[
  {
    "objectID": "check_meta.html",
    "href": "check_meta.html",
    "title": "EECE7398 Fall 2024",
    "section": "",
    "text": "# gather all the meta info from all  the files -  check for errors \nimport yaml\nfrom pathlib import Path\n\n\ndef extract_meta_from_qmd(file_path):\n    \"\"\"\n    Extracts and returns the YAML front matter (meta information) from a QMD file.\n    \n    Parameters:\n    - file_path: Path to the QMD file.\n    \n    Returns:\n    - A dictionary containing the parsed YAML front matter, or None if not found.\n    \"\"\"\n    # Initialize an empty string to hold the YAML content\n    yaml_content = ''\n    # Flag to indicate if we are within the YAML front matter\n    in_yaml = False\n    \n    with open(file_path, 'r') as f:\n        for line in f:\n            # Check for the start/end of the YAML front matter\n            if line.strip() == '---':\n                if in_yaml:\n                    # We found the second ---, stop reading further\n                    break\n                else:\n                    # We found the first ---, start collecting lines\n                    in_yaml = True\n            elif in_yaml:\n                # Add the current line to the YAML content\n                yaml_content += line\n    \n    # Parse the YAML content if any was found\n    if yaml_content:\n        return yaml.safe_load(yaml_content)\n    else:\n        return None\n\n\n\nimport os\n\ndef list_all_files(root_dir):\n    \"\"\"List all files in a directory and its subdirectories.\"\"\"\n    all_files = []\n    for root, dirs, files in os.walk(root_dir):\n        for file in files:\n            all_files.append(os.path.join(root, file))\n    return all_files\n\n\nbase = Path(\"/home/norm/compiler_course_2024fa/\")\n\n\nfrom numpy import isin\n\n\nmeta_union = {}\nfor file in list_all_files(base):\n    file_path = Path(file)\n    if file_path.suffix in [\".pdf\", \".jpg\", \".png\", \".woff\", \".eot\", \".woff2\", \".ttf\", \".so\", \".pyc\"]:\n        continue\n    if file_path.parent.name == \"bin\":\n        continue \n    if any(parent.name == \".venv\" for parent in file_path.parents):\n        continue\n    if any(parent.name == \".git\" for parent in file_path.parents):\n        continue\n    meta_info = extract_meta_from_qmd(file)\n    if meta_info:\n        for key, value in meta_info.items():\n    \n            if isinstance(value, list):\n                value = ', '.join(map(str, value))\n            elif isinstance(value,dict):\n                for sub_key, sub_value in value.items():\n                    combined_key = f\"{key}.{sub_key}\"  # Combine the parent key and sub-key\n                    sub_value = str(sub_value)  # Convert sub-value to string\n                    if combined_key not in meta_union:\n                        meta_union[combined_key] = {sub_value}\n                    else:\n                        meta_union[combined_key].add(sub_value)\n                continue\n            \n            if key not in meta_union:\n                meta_union[key] = {value}\n            else:\n                meta_union[key].add(value)\n\nfor (k,v) in meta_union.items():\n    print (k, v )\n\n    \n\ntitle {'Representation of programs', 'Performance and Measurement', '14_gpu_compilers', '5_hw', 'project', 'EECE7398 Fall 2024', '10 MLIR', '3_hw ', 'Untitled', '_ loop invariant code motion', '9 polyhedral analysis', '11 Whole program', 'EECS7398 Weekly Schedule', 'How to do assignments', 'homework 0', '8 classic loop optimizations', 'Overview of Bril', '_ partial_redundancy elimination', '4_hw', 'EECS7398 Weekly Schedule fa 2024', '_ local value numbering', '4. Data Flow', 'About', 'Schedule', '3 Local Analysis & Optimization', '2.hw', '12_memory.qmd', 'Testing Register allocators', '6- extra credit hw ', '1 Compiler Overview', '1-Homework', 'Static Single Assignment', '13_dynamic_compielrs', '5 Global Analysis'}\nformat {'html'}\ntbl-colwidths {'10, 20, 20, 20, 15, 15'}\nformat.html {'default'}\nformat.revealjs {\"{'chalkboard': True, 'output-file': 'revealjs-licm', 'scrollable': True}\", \"{'chalkboard': True, 'output-file': 'revealjs-rep'}\", \"{'chalkboard': True, 'output-file': 'revealjs-partial-redun', 'scrollable': True}\", \"{'chalkboard': True, 'scrollable': True, 'output-location': 'slide', 'code-line-numbers': True, 'output-file': 'revealjs-bril'}\", \"{'chalkboard': True, 'output-file': 'revealjs-local', 'scrollable': True}\", \"{'chalkboard': True, 'output-file': 'revealjs-ssa', 'scrollable': True}\", \"{'chalkboard': True, 'output-file': 'revealjs-lvn', 'scrollable': True}\", \"{'chalkboard': True, 'output-file': 'revealjs-compiler_overview.html', 'scrollable': True}\", \"{'chalkboard': True, 'output-file': 'revealjs-data-flow', 'scrollable': True}\", \"{'chalkboard': True, 'output-file': 'revealjs-global-anal', 'scrollable': True}\", \"{'chalkboard': True, 'output-file': 'revealjs-performance.html', 'scrollable': True}\"}\nkeep-ipynb {True}\npython {'kaggle_comp'}\nsidebar {False}\nexecute.echo {'True'}\n\n\n\nimport os\nimport yaml\n\nbase = Path(\"/home/norm/compiler_course_2024fa/\")\n\n# Step 1: List all .qmd files\nqmd_files = []\nslide_qmd_files = []\n\nfor root, dirs, files in os.walk(base):  # Adjust '.' to your project directory if necessary\n    revealjs_\n    if \"_site\" in Path(root).parts:\n        continue\n    for file in files:\n        if file.endswith('.qmd'):\n            qmd_files.append(os.path.join(root, file))\n\n# Step 2: Read and parse each .qmd file\nfor qmd_file in qmd_files:\n\n    with open(qmd_file, 'r') as file:\n        content = file.read()\n        # Assuming the YAML metadata is at the top of the file, delimited by ---\n        if content.startswith('---'):\n            end_of_yaml = content.find('---', 3)\n            if end_of_yaml != -1:\n                yaml_content = content[3:end_of_yaml]\n                metadata = yaml.safe_load(yaml_content)  # Parse YAML\n\n                format_data = metadata.get('format')\n                if not format_data:\n                    print(qmd_file, \"no format field\")\n                    continue\n                try:\n                    html_meta_data = format_data.get(\"html\")\n                except Exception as e:\n                    print(qmd_file, e, 'format does not have html')\n                    continue\n\n                if html_meta_data != 'default':\n                    print(qmd_file, \"not html default\")\n                    continue \n                try:\n                    revealjs_meta_data = format_data.get('revealjs')\n                except:\n                    print(\"qmd_file\", \"no revealjs\")\n                    continue\n                if revealjs_meta_data:\n                    chalk = revealjs_meta_data.get(\"chalkboard'\")\n                    if chalk != 'true':\n                        print(qmd_file, \"missing chalkboard\")\n    \n                # get the format\n                # see if it has a subkey, revealjs\n                # if it does check for an deeper subkey of output-file\n\n                # Step 3: Check for format: reveljs\n                if metadata.get('format') == 'revealjs':\n                    # Step 4: Verify the output file name\n                    expected_output = 'reveljs-' + os.path.basename(qmd_file).replace('.qmd', '')\n                    if metadata.get('output-file') == expected_output:\n                        print(f\"{qmd_file}: Success, output file name is correct.\")\n                    else:\n                        print(f\"{qmd_file}: Failure, output file name does not match the expected '{expected_output}'.\")\n\n/home/norm/compiler_course_2024fa/lectures/02a_representation.qmd missing chalkboard\n/home/norm/compiler_course_2024fa/lectures/010_compiler_overview.qmd missing chalkboard\n/home/norm/compiler_course_2024fa/lectures/02b_bril.qmd missing chalkboard\n/home/norm/compiler_course_2024fa/lectures/05_global.qmd missing chalkboard\n/home/norm/compiler_course_2024fa/lectures/01a_performance_measurement.qmd missing chalkboard\n/home/norm/compiler_course_2024fa/lectures/06_ssa.qmd missing chalkboard\n/home/norm/compiler_course_2024fa/lectures/05b_licm.qmd missing chalkboard\n/home/norm/compiler_course_2024fa/lectures/04_data_flow.qmd missing chalkboard\n/home/norm/compiler_course_2024fa/lectures/03b_local_value_numbering.qmd missing chalkboard\n/home/norm/compiler_course_2024fa/lectures/03_local.qmd missing chalkboard\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "homework/2_hw.html",
    "href": "homework/2_hw.html",
    "title": "Homework 2 local optimizations",
    "section": "",
    "text": "part 1: Implement “trivial” dead code elimination in which you delete instructions that are never used before they are reassigned.\npart2: Implement local value numbering. Try pairing it with your dead code elimination code, in the write up be sure to include evidence that your implementation is correct and actually optimizes programs, you might want to use the Brench program, for extra points, extend your implementation to handle some of the tricker examples talked about in class.\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Homework",
      "Homework 2 local optimizations"
    ]
  },
  {
    "objectID": "homework/hw0.html",
    "href": "homework/hw0.html",
    "title": "homework 0",
    "section": "",
    "text": "Write a paragraph to introduce yourself in a reply to the canvas introductions topic. Add a picture of you can. Mention a compilers topic you’d like to learn about someday, either in this class or beyond. Add your info to the canvas introductions discussion topic.\nPick a paper from the weekly schedule whose discussion you will lead. Claim the paper by opening a pull request (at the class github) for the weekly.qmd file, fill in your name in the LEADER: line. (I encurage teams of two to sign up for the same paper)\nOnce everyone has signed up, and I see which papers are covered, I’ll finalize the dates and times.\nAdd a text file containing done to Canvas assignment 0 to indicate you have done the introduction and claimed a paper\n\nFor this assignment you just need to submit a response to the canvas assignment to indicate that you are done after you write your introduction into canvas\nFor other assignments you should:\n\nWrite a blog post describing your work, and submit it via a pull request to the github page\nAdd a response to the the canvas assignment giving the name of your blog post\n\n\nMy plan is that grades, personal details and the like stay in canvas and everything else becomes public and goes on the github website.\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Homework",
      "homework 0"
    ]
  },
  {
    "objectID": "homework/dynamic.html",
    "href": "homework/dynamic.html",
    "title": "homework 7 dynamic compile",
    "section": "",
    "text": "This task is to implement a trace-based speculative optimizer for Bril. You’ll implement the same concept as in a tracing JIT, but in a profile-guided AOT setting: profiling, transformation, and execution will be distinct phases. The idea is to implement the “heavy lifting” for a trace-based JIT without needing all the scaffolding that a complete JIT requires, such as on-stack replacement.\nConcretely, there are three main phases:\n\nModify the reference interpreter to produce traces.\nBuild an optimizer that injects traces back into the original program using the speculation extension to provide a “fast path.”\nCheck that the whole process is correct and had some effect on performance (it needn’t actually be good!).\n\nStart by reading the documentation for the speculation extension (and watch the video!). That should give you an idea of what’s required to augment a program with a speculative execution of an extracted trace. Then make a plan for how you’ll hack the interpreter to produce one of those traces.\nHere’s a recipe:\n\nStart interpreting normally.\nAt some point during execution (at the very beginning of main, for example, or when reaching a backedge), start tracing.\nWhile tracing, record every instruction as it executes. Eliminate jumps; replace branches with guard instructions. Feel free to do the interprocedural version, and to bail out on any other instruction you don’t want to handle.\nStop tracing at some point (after a fixed number of instructions, for example, or at the next backedge) and save the trace to a file.\nFor bonus “points,” statically optimize the trace by eliminating instructions that depend on foregone conclusions enforced by guards.\nTransform the program to stitch the trace back into the program using speculate and commit instructions.\nFor these tasks, unlike some previous lessons, I recommend not attempting to support all the benchmarks. It’s more important that you understand a few programs well than you apply your transformation to a large body of code. (In other words, I recommend that you work depth-first instead of breadth-first.)\n\nIn particular, you do not need to support Bril’s memory extension, which makes things more complicated because it doesn’t get automatically rolled back on speculation aborts. If you are feeling very ambitious, you can try devising a scheme to manually roll back memory modifications on aborts (consider an “undo log” or “redo log,” which are concepts from databases).\nFinally, evaluate your work:\n\nCheck that you didn’t break programs. For at least one benchmark (and ideally a few), create multiple inputs to the program that result in different outputs. Use one input to generate the trace and optimize the program, and use other inputs to check correctness. This approach guards against cases where your tracing optimization “overfits” and you end up with code that only works on one input. Measure performance impact, i.e., the effect of your transformation on the dynamic instruction count.\nfor at least one benchmark, use at least two inputs to evaluate tracing’s impact on executions that are not identical to the traced execution. If you implemented optimizations on the traced code, consider comparing the optimized vs. un-optimized versions. (It’s OK if your tracing apparatus makes programs slower, especially on unseen inputs! We just want to measure the difference.)\n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Homework",
      "homework 7 dynamic compile"
    ]
  },
  {
    "objectID": "homework/1.hw.html",
    "href": "homework/1.hw.html",
    "title": "Homework 1 trying out bril",
    "section": "",
    "text": "you will write a blog post and put in in github using a pull request\nThen will put the name of the blog post into canvas\nYou can ask questions on github discussions\n\nYour goal is to get familiar with Bril.\nPart 1\nWrite a new benchmark.\nYou can write it by hand, use the TypeScript compiler, or generate it some other way. Try running it with brili.\nOpen a pull request to add your new benchmark. ??? Add your code to the the benchmarks directory.\nUse turnt –save yours.bril to create the test outputs for your new benchmark. (See the Turnt README for details.) Mention it in the docs.\npart 2\nWrite a program to analyze or transform Bril programs in some small way. Pick your favorite programming language—there is no “starter code,” so you can start from scratch.\nLoad up a JSON file. You can start with this tiny one! Read the docs.\nDo something unambitious with it: count the number of add instructions, or add a print instruction before every jump, or whatever. Pick something small and contrived! Use Turnt to test your new tool.\nAlong the way, you will run into problems! Ask questions on ???, and open issues and pull requests to describe or fix problems. For example, even super simple benchmarks you might imagine probably can’t be written easily because Bril is too simple. Mention this on ??? discussions, and consider pitching in to help add features.\nThink about how to write a good test, and finally write a post describing your work and submit it to github and submit the name to canvas\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Homework",
      "Homework 1 trying out bril"
    ]
  },
  {
    "objectID": "homework/3_hw.html",
    "href": "homework/3_hw.html",
    "title": "Homework 4 data flow",
    "section": "",
    "text": "Implement one data flow analysis - For Bonus points make it generic so that the same code supports multiple analysis. As always, think about how to test it.\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Homework",
      "Homework  4 data flow"
    ]
  },
  {
    "objectID": "notebooks/llvm.html",
    "href": "notebooks/llvm.html",
    "title": "intro to llvm",
    "section": "",
    "text": "difference between bril and llvm\nlinks\nllvm page\nAdrians tutorial\nllvm doc\ngoogle or github pilot is very useful for this\n\n#as a first step I'm going to show how to install clang and cmake \n\n# step remove any old copies \n# the -S flag to sudo means - read from stdinput\n# the -y flag means always ans yes to apt \n# since sudo needs a password \n# -qq is the very quiet option \n!sudo -S apt purge -y -qq clang cmake &lt;  ~/pw\n!sudo -S apt install -y -qq clang cmake &lt; ~/pw\n\n\n[sudo] password for norm: The following packages were automatically installed and are no longer required:\n  cmake-data dh-elpa-helper emacsen-common libarchive13 libjsoncpp25 librhash0\nUse 'sudo apt autoremove' to remove them.\nThe following packages will be REMOVED:\n  clang* cmake*\n0 upgraded, 0 newly installed, 2 to remove and 48 not upgraded.\nAfter this operation, 21.3 MB disk space will be freed.\n\n(Reading database ... 40226 files and directories currently installed.)\nRemoving clang (1:14.0-55~exp2) ...\nProgress: [  0%] [..........................................................] Progress: [ 11%] [######....................................................] Progress: [ 22%] [############..............................................] Progress: [ 33%] [###################.......................................] Progress: [ 44%] [#########################.................................] emoving cmake (3.22.1-1ubuntu1.22.04.2) ...\nProgress: [ 56%] [################################..........................] Progress: [ 67%] [######################################....................] Progress: [ 78%] [#############################################.............] Progress: [ 89%] [###################################################.......] rocessing triggers for man-db (2.10.2-1) ...\n\n[sudo] password for norm: Suggested packages:\n  cmake-doc ninja-build cmake-format\nThe following NEW packages will be installed:\n  clang cmake\n0 upgraded, 2 newly installed, 0 to remove and 48 not upgraded.\nNeed to get 0 B/5014 kB of archives.\nAfter this operation, 21.3 MB of additional disk space will be used.\n\nSelecting previously unselected package clang.\n(Reading database ... 40203 files and directories currently installed.)\nPreparing to unpack .../clang_1%3a14.0-55~exp2_amd64.deb ...\nProgress: [  0%] [..........................................................] Progress: [ 11%] [######....................................................] Unpacking clang (1:14.0-55~exp2) ...\nProgress: [ 22%] [############..............................................] electing previously unselected package cmake.\nPreparing to unpack .../cmake_3.22.1-1ubuntu1.22.04.2_amd64.deb ...\nProgress: [ 33%] [###################.......................................] Unpacking cmake (3.22.1-1ubuntu1.22.04.2) ...\nProgress: [ 44%] [#########################.................................] etting up clang (1:14.0-55~exp2) ...\nProgress: [ 56%] [################################..........................] Progress: [ 67%] [######################################....................] etting up cmake (3.22.1-1ubuntu1.22.04.2) ...\nProgress: [ 78%] [#############################################.............] Progress: [ 89%] [###################################################.......] rocessing triggers for man-db (2.10.2-1) ...\n\n\n\nlets take a look at llvm ir\n\n%%writefile temp.c\nint main(int argc, char** argv){\n    return argc;\n}\n\nOverwriting temp.c\n\n\n\n# call clang and dump the ir\n# # -emit-llvm  print the ir\n# -S print as text not as binary \n# 0 -  output to stdout \n# \n!clang -emit-llvm -S -o - temp.c\n\n\n; ModuleID = 'temp.c'\nsource_filename = \"temp.c\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n; Function Attrs: noinline nounwind optnone uwtable\ndefine dso_local i32 @main(i32 noundef %0, i8** noundef %1) #0 {\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  ret i32 %6\n}\n\nattributes #0 = { noinline nounwind optnone uwtable \"frame-pointer\"=\"all\" \"min-legal-vector-width\"=\"0\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\n\n!llvm.module.flags = !{!0, !1, !2, !3, !4}\n!llvm.ident = !{!5}\n\n!0 = !{i32 1, !\"wchar_size\", i32 4}\n!1 = !{i32 7, !\"PIC Level\", i32 2}\n!2 = !{i32 7, !\"PIE Level\", i32 2}\n!3 = !{i32 7, !\"uwtable\", i32 1}\n!4 = !{i32 7, !\"frame-pointer\", i32 2}\n!5 = !{!\"Ubuntu clang version 14.0.0-1ubuntu1.1\"}\n\n\nAn LLVM plugin is a shared library that can add additional functionality to the LLVM infrastructure. Plugins can be used to add new passes, analyses, targets, and more.\nPlugins are dynamically loaded into LLVM. Once loaded, a plugin can register new command-line options, passes, etc., that are then available for use in that invocation of the tool.\nThere is a cs6120 package that makes setting up the build process for plugins simple\nllvm ir, has two forms .bc files are bitcode, .ll forms are text versions that look like assembly.\nllvm is not written in C++ but it has a lot of features that look like C++.\n\nllvm does not use char* or std::string, it has something else called a StringRef.\nthere is no std::cout or std::cerr there are outs(), errs()\nlot of built in data structures\ncomplex class hierarchy\n\n\n\n\n\n\nflowchart TD;\nValue --&gt; Argument ;\nValue --&gt; other[\"...\"];\nValue --&gt; User;\nUser --&gt; Constant\nUser--&gt; Operator\nUser--&gt; Instruction\nConstant --&gt; ConstantExpr\nConstant--&gt; ConstantData\nOperator--&gt; ConcreteOperator\nInstruction--&gt; UnaryInst\nConstantData --&gt; ConstantInt\nConstantData --&gt; UndefValue\nInstruction --&gt; BinaryOperator\nInstruction--&gt; CallBase\n\n\n\n\n\n\n\nInstructions are a kind of Value, since everything is in SSA form, so in memory operands are pointers to instructions so if I is an instruction\nouts() &lt;&lt; *(I.getOperand(0)) ; prints an instruction\nGiven a Value* V, what kind of thing is V?\n\nisa(V) true of V is a agument\ncast(V) casts to Argument, assert falure of not Argument\ndyn_cast(V) casts to Argument returns NULL if not an argument\n\nStatic bool isLoopInvariant(const Value *V, const Loop *L) { \n    if (isa&lt;Constant&gt;(V) || isa&lt;Argument&gt;(V) || isa&lt;GlobalValue&lt;(V)) {\n         return true; } \n    //otherwise it must be an instruction…    \n    return !L-&gt;contains(cast&lt;Instruction&gt;(V)-&gt;getParent());\n     … \n}\nNavigating llvm IR - IT Containers\n\nModule - two way linked list of Functions\nFunction - two way linked list of Basic Blocks\nBasic Block - two way linked list of Instructions\n\n%5 = add i32 %4,2\nthis instruction adds two 32 bit ints, input is in register %4 and the constant 2, result goes into register %5\nblog post: Why would a grad student care about llvm\n\n%%bash \nrm -r llvm-pass-skeleton/\ngit clone   https://github.com/sampsyo/llvm-pass-skeleton.git\ncd llvm-pass-skeleton/\nmkdir -p build \ncd build \ncmake ..\nmake\n\n\n# look at  llvm-pass-skeleton/skeleton/Skeleton.cpp\n\n\nCloning into 'llvm-pass-skeleton'...\n\n\nThe function returns PreservedAnalyses::all() to indicate that it didn’t modify M. Later, when we actually transform the program, we’ll need to return something like PreservedAnalyses::none().\nThe ModuleAnalysisManager is responsible for managing the analysis results for Module passes.\nWhen a pass requests an analysis, the ModuleAnalysisManager checks if the analysis result is already available. If it is, the ModuleAnalysisManager returns the cached result. If it’s not, the ModuleAnalysisManager runs the analysis pass, caches the result, and then returns it.\nThis allows LLVM to avoid recomputing analysis results unnecessarily, which can significantly improve the performance of the compiler.\nHere’s an example of how you might use it:\nPreservedAnalyses MyPass::run(Module &M, ModuleAnalysisManager &MAM) {\n    // Request an analysis result.\n    const auto &Result = MAM.getResult&lt;SomeAnalysis&gt;(M);\n\n    // Use the analysis result.\n    // ...\n\n    return PreservedAnalyses::all();\n}\nHere is a second example getting the dominator tree\n    PreservedAnalyses run(Module &M, ModuleAnalysisManager &MAM) {\n        // Get the FunctionAnalysisManager.\n        FunctionAnalysisManager &FAM = MAM.getResult&lt;FunctionAnalysisManagerModuleProxy&gt;(M).getManager();\n\n        for (Function &F : M) {\n            // Skip external functions.\n            if (F.isDeclaration()) continue;\n\n            // Request the dominator tree of the function.\n            const DominatorTree &DT = FAM.getResult&lt;DominatorTreeAnalysis&gt;(F);\n\n            // Use the dominator tree.\n            // ...\n        }\n\n        return PreservedAnalyses::all();\n    }\nnow let look at the containers\n\n%%bash\nrm -r llvm-pass-skeleton/\ngit clone  -b containers  https://github.com/sampsyo/llvm-pass-skeleton.git\ncd llvm-pass-skeleton/\nmkdir -p build \ncd build \ncmake ..\nmake\n\n\nCloning into 'llvm-pass-skeleton'...\n\n\n-- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Performing Test HAVE_FFI_CALL\n-- Performing Test HAVE_FFI_CALL - Success\n-- Found FFI: /usr/lib/x86_64-linux-gnu/libffi.so  \n-- Performing Test Terminfo_LINKABLE\n-- Performing Test Terminfo_LINKABLE - Success\n-- Found Terminfo: /usr/lib/x86_64-linux-gnu/libtinfo.so  \n-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n-- Found LibXml2: /usr/lib/x86_64-linux-gnu/libxml2.so (found version \"2.9.13\") \n-- Linker detection: GNU ld\n-- Registering SkeletonPass as a pass plugin (static build: OFF)\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/norm/llvm/llvm-pass-skeleton/build\n[ 50%] Building CXX object skeleton/CMakeFiles/SkeletonPass.dir/Skeleton.cpp.o\n[100%] Linking CXX shared module SkeletonPass.so\nError while terminating subprocess (pid=71626): \n[100%] Built target SkeletonPass\n\n\n\n# run the plugin \n# \n!clang -fpass-plugin=`echo llvm-pass-skeleton/build/skeleton/SkeletonPass.*` temp.c\n\n\nIn a function called main!\nFunction body:\n; Function Attrs: noinline nounwind optnone uwtable\ndefine dso_local i32 @main(i32 noundef %0, i8** noundef %1) #0 {\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  ret i32 %6\n}\nBasic block:\n\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  ret i32 %6\nInstruction: \n  %3 = alloca i32, align 4\nInstruction: \n  %4 = alloca i32, align 4\nInstruction: \n  %5 = alloca i8**, align 8\nInstruction: \n  store i32 0, i32* %3, align 4\nInstruction: \n  store i32 %0, i32* %4, align 4\nInstruction: \n  store i8** %1, i8*** %5, align 8\nInstruction: \n  %6 = load i32, i32* %4, align 4\nInstruction: \n  ret i32 %6\nI saw a function called main!\n\n\n\n%%writefile temp1.c\nint main(int argc, char** argv){\n    if (argc &gt;2 )\n        return argc;\n    return 0;\n}\n\nOverwriting temp1.c\n\n\n\n!clang -fpass-plugin=`echo llvm-pass-skeleton/build/skeleton/SkeletonPass.*` temp1.c\n\nIn a function called main!\nFunction body:\n; Function Attrs: noinline nounwind optnone uwtable\ndefine dso_local i32 @main(i32 noundef %0, i8** noundef %1) #0 {\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  %7 = icmp sgt i32 %6, 2\n  br i1 %7, label %8, label %10\n\n8:                                                ; preds = %2\n  %9 = load i32, i32* %4, align 4\n  store i32 %9, i32* %3, align 4\n  br label %11\n\n10:                                               ; preds = %2\n  store i32 0, i32* %3, align 4\n  br label %11\n\n11:                                               ; preds = %10, %8\n  %12 = load i32, i32* %3, align 4\n  ret i32 %12\n}\nBasic block:\n\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  %7 = icmp sgt i32 %6, 2\n  br i1 %7, label %8, label %10\nInstruction: \n  %3 = alloca i32, align 4\nInstruction: \n  %4 = alloca i32, align 4\nInstruction: \n  %5 = alloca i8**, align 8\nInstruction: \n  store i32 0, i32* %3, align 4\nInstruction: \n  store i32 %0, i32* %4, align 4\nInstruction: \n  store i8** %1, i8*** %5, align 8\nInstruction: \n  %6 = load i32, i32* %4, align 4\nInstruction: \n  %7 = icmp sgt i32 %6, 2\nInstruction: \n  br i1 %7, label %8, label %10\nBasic block:\n\n8:                                                ; preds = %2\n  %9 = load i32, i32* %4, align 4\n  store i32 %9, i32* %3, align 4\n  br label %11\nInstruction: \n  %9 = load i32, i32* %4, align 4\nInstruction: \n  store i32 %9, i32* %3, align 4\nInstruction: \n  br label %11\nBasic block:\n\n10:                                               ; preds = %2\n  store i32 0, i32* %3, align 4\n  br label %11\nInstruction: \n  store i32 0, i32* %3, align 4\nInstruction: \n  br label %11\nBasic block:\n\n11:                                               ; preds = %10, %8\n  %12 = load i32, i32* %3, align 4\n  ret i32 %12\nInstruction: \n  %12 = load i32, i32* %3, align 4\nInstruction: \n  ret i32 %12\nI saw a function called main!\n\n\n\nusing IRBuilder is a mess, So I’m going to show a trick that makes it much simpler\n\n%%bash\nrm -r llvm-pass-skeleton/\ngit clone  -b rtlib  https://github.com/sampsyo/llvm-pass-skeleton.git\ncd llvm-pass-skeleton/\nmkdir -p build \ncd build \ncmake ..\nmake\n\nCloning into 'llvm-pass-skeleton'...\n\n\n-- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Performing Test HAVE_FFI_CALL\n-- Performing Test HAVE_FFI_CALL - Success\n-- Found FFI: /usr/lib/x86_64-linux-gnu/libffi.so  \n-- Performing Test Terminfo_LINKABLE\n-- Performing Test Terminfo_LINKABLE - Success\n-- Found Terminfo: /usr/lib/x86_64-linux-gnu/libtinfo.so  \n-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n-- Found LibXml2: /usr/lib/x86_64-linux-gnu/libxml2.so (found version \"2.9.13\") \n-- Linker detection: GNU ld\n-- Registering SkeletonPass as a pass plugin (static build: OFF)\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/norm/llvm/llvm-pass-skeleton/build\n[ 50%] Building CXX object skeleton/CMakeFiles/SkeletonPass.dir/Skeleton.cpp.o\n[100%] Linking CXX shared module SkeletonPass.so\n[100%] Built target SkeletonPass\n\n\n\n%%bash \ncat ls ~/llvm/llvm-pass-skeleton/skeleton/Skeleton.cpp \necho done\n\ncat: ls: No such file or directory\n\n\n#include \"llvm/Pass.h\"\n#include \"llvm/Passes/PassBuilder.h\"\n#include \"llvm/Passes/PassPlugin.h\"\n#include \"llvm/Support/raw_ostream.h\"\n#include \"llvm/IR/IRBuilder.h\"\n#include \"llvm/Transforms/Utils/BasicBlockUtils.h\"\nusing namespace llvm;\n\nnamespace {\n\nstruct SkeletonPass : public PassInfoMixin&lt;SkeletonPass&gt; {\n    PreservedAnalyses run(Module &M, ModuleAnalysisManager &AM) {\n        for (auto &F : M.functions()) {\n\n            // Get the function to call from our runtime library.\n            LLVMContext &Ctx = F.getContext();\n            std::vector&lt;Type*&gt; paramTypes = {Type::getInt32Ty(Ctx)};\n            Type *retType = Type::getVoidTy(Ctx);\n            FunctionType *logFuncType = FunctionType::get(retType, paramTypes, false);\n            FunctionCallee logFunc =\n                F.getParent()-&gt;getOrInsertFunction(\"logop\", logFuncType);\n\n            for (auto &B : F) {\n                for (auto &I : B) {\n                    if (auto *op = dyn_cast&lt;BinaryOperator&gt;(&I)) {\n                        // Insert *after* `op`.\n                        IRBuilder&lt;&gt; builder(op);\n                        builder.SetInsertPoint(&B, ++builder.GetInsertPoint());\n\n                        // Insert a call to our function.\n                        Value* args[] = {op};\n                        builder.CreateCall(logFunc, args);\n\n                        return PreservedAnalyses::none();\n                    }\n                }\n            }\n\n        }\n        return PreservedAnalyses::all();\n    }\n};\n\n}\n\nextern \"C\" LLVM_ATTRIBUTE_WEAK ::llvm::PassPluginLibraryInfo\nllvmGetPassPluginInfo() {\n    return {\n        .APIVersion = LLVM_PLUGIN_API_VERSION,\n        .PluginName = \"Skeleton pass\",\n        .PluginVersion = \"v0.1\",\n        .RegisterPassBuilderCallbacks = [](PassBuilder &PB) {\n            PB.registerPipelineStartEPCallback(\n                [](ModulePassManager &MPM, OptimizationLevel Level) {\n                    MPM.addPass(SkeletonPass());\n                });\n        }\n    };\n}\ndone\n\n\n\n%%bash \ncat /home/norm/llvm/llvm-pass-skeleton/rtlib.c\necho\n\n#include &lt;stdio.h&gt;\nvoid logop(int i) {\n    printf(\"computed: %i\\n\", i);\n}\n\n\n\n\n%%writefile llvm-pass-skeleton/test_r.cpp\n#include &lt;stdio.h&gt;\nint main (int argc, char** argv) {\n    printf(\"%d %d\", argc, (argc + 2) * (argc +3));\n}\n\nOverwriting llvm-pass-skeleton/test_r.cpp\n\n\n\n%%bash \ncd llvm-pass-skeleton/\ncc -c rtlib.c\nclang  -fpass-plugin=build/skeleton/SkeletonPass.so -c test_r.cpp\ncc test_r.o rtlib.o\n./a.out 1 2 3 4\necho \n\ncomputed: 7\n5 56\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/02_reps.html",
    "href": "notebooks/02_reps.html",
    "title": "2a Representation",
    "section": "",
    "text": "The representation of a program - is what we read in and read out when transforming a program. What kind of properties make a good representation?\nOne possible representation is called concrete syntax form Programs are text - surface syntax- just what you would type into an editor.\n\nvalue = 8\nresult = 1\nfor i in range(value):\n  result = result + i\nprint(result)\n\n29\n\n\nWhat is good and what is bad about this representation?\nWhat is the level of abstraction? How do you understand the semantics.\nForm 2 - Abstract syntax form\nTree structure - Nodes are parts of the program, edges show how they are connected. We can write this as a list or a graph\n\n\nFunctionDef(\n    name='pgm',\n    args=arguments(\n        posonlyargs=[],\n        args=[],\n        kwonlyargs=[],\n        kw_defaults=[],\n        defaults=[]),\n    body=[\n        Assign(\n            targets=[\n                Name(id='value', ctx=Store())],\n            value=Constant(value=8)),\n        Assign(\n            targets=[\n                Name(id='result', ctx=Store())],\n            value=Constant(value=1)),\n        For(\n            target=Name(id='i', ctx=Store()),\n            iter=Call(\n                func=Name(id='range', ctx=Load()),\n                args=[\n                    Name(id='value', ctx=Load())],\n                keywords=[]),\n            body=[\n                Assign(\n                    targets=[\n                        Name(id='result', ctx=Store())],\n                    value=BinOp(\n                        left=Name(id='result', ctx=Load()),\n                        op=Mult(),\n                        right=Name(id='i', ctx=Load())))],\n            orelse=[]),\n        Expr(\n            value=Call(\n                func=Name(id='print', ctx=Load()),\n                args=[\n                    Name(id='result', ctx=Load())],\n                keywords=[]))],\n    decorator_list=[])\n\n\n\ndot_dia\n\n\n\n\n\n\n\n\nAST tree representation An AST is a tree structure, nodes like if, test, body, assign Each node is one concept from the program\nRecursive function can walk over the tree, one chunk of code for each node.\n\nGood - each type of node is different, making special cases are easy\nBad - each type of node is different so analysis has to know about every type, making general cases hard\n\nThis is the classic way to write an interpreter. Simple (non optimizing) compilers often use this format.\n\n\nPrograms are lists of instructions. Like an assembly instructions. Same sort of representation as LLVM.\n\n\n    let value = 8\n    let result = 1\n    for (let i = 0;i &lt; value;i = i+ 1)\n    {\n        result = result * i\n    }\n    console.log(result )\n\n\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}\n\n\n\n\nos.system('ts2bril images/toy.ts | bril2txt')\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}\n\n\n0\n\n\nLooks like assembly but no limit on registers, no condition codes. fully typed, no complex addressing modes.\nsyntax-\nDeclare functions, labels, instructions\ninstruction:\n1) variable type = opcode arguments 2) opcode list of arguments\n\n\n\nWhat is the abstract syntax form for this?\n\n\n\n\n\nRepresentation is a directed graph. Nodes are instructions, edges indicate possible flow of control, one entry and one exit node.\nHere is a simple program:\n    @main {\n        v: int = const 5;\n        print v;\n    }\n\n\n\n\n\nflowchart LR\n  A[const] --&gt; B[print]\n\n\n\n\n\n\na second example\n    @main {\n        v: int = const 4;\n        jmp  .somewhere;\n        v: int = const 2;\n        .somewhere;\n        print v;\n    }\nWhat does the control flow graph look like?\n\n\n\n\n\nflowchart LR\n  A[const 4] --&gt; B[jmp]\n  B --&gt; C[print]\n  D[const 2] --&gt; C\n\n\n\n\n\n\nnotice label does not produce a node\nEasy to see a dead instruction.\nThird example:\n    @main {\n        v: int = const 4;\n        b: bool = const false;\n        br b .there .here;\n    .here:\n        v: int = const 2;\n    .there;\n        print v;\n    }\n\n\n\n\n\nflowchart LR\n  A[v: int const 4] --&gt; B[b: bool const false]\n  B --&gt; C[br b .there, .false]\n  C --&gt; D[v: const 2]\n  C --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\nwhich is the true and which is the false, could mark the edges or use a convention\nWhich is the entry, which is the exit?\nThere is a long chain of instructions entered at the top, exit at the bottom, no branches inside.\nBasic blocks (cfg form 2) 1) nodes can be a sequence of instructions. 1) jumps and branches can only be at the end of a sequence 1) only label has to be at the start 1) every instruction in the sequence executes the same number of times\n\n\n\n\n\nflowchart LR\n  A[v: int const 4\\nb : bool\\n br ] \n  A --&gt; D[v: const 2]\n  A --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\nAs we construct basic blocks, we can add instructions up till something that ends the block (terminator)\nOption: do all blocks end in a terminator or not?\ngiven a block b, the predecessors of b are the blocks b_in where there is an edge bin-&gt;b. And the successors of B are the b_out where b-&gt;b_out is an edge\n\n\n\n\n\njust find all the basic blocks\nadd the control flow edges\n\npsuedo code\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\nstep 2 we need a map from labels to basic blocks\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\n    \n\nfor block in blocks:\n   last = block[-1]\n   if last is a jmp (one successor)\n      add edge from block to last.dest \n   else if last is a br (two successors)\n      add two edges from block to last.true, last.false \n   else  fall through \n      add edge to next block (if it exists)\n\nwith open(\"images/add.json\", 'r') as f:\n  bril_program = f.read()\n  print(bril_program)\n\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v0\", \"value\": 1 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v1\", \"value\": 2 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"v2\",\n          \"args\": [\"v0\", \"v1\"] },\n        { \"op\": \"print\", \"args\": [\"v2\"] }\n      ],\n      \"args\": []\n    }\n  ]\n}"
  },
  {
    "objectID": "notebooks/02_reps.html#a-more-regular-representation",
    "href": "notebooks/02_reps.html#a-more-regular-representation",
    "title": "2a Representation",
    "section": "",
    "text": "Programs are lists of instructions. Like an assembly instructions. Same sort of representation as LLVM.\n\n\n    let value = 8\n    let result = 1\n    for (let i = 0;i &lt; value;i = i+ 1)\n    {\n        result = result * i\n    }\n    console.log(result )\n\n\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}\n\n\n\n\nos.system('ts2bril images/toy.ts | bril2txt')\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}\n\n\n0\n\n\nLooks like assembly but no limit on registers, no condition codes. fully typed, no complex addressing modes.\nsyntax-\nDeclare functions, labels, instructions\ninstruction:\n1) variable type = opcode arguments 2) opcode list of arguments"
  },
  {
    "objectID": "notebooks/02_reps.html#what-is-good-and-what-is-about-this-reorientation",
    "href": "notebooks/02_reps.html#what-is-good-and-what-is-about-this-reorientation",
    "title": "2a Representation",
    "section": "",
    "text": "What is the abstract syntax form for this?"
  },
  {
    "objectID": "notebooks/02_reps.html#extract-info-from-this-repreentation.",
    "href": "notebooks/02_reps.html#extract-info-from-this-repreentation.",
    "title": "2a Representation",
    "section": "",
    "text": "Representation is a directed graph. Nodes are instructions, edges indicate possible flow of control, one entry and one exit node.\nHere is a simple program:\n    @main {\n        v: int = const 5;\n        print v;\n    }\n\n\n\n\n\nflowchart LR\n  A[const] --&gt; B[print]\n\n\n\n\n\n\na second example\n    @main {\n        v: int = const 4;\n        jmp  .somewhere;\n        v: int = const 2;\n        .somewhere;\n        print v;\n    }\nWhat does the control flow graph look like?\n\n\n\n\n\nflowchart LR\n  A[const 4] --&gt; B[jmp]\n  B --&gt; C[print]\n  D[const 2] --&gt; C\n\n\n\n\n\n\nnotice label does not produce a node\nEasy to see a dead instruction.\nThird example:\n    @main {\n        v: int = const 4;\n        b: bool = const false;\n        br b .there .here;\n    .here:\n        v: int = const 2;\n    .there;\n        print v;\n    }\n\n\n\n\n\nflowchart LR\n  A[v: int const 4] --&gt; B[b: bool const false]\n  B --&gt; C[br b .there, .false]\n  C --&gt; D[v: const 2]\n  C --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\nwhich is the true and which is the false, could mark the edges or use a convention\nWhich is the entry, which is the exit?\nThere is a long chain of instructions entered at the top, exit at the bottom, no branches inside.\nBasic blocks (cfg form 2) 1) nodes can be a sequence of instructions. 1) jumps and branches can only be at the end of a sequence 1) only label has to be at the start 1) every instruction in the sequence executes the same number of times\n\n\n\n\n\nflowchart LR\n  A[v: int const 4\\nb : bool\\n br ] \n  A --&gt; D[v: const 2]\n  A --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\nAs we construct basic blocks, we can add instructions up till something that ends the block (terminator)\nOption: do all blocks end in a terminator or not?\ngiven a block b, the predecessors of b are the blocks b_in where there is an edge bin-&gt;b. And the successors of B are the b_out where b-&gt;b_out is an edge"
  },
  {
    "objectID": "notebooks/02_reps.html#what-is-an-algorithm-that-forms-a-cfg",
    "href": "notebooks/02_reps.html#what-is-an-algorithm-that-forms-a-cfg",
    "title": "2a Representation",
    "section": "",
    "text": "just find all the basic blocks\nadd the control flow edges\n\npsuedo code\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\nstep 2 we need a map from labels to basic blocks\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\n    \n\nfor block in blocks:\n   last = block[-1]\n   if last is a jmp (one successor)\n      add edge from block to last.dest \n   else if last is a br (two successors)\n      add two edges from block to last.true, last.false \n   else  fall through \n      add edge to next block (if it exists)\n\nwith open(\"images/add.json\", 'r') as f:\n  bril_program = f.read()\n  print(bril_program)\n\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v0\", \"value\": 1 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v1\", \"value\": 2 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"v2\",\n          \"args\": [\"v0\", \"v1\"] },\n        { \"op\": \"print\", \"args\": [\"v2\"] }\n      ],\n      \"args\": []\n    }\n  ]\n}"
  },
  {
    "objectID": "check-links.html",
    "href": "check-links.html",
    "title": "EECE7398 Fall 2024",
    "section": "",
    "text": "# check all the links - get a list of all internal links\n# find all the files that are not used\n\n\nimport os\n\ndef get_all_files(base):\n    \"\"\"Returns a set of all paths to all files below base.\"\"\"\n    all_files = set()\n    skips = ['.git', '_site', \".quarto\"]\n    for root, dirs, files in os.walk(base):\n        for skip in skips:\n            if skip in dirs:\n                dirs.remove(skip)\n        for file in files:\n            all_files.add(os.path.join(root, file)[2:])\n    return all_files\n\n\nextra_files = get_all_files(\".\")\n\nprint(extra_files)\n\n{'lectures/images/StaticSingleAssignment_Part48.jpg', 'lectures/images/StaticSingleAssignment_Part73.jpg', 'issues', 'runfirst.py', 'lectures/images/StaticSingleAssignment_Part71.jpg', 'lectures/12_memory.qmd', 'Class_Overview/sylibus.qmd', 'lectures/03b_local_value_numbering.qmd', 'lectures/02b_bril.qmd.saved', 'lectures/100_mlir.qmd', 'lectures/03_local.qmd', 'lectures/05_global.qmd', 'description.txt', 'Class_Overview/about.qmd', 'lectures/04_data_flow.qmd', 'lectures/images/StaticSingleAssignment_Part69.jpg', 'index.qmd', 'notebooks/representation.ipynb', 'lectures/13_dynamic_compilers.qmd', 'lectures/images/my_ast', 'lectures/images/StaticSingleAssignment_Part40.jpg', 'lectures/images/add.json', 'lectures/images/Grace_Hopper_and_UNIVAC.jpg', 'lectures/08_classic_loop_ops.qmd', 'weekly.qmd', 'found_links.csv', 'lectures/02b_bril.qmd', 'lectures/14_gpu_compilers.qmd', 'Links-21-6-2024 83848.csv', 'lectures/junk.qmd', 'lectures/images/StaticSingleAssignment_Part40.pdf', 'Links-21-6-2024 84630.csv', 'notebooks/possible_papers.qmd', 'about.qmd', 'notebooks/02aa_reps.ipynb', 'lectures/images/Digraph.gv', 'Links-21-6-2024 84311.csv', 'lectures/images/StaticSingleAssignment_Part41.jpg', 'lectures/bril.qmd', 'lectures/images/StaticSingleAssignment_Part70.jpg', 'notebooks/llvm.ipynb', '.github/workflows/publish.yml', 'lectures/110_whole_program.qmd', 'lectures/images/Digraph.gv.png', 'requirements.txt', 'lectures/02a_representation.qmd.saved', 'lectures/02a_representation.qmd', 'lectures/06_ssa.qmd', 'lectures/images/StaticSingleAssignment_Part46.jpg', 'lectures/01a_performance_measurement.qmd', '.gitignore', 'lectures/images/toy.ts', '.vscode/settings.json', 'setenv.sh', 'lectures/images/StaticSingleAssignment_Part43.jpg', 'lectures/images/my_ast.png', 'styles.css', 'notebooks/02_reps.ipynb', 'Links-21-6-2024 84544.csv', 'check-links.ipynb', 'Links-21-6-2024 8400.csv', 'Class_Overview/schedule.qmd', 'lectures/02b_bril.ipynb', 'lectures/05b_licm.qmd', 'Links-21-6-2024 84235.csv', '_quarto.yml', 'lectures/09_poly.qmd', 'Class_Overview/What_to_do.qmd', 'lectures/07_llvm.ipynb', 'lectures/images/cfg.png', 'lectures/010_compiler_overview.qmd', 'notebooks/bril.ipynb', 'lectures/02a_representation.ipynb'}\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\n\nclass ctx():\n    def __init__(self, top):\n        self.top = top\n        self.external_links = set()\n        self.missing_files = set()\n        self.extra_files = set()\n        self.seen_urls = set()\n        self.seen_links = set()\n\n\n\n\ndef parseLinks(pageHtml, pageUrl, ctx):\n    soup = BeautifulSoup(pageHtml, 'html.parser')\n\n    #get all the &lt;a&gt; elements from the HTML page\n    allLinks = soup.find_all('a')\n\n    extIntLinks(allLinks, pageUrl, ctx)\n\n\ndef requestMaker(url, ctx):\n    if (url in ctx.seen_urls):\n        return\n    ctx.seen_urls.add(url)\n    try:\n        #make the get request to the url\n        response = requests.get(url)\n\n        #if the request is successful\n        if response.status_code in range(200, 300):\n            #extract the page html content for parsing the links\n            pageHtml = response.text\n            pageUrl = response.url\n\n            #call the parseLink function\n            parseLinks(pageHtml, pageUrl, ctx)\n        \n        else:\n            print(\"Sorry Could not fetch the result status code {response.status_code}!\")\n\n    except Exception as e:\n        print(f\"{e} Could Not Connect to url {url}\")\n\n\nfrom sympy import I\n\n\ndef extIntLinks(allLinks, pageUrl, ctx):\n\n    #go through all the &lt;a&gt; elements list \n    for anchor in allLinks:\n        link = anchor.get(\"href\")   #get the link from the &lt;a&gt; element\n\n        link_orig = link\n\n        if link is None:\n            continue\n        print(f\"link {link}\")\n\n        if link.startswith(\".\"):\n            if link == \"./\":\n                continue\n\n            if link in ctx.seen_links:\n                continue\n            ctx.seen_links.add(link)\n            file = link[2:] # remove ./\n            ## deal with the possible enddings \n\n            if file.startswith(\"revealjs-\"):\n                file = file[10:]\n\n            if file.endswith(\".html\"):\n                file_qmd =  file[:-4]+ \"qmd\"\n                if file_qmd in extra_files:\n                    extra_files.remove(file_qmd)\n                    newurl = ctx.top + '/' + file\n                    requestMaker(newurl, ctx)\n                    continue\n\n        elif link.startswith(ctx.top):\n            print(\"starts with top\")\n            continue\n\n        \n        elif link.startswith(\"#\") :\n            print(\"ref link\")\n\n        elif link.startswith(\"https://capra\"):\n            ctx.external_links.add(link)\n\n        elif link.startswith(\"https://github.com\"):\n            ctx.external_links.add(link)\n\n        elif link.startswith(\"https://quarto.org\"):\n            ctx.external_links.add(link)\n        \n        else: \n            print(\"else \", link, link_orig)\n\n     \n\n\nurl = \"https://normrubin.github.io\"\nrequestMaker(url, ctx(url))\n\nlink ./\nlink ./weekly.html\nlink ./weekly.html\nlink ./weekly.html\nlink ./\nlink ./weekly.html\nlink ./Class_Overview/about.html\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/about.html\nlink ../\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/schedule.html\nlink ../Class_Overview/sylibus.html\nlink ../Class_Overview/What_to_do.html\nlink ../lectures/010_compiler_overview.html\nlink ../lectures/01a_performance_measurement.html\nlink ../lectures/02a_representation.html\nlink ../lectures/02b_bril.html\nlink ../lectures/03_local.html\nlink ../lectures/03b_local_value_numbering.html\nlink ../lectures/04_data_flow.html\nlink ../lectures/05_global.html\nlink ../lectures/05b_licm.html\nlink ../lectures/06_ssa.html\nlink ../lectures/07_llvm.html\nlink ../lectures/08_classic_loop_ops.html\nlink ../lectures/09_poly.html\nlink ../lectures/100_mlir.html\nlink ../lectures/110_whole_program.html\nlink ../lectures/12_memory.html\nlink ../lectures/13_dynamic_compilers.html\nlink ../lectures/14_gpu_compilers.html\nlink ../lectures/bril.html\nlink ../lectures/junk.html\nlink https://capra.cs.cornell.edu/bril/\nlink https://github.com/normrubin/bril\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/about.html\nlink ../weekly.html\nlink ../Class_Overview/schedule.html\nlink https://normrubin.github.io/\nstarts with top\nlink https://github.com/normrubin/normrubin.github.io/edit/main/Class_Overview/about.qmd\nlink https://github.com/normrubin/normrubin.github.io/issues/new\nlink https://quarto.org/\nlink ./Class_Overview/schedule.html\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/schedule.html\nlink ../\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/schedule.html\nlink ../Class_Overview/sylibus.html\nlink ../Class_Overview/What_to_do.html\nlink ../lectures/010_compiler_overview.html\nlink ../lectures/01a_performance_measurement.html\nlink ../lectures/02a_representation.html\nlink ../lectures/02b_bril.html\nlink ../lectures/03_local.html\nlink ../lectures/03b_local_value_numbering.html\nlink ../lectures/04_data_flow.html\nlink ../lectures/05_global.html\nlink ../lectures/05b_licm.html\nlink ../lectures/06_ssa.html\nlink ../lectures/07_llvm.html\nlink ../lectures/08_classic_loop_ops.html\nlink ../lectures/09_poly.html\nlink ../lectures/100_mlir.html\nlink ../lectures/110_whole_program.html\nlink ../lectures/12_memory.html\nlink ../lectures/13_dynamic_compilers.html\nlink ../lectures/14_gpu_compilers.html\nlink ../lectures/bril.html\nlink ../lectures/junk.html\nlink https://capra.cs.cornell.edu/bril/\nlink https://github.com/normrubin/bril\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/schedule.html\nlink ../lectures/010_compiler_overview.html\nlink ../lectures/01a_performance_measurement.html\nlink ../lectures/02a_representation.ipynb\nlink ../lectures/02b_bril.ipynb\nlink ../lectures/03_local.html\nlink ../lectures/04_data_flow.html\nlink ../lectures/05_global.html\nlink ../lectures/06_ssa.html\nlink ../lectures/07_llvm.html\nlink ../lectures/08_classic_loop_ops.html\nlink ../lectures/09_poly.html\nlink ../lectures/100_mlir.html\nlink ../lectures/110_whole_program.html\nlink ../lectures/12_memory.html\nlink ../lectures/13_dynamic_compilers.html\nlink ../lectures/14_gpu_compilers.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/sylibus.html\nlink https://normrubin.github.io/\nstarts with top\nlink https://github.com/normrubin/normrubin.github.io/edit/main/Class_Overview/schedule.qmd\nlink https://github.com/normrubin/normrubin.github.io/issues/new\nlink https://quarto.org/\nlink ./Class_Overview/sylibus.html\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/sylibus.html\nlink ../\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/schedule.html\nlink ../Class_Overview/sylibus.html\nlink ../Class_Overview/What_to_do.html\nlink ../lectures/010_compiler_overview.html\nlink ../lectures/01a_performance_measurement.html\nlink ../lectures/02a_representation.html\nlink ../lectures/02b_bril.html\nlink ../lectures/03_local.html\nlink ../lectures/03b_local_value_numbering.html\nlink ../lectures/04_data_flow.html\nlink ../lectures/05_global.html\nlink ../lectures/05b_licm.html\nlink ../lectures/06_ssa.html\nlink ../lectures/07_llvm.html\nlink ../lectures/08_classic_loop_ops.html\nlink ../lectures/09_poly.html\nlink ../lectures/100_mlir.html\nlink ../lectures/110_whole_program.html\nlink ../lectures/12_memory.html\nlink ../lectures/13_dynamic_compilers.html\nlink ../lectures/14_gpu_compilers.html\nlink ../lectures/bril.html\nlink ../lectures/junk.html\nlink https://capra.cs.cornell.edu/bril/\nlink https://github.com/normrubin/bril\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/sylibus.html\nlink ../lectures/010_compiler_overview.html\nlink ../lectures/01a_performance_measurement.html\nlink ../lectures/02a_representation.ipynb\nlink ../lectures/02b_bril.ipynb\nlink ../lectures/03_local.html\nlink ../lectures/04_data_flow.html\nlink ../lectures/05_global.html\nlink ../lectures/06_ssa.html\nlink ../lectures/07_llvm.html\nlink ../lectures/08_classic_loop_ops.html\nlink ../lectures/09_poly.html\nlink ../lectures/100_mlir.html\nlink ../lectures/110_whole_program.html\nlink ../lectures/12_memory.html\nlink ../lectures/13_dynamic_compilers.html\nlink ../lectures/14_gpu_compilers.html\nlink ../Class_Overview/schedule.html\nlink ../Class_Overview/What_to_do.html\nlink https://normrubin.github.io/\nstarts with top\nlink https://github.com/normrubin/normrubin.github.io/edit/main/Class_Overview/sylibus.qmd\nlink https://github.com/normrubin/normrubin.github.io/issues/new\nlink https://quarto.org/\nlink ./Class_Overview/What_to_do.html\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/What_to_do.html\nlink ../\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/schedule.html\nlink ../Class_Overview/sylibus.html\nlink ../Class_Overview/What_to_do.html\nlink ../lectures/010_compiler_overview.html\nlink ../lectures/01a_performance_measurement.html\nlink ../lectures/02a_representation.html\nlink ../lectures/02b_bril.html\nlink ../lectures/03_local.html\nlink ../lectures/03b_local_value_numbering.html\nlink ../lectures/04_data_flow.html\nlink ../lectures/05_global.html\nlink ../lectures/05b_licm.html\nlink ../lectures/06_ssa.html\nlink ../lectures/07_llvm.html\nlink ../lectures/08_classic_loop_ops.html\nlink ../lectures/09_poly.html\nlink ../lectures/100_mlir.html\nlink ../lectures/110_whole_program.html\nlink ../lectures/12_memory.html\nlink ../lectures/13_dynamic_compilers.html\nlink ../lectures/14_gpu_compilers.html\nlink ../lectures/bril.html\nlink ../lectures/junk.html\nlink https://capra.cs.cornell.edu/bril/\nlink https://github.com/normrubin/bril\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/What_to_do.html\nlink https://quarto.org/\nlink ../Class_Overview/sylibus.html\nlink ../lectures/010_compiler_overview.html\nlink https://normrubin.github.io/\nstarts with top\nlink https://github.com/normrubin/normrubin.github.io/edit/main/Class_Overview/What_to_do.qmd\nlink https://github.com/normrubin/normrubin.github.io/issues/new\nlink https://quarto.org/\nlink ./lectures/010_compiler_overview.html\nlink ../weekly.html\nlink ../lectures/010_compiler_overview.html\nlink ../lectures/010_compiler_overview.html\nlink ../\nlink ../weekly.html\nlink ../Class_Overview/about.html\nlink ../Class_Overview/schedule.html\nlink ../Class_Overview/sylibus.html\nlink ../Class_Overview/What_to_do.html\nlink ../lectures/010_compiler_overview.html\nlink ../lectures/01a_performance_measurement.html\nlink ../lectures/02a_representation.html\nlink ../lectures/02b_bril.html\nlink ../lectures/03_local.html\nlink ../lectures/03b_local_value_numbering.html\nlink ../lectures/04_data_flow.html\nlink ../lectures/05_global.html\nlink ../lectures/05b_licm.html\nlink ../lectures/06_ssa.html\nlink ../lectures/07_llvm.html\nlink ../lectures/08_classic_loop_ops.html\nlink ../lectures/09_poly.html\nlink ../lectures/100_mlir.html\nlink ../lectures/110_whole_program.html\nlink ../lectures/12_memory.html\nlink ../lectures/13_dynamic_compilers.html\nlink ../lectures/14_gpu_compilers.html\nlink ../lectures/bril.html\nlink ../lectures/junk.html\nlink https://capra.cs.cornell.edu/bril/\nlink https://github.com/normrubin/bril\nlink revealjs-compiler_overview.html\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EECE7398 Fall 2024",
    "section": "",
    "text": "EECS 7398 Fall 2024\nInstructor: Dr. Norman Rubin\nemail: n.rubin@northeastern.edu\nOffice hours by appointment\nThis course draws heavily from the CS6120 Advanced Compilers course at Cornell University. Special thanks to Adrian Sampson for granting permission to use his course materials.\nYou can find videos of Adrian Sampson’s lectures on the CS6120 self-guided page. These videos provide an in-depth explanation of many topics we’ll cover in this course.\nI want to take a moment to address an important aspect of this course: its evolving nature. Since this is the first time this course is being offered, please be aware that both the schedule and assignments may change as we progress.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/yashaswini/11-1-2024-HW5-YashaswiniMakaram.html",
    "href": "blogs/yashaswini/11-1-2024-HW5-YashaswiniMakaram.html",
    "title": "Homework5 - Yashaswini",
    "section": "",
    "text": "This pass looks for floating-point division operations (FDivOperator) and injects a call to a function named log_float_division whenever such an instruction is found.\nThe log_float_division function needs to be defined separately (e.g., in a C file) to print a message or log data.\n\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/IRBuilder.h\"\n#include \"llvm/IR/PassManager.h\"\n#include \"llvm/Support/raw_ostream.h\"\n\nusing namespace llvm;\n\nnamespace {\nclass FloatDivLogger : public PassInfoMixin&lt;FloatDivLogger&gt; {\npublic:\n  PreservedAnalyses run(Function &F, FunctionAnalysisManager &FAM) {\n    bool modified = false;\n\n    for (auto &BB : F) {\n      for (auto &I : BB) {\n        // Check if the instruction is a floating-point division\n        if (auto *op = dyn_cast&lt;FDivOperator&gt;(&I)) {\n          IRBuilder&lt;&gt; builder(&I);\n          FunctionCallee logFunc = F.getParent()-&gt;getOrInsertFunction(\n              \"log_float_division\", builder.getVoidTy());\n          builder.CreateCall(logFunc);\n\n          modified = true;\n        }\n      }\n    }\n\n    return (modified ? PreservedAnalyses::none() : PreservedAnalyses::all());\n  }\n};\n} // namespace\n\n// Register the pass with the LLVM Pass Manager\nllvm::PassPluginLibraryInfo getFloatDivLoggerPluginInfo() {\n  return {LLVM_PLUGIN_API_VERSION, \"FloatDivLogger\", LLVM_VERSION_STRING,\n          [](PassBuilder &PB) {\n            PB.registerPipelineParsingCallback(\n                [](StringRef Name, FunctionPassManager &FPM,\n                   ArrayRef&lt;PassBuilder::PipelineElement&gt;) {\n                  if (Name == \"float-div-logger\") {\n                    FPM.addPass(FloatDivLogger());\n                    return true;\n                  }\n                  return false;\n                });\n          }};\n}\n\nextern \"C\" LLVM_ATTRIBUTE_WEAK ::llvm::PassPluginLibraryInfo\nllvmGetPassPluginInfo() {\n  return getFloatDivLoggerPluginInfo();\n}\n#include &lt;stdio.h&gt;\n\nvoid log_float_division() {\n    printf(\"Floating-point division detected!\\n\");\n}\n\n\n\n\nA simple program that uses floating point division:\n#include &lt;stdio.h&gt;\n\nint main() {\n    float a = 10.0;\n    float b = 2.0;\n    float c = a / b;\n    printf(\"Result: %f\\n\", c);\n    return 0;\n}\n##Output\nThe message: “Floating-point division detected!” is printed after every floating point division.\n\n\n\nThe most difficult part of this Homework was getting LLVM properly installed along with cmake and clang.\nAs I am using WSL, the cmake function does not update properly and was outof date for the llvm installation.\nensuring all installations worked and were properly linked together took some time.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework5 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/11-1-2024-HW5-YashaswiniMakaram.html#about-the-code",
    "href": "blogs/yashaswini/11-1-2024-HW5-YashaswiniMakaram.html#about-the-code",
    "title": "Homework5 - Yashaswini",
    "section": "",
    "text": "This pass looks for floating-point division operations (FDivOperator) and injects a call to a function named log_float_division whenever such an instruction is found.\nThe log_float_division function needs to be defined separately (e.g., in a C file) to print a message or log data.\n\n#include \"llvm/IR/Function.h\"\n#include \"llvm/IR/IRBuilder.h\"\n#include \"llvm/IR/PassManager.h\"\n#include \"llvm/Support/raw_ostream.h\"\n\nusing namespace llvm;\n\nnamespace {\nclass FloatDivLogger : public PassInfoMixin&lt;FloatDivLogger&gt; {\npublic:\n  PreservedAnalyses run(Function &F, FunctionAnalysisManager &FAM) {\n    bool modified = false;\n\n    for (auto &BB : F) {\n      for (auto &I : BB) {\n        // Check if the instruction is a floating-point division\n        if (auto *op = dyn_cast&lt;FDivOperator&gt;(&I)) {\n          IRBuilder&lt;&gt; builder(&I);\n          FunctionCallee logFunc = F.getParent()-&gt;getOrInsertFunction(\n              \"log_float_division\", builder.getVoidTy());\n          builder.CreateCall(logFunc);\n\n          modified = true;\n        }\n      }\n    }\n\n    return (modified ? PreservedAnalyses::none() : PreservedAnalyses::all());\n  }\n};\n} // namespace\n\n// Register the pass with the LLVM Pass Manager\nllvm::PassPluginLibraryInfo getFloatDivLoggerPluginInfo() {\n  return {LLVM_PLUGIN_API_VERSION, \"FloatDivLogger\", LLVM_VERSION_STRING,\n          [](PassBuilder &PB) {\n            PB.registerPipelineParsingCallback(\n                [](StringRef Name, FunctionPassManager &FPM,\n                   ArrayRef&lt;PassBuilder::PipelineElement&gt;) {\n                  if (Name == \"float-div-logger\") {\n                    FPM.addPass(FloatDivLogger());\n                    return true;\n                  }\n                  return false;\n                });\n          }};\n}\n\nextern \"C\" LLVM_ATTRIBUTE_WEAK ::llvm::PassPluginLibraryInfo\nllvmGetPassPluginInfo() {\n  return getFloatDivLoggerPluginInfo();\n}\n#include &lt;stdio.h&gt;\n\nvoid log_float_division() {\n    printf(\"Floating-point division detected!\\n\");\n}",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework5 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/11-1-2024-HW5-YashaswiniMakaram.html#testing",
    "href": "blogs/yashaswini/11-1-2024-HW5-YashaswiniMakaram.html#testing",
    "title": "Homework5 - Yashaswini",
    "section": "",
    "text": "A simple program that uses floating point division:\n#include &lt;stdio.h&gt;\n\nint main() {\n    float a = 10.0;\n    float b = 2.0;\n    float c = a / b;\n    printf(\"Result: %f\\n\", c);\n    return 0;\n}\n##Output\nThe message: “Floating-point division detected!” is printed after every floating point division.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework5 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/11-1-2024-HW5-YashaswiniMakaram.html#challanges",
    "href": "blogs/yashaswini/11-1-2024-HW5-YashaswiniMakaram.html#challanges",
    "title": "Homework5 - Yashaswini",
    "section": "",
    "text": "The most difficult part of this Homework was getting LLVM properly installed along with cmake and clang.\nAs I am using WSL, the cmake function does not update properly and was outof date for the llvm installation.\nensuring all installations worked and were properly linked together took some time.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework5 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/10-18-2024-HW4-YashaswiniMakaram.html",
    "href": "blogs/yashaswini/10-18-2024-HW4-YashaswiniMakaram.html",
    "title": "Homework4 - Yashaswini",
    "section": "",
    "text": "#Dominator Computation (find_dominators):\n\nThis function initializes all nodes to be dominated by every other node. Then, using an iterative approach, it refines the set of dominators until convergence.\nThe entry node is assumed to be node 0, and it dominates itself.\nThe dominators of each node are found by intersecting the dominator sets of its predecessors and adding the node itself.\n\n#Dominance Tree Construction (construct_dominance_tree):\n\nThis function builds the dominance tree from the dominator sets.\nFor each node, the immediate dominator (idom) is the node with the highest index in its dominator set that isn’t itself. The tree is constructed using these relationships.\n\n#Dominance Frontier Computation (compute_dominance_frontier):\n\nThe dominance frontier of a node is the set of nodes where the node dominates one of their predecessors but does not strictly dominate the node itself.\nThis is computed using the reverse of the CFG and the dominator sets.\n\nimport json\nfrom collections import defaultdict\n\ndef load_bril_program(filename):\n    \"\"\"Load the Bril program from a JSON file.\"\"\"\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef build_cfg(function):\n    \"\"\"Build the Control Flow Graph (CFG) for a given function.\"\"\"\n    instrs = function['instrs']\n    cfg = defaultdict(list)\n    label_map = {}\n\n    # Map labels to instruction indices\n    for i, instr in enumerate(instrs):\n        if 'label' in instr:\n            label_map[instr['label']] = i\n\n    for i, instr in enumerate(instrs):\n        if 'op' in instr and instr['op'] == 'jmp':\n            target = label_map[instr['labels'][0]]\n            cfg[i].append(target)\n        elif 'op' in instr and instr['op'] == 'br':\n            target1 = label_map[instr['labels'][0]]\n            target2 = label_map[instr['labels'][1]]\n            cfg[i].extend([target1, target2])\n        else:\n            # Fall-through case: connect to the next instruction\n            if i + 1 &lt; len(instrs):\n                cfg[i].append(i + 1)\n\n    return cfg\n\ndef reverse_cfg(cfg):\n    \"\"\"Compute the reverse of the CFG.\"\"\"\n    reverse = defaultdict(list)\n    for node, successors in cfg.items():\n        for succ in successors:\n            reverse[succ].append(node)\n    return reverse\n\ndef find_dominators(function):\n    \"\"\"Find the dominators for each node in the function.\"\"\"\n    cfg = build_cfg(function)\n    nodes = set(cfg.keys())\n    entry = min(nodes)#0  # Assuming the entry node is at index 0\n    dom = {node: nodes.copy() for node in nodes}\n    dom[entry] = {entry}\n\n    changed = True\n    while changed:\n        changed = False\n        for node in nodes - {entry}:\n            #new_dom = set.intersection(*(dom[pred] for pred in reverse_cfg(cfg)[node])) | {node}\n            preds = reverse_cfg(cfg)[node]\n            if preds:\n                # Compute intersection only if there are predecessors\n                new_dom = set.intersection(*(dom[pred] for pred in preds)) | {node}\n            else:\n                # If no predecessors, the dominators are just the node itself\n                new_dom = {node}\n            if dom[node] != new_dom:\n                dom[node] = new_dom\n                changed = True\n\n    return dom\n\ndef construct_dominance_tree(dominators):\n    \"\"\"Construct the dominance tree from the dominators.\"\"\"\n    tree = defaultdict(list)\n    for node, dom_set in dominators.items():\n        if node != min(dom_set):\n            idom = max(dom_set - {node})\n            tree[idom].append(node)\n    return tree\n\ndef compute_dominance_frontier(function, dominators):\n    \"\"\"Compute the dominance frontier for each node.\"\"\"\n    cfg = build_cfg(function)\n    df = defaultdict(set)\n\n    for node in cfg:\n        preds = reverse_cfg(cfg)[node]\n        if len(preds) &gt;= 2:\n            for pred in preds:\n                runner = pred\n                while runner not in dominators[node]:\n                    df[runner].add(node)\n                    #runner = max(dominators[runner] - {runner})\n                    if dominators[runner] - {runner}:\n                        runner = max(dominators[runner] - {runner})\n                    else:\n                        break  # If no other dominators, exit the loop\n\n    return df\n\ndef test_dominance_utilities(function):\n    \"\"\"Test the dominance utilities for a given function.\"\"\"\n    print(f\"Testing function: {function['name']}\")\n    dominators = find_dominators(function)\n    print(f\"Dominators: {dominators}\")\n\n    dominance_tree = construct_dominance_tree(dominators)\n    print(f\"Dominance Tree: {dominance_tree}\")\n\n    dominance_frontier = compute_dominance_frontier(function, dominators)\n    print(f\"Dominance Frontier: {dominance_frontier}\")\n\ndef main():\n    # Load the Bril program from a file\n    #bril_program = load_bril_program('input_bril.json')\n    for bril_program in bril_programs:\n    # Run the tests\n        for function in bril_program['functions']:\n            test_dominance_utilities(function)\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()\n\n\n\nI implemented 8 test cases each targeting a different type of CFG. I then manually checked the against the outputs produced by the code. I had to make some changes when the code was unable to handle empty sets but after the output seems to be work correctly\n\n\n\nThe hardest part of this task was understanding ensuring that the algorithm i used could handle various data flow types.\nOnce I was able to get the code to work for both independent branches and nested branches it became much easier.\n\n\n\nHere are the test cases used and the output from the function after running all of the test cases.\n#Simple Linear Sequence A program with a single linear sequence of instructions. No branches or loops.\n{\n  \"functions\": [\n    {\n      \"name\": \"linear\",\n      \"instrs\": [\n        {\"label\": \"entry\"},\n        {\"op\": \"const\", \"dest\": \"a\", \"type\": \"int\", \"value\": 1},\n        {\"op\": \"add\", \"dest\": \"b\", \"type\": \"int\", \"args\": [\"a\", \"a\"]},\n        {\"op\": \"mul\", \"dest\": \"c\", \"type\": \"int\", \"args\": [\"b\", \"a\"]},\n        {\"label\": \"exit\"}\n      ]\n    }\n  ]\n}\n\n#Simple Branch - A conditional branch leading to two different paths and joining back together.\n{\n  \"functions\": [\n    {\n      \"name\": \"branch\",\n      \"instrs\": [\n        {\"label\": \"entry\"},\n        {\"op\": \"const\", \"dest\": \"x\", \"type\": \"int\", \"value\": 5},\n        {\"op\": \"const\", \"dest\": \"y\", \"type\": \"int\", \"value\": 10},\n        {\"op\": \"br\", \"args\": [\"x\"], \"labels\": [\"true_branch\", \"false_branch\"]},\n        {\"label\": \"true_branch\"},\n        {\"op\": \"add\", \"dest\": \"z\", \"type\": \"int\", \"args\": [\"x\", \"y\"]},\n        {\"op\": \"jmp\", \"labels\": [\"join\"]},\n        {\"label\": \"false_branch\"},\n        {\"op\": \"sub\", \"dest\": \"z\", \"type\": \"int\", \"args\": [\"y\", \"x\"]},\n        {\"label\": \"join\"},\n        {\"op\": \"mul\", \"dest\": \"w\", \"type\": \"int\", \"args\": [\"z\", \"x\"]}\n      ]\n    }\n  ]\n}\n\n#Loop with Back Edge -A basic loop where a variable is incremented until a condition is met.\n{\n  \"functions\": [\n    {\n      \"name\": \"loop\",\n      \"instrs\": [\n        {\"label\": \"entry\"},\n        {\"op\": \"const\", \"dest\": \"i\", \"type\": \"int\", \"value\": 0},\n        {\"label\": \"loop_start\"},\n        {\"op\": \"lt\", \"dest\": \"cond\", \"type\": \"bool\", \"args\": [\"i\", \"10\"]},\n        {\"op\": \"br\", \"args\": [\"cond\"], \"labels\": [\"body\", \"exit\"]},\n        {\"label\": \"body\"},\n        {\"op\": \"add\", \"dest\": \"i\", \"type\": \"int\", \"args\": [\"i\", \"1\"]},\n        {\"op\": \"jmp\", \"labels\": [\"loop_start\"]},\n        {\"label\": \"exit\"}\n      ]\n    }\n  ]\n}\n#Nested Branches -Two nested conditional branches.\n{\n  \"functions\": [\n    {\n      \"name\": \"nested_branches\",\n      \"instrs\": [\n        {\"label\": \"entry\"},\n        {\"op\": \"const\", \"dest\": \"a\", \"type\": \"int\", \"value\": 1},\n        {\"op\": \"const\", \"dest\": \"b\", \"type\": \"int\", \"value\": 2},\n        {\"op\": \"br\", \"args\": [\"a\"], \"labels\": [\"branch1\", \"merge\"]},\n        {\"label\": \"branch1\"},\n        {\"op\": \"br\", \"args\": [\"b\"], \"labels\": [\"branch2\", \"merge\"]},\n        {\"label\": \"branch2\"},\n        {\"op\": \"mul\", \"dest\": \"x\", \"type\": \"int\", \"args\": [\"a\", \"b\"]},\n        {\"op\": \"jmp\", \"labels\": [\"merge\"]},\n        {\"label\": \"merge\"},\n        {\"op\": \"add\", \"dest\": \"c\", \"type\": \"int\", \"args\": [\"a\", \"b\"]}\n      ]\n    }\n  ]\n}\n#Multiple Independent Branches - A program with two independent branches and a join.\n{\n  \"functions\": [\n    {\n      \"name\": \"independent_branches\",\n      \"instrs\": [\n        {\"label\": \"entry\"},\n        {\"op\": \"const\", \"dest\": \"a\", \"type\": \"int\", \"value\": 3},\n        {\"op\": \"const\", \"dest\": \"b\", \"type\": \"int\", \"value\": 6},\n        {\"op\": \"br\", \"args\": [\"a\"], \"labels\": [\"branch1\", \"merge\"]},\n        {\"label\": \"branch1\"},\n        {\"op\": \"add\", \"dest\": \"c\", \"type\": \"int\", \"args\": [\"a\", \"b\"]},\n        {\"op\": \"jmp\", \"labels\": [\"merge\"]},\n        {\"label\": \"branch2\"},\n        {\"op\": \"sub\", \"dest\": \"c\", \"type\": \"int\", \"args\": [\"b\", \"a\"]},\n        {\"op\": \"jmp\", \"labels\": [\"merge\"]},\n        {\"label\": \"merge\"},\n        {\"op\": \"mul\", \"dest\": \"d\", \"type\": \"int\", \"args\": [\"c\", \"a\"]}\n      ]\n    }\n  ]\n}\n#Simple Two-Way Loop - A loop with a branch that exits early based on a condition.\n{\n  \"functions\": [\n    {\n      \"name\": \"two_way_loop\",\n      \"instrs\": [\n        {\"label\": \"entry\"},\n        {\"op\": \"const\", \"dest\": \"x\", \"type\": \"int\", \"value\": 0},\n        {\"label\": \"loop_start\"},\n        {\"op\": \"lt\", \"dest\": \"cond\", \"type\": \"bool\", \"args\": [\"x\", \"5\"]},\n        {\"op\": \"br\", \"args\": [\"cond\"], \"labels\": [\"loop_body\", \"exit\"]},\n        {\"label\": \"loop_body\"},\n        {\"op\": \"add\", \"dest\": \"x\", \"type\": \"int\", \"args\": [\"x\", \"1\"]},\n        {\"op\": \"jmp\", \"labels\": [\"loop_start\"]},\n        {\"label\": \"exit\"},\n        {\"op\": \"const\", \"dest\": \"y\", \"type\": \"int\", \"value\": 1}\n      ]\n    }\n  ]\n}",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework4 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/10-18-2024-HW4-YashaswiniMakaram.html#about-the-code",
    "href": "blogs/yashaswini/10-18-2024-HW4-YashaswiniMakaram.html#about-the-code",
    "title": "Homework4 - Yashaswini",
    "section": "",
    "text": "#Dominator Computation (find_dominators):\n\nThis function initializes all nodes to be dominated by every other node. Then, using an iterative approach, it refines the set of dominators until convergence.\nThe entry node is assumed to be node 0, and it dominates itself.\nThe dominators of each node are found by intersecting the dominator sets of its predecessors and adding the node itself.\n\n#Dominance Tree Construction (construct_dominance_tree):\n\nThis function builds the dominance tree from the dominator sets.\nFor each node, the immediate dominator (idom) is the node with the highest index in its dominator set that isn’t itself. The tree is constructed using these relationships.\n\n#Dominance Frontier Computation (compute_dominance_frontier):\n\nThe dominance frontier of a node is the set of nodes where the node dominates one of their predecessors but does not strictly dominate the node itself.\nThis is computed using the reverse of the CFG and the dominator sets.\n\nimport json\nfrom collections import defaultdict\n\ndef load_bril_program(filename):\n    \"\"\"Load the Bril program from a JSON file.\"\"\"\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef build_cfg(function):\n    \"\"\"Build the Control Flow Graph (CFG) for a given function.\"\"\"\n    instrs = function['instrs']\n    cfg = defaultdict(list)\n    label_map = {}\n\n    # Map labels to instruction indices\n    for i, instr in enumerate(instrs):\n        if 'label' in instr:\n            label_map[instr['label']] = i\n\n    for i, instr in enumerate(instrs):\n        if 'op' in instr and instr['op'] == 'jmp':\n            target = label_map[instr['labels'][0]]\n            cfg[i].append(target)\n        elif 'op' in instr and instr['op'] == 'br':\n            target1 = label_map[instr['labels'][0]]\n            target2 = label_map[instr['labels'][1]]\n            cfg[i].extend([target1, target2])\n        else:\n            # Fall-through case: connect to the next instruction\n            if i + 1 &lt; len(instrs):\n                cfg[i].append(i + 1)\n\n    return cfg\n\ndef reverse_cfg(cfg):\n    \"\"\"Compute the reverse of the CFG.\"\"\"\n    reverse = defaultdict(list)\n    for node, successors in cfg.items():\n        for succ in successors:\n            reverse[succ].append(node)\n    return reverse\n\ndef find_dominators(function):\n    \"\"\"Find the dominators for each node in the function.\"\"\"\n    cfg = build_cfg(function)\n    nodes = set(cfg.keys())\n    entry = min(nodes)#0  # Assuming the entry node is at index 0\n    dom = {node: nodes.copy() for node in nodes}\n    dom[entry] = {entry}\n\n    changed = True\n    while changed:\n        changed = False\n        for node in nodes - {entry}:\n            #new_dom = set.intersection(*(dom[pred] for pred in reverse_cfg(cfg)[node])) | {node}\n            preds = reverse_cfg(cfg)[node]\n            if preds:\n                # Compute intersection only if there are predecessors\n                new_dom = set.intersection(*(dom[pred] for pred in preds)) | {node}\n            else:\n                # If no predecessors, the dominators are just the node itself\n                new_dom = {node}\n            if dom[node] != new_dom:\n                dom[node] = new_dom\n                changed = True\n\n    return dom\n\ndef construct_dominance_tree(dominators):\n    \"\"\"Construct the dominance tree from the dominators.\"\"\"\n    tree = defaultdict(list)\n    for node, dom_set in dominators.items():\n        if node != min(dom_set):\n            idom = max(dom_set - {node})\n            tree[idom].append(node)\n    return tree\n\ndef compute_dominance_frontier(function, dominators):\n    \"\"\"Compute the dominance frontier for each node.\"\"\"\n    cfg = build_cfg(function)\n    df = defaultdict(set)\n\n    for node in cfg:\n        preds = reverse_cfg(cfg)[node]\n        if len(preds) &gt;= 2:\n            for pred in preds:\n                runner = pred\n                while runner not in dominators[node]:\n                    df[runner].add(node)\n                    #runner = max(dominators[runner] - {runner})\n                    if dominators[runner] - {runner}:\n                        runner = max(dominators[runner] - {runner})\n                    else:\n                        break  # If no other dominators, exit the loop\n\n    return df\n\ndef test_dominance_utilities(function):\n    \"\"\"Test the dominance utilities for a given function.\"\"\"\n    print(f\"Testing function: {function['name']}\")\n    dominators = find_dominators(function)\n    print(f\"Dominators: {dominators}\")\n\n    dominance_tree = construct_dominance_tree(dominators)\n    print(f\"Dominance Tree: {dominance_tree}\")\n\n    dominance_frontier = compute_dominance_frontier(function, dominators)\n    print(f\"Dominance Frontier: {dominance_frontier}\")\n\ndef main():\n    # Load the Bril program from a file\n    #bril_program = load_bril_program('input_bril.json')\n    for bril_program in bril_programs:\n    # Run the tests\n        for function in bril_program['functions']:\n            test_dominance_utilities(function)\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework4 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/10-18-2024-HW4-YashaswiniMakaram.html#testing",
    "href": "blogs/yashaswini/10-18-2024-HW4-YashaswiniMakaram.html#testing",
    "title": "Homework4 - Yashaswini",
    "section": "",
    "text": "I implemented 8 test cases each targeting a different type of CFG. I then manually checked the against the outputs produced by the code. I had to make some changes when the code was unable to handle empty sets but after the output seems to be work correctly",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework4 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/10-18-2024-HW4-YashaswiniMakaram.html#challanges",
    "href": "blogs/yashaswini/10-18-2024-HW4-YashaswiniMakaram.html#challanges",
    "title": "Homework4 - Yashaswini",
    "section": "",
    "text": "The hardest part of this task was understanding ensuring that the algorithm i used could handle various data flow types.\nOnce I was able to get the code to work for both independent branches and nested branches it became much easier.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework4 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/10-18-2024-HW4-YashaswiniMakaram.html#test-cases-and-output",
    "href": "blogs/yashaswini/10-18-2024-HW4-YashaswiniMakaram.html#test-cases-and-output",
    "title": "Homework4 - Yashaswini",
    "section": "",
    "text": "Here are the test cases used and the output from the function after running all of the test cases.\n#Simple Linear Sequence A program with a single linear sequence of instructions. No branches or loops.\n{\n  \"functions\": [\n    {\n      \"name\": \"linear\",\n      \"instrs\": [\n        {\"label\": \"entry\"},\n        {\"op\": \"const\", \"dest\": \"a\", \"type\": \"int\", \"value\": 1},\n        {\"op\": \"add\", \"dest\": \"b\", \"type\": \"int\", \"args\": [\"a\", \"a\"]},\n        {\"op\": \"mul\", \"dest\": \"c\", \"type\": \"int\", \"args\": [\"b\", \"a\"]},\n        {\"label\": \"exit\"}\n      ]\n    }\n  ]\n}\n\n#Simple Branch - A conditional branch leading to two different paths and joining back together.\n{\n  \"functions\": [\n    {\n      \"name\": \"branch\",\n      \"instrs\": [\n        {\"label\": \"entry\"},\n        {\"op\": \"const\", \"dest\": \"x\", \"type\": \"int\", \"value\": 5},\n        {\"op\": \"const\", \"dest\": \"y\", \"type\": \"int\", \"value\": 10},\n        {\"op\": \"br\", \"args\": [\"x\"], \"labels\": [\"true_branch\", \"false_branch\"]},\n        {\"label\": \"true_branch\"},\n        {\"op\": \"add\", \"dest\": \"z\", \"type\": \"int\", \"args\": [\"x\", \"y\"]},\n        {\"op\": \"jmp\", \"labels\": [\"join\"]},\n        {\"label\": \"false_branch\"},\n        {\"op\": \"sub\", \"dest\": \"z\", \"type\": \"int\", \"args\": [\"y\", \"x\"]},\n        {\"label\": \"join\"},\n        {\"op\": \"mul\", \"dest\": \"w\", \"type\": \"int\", \"args\": [\"z\", \"x\"]}\n      ]\n    }\n  ]\n}\n\n#Loop with Back Edge -A basic loop where a variable is incremented until a condition is met.\n{\n  \"functions\": [\n    {\n      \"name\": \"loop\",\n      \"instrs\": [\n        {\"label\": \"entry\"},\n        {\"op\": \"const\", \"dest\": \"i\", \"type\": \"int\", \"value\": 0},\n        {\"label\": \"loop_start\"},\n        {\"op\": \"lt\", \"dest\": \"cond\", \"type\": \"bool\", \"args\": [\"i\", \"10\"]},\n        {\"op\": \"br\", \"args\": [\"cond\"], \"labels\": [\"body\", \"exit\"]},\n        {\"label\": \"body\"},\n        {\"op\": \"add\", \"dest\": \"i\", \"type\": \"int\", \"args\": [\"i\", \"1\"]},\n        {\"op\": \"jmp\", \"labels\": [\"loop_start\"]},\n        {\"label\": \"exit\"}\n      ]\n    }\n  ]\n}\n#Nested Branches -Two nested conditional branches.\n{\n  \"functions\": [\n    {\n      \"name\": \"nested_branches\",\n      \"instrs\": [\n        {\"label\": \"entry\"},\n        {\"op\": \"const\", \"dest\": \"a\", \"type\": \"int\", \"value\": 1},\n        {\"op\": \"const\", \"dest\": \"b\", \"type\": \"int\", \"value\": 2},\n        {\"op\": \"br\", \"args\": [\"a\"], \"labels\": [\"branch1\", \"merge\"]},\n        {\"label\": \"branch1\"},\n        {\"op\": \"br\", \"args\": [\"b\"], \"labels\": [\"branch2\", \"merge\"]},\n        {\"label\": \"branch2\"},\n        {\"op\": \"mul\", \"dest\": \"x\", \"type\": \"int\", \"args\": [\"a\", \"b\"]},\n        {\"op\": \"jmp\", \"labels\": [\"merge\"]},\n        {\"label\": \"merge\"},\n        {\"op\": \"add\", \"dest\": \"c\", \"type\": \"int\", \"args\": [\"a\", \"b\"]}\n      ]\n    }\n  ]\n}\n#Multiple Independent Branches - A program with two independent branches and a join.\n{\n  \"functions\": [\n    {\n      \"name\": \"independent_branches\",\n      \"instrs\": [\n        {\"label\": \"entry\"},\n        {\"op\": \"const\", \"dest\": \"a\", \"type\": \"int\", \"value\": 3},\n        {\"op\": \"const\", \"dest\": \"b\", \"type\": \"int\", \"value\": 6},\n        {\"op\": \"br\", \"args\": [\"a\"], \"labels\": [\"branch1\", \"merge\"]},\n        {\"label\": \"branch1\"},\n        {\"op\": \"add\", \"dest\": \"c\", \"type\": \"int\", \"args\": [\"a\", \"b\"]},\n        {\"op\": \"jmp\", \"labels\": [\"merge\"]},\n        {\"label\": \"branch2\"},\n        {\"op\": \"sub\", \"dest\": \"c\", \"type\": \"int\", \"args\": [\"b\", \"a\"]},\n        {\"op\": \"jmp\", \"labels\": [\"merge\"]},\n        {\"label\": \"merge\"},\n        {\"op\": \"mul\", \"dest\": \"d\", \"type\": \"int\", \"args\": [\"c\", \"a\"]}\n      ]\n    }\n  ]\n}\n#Simple Two-Way Loop - A loop with a branch that exits early based on a condition.\n{\n  \"functions\": [\n    {\n      \"name\": \"two_way_loop\",\n      \"instrs\": [\n        {\"label\": \"entry\"},\n        {\"op\": \"const\", \"dest\": \"x\", \"type\": \"int\", \"value\": 0},\n        {\"label\": \"loop_start\"},\n        {\"op\": \"lt\", \"dest\": \"cond\", \"type\": \"bool\", \"args\": [\"x\", \"5\"]},\n        {\"op\": \"br\", \"args\": [\"cond\"], \"labels\": [\"loop_body\", \"exit\"]},\n        {\"label\": \"loop_body\"},\n        {\"op\": \"add\", \"dest\": \"x\", \"type\": \"int\", \"args\": [\"x\", \"1\"]},\n        {\"op\": \"jmp\", \"labels\": [\"loop_start\"]},\n        {\"label\": \"exit\"},\n        {\"op\": \"const\", \"dest\": \"y\", \"type\": \"int\", \"value\": 1}\n      ]\n    }\n  ]\n}",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework4 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/FinalReport.html",
    "href": "blogs/yashaswini/FinalReport.html",
    "title": "Final Project - Yashaswini",
    "section": "",
    "text": "In the context of cryptographic implementations, masking is a countermeasure against side-channel attacks. These attacks exploit physical leakages, such as power consumption or electromagnetic emissions, to infer secret data. Masking mitigates these risks by splitting sensitive data into multiple shares and operating on these shares independently. The operations must be designed so that the leakage of any subset of shares provides no useful information about the original data.\nThere are two primary types of masking: 1. Boolean Masking: Sensitive data is split using XOR operations. 2. Arithmetic Masking: Sensitive data is split using modular arithmetic.\nFor modern cryptographic workloads, the scalability and efficiency of masking techniques are critical. This project evaluates the performance of multi-share masking operations implemented on both CPU and GPU.\n\n\n\n\nThe goal of this project was to evaluate the performance of multi-share masked cryptographic operations when executed on GPU versus CPU. Specifically, we aimed to: 1. Compare the runtime performance of Boolean and Arithmetic masking operations on both architectures. 2. Simulate realistic cryptographic workloads (e.g., S-Box operations, AES rounds, and GF(2^n) arithmetic). 4. Assess scalability as the number of shares increases.\n\n\n\n\n\n\nThe project was structured as follows: 1. Workload Selection: The workloads included: - Arithmatic operations: basic logic and addition operations under masking. - S-Box Operations: A non-linear substitution step used in AES encryption. - GF(2^n) Arithmetic: Common in cryptographic operations like polynomial multiplication. 2. Masking Techniques: - Boolean and Arithmetic masking were applied with varying numbers of shares. 3. Platforms: - CPU: Sequential execution of masked operations. - GPU: Parallel execution using CUDA. 4. Evaluation Metrics: - Runtime performance. - Scalability as the number of shares increases.\n\n\n\n\n\nThe CPU implementation involved: - Sequential loops for operating on shares. - Use of native C++ operations for Boolean masking (XOR) and Arithmetic masking (modular arithmetic). - Timing profiling using standard libraries.\n\n\n\nThe GPU implementation involved: - Parallel execution of masking operations using CUDA. - Separate kernels for S-Box operations and GF(2^n) arithmetic. - Accurate benchmarking using CUDA events.\n\n\n\n\n\nInputs were generated randomly for reproducibility.\nBenchmarks were run for varying numbers of shares (e.g., 2, 4, 8).\nOutputs were verified for correctness across platforms.\n\n\n\n__global__ void arithmetic_share_operations_kernel(uint32_t *inputs, uint32_t *share1, uint32_t *share2, uint32_t *add_results, uint32_t *mul_results, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        // Perform arithmetic operations with shares\n        share1[idx] = inputs[idx] / 2; // Example splitting\n        share2[idx] = inputs[idx] - share1[idx];\n        add_results[idx] = share1[idx] + share2[idx]; // Reconstruction\n        mul_results[idx] = share1[idx] * share2[idx]; // Masked multiplication\n    }\n}\n\nvoid arithmetic_share_operations_cpu(uint32_t *inputs, uint32_t *share1, uint32_t *share2, uint32_t *add_results, uint32_t *mul_results, int n) {\n    for (int i = 0; i &lt; n; i++) {\n        share1[i] = inputs[i] / 2; // Example splitting\n        share2[i] = inputs[i] - share1[i];\n        add_results[i] = share1[i] + share2[i]; // Reconstruction\n        mul_results[i] = share1[i] * share2[i]; // Masked multiplication\n    }\n}\n\n\n\n__global__ void boolean_share_operations_kernel(uint32_t *inputs, uint32_t *share1, uint32_t *share2, uint32_t *xor_results, uint32_t *and_results, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        // Generate shares\n        share1[idx] = inputs[idx] ^ (idx * 0xA5A5A5A5); // Simulated randomness\n        share2[idx] = inputs[idx] ^ share1[idx];\n        \n        // Perform XOR operation securely on shares\n        uint32_t xor1 = share1[idx] ^ 0xA5A5A5A5;\n        uint32_t xor2 = share2[idx] ^ 0x00000000;\n        \n        // Perform AND operation securely on shares\n        uint32_t and1 = share1[idx] & 0x5A5A5A5A;\n        uint32_t and2 = share2[idx] & 0x5A5A5A5A;\n        \n        // Reconstruct XOR and AND results\n        xor_results[idx] = xor1 ^ xor2;\n        and_results[idx] = and1 ^ and2; // XOR to reconstruct AND\n    }\n}\n\n// CPU Implementation for Comparison\nvoid boolean_share_operations_cpu(uint32_t *inputs, uint32_t *share1, uint32_t *share2, uint32_t *xor_results, uint32_t *and_results, int n) {\n    for (int i = 0; i &lt; n; i++) {\n        share1[i] = inputs[i] ^ (i * 0xA5A5A5A5);\n        share2[i] = inputs[i] ^ share1[i];\n        uint32_t xor1 = share1[i] ^ 0xA5A5A5A5;\n        uint32_t xor2 = share2[i] ^ 0x00000000;\n        uint32_t and1 = share1[i] & 0x5A5A5A5A;\n        uint32_t and2 = share2[i] & 0x5A5A5A5A;\n        xor_results[i] = xor1 ^ xor2;\n        and_results[i] = and1 ^ and2;\n    }\n}\n\n\n\n\n__device__ uint8_t sbox[SBOX_SIZE] = {\n    0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5, 0x30, 0x01, 0x67, 0x2b, 0xfe, 0xd7, 0xab, 0x76,\n    // Populate with actual S-Box values for AES\n};\n\n__global__ void gpu_sbox_operations(uint8_t *shares, uint8_t *outputs, int num_shares, int num_elements) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; num_elements) {\n        for (int share = 0; share &lt; num_shares; ++share) {\n            outputs[idx * num_shares + share] = sbox[shares[idx * num_shares + share]];\n        }\n    }\n}\n\n__global__ void gpu_gf2n_arithmetic(uint8_t *shares, uint8_t *outputs, int num_shares, int num_elements) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; num_elements) {\n        for (int share = 0; share &lt; num_shares; ++share) {\n            outputs[idx * num_shares + share] = shares[idx * num_shares + share] ^ (shares[idx * num_shares + share] &lt;&lt; 1);\n        }\n    }\n}\n\nvoid cpu_sbox_operations(uint8_t *shares, uint8_t *outputs, int num_shares, int num_elements) {\n    for (int idx = 0; idx &lt; num_elements; ++idx) {\n        for (int share = 0; share &lt; num_shares; ++share) {\n            outputs[idx * num_shares + share] = sbox[shares[idx * num_shares + share]];\n        }\n    }\n}\n\nvoid cpu_gf2n_arithmetic(uint8_t *shares, uint8_t *outputs, int num_shares, int num_elements) {\n    for (int idx = 0; idx &lt; num_elements; ++idx) {\n        for (int share = 0; share &lt; num_shares; ++share) {\n            outputs[idx * num_shares + share] = shares[idx * num_shares + share] ^ (shares[idx * num_shares + share] &lt;&lt; 1);\n        }\n    }\n}\n\n\n\n\n\n\n\nCUDA-Specific Debugging: Debugging CUDA kernels required careful attention to memory allocation, thread indexing, and synchronization.\nTiming Measurements: Measuring GPU execution time accurately required synchronizing kernels and using CUDA events.\nScalability: Ensuring that both CPU and GPU implementations scaled efficiently with the number of shares.\n\n\n\n\n\n\n\nin the timing measurements, the gpu implementation had close to 100x improvement over the CPU implementation for just 2 shares - Observation: GPU implementations significantly outperformed CPU implementations for large datasets due to parallelism. - Trends: As the number of shares increased, the GPU’s advantage became more pronounced.\n\n\n\n\nObservation: GPU implementations scaled efficiently with increased input sizes and number of shares.\n\n\n\n\n\n\nThe project successfully implemented and benchmarked multi-share masked cryptographic operations on both CPU and GPU. Key findings include: - GPU implementations demonstrated significant speedups compared to CPUs, especially for larger datasets and higher numbers of shares. - The increased capabilites were pronounced not only for basic operations, but for cryptographic methods like sbox operations. - Decreased timing can allow for stronger masking to be applied. This will make cryptographic methods more secure against sidechannel attacks without greatily increasing runtime.\n\n\n\n\n\nCan these masking implementations be optimized further for specific cryptographic algorithms like AES?\nWhat are the trade-offs when applying these techniques to hardware-constrained devices?\nHow do masking operations affect power consumption and energy efficiency?",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Final Project - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/FinalReport.html#introduction-to-masking",
    "href": "blogs/yashaswini/FinalReport.html#introduction-to-masking",
    "title": "Final Project - Yashaswini",
    "section": "",
    "text": "In the context of cryptographic implementations, masking is a countermeasure against side-channel attacks. These attacks exploit physical leakages, such as power consumption or electromagnetic emissions, to infer secret data. Masking mitigates these risks by splitting sensitive data into multiple shares and operating on these shares independently. The operations must be designed so that the leakage of any subset of shares provides no useful information about the original data.\nThere are two primary types of masking: 1. Boolean Masking: Sensitive data is split using XOR operations. 2. Arithmetic Masking: Sensitive data is split using modular arithmetic.\nFor modern cryptographic workloads, the scalability and efficiency of masking techniques are critical. This project evaluates the performance of multi-share masking operations implemented on both CPU and GPU.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Final Project - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/FinalReport.html#project-goal",
    "href": "blogs/yashaswini/FinalReport.html#project-goal",
    "title": "Final Project - Yashaswini",
    "section": "",
    "text": "The goal of this project was to evaluate the performance of multi-share masked cryptographic operations when executed on GPU versus CPU. Specifically, we aimed to: 1. Compare the runtime performance of Boolean and Arithmetic masking operations on both architectures. 2. Simulate realistic cryptographic workloads (e.g., S-Box operations, AES rounds, and GF(2^n) arithmetic). 4. Assess scalability as the number of shares increases.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Final Project - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/FinalReport.html#design-and-implementation",
    "href": "blogs/yashaswini/FinalReport.html#design-and-implementation",
    "title": "Final Project - Yashaswini",
    "section": "",
    "text": "The project was structured as follows: 1. Workload Selection: The workloads included: - Arithmatic operations: basic logic and addition operations under masking. - S-Box Operations: A non-linear substitution step used in AES encryption. - GF(2^n) Arithmetic: Common in cryptographic operations like polynomial multiplication. 2. Masking Techniques: - Boolean and Arithmetic masking were applied with varying numbers of shares. 3. Platforms: - CPU: Sequential execution of masked operations. - GPU: Parallel execution using CUDA. 4. Evaluation Metrics: - Runtime performance. - Scalability as the number of shares increases.\n\n\n\n\n\nThe CPU implementation involved: - Sequential loops for operating on shares. - Use of native C++ operations for Boolean masking (XOR) and Arithmetic masking (modular arithmetic). - Timing profiling using standard libraries.\n\n\n\nThe GPU implementation involved: - Parallel execution of masking operations using CUDA. - Separate kernels for S-Box operations and GF(2^n) arithmetic. - Accurate benchmarking using CUDA events.\n\n\n\n\n\nInputs were generated randomly for reproducibility.\nBenchmarks were run for varying numbers of shares (e.g., 2, 4, 8).\nOutputs were verified for correctness across platforms.\n\n\n\n__global__ void arithmetic_share_operations_kernel(uint32_t *inputs, uint32_t *share1, uint32_t *share2, uint32_t *add_results, uint32_t *mul_results, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        // Perform arithmetic operations with shares\n        share1[idx] = inputs[idx] / 2; // Example splitting\n        share2[idx] = inputs[idx] - share1[idx];\n        add_results[idx] = share1[idx] + share2[idx]; // Reconstruction\n        mul_results[idx] = share1[idx] * share2[idx]; // Masked multiplication\n    }\n}\n\nvoid arithmetic_share_operations_cpu(uint32_t *inputs, uint32_t *share1, uint32_t *share2, uint32_t *add_results, uint32_t *mul_results, int n) {\n    for (int i = 0; i &lt; n; i++) {\n        share1[i] = inputs[i] / 2; // Example splitting\n        share2[i] = inputs[i] - share1[i];\n        add_results[i] = share1[i] + share2[i]; // Reconstruction\n        mul_results[i] = share1[i] * share2[i]; // Masked multiplication\n    }\n}\n\n\n\n__global__ void boolean_share_operations_kernel(uint32_t *inputs, uint32_t *share1, uint32_t *share2, uint32_t *xor_results, uint32_t *and_results, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; n) {\n        // Generate shares\n        share1[idx] = inputs[idx] ^ (idx * 0xA5A5A5A5); // Simulated randomness\n        share2[idx] = inputs[idx] ^ share1[idx];\n        \n        // Perform XOR operation securely on shares\n        uint32_t xor1 = share1[idx] ^ 0xA5A5A5A5;\n        uint32_t xor2 = share2[idx] ^ 0x00000000;\n        \n        // Perform AND operation securely on shares\n        uint32_t and1 = share1[idx] & 0x5A5A5A5A;\n        uint32_t and2 = share2[idx] & 0x5A5A5A5A;\n        \n        // Reconstruct XOR and AND results\n        xor_results[idx] = xor1 ^ xor2;\n        and_results[idx] = and1 ^ and2; // XOR to reconstruct AND\n    }\n}\n\n// CPU Implementation for Comparison\nvoid boolean_share_operations_cpu(uint32_t *inputs, uint32_t *share1, uint32_t *share2, uint32_t *xor_results, uint32_t *and_results, int n) {\n    for (int i = 0; i &lt; n; i++) {\n        share1[i] = inputs[i] ^ (i * 0xA5A5A5A5);\n        share2[i] = inputs[i] ^ share1[i];\n        uint32_t xor1 = share1[i] ^ 0xA5A5A5A5;\n        uint32_t xor2 = share2[i] ^ 0x00000000;\n        uint32_t and1 = share1[i] & 0x5A5A5A5A;\n        uint32_t and2 = share2[i] & 0x5A5A5A5A;\n        xor_results[i] = xor1 ^ xor2;\n        and_results[i] = and1 ^ and2;\n    }\n}\n\n\n\n\n__device__ uint8_t sbox[SBOX_SIZE] = {\n    0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5, 0x30, 0x01, 0x67, 0x2b, 0xfe, 0xd7, 0xab, 0x76,\n    // Populate with actual S-Box values for AES\n};\n\n__global__ void gpu_sbox_operations(uint8_t *shares, uint8_t *outputs, int num_shares, int num_elements) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; num_elements) {\n        for (int share = 0; share &lt; num_shares; ++share) {\n            outputs[idx * num_shares + share] = sbox[shares[idx * num_shares + share]];\n        }\n    }\n}\n\n__global__ void gpu_gf2n_arithmetic(uint8_t *shares, uint8_t *outputs, int num_shares, int num_elements) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx &lt; num_elements) {\n        for (int share = 0; share &lt; num_shares; ++share) {\n            outputs[idx * num_shares + share] = shares[idx * num_shares + share] ^ (shares[idx * num_shares + share] &lt;&lt; 1);\n        }\n    }\n}\n\nvoid cpu_sbox_operations(uint8_t *shares, uint8_t *outputs, int num_shares, int num_elements) {\n    for (int idx = 0; idx &lt; num_elements; ++idx) {\n        for (int share = 0; share &lt; num_shares; ++share) {\n            outputs[idx * num_shares + share] = sbox[shares[idx * num_shares + share]];\n        }\n    }\n}\n\nvoid cpu_gf2n_arithmetic(uint8_t *shares, uint8_t *outputs, int num_shares, int num_elements) {\n    for (int idx = 0; idx &lt; num_elements; ++idx) {\n        for (int share = 0; share &lt; num_shares; ++share) {\n            outputs[idx * num_shares + share] = shares[idx * num_shares + share] ^ (shares[idx * num_shares + share] &lt;&lt; 1);\n        }\n    }\n}",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Final Project - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/FinalReport.html#challenges-and-difficulties",
    "href": "blogs/yashaswini/FinalReport.html#challenges-and-difficulties",
    "title": "Final Project - Yashaswini",
    "section": "",
    "text": "CUDA-Specific Debugging: Debugging CUDA kernels required careful attention to memory allocation, thread indexing, and synchronization.\nTiming Measurements: Measuring GPU execution time accurately required synchronizing kernels and using CUDA events.\nScalability: Ensuring that both CPU and GPU implementations scaled efficiently with the number of shares.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Final Project - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/FinalReport.html#empirical-evaluation",
    "href": "blogs/yashaswini/FinalReport.html#empirical-evaluation",
    "title": "Final Project - Yashaswini",
    "section": "",
    "text": "in the timing measurements, the gpu implementation had close to 100x improvement over the CPU implementation for just 2 shares - Observation: GPU implementations significantly outperformed CPU implementations for large datasets due to parallelism. - Trends: As the number of shares increased, the GPU’s advantage became more pronounced.\n\n\n\n\nObservation: GPU implementations scaled efficiently with increased input sizes and number of shares.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Final Project - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/FinalReport.html#conclusion-success-evaluation",
    "href": "blogs/yashaswini/FinalReport.html#conclusion-success-evaluation",
    "title": "Final Project - Yashaswini",
    "section": "",
    "text": "The project successfully implemented and benchmarked multi-share masked cryptographic operations on both CPU and GPU. Key findings include: - GPU implementations demonstrated significant speedups compared to CPUs, especially for larger datasets and higher numbers of shares. - The increased capabilites were pronounced not only for basic operations, but for cryptographic methods like sbox operations. - Decreased timing can allow for stronger masking to be applied. This will make cryptographic methods more secure against sidechannel attacks without greatily increasing runtime.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Final Project - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/FinalReport.html#future-work",
    "href": "blogs/yashaswini/FinalReport.html#future-work",
    "title": "Final Project - Yashaswini",
    "section": "",
    "text": "Can these masking implementations be optimized further for specific cryptographic algorithms like AES?\nWhat are the trade-offs when applying these techniques to hardware-constrained devices?\nHow do masking operations affect power consumption and energy efficiency?",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Final Project - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/10-11-2024-HW3-YashaswiniMakaram.html",
    "href": "blogs/yashaswini/10-11-2024-HW3-YashaswiniMakaram.html",
    "title": "Homework3 - Yashaswini",
    "section": "",
    "text": "creates a flow diagram for a given bril code. can choose whether to ggo forwards,ay must, ect.\n\n’’’ import json from collections import defaultdict import graphviz\ndef load_bril_program(filename): “““Load the Bril program from a JSON file.”“” with open(filename, ‘r’) as file: return json.load(file)\ndef build_cfg(function): “““Build the Control Flow Graph (CFG) for a given function.”“” instrs = function[‘instrs’] cfg = defaultdict(list) label_map = {}\n# Map labels to instruction indices\nfor i, instr in enumerate(instrs):\n    if 'label' in instr:\n        label_map[instr['label']] = i\n\nfor i, instr in enumerate(instrs):\n    if 'op' in instr and instr['op'] == 'jmp':\n        target = label_map[instr['labels'][0]]\n        cfg[i].append(target)\n    elif 'op' in instr and instr['op'] == 'br':\n        target1 = label_map[instr['labels'][0]]\n        target2 = label_map[instr['labels'][1]]\n        cfg[i].extend([target1, target2])\n    else:\n        # Fall-through case: connect to the next instruction\n        if i + 1 &lt; len(instrs):\n            cfg[i].append(i + 1)\n\nreturn cfg\ndef predecessors(cfg): “““Find predecessors for each node in the CFG.”“” pred = defaultdict(list) for node, successors in cfg.items(): for succ in successors: pred[succ].append(node) return pred\ndef generic_data_flow_analysis(function, transfer_fn, gen_fn, kill_fn, direction=“forward”, merge=“union”): “““Perform a generic data flow analysis on the given function.”“” cfg = build_cfg(function) preds = predecessors(cfg)\n# Initialize IN and OUT sets for all nodes\nin_sets = defaultdict(set)\nout_sets = defaultdict(set)  # Initialize with defaultdict to avoid KeyError\n\n# Ensure every node in the function has an entry in out_sets\nfor node in range(len(function['instrs'])):\n    out_sets[node] = gen_fn(node)  # Start with GEN[n] if desired, or an empty set\n\nchanged = set(cfg.keys())\n\nwhile changed:\n    n = changed.pop()\n\n    # Calculate IN[n] or OUT[n] based on direction\n    if direction == \"forward\":\n        in_sets[n] = set()\n        for p in preds[n]:\n            if merge == \"union\":\n                in_sets[n] |= out_sets[p]\n            elif merge == \"intersection\" and out_sets[p]:\n                in_sets[n] &= out_sets[p]\n    else:  # Backward analysis\n        out_sets[n] = set()\n        for s in cfg[n]:\n            if merge == \"union\":\n                out_sets[n] |= in_sets[s]\n            elif merge == \"intersection\" and in_sets[s]:\n                out_sets[n] &= in_sets[s]\n\n    old_out = out_sets[n].copy() if direction == \"forward\" else in_sets[n].copy()\n    \n    # Update OUT[n] or IN[n] using the transfer function\n    if direction == \"forward\":\n        out_sets[n] = transfer_fn(in_sets[n], gen_fn(n), kill_fn(n))\n    else:\n        in_sets[n] = transfer_fn(out_sets[n], gen_fn(n), kill_fn(n))\n\n    # If there's a change, update the changed set\n    if direction == \"forward\" and old_out != out_sets[n]:\n        for s in cfg[n]:\n            changed.add(s)\n    elif direction == \"backward\" and old_out != in_sets[n]:\n        for p in preds[n]:\n            changed.add(p)\n\nreturn in_sets, out_sets\ndef reaching_definitions_transfer(in_set, gen_set, kill_set): “““Transfer function for Reaching Definitions: OUT[n] = GEN[n] U (IN[n] - KILL[n]).”“” return gen_set | (in_set - kill_set)\ndef reaching_definitions_gen_kill(instr, idx): “““GEN and KILL sets for Reaching Definitions.”“” if ‘dest’ in instr: gen_set = {(instr[‘dest’], idx)} kill_set = {var for var, _ in gen_set} return gen_set, kill_set return set(), set()\ndef create_cfg_visualization(function, in_sets, out_sets): “““Create a graphical view of the CFG with IN and OUT sets.”“” cfg = build_cfg(function) dot = graphviz.Digraph(comment=function[‘name’])\nfor node, instr in enumerate(function['instrs']):\n    label = f\"Instr {node}: {instr}\\nIN: {in_sets[node]}\\nOUT: {out_sets[node]}\"\n    dot.node(str(node), label)\n\nfor node, successors in cfg.items():\n    for succ in successors:\n        dot.edge(str(node), str(succ))\n\n# Save the CFG visualization as a PDF\ndot.render(f\"cfg_{function['name']}\", format=\"pdf\", cleanup=True)\ndef run_analysis(program): “““Run the generic data flow analysis on each function in the Bril program.”“” for function in program[‘functions’]: print(f”Analyzing function: {function[‘name’]}“)\n    def gen_fn(n):\n        instr = function['instrs'][n]\n        return reaching_definitions_gen_kill(instr, n)[0]\n\n    def kill_fn(n):\n        instr = function['instrs'][n]\n        return reaching_definitions_gen_kill(instr, n)[1]\n\n    in_sets, out_sets = generic_data_flow_analysis(\n        function, reaching_definitions_transfer, gen_fn, kill_fn, direction=\"forward\", merge=\"union\"\n    )\n\n    print(f\"IN sets: {in_sets}\")\n    print(f\"OUT sets: {out_sets}\")\n\n    # Create a graphical view of the CFG\n    create_cfg_visualization(function, in_sets, out_sets)\ndef main(): # Load the Bril program from a file bril_program = load_bril_program(‘add.bril’)\n# Run the analysis\nrun_analysis(bril_program)",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework3 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/10-11-2024-HW3-YashaswiniMakaram.html#about-the-code",
    "href": "blogs/yashaswini/10-11-2024-HW3-YashaswiniMakaram.html#about-the-code",
    "title": "Homework3 - Yashaswini",
    "section": "",
    "text": "creates a flow diagram for a given bril code. can choose whether to ggo forwards,ay must, ect.\n\n’’’ import json from collections import defaultdict import graphviz\ndef load_bril_program(filename): “““Load the Bril program from a JSON file.”“” with open(filename, ‘r’) as file: return json.load(file)\ndef build_cfg(function): “““Build the Control Flow Graph (CFG) for a given function.”“” instrs = function[‘instrs’] cfg = defaultdict(list) label_map = {}\n# Map labels to instruction indices\nfor i, instr in enumerate(instrs):\n    if 'label' in instr:\n        label_map[instr['label']] = i\n\nfor i, instr in enumerate(instrs):\n    if 'op' in instr and instr['op'] == 'jmp':\n        target = label_map[instr['labels'][0]]\n        cfg[i].append(target)\n    elif 'op' in instr and instr['op'] == 'br':\n        target1 = label_map[instr['labels'][0]]\n        target2 = label_map[instr['labels'][1]]\n        cfg[i].extend([target1, target2])\n    else:\n        # Fall-through case: connect to the next instruction\n        if i + 1 &lt; len(instrs):\n            cfg[i].append(i + 1)\n\nreturn cfg\ndef predecessors(cfg): “““Find predecessors for each node in the CFG.”“” pred = defaultdict(list) for node, successors in cfg.items(): for succ in successors: pred[succ].append(node) return pred\ndef generic_data_flow_analysis(function, transfer_fn, gen_fn, kill_fn, direction=“forward”, merge=“union”): “““Perform a generic data flow analysis on the given function.”“” cfg = build_cfg(function) preds = predecessors(cfg)\n# Initialize IN and OUT sets for all nodes\nin_sets = defaultdict(set)\nout_sets = defaultdict(set)  # Initialize with defaultdict to avoid KeyError\n\n# Ensure every node in the function has an entry in out_sets\nfor node in range(len(function['instrs'])):\n    out_sets[node] = gen_fn(node)  # Start with GEN[n] if desired, or an empty set\n\nchanged = set(cfg.keys())\n\nwhile changed:\n    n = changed.pop()\n\n    # Calculate IN[n] or OUT[n] based on direction\n    if direction == \"forward\":\n        in_sets[n] = set()\n        for p in preds[n]:\n            if merge == \"union\":\n                in_sets[n] |= out_sets[p]\n            elif merge == \"intersection\" and out_sets[p]:\n                in_sets[n] &= out_sets[p]\n    else:  # Backward analysis\n        out_sets[n] = set()\n        for s in cfg[n]:\n            if merge == \"union\":\n                out_sets[n] |= in_sets[s]\n            elif merge == \"intersection\" and in_sets[s]:\n                out_sets[n] &= in_sets[s]\n\n    old_out = out_sets[n].copy() if direction == \"forward\" else in_sets[n].copy()\n    \n    # Update OUT[n] or IN[n] using the transfer function\n    if direction == \"forward\":\n        out_sets[n] = transfer_fn(in_sets[n], gen_fn(n), kill_fn(n))\n    else:\n        in_sets[n] = transfer_fn(out_sets[n], gen_fn(n), kill_fn(n))\n\n    # If there's a change, update the changed set\n    if direction == \"forward\" and old_out != out_sets[n]:\n        for s in cfg[n]:\n            changed.add(s)\n    elif direction == \"backward\" and old_out != in_sets[n]:\n        for p in preds[n]:\n            changed.add(p)\n\nreturn in_sets, out_sets\ndef reaching_definitions_transfer(in_set, gen_set, kill_set): “““Transfer function for Reaching Definitions: OUT[n] = GEN[n] U (IN[n] - KILL[n]).”“” return gen_set | (in_set - kill_set)\ndef reaching_definitions_gen_kill(instr, idx): “““GEN and KILL sets for Reaching Definitions.”“” if ‘dest’ in instr: gen_set = {(instr[‘dest’], idx)} kill_set = {var for var, _ in gen_set} return gen_set, kill_set return set(), set()\ndef create_cfg_visualization(function, in_sets, out_sets): “““Create a graphical view of the CFG with IN and OUT sets.”“” cfg = build_cfg(function) dot = graphviz.Digraph(comment=function[‘name’])\nfor node, instr in enumerate(function['instrs']):\n    label = f\"Instr {node}: {instr}\\nIN: {in_sets[node]}\\nOUT: {out_sets[node]}\"\n    dot.node(str(node), label)\n\nfor node, successors in cfg.items():\n    for succ in successors:\n        dot.edge(str(node), str(succ))\n\n# Save the CFG visualization as a PDF\ndot.render(f\"cfg_{function['name']}\", format=\"pdf\", cleanup=True)\ndef run_analysis(program): “““Run the generic data flow analysis on each function in the Bril program.”“” for function in program[‘functions’]: print(f”Analyzing function: {function[‘name’]}“)\n    def gen_fn(n):\n        instr = function['instrs'][n]\n        return reaching_definitions_gen_kill(instr, n)[0]\n\n    def kill_fn(n):\n        instr = function['instrs'][n]\n        return reaching_definitions_gen_kill(instr, n)[1]\n\n    in_sets, out_sets = generic_data_flow_analysis(\n        function, reaching_definitions_transfer, gen_fn, kill_fn, direction=\"forward\", merge=\"union\"\n    )\n\n    print(f\"IN sets: {in_sets}\")\n    print(f\"OUT sets: {out_sets}\")\n\n    # Create a graphical view of the CFG\n    create_cfg_visualization(function, in_sets, out_sets)\ndef main(): # Load the Bril program from a file bril_program = load_bril_program(‘add.bril’)\n# Run the analysis\nrun_analysis(bril_program)",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework3 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/aymane/readme5.html",
    "href": "blogs/aymane/readme5.html",
    "title": "Homework 5 – Implementing a simple LLVM pass",
    "section": "",
    "text": "For this assignment, I implemented an LLVM pass to count the number of instructions in each basic block of all functions in a module.\nstruct InstrCountPass : public PassInfoMixin&lt;InstrCountPass&gt; {\n    PreservedAnalyses run(Module &M, ModuleAnalysisManager &AM) {\n        for (auto &F : M.functions()) {\n            // print the name of each function it the module\n            errs() &lt;&lt; \"Function: \" &lt;&lt; F.getName() &lt;&lt; \"\\n\";\n            for (auto &BB : F) {\n                // print the name of the basic block\n                errs() &lt;&lt; \"  Basic Block: \";\n                BB.printAsOperand(errs(), false);\n\n\n                // Count and print the number of instructions in the basic block\n                int InstrCount = 0;\n                for (auto &I : BB) {\n                    InstrCount++;\n                }\n                errs() &lt;&lt; \"    Number of instructions: \" &lt;&lt; InstrCount &lt;&lt; \"\\n\";\n            }\n        }\n        return PreservedAnalyses::all();\n    }\n};\nAfter compiling the pass and running it, we can see all function names in the module, as well as all the blocks along with the number of instructions per block.\n$ clang -fpass-plugin=/home/eljeraria/compilers/instr_count_pass/build/instr_pass/InstrCountPass.so vec_add.c\nFunction: vec_add\n  Basic Block: %0    Number of instructions: 24\n  Basic Block: %18    Number of instructions: 4\n  Basic Block: %22    Number of instructions: 9\n  Basic Block: %29    Number of instructions: 4\n  Basic Block: %32    Number of instructions: 2\n  Basic Block: %33    Number of instructions: 4\n  Basic Block: %37    Number of instructions: 14\n  Basic Block: %50    Number of instructions: 4\n  Basic Block: %53    Number of instructions: 3\nFunction: llvm.stacksave.p0\nFunction: llvm.stackrestore.p0\nFunction: main\n  Basic Block: %0    Number of instructions: 4\nMy first attempt was to write a loop unrolling pass and compare the performance improvement of different unrolling factors. Unfortunately, I was not able to implement the pass successfully.\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Homework 5 -- Implementing a simple LLVM pass"
    ]
  },
  {
    "objectID": "blogs/aymane/readme3.html",
    "href": "blogs/aymane/readme3.html",
    "title": "Homework 3 – Implementing Liveness Dataflow Analysis",
    "section": "",
    "text": "In order to perform data flow analysis on a bril program, we need to perform certain preprocessing operations to create a control flow graph. Once the control flow graph is built, we need to run the iterative worklist algorithm to solve the dataflow analysis. For this assignment, I used certain functions from the bril repository to faciliate the generation of the block mapping. I focused on implementing the Liveness data flow analysis.\nKey Components of the DFA Program for this assignment are:\n\nGetting the successors of each block\n    def get_successors(self, block):\n        # last instr in block\n        instr = block[-1]\n        # get successors\n        if instr['op'] in ['jmp', 'br']:\n            return instr['labels']\n        # no successor\n        elif instr['op'] == 'ret':\n            return []\n        else: \n            return\nThis function determines the successors of a block (basic block of instructions). It does so based on the last instruction in a block, if it’s a jmp or br (branch) operation, the function returns the successors (.ie next block in the control flow graph (cfg)). If it’s a ret (return), there are no successors.\n\n\nGenerating the graph\n    def generate_graph(self, blocks):\n        # init all to empty lists\n        self.predecessors = {blk_id: [] for blk_id in blocks}\n        self.successors = {blk_id: [] for blk_id in blocks}\n        for name, block in blocks.items():\n            # get all succcessors\n            successors = self.get_successors(block)\n            self.successors[name].extend(successors) \n            \n            # make current block predecessors to all its successors\n            for s in successors:\n                self.predecessors[s].append(name)\nThis function builds the cfg for the program. It initializes empty lists for the predecessors and successors of each block. For each block, it determines the block’s successors and makes the current block a predecessor to all of its successors. This forms the graph that the DFA algorithm will traverse.\n    def analyze_dataflow(self, blocks):\n        # generate successors and predecessors for every block\n        self.generate_graph(blocks)\n\n        # start from the last block\n        blk_start = list(blocks.keys())[-1]\n        in_set = self.successors\n        out_set = self.predecessors\n\n        # init first block\n        self.in_set = {blk_start: set()}\n        self.out_set = {blk: set() for blk in blocks}\n\n        # iterative worklist dataflow algorithm\n        worklist = list(blocks.keys())\n        while worklist:\n            blk = worklist.pop(0)\n\n            # in values = merge all previous value \n            in_val = self.union_op(self.out_set[n] for n in in_set[blk])\n            self.in_set[blk] = in_val\n            \n            # out values generated using the transfer function\n            out_val = self.transfer_func(blocks[blk], in_val)\n\n            # if there was a change\n            if out_val != self.out_set[blk]:\n                # update\n                self.out_set[blk] = out_val\n                # add item to the workloist\n                worklist += out_set[blk]\n\n        return self.out_set, self.in_set\nThis is the function that conducts the data flow analysis by iterating over the control flow graph. Since liveness is a backwards algorithm, we start from the last block. The function uses the worklist algorithm to iteratively merge information using the transfer function.\nThe transfer function for liveness is\n\\[IN = GEN \\space \\cup \\space (OUT - KILL)\\]\nThe bril repository contains programs on which to perform data flow analysis. Let’s look at the cond.bril program:\n@main {\n  a: int = const 47;\n  b: int = const 42;\n  cond: bool = const true;\n  br cond .left .right;\n.left:\n  b: int = const 1;\n  c: int = const 5;\n  jmp .end;\n.right:\n  a: int = const 2;\n  c: int = const 10;\n  jmp .end;\n.end:\n  d: int = sub a c;\n  print d;\n}\nWithout performing any data flow analysis, we can, just by looking at this program, know that the only variables that reach a use are a and c when executing this instruction: d: int = sub a c. On the other hand, we expect not to see b in the liveness data flow analysis, since not instruction uses b as an argument. Below is the output for the livenes pass:\n$ bril2json &lt; cond.bril | python3 dfa.py\nb1:\n  in:  ∅\n  out: a\nleft:\n  in:  a\n  out: a, c\nright:\n  in:  ∅\n  out: a, c\nend:\n  in:  a, c\n  out: ∅\nI started the assignment by attempting to implement a generic approach, but it turned out to be more challenging than I expected. Eventually, I focused on implementing dataflow analysis specifically for liveness. The complexity mainly came from trying to create a modular worklist algorithm that could be parametrized with different inputs, such as the merge operation, transfer function, and whether the analysis is forward or backward.\n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Homework 3 -- Implementing Liveness Dataflow Analysis"
    ]
  },
  {
    "objectID": "blogs/aymane/readme4.html",
    "href": "blogs/aymane/readme4.html",
    "title": "Homework 4 – Implementing Dominance Algorithms",
    "section": "",
    "text": "I implemented the dominator functions inside dominator.py. There are some utility functions in the utils.py file. Functions to draw the graphs are located inside graph.py. To run the program, pass a bril file through stdin bril2json &lt; file.bril | python3 dominatory.py --doms --doms_tree --dom_frontier --test_dom --nodes nodeA nodeB\n--doms computes the dominators for each block and generates a cfg image. --dom_tree computes the dominator tree and generates the image. --dom_frontier computes the dominance frontier of all nodes and prints it out. --test_dom --nodes nodeA nodeB tests if nodeA dominates nodeB.\nKey Concepts\n•   Dominators: A block B1 dominates another block B2 if every path from the entry block to B2 must go through B1.\n•   Immediate Dominators: The closest dominator of a block B is called its immediate dominator (idom).\n•   Dominance Frontier: The set of blocks where the dominance relationship breaks.\nThe get_dominators() function returns a dictionary mapping a node to its dominators. We find a node’s dominators by finding the common ancestry of all it’s predecessors.\ndef get_dominators(blocks):\n    # get block list and block set structures\n    all_blks_l = list(blocks.keys())\n    all_blks_s = set(all_blks_l)\n\n    # get the firs block\n    entry = next(iter(blocks))\n\n    # init dominator dict\n    dom = {name: all_blks_s for name in blocks.keys()} \n    dom[entry] = {entry}\n\n    _, predecessors = generate_graph(blocks)\n\n    changed = True\n    while changed:\n        changed = False\n        # all vertices except the entry point\n        for v in all_blks_l[1:]:\n            intrsct = all_blks_s\n\n            # get the common ancestors of all predecessors\n            for pred in predecessors[v]:\n                intrsct = intrsct.intersection(dom[pred])\n\n            # self dominates\n            new_dom = intrsct.union({v})\n\n            # update the dominator tree entries\n            if new_dom != dom[v]:\n                dom[v] = new_dom\n                changed = True\n\n    return dom\nWe build the dominance tree by finding the immediate dominators of each block. A node a immediately dominates a node b if a != b and no node c (with c != a and c != b) exists such that a dominates c and c dominates b.\ndef strictly_dominates(b1, b2, doms):\n    return b1 in doms[b2] and b1 != b2\n\ndef build_dominance_tree(dom):\n    # init dominator tree\n    dom_tree = {node: [] for node in dom.keys()}\n\n    for a in dom.keys():\n        for b, b_doms in dom.items():\n            if a != b:\n                E_a_stric_dom_c = False\n                # look at all dominators of b\n                for c in b_doms:\n                    # c != b\n                    if c != b:\n                        # check if a strictly dominates any of b's dominators\n                        E_a_stric_dom_c = E_a_stric_dom_c or strictly_dominates(a, c, dom)\n\n                # b1 strictly dominates \n                if a in b_doms and not E_a_stric_dom_c:\n                    dom_tree[a].append(b)\n    return dom_tree\nFinally, the get_dominance_frontier() function returns a dictionary mapping each block to the blocks that lie within its dominance frontier. First we compute the intersection between the dominators of all predecessors of the node, then compute the union for the same set. Subtracting the intersection from the union results in the dominance frontier for a given node.\ndef get_dominance_frontier(blks):\n    # get predecessors and dominators for each block\n    _, p = generate_graph(blks)\n    doms = get_dominators(blks)\n\n    # init the frontier dict\n    frontier = {name: [] for name in doms.keys()}\n\n    # loop over all nodes\n    for node in doms:\n        # get the dominators of all predecessors of the current node\n        pred_doms = [doms[pred] for pred in p[node]]\n        # if none, we can skip to the next node\n        if len(pred_doms) == 0:\n            continue\n\n        # get the set intersection for dominators of all predecessors of current node\n        intersection = set.intersection(*map(set,pred_doms))\n        # get the set union for dominators of all predecessors of the current node\n        union = set.union(*map(set,pred_doms)) \n        frontier_set = union - intersection\n        \n        # add nodes to the frontier dict\n        for block in frontier_set:\n            frontier[block].append(node)\n\n    return frontier\n\nLet’s look at some examples.\ntest1.bril\n# ARGS: 8\n@main(input: int) {\n  value: int = id input;\n  v1: int = const 1;\n  result: int = id v1;\n  v3: int = id value;\n  i: int = id v3;\n.for.cond.2:\n  v4: int = id i;\n  v5: int = const 0;\n  v6: bool = gt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: int = id result;\n  v8: int = id i;\n  v9: int = mul v7 v8;\n  result: int = id v9;\n  v10: int = id i;\n  v11: int = const 1;\n  v12: int = sub v10 v11;\n  i: int = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: int = id result;\n  print v13;\n  v14: int = const 0;\n}\n\nCFG with Dominators\n #### Dominance Tree \nWe can run the following command to generate these two graphs, compute the dominance frontier and check whether node b1 dominates for.cond.2.\nbril2json &lt; test1.bril | python3 dominator.py --doms --dom_tree --dom_frontier --test_dom --nodes b1 for.cond.2\n\nDominance Frontier\n{'b1': [], 'for.cond.2': ['for.cond.2'], 'for.body.2': ['for.cond.2'], 'for.end.2': []}\n\nb1 dominates for.cond.2\ntest7.bril generates the same graph as one of the examples shown in the class slides.\n@main{\n.b0:\n    a: int = const 1;\n    b: int = const 0;\n    cond1: bool = a;\n    br cond1 .b5 .b1;\n.b5:\n    cond2: bool = a;\n    br cond1 .b6 .b7;\n.b1:\n    cond3: bool = a;\n    br cond1 .b2 .b3;\n.b2:\n    jmp .b4;\n.b4:\n    jmp .b8;\n.b6:\n    jmp .b4;\n.b3:\n    jmp .b8;\n.b7:\n    jmp .b8;\n.b8:\n    print a;\n}\nWe can verify that block b4 doesn’t dominate block b0 by running:\nbril2json &lt; test7.bril | python3 dominator.py --doms --dom_tree --test_dom --nodes b4 b0\nb4 doesn't dominate b0\n\n\nCFG with Dominators\n #### Dominance Tree \ntest4.bril incorporate phi nodes:\n@main {\n  x: int = const 1;\n  y: int = const 2;\n  \n.entry:\n  z: int = add x y;\n  \n.branch1:\n  cond1: bool = le x y;\n  br cond1 .then1 .else1;\n\n.then1:\n  t1: int = mul x z;\n  jmp .merge;\n\n.else1:\n  t2: int = sub y z;\n  \n.branch2:\n  cond2: bool = gt t2 x;\n  br cond2 .then2 .else2;\n\n.then2:\n  t3: int = add t2 x;\n  \n.else2:\n  t4: int = sub t2 y;\n\n.merge:\n  u: int = phi t1 .then1 t3 .then2 t4 .else2;\n  \n.exit:\n  print u;\n}\n\n\nCFG with Dominators\n #### Dominance Tree \n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Homework 4 -- Implementing Dominance Algorithms"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW5.html",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW5.html",
    "title": "Homework 5: LLVM - Spinning Up",
    "section": "",
    "text": "New dependency packages are required to build targets in this Homework. I choose using conda for clear isolation with the system build toolchain. However, things were much more difficult than ever if the idiot VSCode (and unreliable official plugins) came to the context: VSCode IntelliSense could not easily recognize the header file, although LSP had already found out where to go; copilot continued to ask me to fix the include path to clear these import error; and the corresponding items in setting panel I have specified and pointed to the correct direction is always ignored or overwritten by their default location, which is point to the system default path. After trying a bunch of suggestions posted in the VSCode community forum, I finally gave up on making the autocomplete and type-checking stuff work.\nconda install conda-forge::clang\nconda install conda-forge::cmake\nconda install -c conda-forge llvmdev;  # header file, cmake recipe\nAfter experiencing these kinds of frustrating config stuff, I strongly recommend installing/updating the system’s default LLVM toolchain to make VSCode find the definition of the functions and variables, and not draw red underlines to interrupt your mind.\n\n\n\nIf the LLVM toolchain is installed in the system-level default location, there’s no need to do this extra step.\nexport LLVM_DIR=$CONDA_PREFIX/lib/cmake/llvm\nexport LD_LIBRARY_PATH=&lt;PRJ_DIR&gt;/HW5/llvm-passes/show-float-div/build:$LD_LIBRARY_PATH;",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 5: LLVM - Spinning Up"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW5.html#env",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW5.html#env",
    "title": "Homework 5: LLVM - Spinning Up",
    "section": "",
    "text": "New dependency packages are required to build targets in this Homework. I choose using conda for clear isolation with the system build toolchain. However, things were much more difficult than ever if the idiot VSCode (and unreliable official plugins) came to the context: VSCode IntelliSense could not easily recognize the header file, although LSP had already found out where to go; copilot continued to ask me to fix the include path to clear these import error; and the corresponding items in setting panel I have specified and pointed to the correct direction is always ignored or overwritten by their default location, which is point to the system default path. After trying a bunch of suggestions posted in the VSCode community forum, I finally gave up on making the autocomplete and type-checking stuff work.\nconda install conda-forge::clang\nconda install conda-forge::cmake\nconda install -c conda-forge llvmdev;  # header file, cmake recipe\nAfter experiencing these kinds of frustrating config stuff, I strongly recommend installing/updating the system’s default LLVM toolchain to make VSCode find the definition of the functions and variables, and not draw red underlines to interrupt your mind.\n\n\n\nIf the LLVM toolchain is installed in the system-level default location, there’s no need to do this extra step.\nexport LLVM_DIR=$CONDA_PREFIX/lib/cmake/llvm\nexport LD_LIBRARY_PATH=&lt;PRJ_DIR&gt;/HW5/llvm-passes/show-float-div/build:$LD_LIBRARY_PATH;",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 5: LLVM - Spinning Up"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW5.html#detail-design",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW5.html#detail-design",
    "title": "Homework 5: LLVM - Spinning Up",
    "section": "Detail Design",
    "text": "Detail Design\nThere’re 3 passes implemented in this Homework: - loop-unroll: Implemented a straightforward loop unrolling pass, only unrolling once if it finds a valid loop. (An ambitious task as the instruction of this homework suggested.) - show-bin-op: Display when there is a binary operation, log it out when doing compile, and call a stab function in rtlib during runtime to print the value stored in the destination. - show-float-div: As the instruction of this homework suggested, this pass is an example of an unambitious task, which displays only the fdiv operation, and acts the very same as the show-bin-op pass does when encountering fdiv operation.\n* Check Passes Source Code for details.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 5: LLVM - Spinning Up"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW5.html#integration-testing",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW5.html#integration-testing",
    "title": "Homework 5: LLVM - Spinning Up",
    "section": "Integration, Testing",
    "text": "Integration, Testing\nSome examples of the real-ish C/C++ program are provided in this homework. - example.c in each pass: Unit Tests to make sure passes are performing the correct results as wished. - matmul.cpp: A square matrix multiply testbench, which would be a great scenario to test the loop unrolling pass, but also can test the other two display passes. - projectile_motion.cpp: Simulates the motion of a projectile launched at a given angle and initial velocity within finite N steps.\nTo build the tests:\nFor examples.c and the pass library, go to each pass folder, and run make. All the results will be generated, including: - $(BUILD_DIR)/$(PASS_NAME)/$(PASS_NAME)Pass.so: Pass plugin as a dynamic lib. - $(BUILD_DIR)/lib$(RTLIB_LIB_NAME).so: Additional Runtime functions (i.e. Stab call funcs). - example: Executable of examples.c. To run the example, use make run instead of make.\nFor matmul.cpp and projectile_motion.cpp, go to the app folder, and run make. Executable will be generated in the same folder. Use make run to test the execution.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 5: LLVM - Spinning Up"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW5.html#results-and-analysis",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW5.html#results-and-analysis",
    "title": "Homework 5: LLVM - Spinning Up",
    "section": "Results and Analysis",
    "text": "Results and Analysis\n\nMatrix Multiply - Loop Unroll\nSlightly different in wall clock speed.\n❯ make run\nRunning target with custom LLVM pass...\ntime ./matmul_with_pass\nC[0][0] = 200\n0.02user 0.00system 0:00.02elapsed 96%CPU (0avgtext+0avgdata 2772maxresident)k\n0inputs+0outputs (0major+149minor)pagefaults 0swaps\nRunning target without custom LLVM pass...\ntime ./matmul_without_pass\nC[0][0] = 200\n0.01user 0.00system 0:00.02elapsed 95%CPU (0avgtext+0avgdata 2896maxresident)k\n0inputs+0outputs (0major+153minor)pagefaults 0swaps\n\n\nExample - Show FDiv\n# ~/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div    main !8 ?7      ✔  15s   course-eece7398-st-compiler   jiang@builder  04:29:01  \n\n❯ make clean\n❯ make run\nmkdir -p build\nBuilding RTLib...\nmake[1]: Entering directory '/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div'\nmake[1]: 'build' is up to date.\nmake[1]: Leaving directory '/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div'\nclang -fPIC -shared rtlib.c -o build/librtlib.so\nBuilding LLVM Pass...\nmake[1]: Entering directory '/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div'\nmake[1]: 'build' is up to date.\nmake[1]: Leaving directory '/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div'\ncd build && cmake .. && make\n-- The C compiler identification is GNU 14.2.0\n-- The CXX compiler identification is GNU 9.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /home/jiang/miniconda3/envs/course-eece7398-st-compiler/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\")\n-- Found zstd: /home/jiang/miniconda3/envs/course-eece7398-st-compiler/lib/libzstd.so\n-- Found LibXml2: /home/jiang/miniconda3/envs/course-eece7398-st-compiler/lib/libxml2.so (found version \"2.13.1\")\n-- Linker detection: GNU ld\n-- Registering ShowFDivPass as a pass plugin (static build: OFF)\n-- Configuring done (0.7s)\n-- Generating done (0.0s)\n-- Build files have been written to: /home/jiang/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div/build\nmake[1]: Entering directory '/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div/build'\nmake[2]: Entering directory '/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div/build'\nmake[3]: Entering directory '/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div/build'\nmake[3]: Leaving directory '/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div/build'\nmake[3]: Entering directory '/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div/build'\n[ 50%] Building CXX object ShowFDiv/CMakeFiles/ShowFDivPass.dir/ShowFDiv.cpp.o\n[100%] Linking CXX shared module ShowFDivPass.so\nmake[3]: Leaving directory '/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div/build'\n[100%] Built target ShowFDivPass\nmake[2]: Leaving directory '/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div/build'\nmake[1]: Leaving directory '/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div/build'\nCompiling target...\nclang -fpass-plugin=build/ShowFDiv/ShowFDivPass.so -c example.c\n\nShowFDiv.cpp:34: Found FDiv operator:   %38 = fdiv double %37, 2.000000e+00\n  %38 = fdiv double %37, 2.000000e+00\nOpcodeName: fdiv\n\nclang example.o -Lbuild -lrtlib -o example\nrm -f example.o\nRunning target...\nexport LD_LIBRARY_PATH=/home/jiang/scratch/work/course/EECE7398_ST_Compiler/HW5/llvm-passes/show-float-div/build:$LD_LIBRARY_PATH && \\\n./example\nEnter a number: 123\n125\n121\n246\n61\nEnter a float number: 123.456\n125.456001\n121.456001\n246.912003\n[logfdiv: 61.728001 = 123.456001 / 2.000000]\n61.728001",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 5: LLVM - Spinning Up"
    ]
  },
  {
    "objectID": "blogs/Qucheng/Qucheng-Paper-MLIR.html#motivation",
    "href": "blogs/Qucheng/Qucheng-Paper-MLIR.html#motivation",
    "title": "Paper Discussion: SODA-OPT",
    "section": "Motivation",
    "text": "Motivation\n\nMapping applications into Custom Hardware.\nExtract Performance.\nHeavy manual intervention.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Paper Discussion: SODA-OPT"
    ]
  },
  {
    "objectID": "blogs/Qucheng/Qucheng-Paper-MLIR.html#soda-opt",
    "href": "blogs/Qucheng/Qucheng-Paper-MLIR.html#soda-opt",
    "title": "Paper Discussion: SODA-OPT",
    "section": "SODA-OPT",
    "text": "SODA-OPT\n\nMLIR-based inputs\n\nSupports any high-level application that can be converted into linalg, affine dialects\n\nSystem-Level Design\nhigh-level optimizations for the HLS backends\nDSE of compiler options\n\n\n\nWorkflow (in paper)\n\nML model in python (Tensorflow)\nConvert model to MLIR (tf dialect) & Lower to TOSA\nLower to linalg MLIR dialect\nThis work\n\nSelect MLIR code for custom accelerator generation\nOptimize kernel code and generate IR for HLS (Bambu)\n\nSynthesize baseline and optimized code into Verilog\nPlace and route synthesized code, generate final GDSII",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Paper Discussion: SODA-OPT"
    ]
  },
  {
    "objectID": "blogs/Qucheng/Qucheng-Paper-MLIR.html#background",
    "href": "blogs/Qucheng/Qucheng-Paper-MLIR.html#background",
    "title": "Paper Discussion: SODA-OPT",
    "section": "Background",
    "text": "Background\n\nMulti-Level Intermediate Representation Compiler Infrastructure\n\nOpen-source\nProgressive lowering between existing and new operations\nReuse of abstractions and compiler transformations\nEnables co-existence of different abstractions\n\n\n\nMLIR - Example\n// Linalg abstraction\nfunc.func dot(%A: memref&lt;100xf32&gt;, %B: memref&lt;100xf32&gt;, %out: memref&lt;f32&gt;){\n    linalg.dot ins(A, B: memref&lt;100xf32&gt;, memref&lt;100xf32&gt;) outs(%out: memref&lt;f32&gt;)\n    return\n}\n// SCF abstraction\nfunc.func @dot(%A: memref&lt;100xf32&gt;, %B: memref&lt;100xf32&gt;, %out: memref&lt;f32&gt;){\n    %c0 = arith.constant 0 : index\n    %c100 = arith.constant 100 : index\n    %cl = arith.constant 1 : index\n    scf.for %arg3 = %c0 to %c100 step %c1 {\n        %0 = memref.load %A[%arg3] : memref&lt;100xf32&gt;\n        %1 = memref.load %B[%arg3] : memref&lt;100xf32&gt;\n        %2 = memref.load %out[] : memref&lt;f32&gt;\n        %3 = arith.mulf %0, %1 : f32\n        %4 = arith.addf %2, %3 : f32\n        memref.store %4 , %out[] : memref&lt;f32&gt;\n    }\n    return\n}\n// CF abstraction\nfunc.func @dot(%A: memref&lt;100xf32&gt;, %B: memref&lt;100xf32&gt;, %out: memref&lt;f32&gt;){\n    %c0 = arith.constant 0 : index\n    %c100 = arith.constant 100 : index\n    %c1 = arith.constant 1 : index\n    cf.br ^bb1(&c0 : index)\n^bb1(%0: index):  // 2 preds: ^bb0, ^bb2\n    %1 = arith.cmpi slt, %0, %c100 : index\n    cf.cond_br %1, ^bb2, ^bb3\n^bb2:  // pred: ^bb1\n        %2 = memref.load %A[%arg3] : memref&lt;100xf32&gt;\n        %3 = memref.load %B[%arg3] : memref&lt;100xf32&gt;\n        %4 = memref.load %out[] : memref&lt;f32&gt;\n        %5 = arith.mulf %2, %3 : f32\n        %6 = arith.addf %4, %5 : f32\n        memref.store %6 , %out[] : memref&lt;f32&gt;\n        %7 = arith.addi %0, %c1 : index\n        cf.br ^bb1(%7 : index)\n^bb3:  // pred: ^bb1\n    return\n}\n\n\n\nOptimization",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Paper Discussion: SODA-OPT"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-09-27-Qucheng-HW2.html",
    "href": "blogs/Qucheng/2024-09-27-Qucheng-HW2.html",
    "title": "Homework 2: Optimizing with DCE and LVN",
    "section": "",
    "text": "This assignment aims to implement two key local optimizations for the Bril intermediate language: Trivial Dead Code Elimination (DCE) and Local Value Numbering (LVN). These optimizations enhance the efficiency of Bril programs by removing unnecessary instructions and consolidating equivalent expressions. This report outlines the development process, testing methodology, and analysis of the implemented optimizations.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 2: Optimizing with DCE and LVN"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-09-27-Qucheng-HW2.html#introduction",
    "href": "blogs/Qucheng/2024-09-27-Qucheng-HW2.html#introduction",
    "title": "Homework 2: Optimizing with DCE and LVN",
    "section": "",
    "text": "This assignment aims to implement two key local optimizations for the Bril intermediate language: Trivial Dead Code Elimination (DCE) and Local Value Numbering (LVN). These optimizations enhance the efficiency of Bril programs by removing unnecessary instructions and consolidating equivalent expressions. This report outlines the development process, testing methodology, and analysis of the implemented optimizations.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 2: Optimizing with DCE and LVN"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-09-27-Qucheng-HW2.html#detail-design",
    "href": "blogs/Qucheng/2024-09-27-Qucheng-HW2.html#detail-design",
    "title": "Homework 2: Optimizing with DCE and LVN",
    "section": "Detail Design",
    "text": "Detail Design\nCode: For details, check the code repository.\n\nAbstraction Classes\nThe core of the Bril language representation in this project is implemented in the bril_model/_bril_struct.py file. This module provides a structured and object-oriented way to represent Bril programs (BrilScript), functions (BrilFunction), and instructions (BrilInstruction). Each Bril instruction is modeled as a Python object, enabling intuitive manipulation and transformation of the program during optimization.\nThe primary classes include:\nBril Program: Represents the entire Bril program, containing multiple functions.\nBril Function: Encapsulates a sequence of instructions and metadata for a function within the program.\nBril Instruction: Models individual instructions, differentiating between operations (e.g., addition, multiplication) and effectful statements (e.g., print).\nFirst, the BrilInstruction class is the base class for all instructions. Its constructor accepts a dictionary raw and deeply copies its contents into the instance variable _raw. It also initializes a mark dictionary _marks to track whether the instruction has been deleted, modified, or replaced. The constructor also checks whether the key in the raw dictionary is in the allowed key set AVAILABLE_KEYS, and throws NotImplementedError if there is an unrecognized key. The class also defines a series of properties and methods for accessing and modifying various parts of the instruction, such as dest, type, op, args, etc.\nNext are several subclasses inherited from BrilInstruction, which handle different types of instructions. The BrilInstruction_Const class is used to handle constant instructions. It checks whether the opcode is const in the constructor and verifies whether the key in the dictionary is in the allowed set. The BrilInstruction_Label class is used to handle label instructions. It checks whether the label exists in the constructor and verifies the key in the dictionary. The BrilInstruction_ValOp class is used to handle operation instructions with a return value. It checks whether the opcode is in the allowed set and verifies the key in the dictionary in the constructor. The BrilInstruction_EffOp class is used to handle operation instructions without a return value. It performs similar checks and verifications in the constructor.\nThe BrilFunction class is used to represent a Bril function. Its constructor accepts a dictionary raw and extracts the instruction list instrs from it, converting each instruction into a corresponding BrilInstruction subclass instance. It also checks whether the key in the dictionary is in the allowed set. The class also defines some properties and methods for accessing and modifying various parts of the function, such as name, type, args, etc.\nFinally, the BrilScript class is used to represent a Bril script. Its constructor accepts a dictionary raw and extracts the function list functions from it, converting each function into a BrilFunction instance. It also checks whether the key in the dictionary is in the allowed set. The class also defines some properties and methods for accessing and modifying various parts of the script, such as functions, etc.\nTogether, these classes form a framework for processing Bril language instructions and scripts, providing a wealth of properties and methods to access and modify various parts of instructions and scripts.\n\n\nTrivial Dead Code Elimination (DCE)\nThe Trivial-DCE implementation is encapsulated in tdce.py. This script scans through the instructions of a Bril function and removes any instructions whose results are not used before being overwritten. The optimization process involves:\nThe trivial_dce_once function performs a dead code elimination operation. It traverses all instructions in the function, collects all used variables, and marks unused instructions for deletion. Then, it updates the function’s instruction list, deletes the instructions marked for deletion, and returns a Boolean value indicating whether the instruction is deleted.\nThe trivial_dce function is a loop that repeatedly calls the trivial_dce_once function until no more instructions are deleted.\nThe _block_mark_reassign_before_use function deletes instructions that are reassigned before use within a single basic block. It maintains a dictionary to track the most recent unused assignment instructions and deletes old assignment instructions before reassignment is found.\nThe rm_reassign_before_use function traverses all basic blocks of the function, calls the _block_mark_reassign_before_use function to delete instructions that are reassigned before use, and updates the function’s instruction list.\nThe trivial_dce_plus function combines function-level dead code elimination with instruction removal before reassignment until there are no more instructions to remove.\nThis straightforward approach efficiently eliminates redundant calculations, contributing to a cleaner and faster program.\n\n\nLocal Value Numbering (LVN)\nLocal Value Numbering optimization is implemented in lvn.py, LVN assigns unique numbers to distinct computations to detect and eliminate redundant expressions within a single block. Key steps include:\nNumbering Expressions: LVN assigns a unique identifier to each distinct computation. This identifier, or value number, is used to track expressions within a block. If an identical computation is encountered later, the existing value number is reused, eliminating the need to recompute the expression. For example, if the expression a + b is already computed and assigned a value number, any subsequent occurrence of a + b will directly use the same value number instead of recalculating the result.\nSimplification and Substitution: The script replaces redundant operations with precomputed values or previously assigned variables, reducing the number of cpu-intense operations in the block. This is done by looking up the value number table for existing computations and substituting them wherever possible. By minimizing redundant calculations, LVN not only optimizes runtime performance but also simplifies the code.\nConstant Folding: Constant folding is a compile-time optimization technique that evaluates expressions with known constant values. For instance, an expression like const 2 + const 3 is directly replaced with const 5. In the script, a dictionary named FOLDABLE_OPS maps operation names (e.g., add, mul, sub) to their respective lambda functions that perform the computation. When the LVN process encounters an expression with constant arguments, it applies the corresponding function to compute the result immediately. Credit: Bril Official Example\nExpression Tracking and Reuse: An Expr class is defined to uniquely represent computations in terms of operations and their arguments. This allows the LVN process to easily check if a computation has been previously performed and reuse the result if possible. The implementation also handles commutative operations (e.g., add, mul, and, or, eq) by normalizing the order of arguments, ensuring that expressions like a + b and b + a are recognized as identical.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 2: Optimizing with DCE and LVN"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-09-27-Qucheng-HW2.html#integration-and-testing",
    "href": "blogs/Qucheng/2024-09-27-Qucheng-HW2.html#integration-and-testing",
    "title": "Homework 2: Optimizing with DCE and LVN",
    "section": "Integration and Testing",
    "text": "Integration and Testing\nThe brench tool is used for batch testing both optimizations across a variety of Bril programs. The brench_test_lvn.toml and brench_test_tdce.toml configuration files specify the programs and expected results for LVN and DCE tests, respectively. This ensures that both optimizations work correctly and consistently.\nAs defined in brench_test_lvn.toml, we specifies the path to the Bril benchmark programs to be tested, which are all the *.bril located in the ../bril/benchmarks/core/.\nThe configuration file defines multiple test run pipelines, each of which is defined in the [runs.&lt;name&gt;] section. Each pipeline contains a pipeline field that lists the sequence of commands to be executed.\nThe baseline pipeline is a baseline test without any optimization. It first converts the Bril program to JSON format and then runs the program using the brili interpreter.\nThe hw2p1_s0 pipeline is used to test whether the input and output structures are correct, but it does not perform any optimization. The purpose of this pipeline is to test if the Bril Construction work has no bug in it.\nThe hw2p1_s1 pipeline tests simple dead code elimination optimization. It calls the tdce.py script and passes the tdce parameter to enable function-level dead code elimination.\nThe hw2p1_s2 pipeline tests the instruction removal optimization used before revalue. It calls the tdce.py script and passes the raby argument to enable the rm_reassign_before_use pass optimization.\nThe hw2p1_s3 pipeline combines function-level dead code elimination and instruction removal optimization used before revalue. It calls the tdce.py script and passes the tdce+ argument to enable both optimizations.\nEach pipeline ultimately runs the optimized Bril program using the brili interpreter, passing arguments via the -p {args} option.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 2: Optimizing with DCE and LVN"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-09-27-Qucheng-HW2.html#results-and-analysis",
    "href": "blogs/Qucheng/2024-09-27-Qucheng-HW2.html#results-and-analysis",
    "title": "Homework 2: Optimizing with DCE and LVN",
    "section": "Results and Analysis",
    "text": "Results and Analysis\nThe optimizations were tested using brench tool, which verifies the correctness and performance of the implemented passes. The tool checks the brili Standard Output (STDOUT) for each test specification to ensure that the optimized programs produce the same results as their unoptimized counterparts. This rigorous testing confirms that the optimization passes do not alter the semantics of the Bril programs.\nThe testing demonstrated significant improvements in reducing the number of executed instructions for each Bril program. Programs with redundant calculations and unused instructions were simplified, leading to a reduced instruction count and faster execution times (based on Instructions Executed reported by brili -p).\nFor example, in the original benchmark check-primes, the program required 8,468 instructions to complete its execution. After applying the hw2p1_s3 optimization pipeline (DCE-only), the number of instructions executed was reduced to 8,419. Furthermore, using the hw2p2_s2 optimization pipeline (LVN+DCE), the total instructions executed dropped significantly to 4,238, representing a reduction to 50.05% of the original execution time. This demonstrates the substantial impact of combining LVN and DCE optimizations on program efficiency.\n\nRaw test log\n❯ brench brench_test_tdce.toml\nbenchmark,run,result\nfact,baseline,229\nfact,hw2p1_s0,229\nfact,hw2p1_s1,228\nfact,hw2p1_s2,229\nfact,hw2p1_s3,228\nquadratic,baseline,785\nquadratic,hw2p1_s0,785\nquadratic,hw2p1_s1,783\nquadratic,hw2p1_s2,785\nquadratic,hw2p1_s3,783\nrecfact,baseline,104\nrecfact,hw2p1_s0,104\nrecfact,hw2p1_s1,103\nrecfact,hw2p1_s2,104\nrecfact,hw2p1_s3,103\npascals-row,baseline,146\npascals-row,hw2p1_s0,146\npascals-row,hw2p1_s1,139\npascals-row,hw2p1_s2,146\npascals-row,hw2p1_s3,139\nloopfact,baseline,116\nloopfact,hw2p1_s0,116\nloopfact,hw2p1_s1,115\nloopfact,hw2p1_s2,116\nloopfact,hw2p1_s3,115\nsum-sq-diff,baseline,3038\nsum-sq-diff,hw2p1_s0,3038\nsum-sq-diff,hw2p1_s1,3036\nsum-sq-diff,hw2p1_s2,3038\nsum-sq-diff,hw2p1_s3,3036\nbirthday,baseline,484\nbirthday,hw2p1_s0,484\nbirthday,hw2p1_s1,483\nbirthday,hw2p1_s2,484\nbirthday,hw2p1_s3,483\nmod_inv,baseline,558\nmod_inv,hw2p1_s0,558\nmod_inv,hw2p1_s1,555\nmod_inv,hw2p1_s2,558\nmod_inv,hw2p1_s3,555\ncheck-primes,baseline,8468\ncheck-primes,hw2p1_s0,8468\ncheck-primes,hw2p1_s1,8419\ncheck-primes,hw2p1_s2,8468\ncheck-primes,hw2p1_s3,8419\nbitwise-ops,baseline,1690\nbitwise-ops,hw2p1_s0,1690\nbitwise-ops,hw2p1_s1,1689\nbitwise-ops,hw2p1_s2,1690\nbitwise-ops,hw2p1_s3,1689\nrelative-primes,baseline,1923\nrelative-primes,hw2p1_s0,1923\nrelative-primes,hw2p1_s1,1914\nrelative-primes,hw2p1_s2,1923\nrelative-primes,hw2p1_s3,1914\neuclid,baseline,563\neuclid,hw2p1_s0,563\neuclid,hw2p1_s1,562\neuclid,hw2p1_s2,563\neuclid,hw2p1_s3,562\nprimes-between,baseline,574100\nprimes-between,hw2p1_s0,574100\nprimes-between,hw2p1_s1,574100\nprimes-between,hw2p1_s2,574100\nprimes-between,hw2p1_s3,574100\nsum-check,baseline,5018\nsum-check,hw2p1_s0,5018\nsum-check,hw2p1_s1,5018\nsum-check,hw2p1_s2,5018\nsum-check,hw2p1_s3,5018\narmstrong,baseline,133\narmstrong,hw2p1_s0,133\narmstrong,hw2p1_s1,130\narmstrong,hw2p1_s2,133\narmstrong,hw2p1_s3,130\n\n❯ brench brench_test_lvn.toml\nbenchmark,run,result\nfact,baseline,229\nfact,hw2p2_s1,229\nfact,hw2p2_s2,167\nis-decreasing,baseline,127\nis-decreasing,hw2p2_s1,127\nis-decreasing,hw2p2_s2,123\nquadratic,baseline,785\nquadratic,hw2p2_s1,785\nquadratic,hw2p2_s2,500\nrecfact,baseline,104\nrecfact,hw2p2_s1,104\nrecfact,hw2p2_s2,64\nfizz-buzz,baseline,3652\nfizz-buzz,hw2p2_s1,3652\nfizz-buzz,hw2p2_s2,2103\npascals-row,baseline,146\npascals-row,hw2p2_s1,146\npascals-row,hw2p2_s2,68\nloopfact,baseline,116\nloopfact,hw2p2_s1,116\nloopfact,hw2p2_s2,78\nreverse,baseline,46\nreverse,hw2p2_s1,46\nreverse,hw2p2_s2,38\nsum-sq-diff,baseline,3038\nsum-sq-diff,hw2p2_s1,3038\nsum-sq-diff,hw2p2_s2,1717\nbirthday,baseline,484\nbirthday,hw2p2_s1,484\nbirthday,hw2p2_s2,278\nmod_inv,baseline,558\nmod_inv,hw2p2_s1,558\nmod_inv,hw2p2_s2,304\ncheck-primes,baseline,8468\ncheck-primes,hw2p2_s1,8468\ncheck-primes,hw2p2_s2,4238\nbitwise-ops,baseline,1690\nbitwise-ops,hw2p2_s1,1690\nbitwise-ops,hw2p2_s2,1689\nrelative-primes,baseline,1923\nrelative-primes,hw2p2_s1,1923\nrelative-primes,hw2p2_s2,1207\neuclid,baseline,563\neuclid,hw2p2_s1,563\neuclid,hw2p2_s2,272\nprimes-between,baseline,574100\nprimes-between,hw2p2_s1,574100\nprimes-between,hw2p2_s2,571439\nperfect,baseline,232\nperfect,hw2p2_s1,232\nperfect,hw2p2_s2,231\nbitshift,baseline,167\nbitshift,hw2p2_s1,167\nbitshift,hw2p2_s2,104\nsum-check,baseline,5018\nsum-check,hw2p2_s1,5018\nsum-check,hw2p2_s2,5018\narmstrong,baseline,133\narmstrong,hw2p2_s1,133\narmstrong,hw2p2_s2,130",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 2: Optimizing with DCE and LVN"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-09-27-Qucheng-HW2.html#conclusion",
    "href": "blogs/Qucheng/2024-09-27-Qucheng-HW2.html#conclusion",
    "title": "Homework 2: Optimizing with DCE and LVN",
    "section": "Conclusion",
    "text": "Conclusion\nThe implementation successfully optimizes Bril programs through Trivial Dead Code Elimination and Local Value Numbering. Both techniques were integrated and tested using brench, confirming their effectiveness. Future work could focus on extending these optimizations to handle more complex cases, such as multi-block programs and interprocedural optimizations.\nThis project demonstrates the power of local optimizations in streamlining intermediate representations and serves as a solid foundation for further exploration of compiler optimization techniques.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 2: Optimizing with DCE and LVN"
    ]
  },
  {
    "objectID": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html",
    "href": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html",
    "title": "EECE7309 Homework 5 – LLVM",
    "section": "",
    "text": "This assignment prompts us to explore LLVM, and to write a basic pass which either analyzes or modifies some input source code.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 5 -- LLVM"
    ]
  },
  {
    "objectID": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#introduction",
    "href": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#introduction",
    "title": "EECE7309 Homework 5 – LLVM",
    "section": "",
    "text": "This assignment prompts us to explore LLVM, and to write a basic pass which either analyzes or modifies some input source code.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 5 -- LLVM"
    ]
  },
  {
    "objectID": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#llvm",
    "href": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#llvm",
    "title": "EECE7309 Homework 5 – LLVM",
    "section": "LLVM",
    "text": "LLVM\nWe are interested in hacking the LLVM compiler in this assignment, specificially by writing a pass. LLVM gives interested developers a straightforward interface by which to do this, namely by building a shared library object which users can prompt clang/LLVM to dynamically link against. One of the reasons LLVM has become so popular is because of this ability, as well as being used by the largest company in the world.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 5 -- LLVM"
    ]
  },
  {
    "objectID": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#design",
    "href": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#design",
    "title": "EECE7309 Homework 5 – LLVM",
    "section": "Design",
    "text": "Design\nHere, I developed a llvm pass which identifies all floating-point arithmetic operations in the llvm-IR. It simply prints these out to stdout. This is not a complex pass by any means, but it does require understanding how to use the LLVM libraries to develop a pass.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 5 -- LLVM"
    ]
  },
  {
    "objectID": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#challenges-faced",
    "href": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#challenges-faced",
    "title": "EECE7309 Homework 5 – LLVM",
    "section": "Challenges Faced",
    "text": "Challenges Faced\nOne challenge I faced was when attempting to implement constant folding. I wrote up the appropriate llvm library to perform constant folding, but was not seeing any code modifications no matter what input I provided. Shortly after, I learned that Clang does constant folding automatically in the frontend, and this feature cannot be turned off.\nA second challenge I faced was when performing the extension of constant folding, which was to put together a constant propagation optimizer. The problem here, unfortunately, I believe lies between chair and keyboard, as I was having issues which I believe are related to my pointer usage. Admittedly, I think I could have finished this (and still may in the future), but ran out of steam to get it done by this assignment deadline.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 5 -- LLVM"
    ]
  },
  {
    "objectID": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#testing",
    "href": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#testing",
    "title": "EECE7309 Homework 5 – LLVM",
    "section": "Testing",
    "text": "Testing\nI tested my program on five different programatic inputs, all which contain a handful of floating point operations. These programs included one addition heavy program, one which had subtraction and division, one which had many multiplications, and two which had floating point arithmetic but also more complex control flow. These programs were largely created by ChatGPT and edited by myself for usefulness and correctness.\nAdditionally, I generated expected test results by writing a basic python text parsing tool, which isolated all floating point arithmetic instructions from the program’s emmitted LLVM. This digest was then directed into .out files, which could be used with turnt. This allowed for easy automated testing, comparing the results delivered by the compiler pass to those delivered by the python script (which is easier to confirm by hand).\nThe test results are shown in the Appendix rather than here, as the program itself is fairly trivial and results are as well. The code can be further verified using the link in the next section.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 5 -- LLVM"
    ]
  },
  {
    "objectID": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#code",
    "href": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#code",
    "title": "EECE7309 Homework 5 – LLVM",
    "section": "Code",
    "text": "Code\nThe code used to generate the above results, as well as the test cases and tooling, can be found here.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 5 -- LLVM"
    ]
  },
  {
    "objectID": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#appendix-test-results",
    "href": "blogs/michael/11-01-2024-HW5-MichaelMaurer.html#appendix-test-results",
    "title": "EECE7309 Homework 5 – LLVM",
    "section": "Appendix: Test Results",
    "text": "Appendix: Test Results\nBelow, we will review the test cases and expected results.\nFirst, let’s look at a simple program which does a floating point addition and a floating point multiplication.\naddition.c\n#include &lt;stdio.h&gt;\n\nint main() {\n    float a = 3.5, b = 2.5, c;\n    c = a + b;\n    c = c * 1.5;\n    printf(\"Result: %f\\n\", c);\n    return 0;\n}\nWith the resulting llvm.\naddition.s\n; ModuleID = 'tests/addition.c'\nsource_filename = \"tests/addition.c\"\ntarget datalayout = \"e-m:o-i64:64-i128:128-n32:64-S128\"\ntarget triple = \"arm64-apple-macosx15.0.0\"\n\n@.str = private unnamed_addr constant [12 x i8] c\"Result: %f\\0A\\00\", align 1\n\n; Function Attrs: noinline nounwind optnone ssp uwtable(sync)\ndefine i32 @main() #0 {\n  %1 = alloca i32, align 4\n  %2 = alloca float, align 4\n  %3 = alloca float, align 4\n  %4 = alloca float, align 4\n  store i32 0, ptr %1, align 4\n  store float 3.500000e+00, ptr %2, align 4\n  store float 2.500000e+00, ptr %3, align 4\n  %5 = load float, ptr %2, align 4\n  %6 = load float, ptr %3, align 4\n  %7 = fadd float %5, %6\n  store float %7, ptr %4, align 4\n  %8 = load float, ptr %4, align 4\n  %9 = fpext float %8 to double\n  %10 = fmul double %9, 1.500000e+00\n  %11 = fptrunc double %10 to float\n  store float %11, ptr %4, align 4\n  %12 = load float, ptr %4, align 4\n  %13 = fpext float %12 to double\n  %14 = call i32 (ptr, ...) @printf(ptr noundef @.str, double noundef %13)\n  ret i32 0\n}\n\ndeclare i32 @printf(ptr noundef, ...) #1\n\nattributes #0 = { noinline nounwind optnone ssp uwtable(sync) \"frame-pointer\"=\"non-leaf\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"apple-m1\" \"target-features\"=\"+aes,+complxnum,+crc,+dotprod,+fp-armv8,+fp16fml,+fullfp16,+jsconv,+lse,+neon,+pauth,+ras,+rcpc,+rdm,+sha2,+sha3,+v8.1a,+v8.2a,+v8.3a,+v8.4a,+v8.5a,+v8a,+zcm,+zcz\" }\nattributes #1 = { \"frame-pointer\"=\"non-leaf\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"apple-m1\" \"target-features\"=\"+aes,+complxnum,+crc,+dotprod,+fp-armv8,+fp16fml,+fullfp16,+jsconv,+lse,+neon,+pauth,+ras,+rcpc,+rdm,+sha2,+sha3,+v8.1a,+v8.2a,+v8.3a,+v8.4a,+v8.5a,+v8a,+zcm,+zcz\" }\n\n!llvm.module.flags = !{!0, !1, !2, !3}\n!llvm.ident = !{!4}\n\n!0 = !{i32 1, !\"wchar_size\", i32 4}\n!1 = !{i32 8, !\"PIC Level\", i32 2}\n!2 = !{i32 7, !\"uwtable\", i32 1}\n!3 = !{i32 7, !\"frame-pointer\", i32 1}\n!4 = !{!\"Homebrew clang version 18.1.8\"}\nThis gives the following result, as expected given that there are only two floating point operations.\naddition.out\n  %7 = fadd float %5, %6\n  %10 = fmul double %9, 1.500000e+00\nNow for a program with floating point division and subtraction:\ndivision.c\n#include &lt;stdio.h&gt;\n\nint main() {\n    float a = 10.0, b = 4.0, result;\n    result = a / b;\n    result = result - 1.5;\n    printf(\"Result: %f\\n\", result);\n    return 0;\n}\nwhich has the following llvm:\ndivision.s\n; ModuleID = 'tests/division.c'\nsource_filename = \"tests/division.c\"\ntarget datalayout = \"e-m:o-i64:64-i128:128-n32:64-S128\"\ntarget triple = \"arm64-apple-macosx15.0.0\"\n\n@.str = private unnamed_addr constant [12 x i8] c\"Result: %f\\0A\\00\", align 1\n\n; Function Attrs: noinline nounwind optnone ssp uwtable(sync)\ndefine i32 @main() #0 {\n  %1 = alloca i32, align 4\n  %2 = alloca float, align 4\n  %3 = alloca float, align 4\n  %4 = alloca float, align 4\n  store i32 0, ptr %1, align 4\n  store float 1.000000e+01, ptr %2, align 4\n  store float 4.000000e+00, ptr %3, align 4\n  %5 = load float, ptr %2, align 4\n  %6 = load float, ptr %3, align 4\n  %7 = fdiv float %5, %6\n  store float %7, ptr %4, align 4\n  %8 = load float, ptr %4, align 4\n  %9 = fpext float %8 to double\n  %10 = fsub double %9, 1.500000e+00\n  %11 = fptrunc double %10 to float\n  store float %11, ptr %4, align 4\n  %12 = load float, ptr %4, align 4\n  %13 = fpext float %12 to double\n  %14 = call i32 (ptr, ...) @printf(ptr noundef @.str, double noundef %13)\n  ret i32 0\n}\n\ndeclare i32 @printf(ptr noundef, ...) #1\n\nattributes #0 = { noinline nounwind optnone ssp uwtable(sync) \"frame-pointer\"=\"non-leaf\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"apple-m1\" \"target-features\"=\"+aes,+complxnum,+crc,+dotprod,+fp-armv8,+fp16fml,+fullfp16,+jsconv,+lse,+neon,+pauth,+ras,+rcpc,+rdm,+sha2,+sha3,+v8.1a,+v8.2a,+v8.3a,+v8.4a,+v8.5a,+v8a,+zcm,+zcz\" }\nattributes #1 = { \"frame-pointer\"=\"non-leaf\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"apple-m1\" \"target-features\"=\"+aes,+complxnum,+crc,+dotprod,+fp-armv8,+fp16fml,+fullfp16,+jsconv,+lse,+neon,+pauth,+ras,+rcpc,+rdm,+sha2,+sha3,+v8.1a,+v8.2a,+v8.3a,+v8.4a,+v8.5a,+v8a,+zcm,+zcz\" }\n\n!llvm.module.flags = !{!0, !1, !2, !3}\n!llvm.ident = !{!4}\n\n!0 = !{i32 1, !\"wchar_size\", i32 4}\n!1 = !{i32 8, !\"PIC Level\", i32 2}\n!2 = !{i32 7, !\"uwtable\", i32 1}\n!3 = !{i32 7, !\"frame-pointer\", i32 1}\n!4 = !{!\"Homebrew clang version 18.1.8\"}\nAgain, we see quickly that there are only two floating point arithmetic operations. We do see that there are a few floating point operations that are not arithmetic, such as fpext and fptrunc, but these were not omitted in our analysis. That said, it would be trivial to add them to our analysis, as it just expands the size of the switch statement in use.\ndivision.out\n  %7 = fdiv float %5, %6\n  %10 = fsub double %9, 1.500000e+00\nNext, we can try a program which has nested conditionals, to see if this analysis works even with multiple basic blocks.\nnested-conditionals.c\n#include &lt;stdio.h&gt;\n#include &lt;math.h&gt;\n\nint main() {\n    float a = 3.7, b = 2.5, result = 0.0;\n\n    if (a &gt; b) {\n        result = pow(a, 2) - sqrt(b);\n    } else {\n        result = log(a + b) * sin(a);\n    }\n\n    if (result &gt; 5.0) {\n        result /= 2.0;\n    } else if (result &lt; -5.0) {\n        result *= -1.0;\n    } else {\n        result += 1.5;\n    }\n\n    printf(\"Final result: %f\\n\", result);\n    return 0;\n}\nnested-conditionals.s\n; ModuleID = 'tests/nested-conditionals.c'\nsource_filename = \"tests/nested-conditionals.c\"\ntarget datalayout = \"e-m:o-i64:64-i128:128-n32:64-S128\"\ntarget triple = \"arm64-apple-macosx15.0.0\"\n\n@.str = private unnamed_addr constant [18 x i8] c\"Final result: %f\\0A\\00\", align 1\n\n; Function Attrs: noinline nounwind optnone ssp uwtable(sync)\ndefine i32 @main() #0 {\n  %1 = alloca i32, align 4\n  %2 = alloca float, align 4\n  %3 = alloca float, align 4\n  %4 = alloca float, align 4\n  store i32 0, ptr %1, align 4\n  store float 0x400D9999A0000000, ptr %2, align 4\n  store float 2.500000e+00, ptr %3, align 4\n  store float 0.000000e+00, ptr %4, align 4\n  %5 = load float, ptr %2, align 4\n  %6 = load float, ptr %3, align 4\n  %7 = fcmp ogt float %5, %6\n  br i1 %7, label %8, label %17\n\n8:                                                ; preds = %0\n  %9 = load float, ptr %2, align 4\n  %10 = fpext float %9 to double\n  %11 = call double @llvm.pow.f64(double %10, double 2.000000e+00)\n  %12 = load float, ptr %3, align 4\n  %13 = fpext float %12 to double\n  %14 = call double @llvm.sqrt.f64(double %13)\n  %15 = fsub double %11, %14    \n  %16 = fptrunc double %15 to float\n  store float %16, ptr %4, align 4\n  br label %28\n\n17:                                               ; preds = %0\n  %18 = load float, ptr %2, align 4\n  %19 = load float, ptr %3, align 4\n  %20 = fadd float %18, %19\n  %21 = fpext float %20 to double\n  %22 = call double @llvm.log.f64(double %21)\n  %23 = load float, ptr %2, align 4\n  %24 = fpext float %23 to double\n  %25 = call double @llvm.sin.f64(double %24)\n  %26 = fmul double %22, %25\n  %27 = fptrunc double %26 to float\n  store float %27, ptr %4, align 4\n  br label %28\n\n28:                                               ; preds = %17, %8\n  %29 = load float, ptr %4, align 4\n  %30 = fpext float %29 to double\n  %31 = fcmp ogt double %30, 5.000000e+00\n  br i1 %31, label %32, label %37\n\n32:                                               ; preds = %28\n  %33 = load float, ptr %4, align 4\n  %34 = fpext float %33 to double\n  %35 = fdiv double %34, 2.000000e+00\n  %36 = fptrunc double %35 to float\n  store float %36, ptr %4, align 4\n  br label %52\n\n37:                                               ; preds = %28\n  %38 = load float, ptr %4, align 4\n  %39 = fpext float %38 to double\n  %40 = fcmp olt double %39, -5.000000e+00\n  br i1 %40, label %41, label %46\n\n41:                                               ; preds = %37\n  %42 = load float, ptr %4, align 4\n  %43 = fpext float %42 to double\n  %44 = fmul double %43, -1.000000e+00\n  %45 = fptrunc double %44 to float\n  store float %45, ptr %4, align 4\n  br label %51\n\n46:                                               ; preds = %37\n  %47 = load float, ptr %4, align 4\n  %48 = fpext float %47 to double\n  %49 = fadd double %48, 1.500000e+00\n  %50 = fptrunc double %49 to float\n  store float %50, ptr %4, align 4\n  br label %51\n\n51:                                               ; preds = %46, %41\n  br label %52\n\n52:                                               ; preds = %51, %32\n  %53 = load float, ptr %4, align 4\n  %54 = fpext float %53 to double\n  %55 = call i32 (ptr, ...) @printf(ptr noundef @.str, double noundef %54)\n  ret i32 0\n}\n\n; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)\ndeclare double @llvm.pow.f64(double, double) #1\n\n; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)\ndeclare double @llvm.sqrt.f64(double) #1\n\n; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)\ndeclare double @llvm.log.f64(double) #1\n\n; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)\ndeclare double @llvm.sin.f64(double) #1\n\ndeclare i32 @printf(ptr noundef, ...) #2\n\nattributes #0 = { noinline nounwind optnone ssp uwtable(sync) \"frame-pointer\"=\"non-leaf\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"apple-m1\" \"target-features\"=\"+aes,+complxnum,+crc,+dotprod,+fp-armv8,+fp16fml,+fullfp16,+jsconv,+lse,+neon,+pauth,+ras,+rcpc,+rdm,+sha2,+sha3,+v8.1a,+v8.2a,+v8.3a,+v8.4a,+v8.5a,+v8a,+zcm,+zcz\" }\nattributes #1 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }\nattributes #2 = { \"frame-pointer\"=\"non-leaf\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"apple-m1\" \"target-features\"=\"+aes,+complxnum,+crc,+dotprod,+fp-armv8,+fp16fml,+fullfp16,+jsconv,+lse,+neon,+pauth,+ras,+rcpc,+rdm,+sha2,+sha3,+v8.1a,+v8.2a,+v8.3a,+v8.4a,+v8.5a,+v8a,+zcm,+zcz\" }\n\n!llvm.module.flags = !{!0, !1, !2, !3}\n!llvm.ident = !{!4}\n\n!0 = !{i32 1, !\"wchar_size\", i32 4}\n!1 = !{i32 8, !\"PIC Level\", i32 2}\n!2 = !{i32 7, !\"uwtable\", i32 1}\n!3 = !{i32 7, !\"frame-pointer\", i32 1}\n!4 = !{!\"Homebrew clang version 18.1.8\"}\nAnd as we can see, the program finds the floating point arithmetic operations in question.\nnested-conditionals.out\n  %15 = fsub double %11, %14\n  %20 = fadd float %18, %19\n  %26 = fmul double %22, %25\n  %35 = fdiv double %34, 2.000000e+00\n  %44 = fmul double %43, -1.000000e+00\n  %49 = fadd double %48, 1.500000e+00\nThe next program contains many multiplication operations, so we expect the result to contain ten fmul operations.\nmultiplication.c\n#include &lt;stdio.h&gt;\n\nint main()\n{\n    float a = 1.1, b = 2.2, c = 3.3, d = 4.4, e = 5.5;\n    float f = 6.6, g = 7.7, h = 8.8, i = 9.9, j = 10.1;\n\n    float result1 = a * b;\n    float result2 = c * d;\n    float result3 = e * f;\n    float result4 = g * h;\n    float result5 = i * j;\n\n    float result6 = a * c;\n    float result7 = e * g;\n    float result8 = i * a;\n    float result9 = b * f;\n    float result10 = d * h;\n\n    return 0;\n}\nmultiplication.s\n; ModuleID = 'tests/multiplication.c'\nsource_filename = \"tests/multiplication.c\"\ntarget datalayout = \"e-m:o-i64:64-i128:128-n32:64-S128\"\ntarget triple = \"arm64-apple-macosx15.0.0\"\n\n; Function Attrs: noinline nounwind optnone ssp uwtable(sync)\ndefine i32 @main() #0 {\n  %1 = alloca i32, align 4\n  %2 = alloca float, align 4\n  %3 = alloca float, align 4\n  %4 = alloca float, align 4\n  %5 = alloca float, align 4\n  %6 = alloca float, align 4\n  %7 = alloca float, align 4\n  %8 = alloca float, align 4\n  %9 = alloca float, align 4\n  %10 = alloca float, align 4\n  %11 = alloca float, align 4\n  %12 = alloca float, align 4\n  %13 = alloca float, align 4\n  %14 = alloca float, align 4\n  %15 = alloca float, align 4\n  %16 = alloca float, align 4\n  %17 = alloca float, align 4\n  %18 = alloca float, align 4\n  %19 = alloca float, align 4\n  %20 = alloca float, align 4\n  %21 = alloca float, align 4\n  store i32 0, ptr %1, align 4\n  store float 0x3FF19999A0000000, ptr %2, align 4\n  store float 0x40019999A0000000, ptr %3, align 4\n  store float 0x400A666660000000, ptr %4, align 4\n  store float 0x40119999A0000000, ptr %5, align 4\n  store float 5.500000e+00, ptr %6, align 4\n  store float 0x401A666660000000, ptr %7, align 4\n  store float 0x401ECCCCC0000000, ptr %8, align 4\n  store float 0x40219999A0000000, ptr %9, align 4\n  store float 0x4023CCCCC0000000, ptr %10, align 4\n  store float 0x4024333340000000, ptr %11, align 4\n  %22 = load float, ptr %2, align 4\n  %23 = load float, ptr %3, align 4\n  %24 = fmul float %22, %23\n  store float %24, ptr %12, align 4\n  %25 = load float, ptr %4, align 4\n  %26 = load float, ptr %5, align 4\n  %27 = fmul float %25, %26\n  store float %27, ptr %13, align 4\n  %28 = load float, ptr %6, align 4\n  %29 = load float, ptr %7, align 4\n  %30 = fmul float %28, %29\n  store float %30, ptr %14, align 4\n  %31 = load float, ptr %8, align 4\n  %32 = load float, ptr %9, align 4\n  %33 = fmul float %31, %32\n  store float %33, ptr %15, align 4\n  %34 = load float, ptr %10, align 4\n  %35 = load float, ptr %11, align 4\n  %36 = fmul float %34, %35\n  store float %36, ptr %16, align 4\n  %37 = load float, ptr %2, align 4\n  %38 = load float, ptr %4, align 4\n  %39 = fmul float %37, %38\n  store float %39, ptr %17, align 4\n  %40 = load float, ptr %6, align 4\n  %41 = load float, ptr %8, align 4\n  %42 = fmul float %40, %41\n  store float %42, ptr %18, align 4\n  %43 = load float, ptr %10, align 4\n  %44 = load float, ptr %2, align 4\n  %45 = fmul float %43, %44\n  store float %45, ptr %19, align 4\n  %46 = load float, ptr %3, align 4\n  %47 = load float, ptr %7, align 4\n  %48 = fmul float %46, %47\n  store float %48, ptr %20, align 4\n  %49 = load float, ptr %5, align 4\n  %50 = load float, ptr %9, align 4\n  %51 = fmul float %49, %50\n  store float %51, ptr %21, align 4\n  ret i32 0\n}\n\nattributes #0 = { noinline nounwind optnone ssp uwtable(sync) \"frame-pointer\"=\"non-leaf\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"apple-m1\" \"target-features\"=\"+aes,+complxnum,+crc,+dotprod,+fp-armv8,+fp16fml,+fullfp16,+jsconv,+lse,+neon,+pauth,+ras,+rcpc,+rdm,+sha2,+sha3,+v8.1a,+v8.2a,+v8.3a,+v8.4a,+v8.5a,+v8a,+zcm,+zcz\" }\n\n!llvm.module.flags = !{!0, !1, !2, !3}\n!llvm.ident = !{!4}\n\n!0 = !{i32 1, !\"wchar_size\", i32 4}\n!1 = !{i32 8, !\"PIC Level\", i32 2}\n!2 = !{i32 7, !\"uwtable\", i32 1}\n!3 = !{i32 7, !\"frame-pointer\", i32 1}\n!4 = !{!\"Homebrew clang version 18.1.8\"}\nAnd we observe what is expected: multiplication.out\n  %24 = fmul float %22, %23\n  %27 = fmul float %25, %26\n  %30 = fmul float %28, %29\n  %33 = fmul float %31, %32\n  %36 = fmul float %34, %35\n  %39 = fmul float %37, %38\n  %42 = fmul float %40, %41\n  %45 = fmul float %43, %44\n  %48 = fmul float %46, %47\n  %51 = fmul float %49, %50\nFinally, we look at a program with a looping control flow, whihc performs floating point divisions, additions, and multiplications. looping_addition.c\n#include &lt;stdio.h&gt;\n\nint main() {\n    float x = 1.0, sum = 0.0;\n    for (int i = 0; i &lt; 5; i++) {\n        sum += x / (i + 1);\n        if (sum &gt; 2.0) {\n            sum *= 0.5;\n        }\n        x += 1.0;\n    }\n    printf(\"Final sum: %f\\n\", sum);\n    return 0;\n}\nlooping_addition.s\n; ModuleID = 'tests/looping_addition.c'\nsource_filename = \"tests/looping_addition.c\"\ntarget datalayout = \"e-m:o-i64:64-i128:128-n32:64-S128\"\ntarget triple = \"arm64-apple-macosx15.0.0\"\n\n@.str = private unnamed_addr constant [15 x i8] c\"Final sum: %f\\0A\\00\", align 1\n\n; Function Attrs: noinline nounwind optnone ssp uwtable(sync)\ndefine i32 @main() #0 {\n  %1 = alloca i32, align 4\n  %2 = alloca float, align 4\n  %3 = alloca float, align 4\n  %4 = alloca i32, align 4\n  store i32 0, ptr %1, align 4\n  store float 1.000000e+00, ptr %2, align 4\n  store float 0.000000e+00, ptr %3, align 4\n  store i32 0, ptr %4, align 4\n  br label %5\n\n5:                                                ; preds = %29, %0\n  %6 = load i32, ptr %4, align 4\n  %7 = icmp slt i32 %6, 5\n  br i1 %7, label %8, label %32\n\n8:                                                ; preds = %5\n  %9 = load float, ptr %2, align 4\n  %10 = load i32, ptr %4, align 4\n  %11 = add nsw i32 %10, 1\n  %12 = sitofp i32 %11 to float\n  %13 = fdiv float %9, %12\n  %14 = load float, ptr %3, align 4\n  %15 = fadd float %14, %13\n  store float %15, ptr %3, align 4\n  %16 = load float, ptr %3, align 4\n  %17 = fpext float %16 to double\n  %18 = fcmp ogt double %17, 2.000000e+00\n  br i1 %18, label %19, label %24\n\n19:                                               ; preds = %8\n  %20 = load float, ptr %3, align 4\n  %21 = fpext float %20 to double\n  %22 = fmul double %21, 5.000000e-01\n  %23 = fptrunc double %22 to float\n  store float %23, ptr %3, align 4\n  br label %24\n\n24:                                               ; preds = %19, %8\n  %25 = load float, ptr %2, align 4\n  %26 = fpext float %25 to double\n  %27 = fadd double %26, 1.000000e+00\n  %28 = fptrunc double %27 to float\n  store float %28, ptr %2, align 4\n  br label %29\n\n29:                                               ; preds = %24\n  %30 = load i32, ptr %4, align 4\n  %31 = add nsw i32 %30, 1\n  store i32 %31, ptr %4, align 4\n  br label %5, !llvm.loop !5\n\n32:                                               ; preds = %5\n  %33 = load float, ptr %3, align 4\n  %34 = fpext float %33 to double\n  %35 = call i32 (ptr, ...) @printf(ptr noundef @.str, double noundef %34)\n  ret i32 0\n}\n\ndeclare i32 @printf(ptr noundef, ...) #1\n\nattributes #0 = { noinline nounwind optnone ssp uwtable(sync) \"frame-pointer\"=\"non-leaf\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"apple-m1\" \"target-features\"=\"+aes,+complxnum,+crc,+dotprod,+fp-armv8,+fp16fml,+fullfp16,+jsconv,+lse,+neon,+pauth,+ras,+rcpc,+rdm,+sha2,+sha3,+v8.1a,+v8.2a,+v8.3a,+v8.4a,+v8.5a,+v8a,+zcm,+zcz\" }\nattributes #1 = { \"frame-pointer\"=\"non-leaf\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"apple-m1\" \"target-features\"=\"+aes,+complxnum,+crc,+dotprod,+fp-armv8,+fp16fml,+fullfp16,+jsconv,+lse,+neon,+pauth,+ras,+rcpc,+rdm,+sha2,+sha3,+v8.1a,+v8.2a,+v8.3a,+v8.4a,+v8.5a,+v8a,+zcm,+zcz\" }\n\n!llvm.module.flags = !{!0, !1, !2, !3}\n!llvm.ident = !{!4}\n\n!0 = !{i32 1, !\"wchar_size\", i32 4}\n!1 = !{i32 8, !\"PIC Level\", i32 2}\n!2 = !{i32 7, !\"uwtable\", i32 1}\n!3 = !{i32 7, !\"frame-pointer\", i32 1}\n!4 = !{!\"Homebrew clang version 18.1.8\"}\n!5 = distinct !{!5, !6}\n!6 = !{!\"llvm.loop.mustprogress\"}\nAnd we see all four floating point arithmetic operations are found. looping_addition.out\n  %13 = fdiv float %9, %12\n  %15 = fadd float %14, %13\n  %22 = fmul double %21, 5.000000e-01\n  %27 = fadd double %26, 1.000000e+00",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 5 -- LLVM"
    ]
  },
  {
    "objectID": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#code-availability",
    "href": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#code-availability",
    "title": "Final Project Report – Register Allocator and minimal RISC-V Backend",
    "section": "Code Availability",
    "text": "Code Availability\nThe code is available at https://github.com/maurermi/compilers-final-project\nTo run the register allocator on a given Bril file, run the following:\npython relabel.py {filename}.bril &gt; {filename}-relabeled.bril\nbril2json &lt; {filename}-relabeled.bril &gt; {filename}.json\npython cfg.py {filename}.json\nTo generate results, run\n./test-all.sh",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Final Project Report -- Register Allocator and minimal RISC-V Backend"
    ]
  },
  {
    "objectID": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#introduction",
    "href": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#introduction",
    "title": "Final Project Report – Register Allocator and minimal RISC-V Backend",
    "section": "Introduction",
    "text": "Introduction\nThis project provides code which converts Bril programs to SSA, computes live variables by block, and then allocates registers to all instructions in the program. Two register allocation algorithms are implemented, a linear scan allocator, and a graph-coloring based allocator. Additionally, I developed a working backend for the RISC-V architecture, and can generate code which runs on a RISC-V simulator.\nThis allocator is designed specifically for a RISC-V based system, namely RV32E (which has 32 32-bit register).\nThe allocator here generates code which can be run directly in RISC-V, and was tested on this simulator: https://ascslab.org/research/briscv/simulator/simulator.html.\nOne goal of this project was to compare head to head the time incurred by the two algorithms. Namely, I was interested in benchmarking the time elapsed by the graph coloring allocator as opposed to the linear scan allocator.\nAnother goal was to implement a working backend for the RISC-V architecture, and to generate code which runs on a RISC-V simulator.\n\nPreface\nA large reason I chose this course was because I have for a long time been interested in learning more about compilers, but namely because compilers were often a black box to me. In terms of the pipeline which goes from code to electrons, compilers were really the only part I had not yet explored. I chose a register allocator as this project because I feel that it completes the chain, and now I feel as though I can connect implementation ideas all the way through the chain, from high level language design to the hardware.\nWhen proposing this project, I actually had a seperate project in mind, which I worked on for a while, but realized that there was a very slim chance I complete it before the end of this semester. This was a transpiler of high level language into Circom, a language which defines a set of constraints for a proof. This project is still ongoing, however in a seperate capacity these days. When I realized that this wouldn’t be feasible for this project, I switched to this register allocator project as an alternative.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Final Project Report -- Register Allocator and minimal RISC-V Backend"
    ]
  },
  {
    "objectID": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#goals",
    "href": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#goals",
    "title": "Final Project Report – Register Allocator and minimal RISC-V Backend",
    "section": "Goals",
    "text": "Goals\nThe main goal of this project is to gain hands-on familiarity with the algorithms used in compiler implementation, as well as to explore some of the topics discussed in class, not previously covered in assignments. As such, I felt that using Bril would be more educational than LLVM, as it required me to implement essentially everything from scratch. In a follow on work, I plan to do the same in LLVM, because it would be nice to have it around.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Final Project Report -- Register Allocator and minimal RISC-V Backend"
    ]
  },
  {
    "objectID": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#challenges",
    "href": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#challenges",
    "title": "Final Project Report – Register Allocator and minimal RISC-V Backend",
    "section": "Challenges",
    "text": "Challenges\nOne unexpected challenge during this work was how tricky it was to convert to SSA. Adrian Sampson, in his lecture on SSA, does actually mention that converting to SSA is trickier than one may expect, and I can confirm that this is the case. Conceptually, it is not terribly challenging, but while implementing it, I developed a massive chunk of code, that was difficult to effectively partition, and this made it especially difficult to debug. Namely, the part which was challenging was not adding in phi nodes, but rather renaming variables properly. To me, it does not seem as though this was a conceptual issue, more of a software engineering one. The amount of code I wrote to support SSA is a significant chunk of this code, and got to the point at which I was spending more time debugging the SSA conversion than the register allocation, which is not what I was aiming for. So, that being said, the SSA conversion code in this project is not fully working, because of the rename step. However, I did write a significant amount of infrastructure to perform register allocation once SSA form is achieved.\nAnother challenge in this project was determining whether I should do this work using LLVM or Bril. In retrospect, it would have been better to use LLVM as the main reason I wanted to use Bril was to force myself to better understand the algorithms at play here. However, given that the main advantage LLVM would have given is SSA formatted code, it may have been wiser (and admittedly, I’m more comfortable with C++ than Python). However given what I learned during this project, I have no concerns about taking this work and applying it to an LLVM based register allocator.\nOne lesson is that I should get better at writing unit tests.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Final Project Report -- Register Allocator and minimal RISC-V Backend"
    ]
  },
  {
    "objectID": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#implementation",
    "href": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#implementation",
    "title": "Final Project Report – Register Allocator and minimal RISC-V Backend",
    "section": "Implementation",
    "text": "Implementation\n\nRegister Allocation Algorithms\nThere are two register allocation algorithms implemented here, a linear scan allocator, and a graph coloring allocator. Linear scan is a simpler algorithm, which is generally faster, but does not always perform as well as graph coloring. Previously, I have read and implemented a similar algorithm for scheduling tasks in a cloud computing environment, based on a paper from Northeastern’s Xue Lin.\nThe basis of the linear scan algorithm is essentially two phases: 1. Computation of the interference graph 2. Register Assignment\nIn the first phase, one must compute the liveness of variables by block, and determine which variables are live at the same time as one another. This analyisis is very similar to some of the data flow analyses we have done in class. Essentially, a worklist of basic blocks is created, in reverse post-order. Then, for each block we encounter, we compute the live out variables, which are the union of live in variables in all successors. Then, we compute the live in variables, which are the union of used variables, and the set difference between live out variables and defined variables in the block. In my implementation, I included some timing information for this analysis.\n\n    def compute_liveness(self):\n        import time\n        # Initialize the blocks\n        for block in self.blocks.values():\n            block.compute_used_and_defined_vars()\n            block.live_out = set()\n            block.live_in = set()\n        # Create worklist\n        worklist = self.terminating_blocks\n        start_time = time.time()\n        while True:\n            # Continue until no changes are made\n            changed = False\n            # This prevents infinite loops\n            blocks_seen = set()\n            while len(worklist) &gt; 0:\n                # Work through blocks in reverse-post order\n                block = self.blocks[worklist.pop()]\n                old_live_in = copy.deepcopy(block.live_in)\n                old_live_out = copy.deepcopy(block.live_out)\n                blocks_seen.add(block.name())\n                # Compute live out variables\n                for succ in block.succs:\n                    block.live_out = block.live_out.union(self.blocks[succ].live_in)\n                # Compute live in variables\n                block.live_in = block.used_vars.union(block.live_out - block.defined_vars)\n                # Check if changes were made\n                if old_live_in != block.live_in or old_live_out != block.live_out:\n                    changed = True\n                # Add predecessors to worklist\n                for pred in block.preds:\n                    # This prevents infinite loops\n                    if pred not in blocks_seen:\n                        worklist.add(pred)\n            if not changed:\n                break\n        end_time = time.time() \n        print(\"Computing Liveness Time taken: \", end_time - start_time)\n        return\n\nThe next step is register allocation. In the linear scan algorithm, we iterate through instructions and assign registers to the destinations. Further, at each instruction, we scan the allocated registers to see if any can be freed (whether the given instruction is past the last use). Allocation is done by choosing the first free register if there is one, and assigning a value to it. If there are none available, there must be a spill. Spilling is typically done based on certain heuristics. Here, I experimented with two heuristics, one being random choice, and the other being choosing the register which has the latest “last use”. Apparently, this is a popular heuristic.\nA shortcoming of the naive linear scan register allocation strategy is that it does do much in the way of wisely allocating registers. Namely, it’s possible that fewer registers are required than what the allocation scheme calls for. So if register pressure is a major concern, linear scan register allocation is not a great choice. However, it is reported to be quite a bit faster than many of the existing ahead of time register allocation strategies (namely graph coloring).\nOne worthwhile note is that the linear scan register allocator requires the ability to iterate through blocks in order. My CFG class however uses a dictionary for storing blocks, which hashes entries, so either when initializing the CFG I would need to tag instructions with their order in the program, or I could preprocess the program to add labels before certain instructions. I ended up choosing the latter, so there is a tool with this work called relabel.py aimed at renaming labels by the number instruction they are in the program.\nThe second register allocation strategy implemented was a graph coloring algorithm (Chiatin’s algorithm). This is a fairly famous technique, which we discussed in class and is also discussed heavily in the literature. The idea here is to treat the program as a graph, with each variable as a vertex. Then, each edge connects two verticies which are alive at the same time.\nThe first step in this scheme is therefore, to create the interference graph. This work was inherited directly from the linear scan register allocation.\nNext, a copy of the graph is made, and is “simplified”. Simplificaiton refers to an iteritave process by which nodes and edges are removed from the graph. Verticies are removed by determining, first, whether there are any verticies that exist in the graph which have fewer than k edges, where k is the number of registers available to the program. If so, the first (in my implementation) vertex found meeting this criterion is removed from the graph, and it along with its outgoing edges are pushed onto a stack, with a flag indicating that this vertex is to be “colored”, or to have a register allocated to it. If no vertex exists meeting this criterion, then a vertex is chosen at random. This vertex and its edges are removed, and pushed onto the stack with a flag indicating that it should not be colored. This is continued until the fixed point when there are no remaining verticies in the graph copy, is reached.\nNext, verticies are added back into the graph, by popping them from the aforementioned stack. If the vertex popped is marked to be colored, it is assigned a color (register) which is different from any verticies with which it shares an edge. If it is marked to not be colored, first, it is checked whether there are any free registers (not in use by neighbors). If so, this register is assigned, and the variable does not actually need to be spilled. If not, then the variable is marked to be spilled to memory. If it is spilled, then it is not added back into the graph. If spills occur, this means that loads and stores must be inserted into the resultant code. This is handled in the actual register assignment code.\nOne implementation note, is that these algorithms are implemented to operate on a set number of registers, but are not yet provided the actual register IDs. This is because I wanted these algorithms to be machine independent. In the next step, these register tags are converted to true physical registers, following the requirements of a given architecture.\nAnother quirk is that the register allocation is done on a per-function basis. So there is no cross-function register allocation optimization done in this process. This would make the analysis much tricker (namely because of the control flow changes), but is an interesting avenue of future work.\n\n\nRegister Assignment on Machine\nFor this project, I focused on a RISC-V architecture, and this code, with some exceptions, can develop code which runs on a RISC-V simulator. This was not really part of my initial plan, but I found it quite exciting to be able to generate code that can actually run, and do what it claims to on a target machine.\nThere are some exceptions, namely operations which are not supported natively by RISC-V (or by the limited instruction set provided by RV32I). For some operations (such as Bril’s br operation), I added support through the use of a mixture of non-equivalent operations. These were some common instructions used that have natural analog constructions in RISC-V. One example would be the eq instruction. This has no analog in RV32I, but an equivalent can be achieved with a subtraction and logical inverse. This is also commonly seen in tandem with br, so a branch based on the result of an eq can be replaced by a sub operation followed by beq _ zero in RISC-V. Some operations have no real equivalent, such as print, so those are translated directly for the time being.\nFor the code generated, I reccomend giving it a try on BU’s RISC-V simulator. For workloads containing only instructions supported by RV32I, the code should run and produce results!\nI used the following register constraints in my setup: Argument Registers = [‘a0’, ‘a1’, ‘a2’, ‘a3’, ‘a4’, ‘a5’, ‘a6’, ‘a7’] Return Register = [‘ra’] General Purpose Registers = [‘t0’, ‘t1’, ‘t2’, ‘s1’, ‘s2’, ‘s3’, ‘s4’, ‘s5’, ‘s6’, ‘s7’, ‘s8’, ‘s9’, ‘s10’, ‘s11’, ‘t3’, ‘t4’, ‘t5’, ‘t6’] Special Registers = [‘fp’, ‘sp’, ‘gp’, ‘tp’, ‘zero’]\nThe backend really only conisders the argument registers, return registers, and general purpose registers. The stack pointer is used for inserting load/store instructions, but otherwise is left alone.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Final Project Report -- Register Allocator and minimal RISC-V Backend"
    ]
  },
  {
    "objectID": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#evaluation",
    "href": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#evaluation",
    "title": "Final Project Report – Register Allocator and minimal RISC-V Backend",
    "section": "Evaluation",
    "text": "Evaluation\nI used SIGPLAN to help develop reasonable evaluation workloads for this project. For sufficient breadth of testing and number of trials, I evaluated this work on a large portion of Bril examples provided in the Bril repository (under benchmarks/core) totaling 35 different workloads. Those used are provided alongside this code.\nThere are two forms of evaluation used hereafter. One is determining whether the code used assigns registers legally (i.e. does not double-assign a register, spills when appropriate). The other evaluation is a head to head comparison of the register assignment techniques used. Here, we compare the number of registers used by the assignment strategy (at maximum), and the number of spills for a given workload and number of available registers. Additionally, we measure the time taken to complete the register assignment operation. For graph coloring, I expect it to be on average slower than linear-scan, but I expect graph-coloring on average to have fewer spills and use fewer registers.\n** Note: I chose to manually pick the number of available registers rather than giving the system default to stress test the system. Further, in multithreaded applications this behavior could be useful.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Final Project Report -- Register Allocator and minimal RISC-V Backend"
    ]
  },
  {
    "objectID": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#results",
    "href": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#results",
    "title": "Final Project Report – Register Allocator and minimal RISC-V Backend",
    "section": "Results",
    "text": "Results\n\nPerformance Results\nHere, we validate the perofrmance of the register allocator on systems with 5, 7, and 10 available genereal purpose registers.\nFirst, we confirm that all test cases produce valid register assignments. Using the provided test script (test-all.sh), we can confirm that all tests pass, and generate a set of test results to operate over.\nNext, we investigate performance differences between the linear scan register allocation and graph-coloring based register allocation.\nFirst, the top line numbers are as follows: - Linear Scan register allocation is on average, 9.75x faster than graph coloring - Graph coloring on average uses 84.4% of the registers used by linear scan - Graph coloring on average has 0.141 fewer spills than linear scan allocation\nThese results tend to align with what I expected. Investigating the results further, it appears that generally, the linear scan-based register allocator and graph-coloring based allocator have a similar number of spills (often the same). The number of registers allocated by the linear-scan register allocator is often larger than the number of registers allocated by the graph-based allocator. Now most notably, the linear scan register allocator is nearly 10x as fast as the graph colloring allocator on average. This is clear evidence for why it is typically used in JIT systems, whereas graph coloring is really only used in ahead of time compilation.\nAnother note is that randomly spilling registers (when a spill is required) appears to perform slightly worse than spilling the register that has the longest time to be free. The difference is not large however (on average, 0.02 more spills with random spillage).\n\n\nRISC-V Code Generation\nThe other chunk of this program was implementing a backend to convert from bril to RISC-V. Admittedly, this addition was somewhat of an afterthought, and I was not able to automate the test infrastructure to determine correctness of these RISC-V programs. Additionally, the backend does not support all Bril instructions, so much of the testing here was done by hand on curated examples.\nIn the future, I am hoping to further flesh this out, because I think this was quite fun and interesting. In hindsight, I would have preferred to focus more of my project on this aspect.\nTo show that the code works, here is an example RISC-V program generated by my tools, and the corresponding Bril which gave rise to it.\nSample RISC-V Program:\nmain:\n.l0:\n    li t1, 10\n.l1:\n    li t0, 1\n.l2:\n    li t2, 1\n.l3:\n    add t0, t1, t0\n.l4:\n    li s1, 50\n.l5:\n    beq s1, zero, .l6\n    j l7\n.l6:\n    j .l11\n.l7:\n    add t0, t0, t2\n.l11:\n    add t0, t2, t0\n.l8:\n    beq s1, zero, .l9\n    j l10\n.l9:\n    j .l11\n.l10:\n    mul t0, t0, t2\n.l12:\n    add t2, t2, t2\n.l13:\n    add t0, t0, t2\n.l14:\n    add t0, t0, t1\n.l15:\n    mv a0, t0\n    jr ra\nCorresponding Bril:\n@main() {\n.l0:\n  n: int = const 10;\n.l1:\n  one: int = const 1;\n.l2:\n  a: int = const 1;\n.l3:\n  b: int = add n one;\n.l4:\n  magic: int = const 50;\n.l5:\n  branch: bool = gt b a;\n  br branch .l6 .l7;\n.l6:\n  b = id a;\n  jmp .l11;\n.l7:\n  b = add b a;\n.l8:\n  branch: bool = gt b magic;\n  br branch .l9 .l10;\n.l9:\n  b = sub b a;\n  jmp .l11;\n.l10:\n  b = mul b a;\n# There should be a 3-way phi node here.;\n.l11:\n  c: int = add a b;\n.l12:\n  d: int = add a a;\n.l13:\n  e: int = add c d;\n.l14:\n  f: int = add e n;\n.l15:\n  ret f;\n}",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Final Project Report -- Register Allocator and minimal RISC-V Backend"
    ]
  },
  {
    "objectID": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#conclusion",
    "href": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#conclusion",
    "title": "Final Project Report – Register Allocator and minimal RISC-V Backend",
    "section": "Conclusion",
    "text": "Conclusion\nThis project implements two register allocation techniques, compares the results, and further creates a limited scope backend which generates actual, runnable RISC-V code. The results of this were as expected and line up with the literature, in that linear scan register allocation is much faster than graph-coloring based allocation, but generally speaking it performs worse. I found it interesting to verify this, because although it’s clear algorithmically, it’s nice to see that on actual workloads the behavior holds. I learned quite a bit throughout this project, and am glad I focused on a register allocator. Now I feel as though I have a good sense for every step going from high level code to hardware. Additionally, this work prompted some exploration of graph theory on my end, which I would like to pursue more.\nFor this project, I would consider it largely a success, but there are notable shortcomings. One of these is that I was not able to compare against an SSA-based register allocator. I think this would have helped quite a bit. However, I was able to develop two working register allocators, and was able to evaluate the relative performance of each. Further, I was able to impement much of a RISC-V backend, which is a nice tool to use going forward. In summary, I would say this project was a success, but there is more I could have done to make it better.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Final Project Report -- Register Allocator and minimal RISC-V Backend"
    ]
  },
  {
    "objectID": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#future-work",
    "href": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#future-work",
    "title": "Final Project Report – Register Allocator and minimal RISC-V Backend",
    "section": "Future Work",
    "text": "Future Work\nI would like to perform this exercise on LLVM instead of Bril in the future. I think leveraging the existing tooling would have allowed me to iterate much faster on this work, and possibly come up with more interesting results. However, I think that using Bril as the basis here really allowed me to get a good understanding of the algorithms and techniques at play.\nIt would be interesting to expand the backend to support additional architectures, namely ones with more complex register behavior. One example would be x86. Additionally, adding more instruction support would be very helpful, as then arbitrary bril programs could be converted to RISC-V using this tool.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Final Project Report -- Register Allocator and minimal RISC-V Backend"
    ]
  },
  {
    "objectID": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#references",
    "href": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#references",
    "title": "Final Project Report – Register Allocator and minimal RISC-V Backend",
    "section": "References",
    "text": "References\n\nhttps://ieeexplore.ieee.org/abstract/document/6985666\nhttps://www.youtube.com/watch?v=eeXk_ec1n6g&t=1039s\nhttps://www.youtube.com/watch?v=eWp_-XCwN1A\nhttps://www.cs.cornell.edu/courses/cs6120/2023fa/lesson/6/\nhttps://www.cs.cornell.edu/courses/cs6120/2023fa/lesson/4/\nhttps://capra.cs.cornell.edu/bril/lang/ssa.html\nhttps://normrubin.github.io/lectures/04_data_flow.html\nhttps://normrubin.github.io/lectures/register_allocation.html\nhttps://anoopsarkar.github.io/compilers-class/assets/lectures/opt3-regalloc-linearscan.pdf\nhttps://homepages.dcc.ufmg.br/~fernando/classes/dcc888/ementa/Questions/RegisterAllocation0.pdf\nhttps://compilers.cs.uni-saarland.de/papers/ssara.pdf\nhttps://en.wikipedia.org/wiki/Register_allocation\nhttps://web.stanford.edu/class/archive/cs/cs143/cs143.1128/lectures/17/Slides17.pdf\nhttps://github.com/riscv/riscv-isa-manual/blob/main/src/rv32.adoc",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Final Project Report -- Register Allocator and minimal RISC-V Backend"
    ]
  },
  {
    "objectID": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#appendix",
    "href": "blogs/michael/12-13-2024-Project-MichaelMaurer.html#appendix",
    "title": "Final Project Report – Register Allocator and minimal RISC-V Backend",
    "section": "Appendix",
    "text": "Appendix\n\nSample Graph Coloring Register Allocation Visualization\n\n\n\n\n\ngraph TD    \n    magic:0 --&gt; n:1\n    d:0 --&gt; n:1\n    e:0 --&gt; n:1\n    branch:2 --&gt; n:1\n    branch:2 --&gt; magic:0\n    b:3 --&gt; n:1\n    magic:0 --&gt; b:3\n    branch:2 --&gt; b:3\n    a:4 --&gt; n:1\n    b:3 --&gt; a:4\n    magic:0 --&gt; a:4\n    branch:2 --&gt; a:4\n    c:2 --&gt; n:1\n    c:2 --&gt; a:4\n    d:0 --&gt; c:2\n    one:0 --&gt; n:1\n    a:4 --&gt; one:0\n\n\n\n\n\n\n\n\nSample Linear Scan Register Allocation Visualization\n# tests/sample.bril\n0: n        1:          2:          3:          4:          5:          6:          \n0: n        1: one      2:          3:          4:          5:          6:          \n0: n        1: one      2: a        3:          4:          5:          6:          \n0: n        1:          2: a        3: b        4:          5:          6:          \n0: n        1: magic    2: a        3: b        4:          5:          6:          \n0: n        1: magic    2: a        3: b        4: branch   5:          6:          \n0: n        1: magic    2: a        3: b        4: branch   5:          6:          \n0: n        1: magic    2: a        3: b        4: branch   5:          6:          \n0: n        1: magic    2: a        3: b        4: branch   5:          6:          \n0: n        1: magic    2: a        3: b        4: branch   5:          6:          \n0: n        1:          2: a        3: b        4: branch   5: c        6:          \n0: n        1: d        2: a        3: b        4:          5: c        6:          \n0: n        1: d        2: a        3: b        4: e        5: c        6:          \n0: n        1: d        2: a        3: b        4: e        5: c        6: f        \n0: n        1: d        2: a        3: b        4: e        5: c        6: f        \n0: n        1: d        2: a        3:          4: e        5: c        6: f\n\n\nSample Test Result Data\nHead to head comparison\nName: ackermann_ack_10 Graph Execution Time:  0.00449681282043457 Linear Execution Time:  0.0001437664031982422 Speedup:  31.27860696517413\nName: ackermann_ack_7 Graph Execution Time:  0.0022280216217041016 Linear Execution Time:  0.0002560615539550781 Speedup:  8.701117318435754\nName: ackermann_ack_5 Graph Execution Time:  0.002377033233642578 Linear Execution Time:  0.00023508071899414062 Speedup:  10.111561866125761\nName: ackermann_main_10 Graph Execution Time:  9.799003601074219e-05 Linear Execution Time:  3.0040740966796875e-05 Speedup:  3.261904761904762\nName: ackermann_main_7 Graph Execution Time:  8.177757263183594e-05 Linear Execution Time:  2.288818359375e-05 Speedup:  3.5729166666666665\nName: ackermann_main_5 Graph Execution Time:  7.414817810058594e-05 Linear Execution Time:  1.9788742065429688e-05 Speedup:  3.746987951807229\nName: binary_main_10 Graph Execution Time:  5.412101745605469e-05 Linear Execution Time:  2.09808349609375e-05 Speedup:  2.5795454545454546\nName: binary_main_7 Graph Execution Time:  4.506111145019531e-05 Linear Execution Time:  5.91278076171875e-05 Speedup:  0.7620967741935484\nName: binary_main_5 Graph Execution Time:  3.409385681152344e-05 Linear Execution Time:  1.3113021850585938e-05 Speedup:  2.6\nName: binary_printBinary_10 Graph Execution Time:  0.0008351802825927734 Linear Execution Time:  6.413459777832031e-05 Speedup:  13.022304832713754\nName: binary_printBinary_7 Graph Execution Time:  0.0008020401000976562 Linear Execution Time:  0.00011515617370605469 Speedup:  6.9648033126294\nName: binary_printBinary_5 Graph Execution Time:  0.0007398128509521484 Linear Execution Time:  9.799003601074219e-05 Speedup:  7.549878345498784\n...\nName: totient_totient_10 Graph Execution Time:  0.0045719146728515625 Linear Execution Time:  0.0003387928009033203 Speedup:  13.494722026741732\nName: totient_totient_7 Graph Execution Time:  0.00251007080078125 Linear Execution Time:  0.000286102294921875 Speedup:  8.773333333333333\nName: totient_totient_5 Graph Execution Time:  0.0029671192169189453 Linear Execution Time:  0.00013017654418945312 Speedup:  22.793040293040292\nName: totient_mod_10 Graph Execution Time:  0.0002512931823730469 Linear Execution Time:  4.100799560546875e-05 Speedup:  6.127906976744186\nName: totient_mod_7 Graph Execution Time:  0.0002460479736328125 Linear Execution Time:  3.409385681152344e-05 Speedup:  7.216783216783217\nName: totient_mod_5 Graph Execution Time:  0.0002429485321044922 Linear Execution Time:  3.0994415283203125e-05 Speedup:  7.838461538461538\nName: up_main_10 Graph Execution Time:  0.0001239776611328125 Linear Execution Time:  3.409385681152344e-05 Speedup:  3.6363636363636362\nName: up_main_7 Graph Execution Time:  0.00014209747314453125 Linear Execution Time:  2.7179718017578125e-05 Speedup:  5.228070175438597\nName: up_main_5 Graph Execution Time:  0.00010538101196289062 Linear Execution Time:  2.4080276489257812e-05 Speedup:  4.376237623762377\nName: up_up_arrow_10 Graph Execution Time:  0.0010821819305419922 Linear Execution Time:  0.00011086463928222656 Speedup:  9.761290322580646\nName: up_up_arrow_7 Graph Execution Time:  0.0010287761688232422 Linear Execution Time:  8.916854858398438e-05 Speedup:  11.537433155080214\nName: up_up_arrow_5 Graph Execution Time:  0.0010311603546142578 Linear Execution Time:  7.987022399902344e-05 Speedup:  12.91044776119403\nAverage speedup of Linear:  9.752766747717136x\nName: ackermann_ack_10 Graph Registers:  4 Linear Registers:  6 Ratio:  0.6666666666666666\nName: ackermann_ack_7 Graph Registers:  4 Linear Registers:  6 Ratio:  0.6666666666666666\nName: ackermann_ack_5 Graph Registers:  4 Linear Registers:  5 Ratio:  0.8\nName: ackermann_main_10 Graph Registers:  1 Linear Registers:  1 Ratio:  1.0\nName: ackermann_main_7 Graph Registers:  1 Linear Registers:  1 Ratio:  1.0\nName: ackermann_main_5 Graph Registers:  1 Linear Registers:  1 Ratio:  1.0\nName: binary_main_10 Graph Registers:  1 Linear Registers:  1 Ratio:  1.0\nName: binary_main_7 Graph Registers:  1 Linear Registers:  1 Ratio:  1.0\nName: binary_main_5 Graph Registers:  1 Linear Registers:  1 Ratio:  1.0\nName: binary_printBinary_10 Graph Registers:  3 Linear Registers:  3 Ratio:  1.0\nName: binary_printBinary_7 Graph Registers:  3 Linear Registers:  3 Ratio:  1.0\nName: binary_printBinary_5 Graph Registers:  3 Linear Registers:  3 Ratio:  1.0\n...\nName: totient_main_10 Graph Registers:  1 Linear Registers:  1 Ratio:  1.0\nName: totient_main_7 Graph Registers:  1 Linear Registers:  1 Ratio:  1.0\nName: totient_main_5 Graph Registers:  1 Linear Registers:  1 Ratio:  1.0\nName: totient_totient_10 Graph Registers:  6 Linear Registers:  7 Ratio:  0.8571428571428571\nName: totient_totient_7 Graph Registers:  6 Linear Registers:  7 Ratio:  0.8571428571428571\nName: totient_totient_5 Graph Registers:  5 Linear Registers:  5 Ratio:  1.0\nName: totient_mod_10 Graph Registers:  2 Linear Registers:  3 Ratio:  0.6666666666666666\nName: totient_mod_7 Graph Registers:  2 Linear Registers:  3 Ratio:  0.6666666666666666\nName: totient_mod_5 Graph Registers:  2 Linear Registers:  3 Ratio:  0.6666666666666666\nName: up_main_10 Graph Registers:  1 Linear Registers:  1 Ratio:  1.0\nName: up_main_7 Graph Registers:  1 Linear Registers:  1 Ratio:  1.0\nName: up_main_5 Graph Registers:  1 Linear Registers:  1 Ratio:  1.0\nName: up_up_arrow_10 Graph Registers:  4 Linear Registers:  6 Ratio:  0.6666666666666666\nName: up_up_arrow_7 Graph Registers:  4 Linear Registers:  6 Ratio:  0.6666666666666666\nName: up_up_arrow_5 Graph Registers:  4 Linear Registers:  5 Ratio:  0.8\nAverage Registers used by Graph vs Linear:  0.8438802083333327x\nName: ackermann_ack_10 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: ackermann_ack_7 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: ackermann_ack_5 Graph Spills:  0 Linear Spills:  1 Difference:  1\nName: ackermann_main_10 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: ackermann_main_7 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: ackermann_main_5 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: binary_main_10 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: binary_main_7 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: binary_main_5 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: binary_printBinary_10 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: binary_printBinary_7 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: binary_printBinary_5 Graph Spills:  0 Linear Spills:  0 Difference:  0\n...\nName: totient_main_10 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: totient_main_7 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: totient_main_5 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: totient_totient_10 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: totient_totient_7 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: totient_totient_5 Graph Spills:  1 Linear Spills:  1 Difference:  0\nName: totient_mod_10 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: totient_mod_7 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: totient_mod_5 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: up_main_10 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: up_main_7 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: up_main_5 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: up_up_arrow_10 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: up_up_arrow_7 Graph Spills:  0 Linear Spills:  0 Difference:  0\nName: up_up_arrow_5 Graph Spills:  0 Linear Spills:  1 Difference:  1\nAverage Spills used by Linear vs Graph:  0.140625",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Final Project Report -- Register Allocator and minimal RISC-V Backend"
    ]
  },
  {
    "objectID": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html",
    "href": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html",
    "title": "EECE7309 Homework 3 – Data Flow",
    "section": "",
    "text": "This assignment asks us to investigate data flow analysis algorithms. Here, we discuss the algorithm implemented, and describe the process of developing this code. We primarily investigate “reaching definitions” here. Further, this code defines an Analysis class which allows the user to decelare a merge and transfer function for data flow analysis, and further allows them to indicate whether the analysis is forward or reverse. This was considered extra credit for this assignment.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 3 -- Data Flow"
    ]
  },
  {
    "objectID": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#introduction",
    "href": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#introduction",
    "title": "EECE7309 Homework 3 – Data Flow",
    "section": "",
    "text": "This assignment asks us to investigate data flow analysis algorithms. Here, we discuss the algorithm implemented, and describe the process of developing this code. We primarily investigate “reaching definitions” here. Further, this code defines an Analysis class which allows the user to decelare a merge and transfer function for data flow analysis, and further allows them to indicate whether the analysis is forward or reverse. This was considered extra credit for this assignment.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 3 -- Data Flow"
    ]
  },
  {
    "objectID": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#reaching-definitions",
    "href": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#reaching-definitions",
    "title": "EECE7309 Homework 3 – Data Flow",
    "section": "Reaching Definitions",
    "text": "Reaching Definitions\nWe wish to analyze whether a definition “reaches” a particular part of the program. This allows us to determine what definitions have a valid path to arrive at a specific block in the program. This can be very helpful for other optimizations such as dead code elimination. Reaching definition data flow analysis is a forward DFA algorithm, and employs a “may” merger strategy (i.e. a union). At a high level, this algorithm determines what definitions may reach the start of a basic block, and determines which definitions may leave the block, which can be deduced to the definitions at the start of the block minus those definitions which were killed (generally by reassignments).",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 3 -- Data Flow"
    ]
  },
  {
    "objectID": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#challenges-faced",
    "href": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#challenges-faced",
    "title": "EECE7309 Homework 3 – Data Flow",
    "section": "Challenges Faced",
    "text": "Challenges Faced\nThere were a couple of challenges which I faced developing this code. The first was in determining how to represent the CFG of a given program. The previous assignments did leverage basic blocks, but only limited interconnection of them. As such, I was able to get by without defining more formal structures. In this assignment, I formalized this structure as one typically does with a graph. This is not inherently a difficult task, however this is the first time I have done so in Python and as such neglected how some common types are passed around, namely whether they are copies or references. I wrote the code under the assumption that they were copies initially, but this introduced a handful of bugs into my code which were not immediately clear. I also learned this the hard way when defining the data flow analysis algorithm itself. This proved particularly confusing when analyzing code with loops.\nAnother challenge I incurred was perhaps one of mistaken thought, where I had convinced myself that calls and returns would need to be handled for this analysis. What I neglected to realize is that at a call or return, the scope changes, and therefore this DFA does not really need to work across functions. The only scenario where this would be necessary is with some kind of global variable, which appears to be out of scope of this type of analysis. So, this was a bit of a tangent I sent myself on.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 3 -- Data Flow"
    ]
  },
  {
    "objectID": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#implementation",
    "href": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#implementation",
    "title": "EECE7309 Homework 3 – Data Flow",
    "section": "Implementation",
    "text": "Implementation\nAs mentioned above, this implementation analyzes whether a definiton reaches a particular part of the program. To do this, we define an algorithm which inspects every basic block in each function in the program. We determine, based on the definitions which enter the block, the definitions that exit the block.\nIn more detail, we determine the definitions which enter the block as the union of all definitions exiting predecessor blocks to the given block (this is the merge step). Then, we determine what the outputs of the basic block are by overwriting any definitions found in b_in (the definitions which reach the block) of variables defined in the current block, which is to say that we overwrite the definitions killed by the current block. This is the transfer step. We iterate through all the basic blocks in the program, moving “forward” through the basic blocks, where forward is moving essentially forward through time in the program. At the end of each block’s analysis, if the outputs of the block have changed, we enqueue the successor blocks of the current block into our worklist. Once the outputs of the block cease to change, this analysis is complete and has converged (it is further provable that this algorithm always converges).\nThe data structure in this analysis is a CFG, containing “Blocks” which are defined as follows:\n\nclass Block:\n    def __init__(self, instrs: list, id, preds: set, succs: set):\n        self.instrs = instrs\n        self.id = id\n        self.preds = preds\n        self.succs = succs\n\nFurther, this code was designed to be compatible with other analyses. It does so by definining the following class:\n\nclass Analysis():\n    def __init__(self, forward: bool, merge: Callable[list, list], transfer: Callable[[dict, blocks.Block], dict]):\n        self.forward = forward\n        self.init = init\n        self.merge = merge\n        self.transfer = transfer\n    \n    def merge(self, b_ins: list):\n        return self.merge(b_ins)\n    \n    def transfer(self, b_in: dict, block: blocks.Block):\n        return self.transfer(b_in, block)\n\n    def data_flow(self, blocks):\n        ... # See linked code",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 3 -- Data Flow"
    ]
  },
  {
    "objectID": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#testing",
    "href": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#testing",
    "title": "EECE7309 Homework 3 – Data Flow",
    "section": "Testing",
    "text": "Testing\nI tested this program on four programs, two of my own, and two found in Bril’s benchmark suite. One of the programs I experimented with was quadratic.bril, which is rather long, and as such I have put those results in the appendix of this blog.\nThe most basic test case was the following code:\n# sample.bril\n@main {\n    a: int = const 4;\n    b: int = const 2;\n    sum1: int = add a b;\n    sum2: int = add a b;\n    prod: int = mul sum1 sum2;\n    prod2: int = mul sum1 sum2;\n    print prod;\n}\nThis code only contains one basic block, and produces fairly trivial results, shown below.\nprinting blocks\n.start \n Instructions: [{'dest': 'a', 'op': 'const', 'type': 'int', 'value': 4}, {'dest': 'b', 'op': 'const', 'type': 'int', 'value': 2}, {'args': ['a', 'b'], 'dest': 'sum1', 'op': 'add', 'type': 'int'}, {'args': ['a', 'b'], 'dest': 'sum2', 'op': 'add', 'type': 'int'}, {'args': ['sum1', 'sum2'], 'dest': 'prod', 'op': 'mul', 'type': 'int'}, {'args': ['sum1', 'sum2'], 'dest': 'prod2', 'op': 'mul', 'type': 'int'}, {'args': ['prod'], 'op': 'print'}]\n ID: .start\n Predecessors: set()\n Successors: set()\n\n\nblock_id:  .start\nin_edges:  {}\nout_edges:  {'a': ['.start_0'], 'b': ['.start_1'], 'sum1': ['.start_2'], 'sum2': ['.start_3'], 'prod': ['.start_4'], 'prod2': ['.start_5']}\nThe next program introduced branching control flow:\n# test\n@main(a: int) {\n  v1: int = const 1;\n  cond: bool = lt v1 x;\n  br cond .then.0 .else.0;\n.then.0:\n  v2: int = id x;\n  jmp .print;\n.else.0:\n  v3: int = id x;\n  v4: int = const 1;\n  v2: int = sub v3 v4;\n.print:\n  print v2;\n}\nThis correctly produced the following result:\nprinting blocks\nthen.0 \n Instructions: [{'args': ['cond'], 'labels': ['then.0', 'else.0'], 'op': 'br'}, {'args': ['x'], 'dest': 'v2', 'op': 'id', 'type': 'int'}, {'labels': ['print'], 'op': 'jmp'}]\n ID: then.0\n Predecessors: {'.start'}\n Successors: {'print'}\n\nprinting blocks\nelse.0 \n Instructions: [{'args': ['cond'], 'labels': ['then.0', 'else.0'], 'op': 'br'}, {'args': ['x'], 'dest': 'v3', 'op': 'id', 'type': 'int'}, {'dest': 'v4', 'op': 'const', 'type': 'int', 'value': 1}, {'args': ['v3', 'v4'], 'dest': 'v2', 'op': 'sub', 'type': 'int'}]\n ID: else.0\n Predecessors: {'.start'}\n Successors: {'print'}\n\nprinting blocks\n.start \n Instructions: [{'dest': 'v1', 'op': 'const', 'type': 'int', 'value': 1}, {'args': ['v1', 'x'], 'dest': 'cond', 'op': 'lt', 'type': 'bool'}, {'args': ['cond'], 'labels': ['then.0', 'else.0'], 'op': 'br'}]\n ID: .start\n Predecessors: set()\n Successors: {'else.0', 'then.0'}\n\nprinting blocks\nprint \n Instructions: [{'labels': ['print'], 'op': 'jmp'}, {'args': ['v2'], 'op': 'print'}]\n ID: print\n Predecessors: {'else.0', 'then.0'}\n Successors: set()\n\n\nblock_id:  then.0\nin_edges:  {'v1': ['.start_0'], 'cond': ['.start_1']}\nout_edges:  {'v1': ['.start_0'], 'cond': ['.start_1'], 'v2': ['then.0_1']}\n\nblock_id:  else.0\nin_edges:  {'v1': ['.start_0'], 'cond': ['.start_1']}\nout_edges:  {'v1': ['.start_0'], 'cond': ['.start_1'], 'v3': ['else.0_1'], 'v4': ['else.0_2'], 'v2': ['else.0_3']}\n\nblock_id:  .start\nin_edges:  {}\nout_edges:  {'v1': ['.start_0'], 'cond': ['.start_1']}\n\nblock_id:  print\nin_edges:  {'v1': ['.start_0'], 'cond': ['.start_1'], 'v3': ['else.0_1'], 'v4': ['else.0_2'], 'v2': ['else.0_3', 'then.0_1']}\nout_edges:  {'v3': ['else.0_1'], 'v4': ['else.0_2'], 'v2': ['else.0_3', 'then.0_1'], 'v1': ['.start_0'], 'cond': ['.start_1']}\nAs we can see, the in and out edges for each block are what we expect. The key observation is in the final print block, definitions from both branches are “reaching”.\nFor a more complex example, we examined the loopfact.bril benchmark from the bril repository.\n# ARGS: 8\n@main(input: int) {\n  value: int = id input;\n  v1: int = const 1;\n  result: int = id v1;\n  v3: int = id value;\n  i: int = id v3;\n.for.cond.2:\n  v4: int = id i;\n  v5: int = const 0;\n  v6: bool = gt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: int = id result;\n  v8: int = id i;\n  v9: int = mul v7 v8;\n  result: int = id v9;\n  v10: int = id i;\n  v11: int = const 1;\n  v12: int = sub v10 v11;\n  i: int = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: int = id result;\n  print v13;\n  v14: int = const 0;\n}\nThis code contains a loop, which is a good test for our analysis. What we expect is all definitions reach the final .for.end.2 block, all definitions except for those in .for.end.2 reach the blocks .for.cond.2 and .for.body.2, and no definitions reach the start of .start. Further, we expect for .for.body.2 to kill the definitions of i and result which will enter the block. No other blocks should kill instructions.\nWe show that the expected results are generated by our program:\nprinting blocks\n.start \n Instructions: [{'args': ['input'], 'dest': 'value', 'op': 'id', 'type': 'int'}, {'dest': 'v1', 'op': 'const', 'type': 'int', 'value': 1}, {'args': ['v1'], 'dest': 'result', 'op': 'id', 'type': 'int'}, {'args': ['value'], 'dest': 'v3', 'op': 'id', 'type': 'int'}, {'args': ['v3'], 'dest': 'i', 'op': 'id', 'type': 'int'}]\n ID: .start\n Predecessors: set()\n Successors: {'for.cond.2'}\n\nprinting blocks\nfor.cond.2 \n Instructions: [{'args': ['i'], 'dest': 'v4', 'op': 'id', 'type': 'int'}, {'dest': 'v5', 'op': 'const', 'type': 'int', 'value': 0}, {'args': ['v4', 'v5'], 'dest': 'v6', 'op': 'gt', 'type': 'bool'}, {'args': ['v6'], 'labels': ['for.body.2', 'for.end.2'], 'op': 'br'}]\n ID: for.cond.2\n Predecessors: {'for.body.2', '.start'}\n Successors: {'for.end.2', 'for.body.2'}\n\nprinting blocks\nfor.body.2 \n Instructions: [{'args': ['v6'], 'labels': ['for.body.2', 'for.end.2'], 'op': 'br'}, {'args': ['result'], 'dest': 'v7', 'op': 'id', 'type': 'int'}, {'args': ['i'], 'dest': 'v8', 'op': 'id', 'type': 'int'}, {'args': ['v7', 'v8'], 'dest': 'v9', 'op': 'mul', 'type': 'int'}, {'args': ['v9'], 'dest': 'result', 'op': 'id', 'type': 'int'}, {'args': ['i'], 'dest': 'v10', 'op': 'id', 'type': 'int'}, {'dest': 'v11', 'op': 'const', 'type': 'int', 'value': 1}, {'args': ['v10', 'v11'], 'dest': 'v12', 'op': 'sub', 'type': 'int'}, {'args': ['v12'], 'dest': 'i', 'op': 'id', 'type': 'int'}, {'labels': ['for.cond.2'], 'op': 'jmp'}]\n ID: for.body.2\n Predecessors: {'for.cond.2'}\n Successors: {'for.cond.2'}\n\nprinting blocks\nfor.end.2 \n Instructions: [{'args': ['v6'], 'labels': ['for.body.2', 'for.end.2'], 'op': 'br'}, {'args': ['result'], 'dest': 'v13', 'op': 'id', 'type': 'int'}, {'args': ['v13'], 'op': 'print'}, {'dest': 'v14', 'op': 'const', 'type': 'int', 'value': 0}]\n ID: for.end.2\n Predecessors: {'for.cond.2'}\n Successors: set()\n\n\nblock_id:  .start\nin_edges:  {}\nout_edges:  {'value': ['.start_0'], 'v1': ['.start_1'], 'result': ['.start_2'], 'v3': ['.start_3'], 'i': ['.start_4']}\n\nblock_id:  for.cond.2\nin_edges:  {'value': ['.start_0'], 'v1': ['.start_1'], 'result': ['.start_2', 'for.body.2_4'], 'v3': ['.start_3'], 'i': ['.start_4', 'for.body.2_8'], 'v4': ['for.cond.2_0'], 'v5': ['for.cond.2_1'], 'v6': ['for.cond.2_2'], 'v7': ['for.body.2_1'], 'v8': ['for.body.2_2'], 'v9': ['for.body.2_3'], 'v10': ['for.body.2_5'], 'v11': ['for.body.2_6'], 'v12': ['for.body.2_7']}\nout_edges:  {'value': ['.start_0'], 'v1': ['.start_1'], 'result': ['.start_2', 'for.body.2_4'], 'v3': ['.start_3'], 'i': ['.start_4', 'for.body.2_8'], 'v7': ['for.body.2_1'], 'v8': ['for.body.2_2'], 'v9': ['for.body.2_3'], 'v10': ['for.body.2_5'], 'v11': ['for.body.2_6'], 'v12': ['for.body.2_7'], 'v4': ['for.cond.2_0'], 'v5': ['for.cond.2_1'], 'v6': ['for.cond.2_2']}\n\nblock_id:  for.body.2\nin_edges:  {'value': ['.start_0'], 'v1': ['.start_1'], 'result': ['.start_2', 'for.body.2_4'], 'v3': ['.start_3'], 'i': ['.start_4', 'for.body.2_8'], 'v7': ['for.body.2_1'], 'v8': ['for.body.2_2'], 'v9': ['for.body.2_3'], 'v10': ['for.body.2_5'], 'v11': ['for.body.2_6'], 'v12': ['for.body.2_7'], 'v4': ['for.cond.2_0'], 'v5': ['for.cond.2_1'], 'v6': ['for.cond.2_2']}\nout_edges:  {'value': ['.start_0'], 'v1': ['.start_1'], 'v3': ['.start_3'], 'v4': ['for.cond.2_0'], 'v5': ['for.cond.2_1'], 'v6': ['for.cond.2_2'], 'v7': ['for.body.2_1'], 'v8': ['for.body.2_2'], 'v9': ['for.body.2_3'], 'result': ['for.body.2_4'], 'v10': ['for.body.2_5'], 'v11': ['for.body.2_6'], 'v12': ['for.body.2_7'], 'i': ['for.body.2_8']}\n\nblock_id:  for.end.2\nin_edges:  {'value': ['.start_0'], 'v1': ['.start_1'], 'result': ['.start_2', 'for.body.2_4'], 'v3': ['.start_3'], 'i': ['.start_4', 'for.body.2_8'], 'v7': ['for.body.2_1'], 'v8': ['for.body.2_2'], 'v9': ['for.body.2_3'], 'v10': ['for.body.2_5'], 'v11': ['for.body.2_6'], 'v12': ['for.body.2_7'], 'v4': ['for.cond.2_0'], 'v5': ['for.cond.2_1'], 'v6': ['for.cond.2_2']}\nout_edges:  {'value': ['.start_0'], 'v1': ['.start_1'], 'result': ['.start_2', 'for.body.2_4'], 'v3': ['.start_3'], 'i': ['.start_4', 'for.body.2_8'], 'v7': ['for.body.2_1'], 'v8': ['for.body.2_2'], 'v9': ['for.body.2_3'], 'v10': ['for.body.2_5'], 'v11': ['for.body.2_6'], 'v12': ['for.body.2_7'], 'v4': ['for.cond.2_0'], 'v5': ['for.cond.2_1'], 'v6': ['for.cond.2_2'], 'v13': ['for.end.2_1'], 'v14': ['for.end.2_3']}\nWe further tested with the quadratic.bril program, and results for that test may be found in the appendix. This program contains multiple functions, loops, and branches, however is not especially interesting for this analysis as there is only one instance of a definition being killed (in the .endif.7 block).",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 3 -- Data Flow"
    ]
  },
  {
    "objectID": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#code",
    "href": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#code",
    "title": "EECE7309 Homework 3 – Data Flow",
    "section": "Code",
    "text": "Code\nThe code for this assignment is contained in a public GitHub repository. The code and testing code + results can be found in this repository.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 3 -- Data Flow"
    ]
  },
  {
    "objectID": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#appendix",
    "href": "blogs/michael/10-11-2024-HW3-MichaelMaurer.html#appendix",
    "title": "EECE7309 Homework 3 – Data Flow",
    "section": "Appendix",
    "text": "Appendix\n# quadratic.bril\n# ARGS: -5 8 21\n@main(a: int, b: int, c: int) {\n  call @quadratic a b c;\n}\n\n@sqrt(x: int): int {\n  v1: int = const 1;\n  i: int = id v1;\n.for.cond.0:\n  v2: int = id i;\n  v3: int = id x;\n  v4: int = const 1;\n  v5: int = sub v3 v4;\n  v6: bool = lt v2 v5;\n  br v6 .for.body.0 .for.end.0;\n.for.body.0:\n  v8: int = id i;\n  v9: int = id i;\n  v10: int = mul v8 v9;\n  v11: int = id x;\n  v12: bool = ge v10 v11;\n  br v12 .then.7 .else.7;\n.then.7:\n  v13: int = id i;\n  ret v13;\n.else.7:\n.endif.7:\n  v14: int = id i;\n  v15: int = const 1;\n  v16: int = add v14 v15;\n  i: int = id v16;\n  jmp .for.cond.0;\n.for.end.0:\n  v17: int = const 0;\n  ret v17;\n}\n\n@quadratic(a: int, b: int, c: int) {\n  v0: int = id b;\n  v1: int = id b;\n  v2: int = mul v0 v1;\n  v3: int = const 4;\n  v4: int = id a;\n  v5: int = mul v3 v4;\n  v6: int = id c;\n  v7: int = mul v5 v6;\n  v8: int = sub v2 v7;\n  s: int = id v8;\n  v9: int = const 2;\n  v10: int = id a;\n  v11: int = mul v9 v10;\n  d: int = id v11;\n  v12: int = const 0;\n  v13: int = id b;\n  v14: int = sub v12 v13;\n  v15: int = id s;\n  v16: int = call @sqrt v15;\n  v17: int = add v14 v16;\n  r1: int = id v17;\n  v18: int = const 0;\n  v19: int = id b;\n  v20: int = sub v18 v19;\n  v21: int = id s;\n  v22: int = call @sqrt v21;\n  v23: int = sub v20 v22;\n  r2: int = id v23;\n  v24: int = id r1;\n  v25: int = id d;\n  v26: int = div v24 v25;\n  print v26;\n  v27: int = const 0;\n  v28: int = id r2;\n  v29: int = id d;\n  v30: int = div v28 v29;\n  print v30;\n  v31: int = const 0;\n}\nResults:\nprinting blocks\n.start \n Instructions: [{'args': ['a', 'b', 'c'], 'funcs': ['quadratic'], 'op': 'call'}]\n ID: .start\n Predecessors: set()\n Successors: set()\n\n\nblock_id:  .start\nin_edges:  {}\nout_edges:  {}\n\nprinting blocks\n.start \n Instructions: [{'dest': 'v1', 'op': 'const', 'type': 'int', 'value': 1}, {'args': ['v1'], 'dest': 'i', 'op': 'id', 'type': 'int'}]\n ID: .start\n Predecessors: set()\n Successors: {'for.cond.0'}\n\nprinting blocks\nfor.cond.0 \n Instructions: [{'args': ['i'], 'dest': 'v2', 'op': 'id', 'type': 'int'}, {'args': ['x'], 'dest': 'v3', 'op': 'id', 'type': 'int'}, {'dest': 'v4', 'op': 'const', 'type': 'int', 'value': 1}, {'args': ['v3', 'v4'], 'dest': 'v5', 'op': 'sub', 'type': 'int'}, {'args': ['v2', 'v5'], 'dest': 'v6', 'op': 'lt', 'type': 'bool'}, {'args': ['v6'], 'labels': ['for.body.0', 'for.end.0'], 'op': 'br'}]\n ID: for.cond.0\n Predecessors: {'.start', 'endif.7'}\n Successors: {'for.body.0', 'for.end.0'}\n\nprinting blocks\nfor.body.0 \n Instructions: [{'args': ['v6'], 'labels': ['for.body.0', 'for.end.0'], 'op': 'br'}, {'args': ['i'], 'dest': 'v8', 'op': 'id', 'type': 'int'}, {'args': ['i'], 'dest': 'v9', 'op': 'id', 'type': 'int'}, {'args': ['v8', 'v9'], 'dest': 'v10', 'op': 'mul', 'type': 'int'}, {'args': ['x'], 'dest': 'v11', 'op': 'id', 'type': 'int'}, {'args': ['v10', 'v11'], 'dest': 'v12', 'op': 'ge', 'type': 'bool'}, {'args': ['v12'], 'labels': ['then.7', 'else.7'], 'op': 'br'}]\n ID: for.body.0\n Predecessors: {'for.cond.0'}\n Successors: {'then.7', 'else.7'}\n\nprinting blocks\nfor.end.0 \n Instructions: [{'args': ['v6'], 'labels': ['for.body.0', 'for.end.0'], 'op': 'br'}, {'dest': 'v17', 'op': 'const', 'type': 'int', 'value': 0}, {'args': ['v17'], 'op': 'ret'}]\n ID: for.end.0\n Predecessors: {'for.cond.0'}\n Successors: set()\n\nprinting blocks\nthen.7 \n Instructions: [{'args': ['v12'], 'labels': ['then.7', 'else.7'], 'op': 'br'}, {'args': ['i'], 'dest': 'v13', 'op': 'id', 'type': 'int'}, {'args': ['v13'], 'op': 'ret'}]\n ID: then.7\n Predecessors: {'for.body.0'}\n Successors: set()\n\nprinting blocks\nelse.7 \n Instructions: [{'args': ['v12'], 'labels': ['then.7', 'else.7'], 'op': 'br'}]\n ID: else.7\n Predecessors: {'for.body.0'}\n Successors: {'endif.7'}\n\nprinting blocks\nendif.7 \n Instructions: [{'args': ['i'], 'dest': 'v14', 'op': 'id', 'type': 'int'}, {'dest': 'v15', 'op': 'const', 'type': 'int', 'value': 1}, {'args': ['v14', 'v15'], 'dest': 'v16', 'op': 'add', 'type': 'int'}, {'args': ['v16'], 'dest': 'i', 'op': 'id', 'type': 'int'}, {'labels': ['for.cond.0'], 'op': 'jmp'}]\n ID: endif.7\n Predecessors: {'else.7'}\n Successors: {'for.cond.0'}\n\n\nblock_id:  .start\nin_edges:  {}\nout_edges:  {'v1': ['.start_0'], 'i': ['.start_1']}\n\nblock_id:  for.cond.0\nin_edges:  {'v1': ['.start_0'], 'i': ['.start_1', 'endif.7_3'], 'v2': ['for.cond.0_0'], 'v3': ['for.cond.0_1'], 'v4': ['for.cond.0_2'], 'v5': ['for.cond.0_3'], 'v6': ['for.cond.0_4'], 'v8': ['for.body.0_1'], 'v9': ['for.body.0_2'], 'v10': ['for.body.0_3'], 'v11': ['for.body.0_4'], 'v12': ['for.body.0_5'], 'v14': ['endif.7_0'], 'v15': ['endif.7_1'], 'v16': ['endif.7_2']}\nout_edges:  {'v1': ['.start_0'], 'i': ['.start_1', 'endif.7_3'], 'v8': ['for.body.0_1'], 'v9': ['for.body.0_2'], 'v10': ['for.body.0_3'], 'v11': ['for.body.0_4'], 'v12': ['for.body.0_5'], 'v14': ['endif.7_0'], 'v15': ['endif.7_1'], 'v16': ['endif.7_2'], 'v2': ['for.cond.0_0'], 'v3': ['for.cond.0_1'], 'v4': ['for.cond.0_2'], 'v5': ['for.cond.0_3'], 'v6': ['for.cond.0_4']}\n\nblock_id:  for.body.0\nin_edges:  {'v1': ['.start_0'], 'i': ['.start_1', 'endif.7_3'], 'v8': ['for.body.0_1'], 'v9': ['for.body.0_2'], 'v10': ['for.body.0_3'], 'v11': ['for.body.0_4'], 'v12': ['for.body.0_5'], 'v14': ['endif.7_0'], 'v15': ['endif.7_1'], 'v16': ['endif.7_2'], 'v2': ['for.cond.0_0'], 'v3': ['for.cond.0_1'], 'v4': ['for.cond.0_2'], 'v5': ['for.cond.0_3'], 'v6': ['for.cond.0_4']}\nout_edges:  {'v1': ['.start_0'], 'i': ['.start_1', 'endif.7_3'], 'v14': ['endif.7_0'], 'v15': ['endif.7_1'], 'v16': ['endif.7_2'], 'v2': ['for.cond.0_0'], 'v3': ['for.cond.0_1'], 'v4': ['for.cond.0_2'], 'v5': ['for.cond.0_3'], 'v6': ['for.cond.0_4'], 'v8': ['for.body.0_1'], 'v9': ['for.body.0_2'], 'v10': ['for.body.0_3'], 'v11': ['for.body.0_4'], 'v12': ['for.body.0_5']}\n\nblock_id:  for.end.0\nin_edges:  {'v1': ['.start_0'], 'i': ['.start_1', 'endif.7_3'], 'v8': ['for.body.0_1'], 'v9': ['for.body.0_2'], 'v10': ['for.body.0_3'], 'v11': ['for.body.0_4'], 'v12': ['for.body.0_5'], 'v14': ['endif.7_0'], 'v15': ['endif.7_1'], 'v16': ['endif.7_2'], 'v2': ['for.cond.0_0'], 'v3': ['for.cond.0_1'], 'v4': ['for.cond.0_2'], 'v5': ['for.cond.0_3'], 'v6': ['for.cond.0_4']}\nout_edges:  {'v1': ['.start_0'], 'i': ['.start_1', 'endif.7_3'], 'v8': ['for.body.0_1'], 'v9': ['for.body.0_2'], 'v10': ['for.body.0_3'], 'v11': ['for.body.0_4'], 'v12': ['for.body.0_5'], 'v14': ['endif.7_0'], 'v15': ['endif.7_1'], 'v16': ['endif.7_2'], 'v2': ['for.cond.0_0'], 'v3': ['for.cond.0_1'], 'v4': ['for.cond.0_2'], 'v5': ['for.cond.0_3'], 'v6': ['for.cond.0_4'], 'v17': ['for.end.0_1']}\n\nblock_id:  then.7\nin_edges:  {'v1': ['.start_0'], 'i': ['.start_1', 'endif.7_3'], 'v14': ['endif.7_0'], 'v15': ['endif.7_1'], 'v16': ['endif.7_2'], 'v2': ['for.cond.0_0'], 'v3': ['for.cond.0_1'], 'v4': ['for.cond.0_2'], 'v5': ['for.cond.0_3'], 'v6': ['for.cond.0_4'], 'v8': ['for.body.0_1'], 'v9': ['for.body.0_2'], 'v10': ['for.body.0_3'], 'v11': ['for.body.0_4'], 'v12': ['for.body.0_5']}\nout_edges:  {'v1': ['.start_0'], 'i': ['.start_1', 'endif.7_3'], 'v14': ['endif.7_0'], 'v15': ['endif.7_1'], 'v16': ['endif.7_2'], 'v2': ['for.cond.0_0'], 'v3': ['for.cond.0_1'], 'v4': ['for.cond.0_2'], 'v5': ['for.cond.0_3'], 'v6': ['for.cond.0_4'], 'v8': ['for.body.0_1'], 'v9': ['for.body.0_2'], 'v10': ['for.body.0_3'], 'v11': ['for.body.0_4'], 'v12': ['for.body.0_5'], 'v13': ['then.7_1']}\n\nblock_id:  else.7\nin_edges:  {'v1': ['.start_0'], 'i': ['.start_1', 'endif.7_3'], 'v14': ['endif.7_0'], 'v15': ['endif.7_1'], 'v16': ['endif.7_2'], 'v2': ['for.cond.0_0'], 'v3': ['for.cond.0_1'], 'v4': ['for.cond.0_2'], 'v5': ['for.cond.0_3'], 'v6': ['for.cond.0_4'], 'v8': ['for.body.0_1'], 'v9': ['for.body.0_2'], 'v10': ['for.body.0_3'], 'v11': ['for.body.0_4'], 'v12': ['for.body.0_5']}\nout_edges:  {'v1': ['.start_0'], 'i': ['.start_1', 'endif.7_3'], 'v14': ['endif.7_0'], 'v15': ['endif.7_1'], 'v16': ['endif.7_2'], 'v2': ['for.cond.0_0'], 'v3': ['for.cond.0_1'], 'v4': ['for.cond.0_2'], 'v5': ['for.cond.0_3'], 'v6': ['for.cond.0_4'], 'v8': ['for.body.0_1'], 'v9': ['for.body.0_2'], 'v10': ['for.body.0_3'], 'v11': ['for.body.0_4'], 'v12': ['for.body.0_5']}\n\nblock_id:  endif.7\nin_edges:  {'v1': ['.start_0'], 'i': ['.start_1', 'endif.7_3'], 'v14': ['endif.7_0'], 'v15': ['endif.7_1'], 'v16': ['endif.7_2'], 'v2': ['for.cond.0_0'], 'v3': ['for.cond.0_1'], 'v4': ['for.cond.0_2'], 'v5': ['for.cond.0_3'], 'v6': ['for.cond.0_4'], 'v8': ['for.body.0_1'], 'v9': ['for.body.0_2'], 'v10': ['for.body.0_3'], 'v11': ['for.body.0_4'], 'v12': ['for.body.0_5']}\nout_edges:  {'v1': ['.start_0'], 'v2': ['for.cond.0_0'], 'v3': ['for.cond.0_1'], 'v4': ['for.cond.0_2'], 'v5': ['for.cond.0_3'], 'v6': ['for.cond.0_4'], 'v8': ['for.body.0_1'], 'v9': ['for.body.0_2'], 'v10': ['for.body.0_3'], 'v11': ['for.body.0_4'], 'v12': ['for.body.0_5'], 'v14': ['endif.7_0'], 'v15': ['endif.7_1'], 'v16': ['endif.7_2'], 'i': ['endif.7_3']}\n\nprinting blocks\n.start \n Instructions: [{'args': ['b'], 'dest': 'v0', 'op': 'id', 'type': 'int'}, {'args': ['b'], 'dest': 'v1', 'op': 'id', 'type': 'int'}, {'args': ['v0', 'v1'], 'dest': 'v2', 'op': 'mul', 'type': 'int'}, {'dest': 'v3', 'op': 'const', 'type': 'int', 'value': 4}, {'args': ['a'], 'dest': 'v4', 'op': 'id', 'type': 'int'}, {'args': ['v3', 'v4'], 'dest': 'v5', 'op': 'mul', 'type': 'int'}, {'args': ['c'], 'dest': 'v6', 'op': 'id', 'type': 'int'}, {'args': ['v5', 'v6'], 'dest': 'v7', 'op': 'mul', 'type': 'int'}, {'args': ['v2', 'v7'], 'dest': 'v8', 'op': 'sub', 'type': 'int'}, {'args': ['v8'], 'dest': 's', 'op': 'id', 'type': 'int'}, {'dest': 'v9', 'op': 'const', 'type': 'int', 'value': 2}, {'args': ['a'], 'dest': 'v10', 'op': 'id', 'type': 'int'}, {'args': ['v9', 'v10'], 'dest': 'v11', 'op': 'mul', 'type': 'int'}, {'args': ['v11'], 'dest': 'd', 'op': 'id', 'type': 'int'}, {'dest': 'v12', 'op': 'const', 'type': 'int', 'value': 0}, {'args': ['b'], 'dest': 'v13', 'op': 'id', 'type': 'int'}, {'args': ['v12', 'v13'], 'dest': 'v14', 'op': 'sub', 'type': 'int'}, {'args': ['s'], 'dest': 'v15', 'op': 'id', 'type': 'int'}, {'args': ['v15'], 'dest': 'v16', 'funcs': ['sqrt'], 'op': 'call', 'type': 'int'}, {'args': ['v14', 'v16'], 'dest': 'v17', 'op': 'add', 'type': 'int'}, {'args': ['v17'], 'dest': 'r1', 'op': 'id', 'type': 'int'}, {'dest': 'v18', 'op': 'const', 'type': 'int', 'value': 0}, {'args': ['b'], 'dest': 'v19', 'op': 'id', 'type': 'int'}, {'args': ['v18', 'v19'], 'dest': 'v20', 'op': 'sub', 'type': 'int'}, {'args': ['s'], 'dest': 'v21', 'op': 'id', 'type': 'int'}, {'args': ['v21'], 'dest': 'v22', 'funcs': ['sqrt'], 'op': 'call', 'type': 'int'}, {'args': ['v20', 'v22'], 'dest': 'v23', 'op': 'sub', 'type': 'int'}, {'args': ['v23'], 'dest': 'r2', 'op': 'id', 'type': 'int'}, {'args': ['r1'], 'dest': 'v24', 'op': 'id', 'type': 'int'}, {'args': ['d'], 'dest': 'v25', 'op': 'id', 'type': 'int'}, {'args': ['v24', 'v25'], 'dest': 'v26', 'op': 'div', 'type': 'int'}, {'args': ['v26'], 'op': 'print'}, {'dest': 'v27', 'op': 'const', 'type': 'int', 'value': 0}, {'args': ['r2'], 'dest': 'v28', 'op': 'id', 'type': 'int'}, {'args': ['d'], 'dest': 'v29', 'op': 'id', 'type': 'int'}, {'args': ['v28', 'v29'], 'dest': 'v30', 'op': 'div', 'type': 'int'}, {'args': ['v30'], 'op': 'print'}, {'dest': 'v31', 'op': 'const', 'type': 'int', 'value': 0}]\n ID: .start\n Predecessors: set()\n Successors: set()\n\n\nblock_id:  .start\nin_edges:  {}\nout_edges:  {'v0': ['.start_0'], 'v1': ['.start_1'], 'v2': ['.start_2'], 'v3': ['.start_3'], 'v4': ['.start_4'], 'v5': ['.start_5'], 'v6': ['.start_6'], 'v7': ['.start_7'], 'v8': ['.start_8'], 's': ['.start_9'], 'v9': ['.start_10'], 'v10': ['.start_11'], 'v11': ['.start_12'], 'd': ['.start_13'], 'v12': ['.start_14'], 'v13': ['.start_15'], 'v14': ['.start_16'], 'v15': ['.start_17'], 'v16': ['.start_18'], 'v17': ['.start_19'], 'r1': ['.start_20'], 'v18': ['.start_21'], 'v19': ['.start_22'], 'v20': ['.start_23'], 'v21': ['.start_24'], 'v22': ['.start_25'], 'v23': ['.start_26'], 'r2': ['.start_27'], 'v24': ['.start_28'], 'v25': ['.start_29'], 'v26': ['.start_30'], 'v27': ['.start_32'], 'v28': ['.start_33'], 'v29': ['.start_34'], 'v30': ['.start_35'], 'v31': ['.start_37']}",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 3 -- Data Flow"
    ]
  },
  {
    "objectID": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html",
    "href": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html",
    "title": "EECE7309 Homework 1 – Trying Out Bril",
    "section": "",
    "text": "This assignment is designed to allow us to get comfortable with Bril, an intermediate representation designed for learning. In this assignment, we add a benchmark program to the suite, and write a tool for analyzing JSON formatted Bril.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 1 -- Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#introduction",
    "href": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#introduction",
    "title": "EECE7309 Homework 1 – Trying Out Bril",
    "section": "",
    "text": "This assignment is designed to allow us to get comfortable with Bril, an intermediate representation designed for learning. In this assignment, we add a benchmark program to the suite, and write a tool for analyzing JSON formatted Bril.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 1 -- Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#part-1-bril-benchmark-program",
    "href": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#part-1-bril-benchmark-program",
    "title": "EECE7309 Homework 1 – Trying Out Bril",
    "section": "Part 1: Bril Benchmark Program",
    "text": "Part 1: Bril Benchmark Program\nA good benchmark for this assignment is one which is new to the repository, but is fairly simple. As such, a benchmark which fits into the benchmark/core suite is likely a good choice.\nMy first idea was to implement a benchmark which computes the Hamming distance between two integers. In a high level language with a shift operator, this is a fairly easy program to write. However, Bril does not have a built-in shift operator. I spent some time writing a function to compute the Hamming distance without shifting, but it quickly became cumbersome and I searched for other ideas to not overcomplicate this work.\nMy next idea was to write a benchmark which computes the \\(nth\\) value in the Fibonacci sequence recursively. This is a sensible benchmark, as it may test a system’s ability to handle recursive programs, and further tests whether a compiler preserves this relationship.\nTo create this, only about 11 lines of TypeScript were required (when paired with the TypeScript compiler).\nAt the end, I created a program to compute the \\(nth\\) value in the Fibonacci sequence recursively, which can be found at benchmarks/core/fibonacci.bril, and can be tested with turnt.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 1 -- Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#problems-faced",
    "href": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#problems-faced",
    "title": "EECE7309 Homework 1 – Trying Out Bril",
    "section": "Problems Faced",
    "text": "Problems Faced\nOne challenge as I learned was that the TS compiler does not recognize many types, and further regards many types as Any when they cannot be easily predicted. This was not much of a challenge to handle, however I am not much of a TS programmer so learning enough of the language was a step in the process.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 1 -- Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#testing",
    "href": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#testing",
    "title": "EECE7309 Homework 1 – Trying Out Bril",
    "section": "Testing",
    "text": "Testing\nTo verify functionality, I first by hand compared the results of execution with different input values of \\(n\\), and additionally developed a turnt test. Results of testing can be observed below:\n$ bril2json &lt; benchmarks/core/fibonacci.bril | brili 1\n1\n$ bril2json &lt; benchmarks/core/fibonacci.bril | brili 5\n5\n$ bril2json &lt; benchmarks/core/fibonacci.bril | brili 10\n55\n$ bril2json &lt; benchmarks/core/fibonacci.bril | brili 30\n832040\n\n# Including timing results as evidence that recursion is happening \n$ time bril2json &lt; benchmarks/core/fibonacci.bril | brili 30\n832040\n\nreal    0m1.836s\nuser    0m1.817s\nsys 0m0.052s\n[15:02:36] michaelmaurer:~/Documents/NEU/EECE7309-compilers/bril$ time bril2json &lt; benchmarks/core/fibonacci.bril | brili 10\n55\n\nreal    0m0.102s\nuser    0m0.096s\nsys 0m0.031s",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 1 -- Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#part-2-bril-tooling",
    "href": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#part-2-bril-tooling",
    "title": "EECE7309 Homework 1 – Trying Out Bril",
    "section": "Part 2: Bril Tooling",
    "text": "Part 2: Bril Tooling\nFor the second part of this assignment, we were asked to design a tool which either analyzed or modified Bril code stored in JSON format. I chose to make a simple program, which for a given file, computes how many of each “type” of instruction the program contains. The three types of instructions are “Constants”, “Value Operations”, and “Effect Operations” as outlined in the docs.\nTo do this, I wrote a Python script which consumed the input JSON, parsed it to get the available instructions, and iterated through the map. Each instruction type has a unique construction, which I used to determine how many of each type of isntruction was contained. This tool only requires one pass through the program JSON to determine the count of each instruction type.\nThis tool could readily be extended to gather information about the types of instructions in a program.\nThe code for this tool is shared below:\n\n#!/usr/bin/env python3\n\nimport json\nimport sys\n\n# Read the input as json\n# Proc Exit: If valid input rec'd\ndef read_json_file(input):\n    try:\n        data = json.load(input)\n        return data\n    except json.JSONDecodeError:\n        print(f\"Error: The input does not contain valid JSON.\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        sys.exit(1)\n\n# Count the instruction types as specified by Bril syntax\ndef count_instr_types(data: dict):\n    db = {'constant': 0, 'value': 0, 'effect': 0}\n    for func in data['functions']:\n        for instr in func['instrs']:\n            # If instr op is const, it must be a constant instruction\n            if 'op' in instr and instr['op'] == 'const':\n                db['constant'] += 1\n            # If instr has a dest and is not a const, it must be a value instruction\n            elif 'dest' in instr:\n                db['value'] += 1\n            # Continue if this is a label\n            elif 'label' in instr:\n                continue\n            # Otherwise this must be an effect instruction\n            else:\n                db['effect'] += 1\n            # Note: this assumes the json is well-formed Bril.\n    return db\n\n# Main\ndef main():\n    data = read_json_file(sys.stdin)\n    print(count_instr_types(data))\n\nif __name__ == \"__main__\":\n    main()",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 1 -- Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#testing-1",
    "href": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#testing-1",
    "title": "EECE7309 Homework 1 – Trying Out Bril",
    "section": "Testing",
    "text": "Testing\nI tested this program on a handful of the provided benchmarks, and verified the results by hand. Some results are shared below:\n$ bril2json &lt; ../benchmarks/core/hanoi.bril | python tool.py \n{'constant': 5, 'value': 2, 'effect': 6}\n[15:10:42] michaelmaurer:~/Documents/NEU/EECE7309-compilers/bril/mytools$ bril2json &lt; ../benchmarks/core/birthday.bril | python tool.py \n{'constant': 9, 'value': 26, 'effect': 4}\n[15:11:04] michaelmaurer:~/Documents/NEU/EECE7309-compilers/bril/mytools$ bril2json &lt; ../benchmarks/core/collatz.bril | python tool.py \n{'constant': 3, 'value': 7, 'effect': 7}",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 1 -- Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#challenges-faced",
    "href": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#challenges-faced",
    "title": "EECE7309 Homework 1 – Trying Out Bril",
    "section": "Challenges faced",
    "text": "Challenges faced\nOriginally, I had planned to construct a program which identified all control flow instructions, and further identified how many basic blocks the program contained. I moved away from this because I was not sure how exactly to handle call and ret instructions at this time, and it seems likely this work is coming in a future assignment.\nAnother challenge was that I had at first not realized that labels are contained within the instructions in the json files. This was causing my program to crash, and required some (brief) debugging.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 1 -- Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#code",
    "href": "blogs/michael/09-20-2024-HW1-MichaelMaurer.html#code",
    "title": "EECE7309 Homework 1 – Trying Out Bril",
    "section": "Code",
    "text": "Code\nThe code for this assignment is contained in the latest commit in my fork of Bril.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 1 -- Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-09-20-HW1.html",
    "href": "blogs/oscar/2024-09-20-HW1.html",
    "title": "HW1 - Trying Out Bril",
    "section": "",
    "text": "Bril is a very simple and educational instruction-based intermediate represnetation (IR) that is represented in JSON. It features a simple syntax that can be converted as JSON, and a suite of tools to execute the program.\nIn this assignment, we implement a basic benchmarking program with Bril, then write a simple analysis program that analyzes a Bril (JSON) program, as well as create tests for both of these using Turnt (a snapshot testing tool).",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW1 - Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-09-20-HW1.html#overview",
    "href": "blogs/oscar/2024-09-20-HW1.html#overview",
    "title": "HW1 - Trying Out Bril",
    "section": "",
    "text": "Bril is a very simple and educational instruction-based intermediate represnetation (IR) that is represented in JSON. It features a simple syntax that can be converted as JSON, and a suite of tools to execute the program.\nIn this assignment, we implement a basic benchmarking program with Bril, then write a simple analysis program that analyzes a Bril (JSON) program, as well as create tests for both of these using Turnt (a snapshot testing tool).",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW1 - Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-09-20-HW1.html#part-1-bril-benchmark-program",
    "href": "blogs/oscar/2024-09-20-HW1.html#part-1-bril-benchmark-program",
    "title": "HW1 - Trying Out Bril",
    "section": "Part 1: Bril Benchmark Program",
    "text": "Part 1: Bril Benchmark Program\nAfter reviewing the list of existing benchmark programs within the Bril repository (under the benchmarks directory), we notice that most benchmarks tend to consist of basic sample programs that runs a handwritten algorithm that returns some output value.\nIn keeping with this pattern, I had originally wanted to write a program that performs a single forward convolutional pass through a large matrix of data, however after noticing the tedious process of packing individual data values into large arrays, I felt that an implementation of this would be too messy. I had also wanted to test recusion in Bril, so naturally an implementation of Merge Sort would fit this purpose well. I chose an in-place version of the algorithm to reduce the complexity a little.\nAs mentioned earlier, creating arrays appear to require painstakingly pack all the individual values one at a time into memory. I had borrowed the @pack function from the bubblesort.bril benchmark to help create this array from the passed arguments for my program, 8 integer values to be stored into an unsorted array. These arguments are passed in when the program is called, as opposed to many other benchmark programs that hardcode the input values into the program.\nTaking an existing implementation of in-place merge sort written in C and converting it to a functional bril program was a relatively simple task, given the procedural and imperative nature of both languages. The difficulty mostly came down to writing proper control flow (inverting condition statements to support &gt;= and &lt;=), the verbosity of accessing individual elements in an array, and keeping track of what variables to modify. Keeping temporary notes in listing what variables are currently used within a certain scope helps keep track of how state may change in a program if a particular variable is used elsewhere.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW1 - Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-09-20-HW1.html#part-2-analysis-tool",
    "href": "blogs/oscar/2024-09-20-HW1.html#part-2-analysis-tool",
    "title": "HW1 - Trying Out Bril",
    "section": "Part 2: Analysis Tool",
    "text": "Part 2: Analysis Tool\nFor the second part of the assignment, we are tasked to write a small program that analyzes or modifies a Bril program in some way. I had originally tried to write my program in Zig for some extra practice, but time constraints locked me back to Python for convenience (JSON handling is a lot easier in Python).\nThe program I had written performs a couple of basic scans through the functions of the specified Bril program, and prints out the following for each function (if applicable):\n\nThe arguments of the function and their types\nThe list of all function calls made and their arguments\nIf recursion is present\nThe number of potential loops found within the function\n\nA potential loop is found by simply scanning for all unique labels that have a jmp or br instruction located somewhere after the label that jumps to said label.\nGathering these statistics for all functions could potentially help internally organize programs into a convenient structure and identify what the most dominating section of code is. I find that it provides a nice synopsis on what a high-level breakdown of a Bril program looks like.\nOriginally, I had also planned to add functionality that will analyze whether variables are unused, but it got needlessly complicated and felt more appropriate for dead code elimination in a later assignment.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW1 - Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-09-20-HW1.html#testing",
    "href": "blogs/oscar/2024-09-20-HW1.html#testing",
    "title": "HW1 - Trying Out Bril",
    "section": "Testing",
    "text": "Testing\nAn in-place merge sort implementation in Bril and a simple program to analyze Bril programs were developed. Both of these programs have associated Turnt tests to ensure that the outputs for these programs are matching the expected result when I had written them. For the analysis tool (flow_detect.py), I have included other existing benchmark Bril programs in testing for a variety of outputs.\nHere are some outputs for our Merge Sort program:\n$ bril2json &lt; mergesort-inplace.bril | brili -p 8 1 2 7 3 6 5 4\n1\n2\n3\n4\n5\n6\n7\n8\ntotal_dyn_inst: 677\n$ bril2json &lt; mergesort-inplace.bril | brili -p 8 7 6 5 4 3 2 1\n1\n2\n3\n4\n5\n6\n7\n8\ntotal_dyn_inst: 803\n$ bril2json &lt; mergesort-inplace.bril | brili -p 2 2 2 2 2 2 2 1\n1\n2\n2\n2\n2\n2\n2\n2\ntotal_dyn_inst: 486\nFor our analysis tool, I had selected a handful of benchmarks that have a variety of control flow. Here is the output for eight-queens:\nmain:\n    args:\n        int: input\n    function calls: 1\n        queen(zero, n, icount, site)\n\nqueen:\n    args:\n        int: n, queens, icount\n        int ptr: site\n    num potential loops: 1\n    function calls: 2\n        valid(n, site)\n        queen(n_1, queens, icount, site)\n    recursion present\n\nvalid:\n    args:\n        int: n\n        int ptr: site\n    num potential loops: 1\nAs shown, for each function a list of arguments, function calls, and indication of loops and/or recursion is demonstrated, if applicable. I have tested the program on my own Merge Sort file as well:\nmain:\n    args:\n        int: n1, n2, n3, n4, n5, n6, n7, n8\n    function calls: 3\n        pack(size, n1, n2, n3, n4, n5, n6, n7, n8)\n        merge_sort(array, zero, upper_bound)\n        print_array(array, size)\n\npack:\n    args:\n        int: size, n1, n2, n3, n4, n5, n6, n7, n8\n\nmerge:\n    args:\n        int ptr: arr\n        int: start, mid, end\n    num potential loops: 2\n\nmerge_sort:\n    args:\n        int ptr: arr\n        int: l, r\n    function calls: 3\n        merge_sort(arr, l, m)\n        merge_sort(arr, mp1, r)\n        merge(arr, l, m, r)\n    recursion present\n\nprint_array:\n    args:\n        int ptr: array\n        int: size\n    num potential loops: 1\nHere, the program correctly identifies the use of recursion in our @merge_sort function, as well as finding the two nested loops within @merge as well.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW1 - Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/matin/2024-12-12-Final-Project.html",
    "href": "blogs/matin/2024-12-12-Final-Project.html",
    "title": "Final Project- Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions",
    "section": "",
    "text": "For the past year or so I’ve been working on a framework to instrument AMD GPU code objects at runtime, similar to NVBit for NVIDIA GPUs and GTPin for Intel GPUs. My final project is essentially a major feature that I wanted to implement in the framework. I think it will be helpful to briefly go over the framework and how it is designed to better understand my final project and the challenges faced when implementing it. ## Background Instrumentation, in a nutshell, entails the modification of a binary in some shape or form, with the goal of gaining a better understanding of how it works. The motivation behind instrumentation ranges from debugging and profiling (e.g. Valgrind, Sanatizers), to architectural research (e.g. recording the addresses accessed by a target workload to better design future caches). Instrumentation has been done for quite some time on the CPU side; Some examples include Intel’s Pin, DynInst, and DynamoRio. In recent years, this capability has been extended to NVIDIA GPUs via NVBit and to Intel GPUs via GTPin. Both these frameworks are capable of “dynamic” instrumentation, which means they don’t require access to the source code and modify the binary directly, just before the code runs. “Static” instrumentation, on the other hand, instruments the binary “offline”, and usually during compile time as part of the compilation process. Generally, dynamic instrumentation is preferred since it doesn’t require the recompilation of the target code from scratch, which is impossible to do for closed-source applications. Another advantage of dynamic frameworks is the ability to switch between instrumented and original versions of the binary (also referred to as “selective instrumentation”), as they have access to both versions. This helps tremendously with reducing the overhead caused by instrumentation, which often times is very significant. Static instrumentation, on the other hand, can generated more efficient instrumented code as they have access to the compilation steps and analysis of the target code otherwise not available to dynamic framworks. They also don’t incur a code-generation penalty compared to dynamic frameworks, as this step happens offline in static frameworks.\nSo far there hasn’t been a successful attempt at creating a framework for dynamically instrumenting AMD GPU applications; Luthier (the framework that I’ve been working on as part of my PhD research), to the best of my (and my collegeues’) knowledege, will be the first ever dynamic instrumentation framework targeting AMD GPUs. In the next section I go over the challenges I faced when designing Luthier, and then go over how it works and how it interfaces with AMD’s hardware and software stack. Since this is a compiler class I will mostly focus on the compiler aspects of Luthier and I only briefly mention details on interfacing with the rest of the ROCm stack. I will make these details available in the near future.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Final Project- Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions"
    ]
  },
  {
    "objectID": "blogs/matin/2024-12-12-Final-Project.html#luthier-how-it-was-designed-and-how-it-works",
    "href": "blogs/matin/2024-12-12-Final-Project.html#luthier-how-it-was-designed-and-how-it-works",
    "title": "Final Project- Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions",
    "section": "Luthier: How It Was Designed, and How It Works",
    "text": "Luthier: How It Was Designed, and How It Works\nThe initial design of Luthier was heavily inspired by NVBit. NVBit tools are .so shared objects that are loaded before the CUDA application using the LD_PRELOAD trick. The tools use CUPTI APIs to get notified when the CUDA application hit certain “events” (e.g. calls cudaMalloc, launches a kernel, etc.) through invoking a “callback” function defined inside the tool. NVBit tools use these callbacks to intercept CUDA kernels before they are launched, and afterwards inspect them using nvdisasm. Luthier tools works similiar to NVBit tools: they are shared objects loaded using the LD_PRELOAD environment variable; To be able to intercept and inspect kernels we designed a similar mechanism using rocprofiler-sdk to notify tools about key events in the application. We then use the ROCr (HSA) runtime APIs to locate where the kernel has been loaded on the target GPU device, and even the ELF (i.e. code object) this kernel was loaded from.\nBut this was where we ran into the following issues: 1. We realized that instrumenting AMDGPU code objects the “NVBit way” is not feasible or at best, very hard to implement: NVBit instruments GPU applications by replacing the target instruction with a JMP to a trampoline region. The trampoline then spills the application’s register onto the thread’s stack, sets up the arguments to a instrumentation device function call, and the proceeds to call it. After returning, the trampoline will restore the registers, execute the original instruction, and then jumps back to the original kernel. This design is successful because SASS (NVIDIA GPU’s hardware assembly) instructions have a fixed size, meaning that it’s very easy to replace a single instruction with a “long jump” to almost any virtual memory address on the device, all without changing the layout of the code. Not changing the code layout is very important in dynamic instrumentation, as it ensures indirect jumps/calls won’t break. This is not the case on AMD GPUs, as CDNA GPUs have a 4-byte short jump, only covering \\(2^{18}\\) bytes, and an 8-byte long jump. To make matters worse, the long jump requires additional instructions to load the jump target into registers. We can argue that we might be able to make the NVBit trampoline work by allocating fixed-address executable memory using a custom allocator (which NVBit seem to also have), but this goes completely against ROCr’s APIs of asking for executable regions of memory using the hsa_executable_t interfaces, and is very hard to implement and manage. Even then this is only a temporary fix, as the aggregation of trampoline logic for each instruction will quickly go over the range managable by the short jump instruction. This meant we needed a completely different approach for instrumentation. 2. Reusing Dead Registers: NVBit inserts calls to pre-compiled instrumentation device functions with a set calling convention, which NVBit has no choice but to obey by spilling/restoring a large number of thread registers at all times. Accessing the stack is not cheap on GPUs, hence we wanted to find a way to adapt the same instrumentation function to each instrumentation point to reuse dead registers in order to speed up instrumented code. 3. Allocating And Accessing Scratch (Local) Memory Is Not Trivial on AMD GPUs: in the NVBit paper, accessing the stack (local memory) of each thread for spilling and restoring registers is mentioned; However, how this access is done without interfering with the application’s local memory is not explained. We assume that NVBit assumes all SASS code adhere to NVIDIA’s calling conventions, which is enforced via their virtual PTX ISA; Hence, it can always have a stack pointer which allows for interleaving the spilled registers with the application’s local memory. AMD GPUs, however, can be programmed directly using hardware assembly, and have multiple calling conventions that don’t enforce presence of a stack pointer register. We needed to find a way to access the stack for instrumentation without the presnce of a stack pointer. 4. Requesting access to scratch might displace the SGPR arguments: According to the AMDGPU LLVM Docs, when a kernel is launched, a set of wavefront-common values are loaded into SGPRs for use by the kernel. This includes the address of the kernel argument buffer or the address of the queue (i.e. command processor) used to launch the kernel. It also includes resources needed to access local/scratch memory in each thread. These resource must be explicitly requested by the kernel, and this table shows the order these SGPRs are setup. One thing to note is that accessing scratch requires access to the “Private Segment Buffer Descriptor” and/or “Flat Scratch Address” of the queue used to launch the kernel, as well as a “wave offset”, which is the offset from the queue’s scratch address to the current wavefront’s scratch space. This makes accessing scratch in instrumentation functions particularly challenging, especially for applications that don’t require scratch and don’t set it up. Luthier must take into account these shift in SGPR arguments and must emit code to set it up for the instrumentation stack and then move the SGPR arguments to their original place. It also have to store the scratch information somewhere to be able to access it later in the instrumentation routines.\nLuthier addresses each of the afformentioned challenge as follows: 1. Instead of carrying out instrumentation by directly modifying the loaded code on the GPU, Luthier opts to create a “standalone executable”, which contains the original code as well as the instrumentation logic. The standalone executable will use ROCr dynamic loader’s features to link against the “static global variables” already loaded in the original code (e.g. variables annotated with __device__ or __managed__ in HIP code). This respects the ROCr APIs the most, not requiring any significant changes to the low level runtime. 2. Instead of calling pre-compiled device functions, Luthier embeds the optimized LLVM IR bitcode of the device instrumentation logic in its device code objects at tool compile time. This ensures that the HIP runtime will load the bitcode for free, allowing the Luthier runtime to do more optimizations on the instrumentation logic and adapt it to each instrumentation point. 3. (and 4.) Luthier defines a new concept called the “State Value Array (SVA)”, an array of 64 32-bit values that can be loaded into a single VGPR of a single wavefront (each wavefront has 64 threads). SVA is in charge of storing the instrumentation stack information, which is always allocated on top of what the application requested originally. It also keeps track of other wavefront-specific values. SVA is setup using a “Kernel preamble code”, which reads the SGPR arguments and saves them into the SVA’s VGPR lanes, right before reverting the SGPR arguments back to their original formation before the target kernel starts executing. SVA is stored in the applications’ dead/unused registers. In most recent hardware, the SVA can be either stored in a single A/VGPR, or spilled on a static point of the kernel stack with a single SGPR pointing to it so it can be loaded later using SCRATCH instructions.\nLuthier implements these designs by heavily leveraging the LLVM project and its AMDGPU backend, which we explain in more detail below:\n\nDisassembling, Lifting, and Inspecting Code Objects\nAs we mentioned earlier, Luthier instruments code by duplicating the target application’s code so that it can freely inject instrumentation logic inside it. To do this, Luthier takes in a single ELF/executable, and inspects its symbols using LLVM’s object utilities. It then identifies the kernels and device functions inside the ELF and disassembles them into LLVM MC instructions. MC is the machine code/assembler layer of LLVM. It is meant to represent “physical” instructions and registers. While disassembling code, Luthier identifies the branch instructions and identifies their targets if possible for later use.\nAfter disassembly is complete Luthier uses the obtained information from the ELF to “lift” it to LLVM Machine IR (MIR). MIR is LLVM’s representation used in its target-independent code generator (i.e. backends). It is a superset of LLVM MC, meaning an LLVM MC instruction can also be easily converted to an LLVM MIR instruction by reusing the same enums for opcodes and registers. The reason behind lifting to LLVM MIR is as follows: 1. MIR has very convenient ways for iterating over the code and querying properties regarding the code and the instructions which is otherwise costly to implement ourselves. 2. MIR’s high-level utilities allow for removing/adding things to the ELF otherwise not possible with ELF-modification frameworks e.g. ELFIO. This makes it easy to modify the kernel specifications and removing symbols that are not used in the target kernel and are unrelated. 3. The compiler machine passes only operate on the MIR representation. Lifting to MIR makes it easier to re-use analysis already available in LLVM’s code generator or add new ones.\nLLVM MIR consists of a set of “pseudo” instructions that is equivalent to a set of “physical”, target-specific instructions in MC; For example, S_ADD_U32 is a pseudo instruction in MIR which maps to S_ADD_U32_vi, S_ADD_32_gfx12, and so on. The same goes for some registers e.g. TTMP or M0. The primary reason for this indirection is to allow for different encoding/decodings on different targets. Although in theory, one should be able to use both pseudo and target-specific opcodes in the MIR, the AMDGPU backend primarily expects the pseudo variant to be present in the code; Hence during its tool library compilation, Luthier uses LLVM’s TableGen to read over all the opcodes/registers and creates an inverse mapping between target-specific opcode and their pseudo equivalents. Luthier then uses this table to convert the opcode and registers to their pseudo variants during runtime.\nJust like LLVM Bolt, Luthier requires the inspected ELFs to have full relocation information. This way, it is able to correctly lift things like reading the address of a symbol:\ns_getpc_b64 s[4:5]\ns_add_u32 s4, s4, func1@rel32@lo+4\ns_addc_u32 s5, s5, func1@rel32@lo+4\nIn the future, we will have to add analysis logic to at least reduce this restriction; For now, however, it is more than enough for a proof-of-concept. The result of the lifting process it a LiftedRepresentation, which is a mapping between the HSA/ROCr symbols of the ELF and their LLVM equivalent.\n\n\nWriting Luthier Tools\nA sample Luthier tool can look like the following:\n\nusing namespace luthier;\n\n/// Kernel instruction counter\n__attribute__((managed)) uint64_t Counter = 0;\n\n/// Macro marking the device module (code object) of this tool to\n/// be a Luthier tool\nMARK_LUTHIER_DEVICE_MODULE\n\nLUTHIER_HOOK_ANNOTATE countInstructionsVector(bool CountWaveFrontLevel) {\n  // Get the exec mask of the wavefront\n  unsigned long long int ExecMask = __builtin_amdgcn_read_exec();\n  // Get the position of the thread in the current wavefront (1-index)\n  const uint32_t LaneId = __lane_id() + 1;\n  // Get the first active thread id inside this wavefront\n  uint32_t FirstActiveThreadId = __ffsll(ExecMask);\n  // Get the number of active threads in this wavefront\n  uint32_t NumActiveThreads = __popcll(ExecMask);\n\n  // Have only the first active thread perform the atomic add\n  if (FirstActiveThreadId == LaneId) {\n    if (CountWaveFrontLevel) {\n      // Num threads can be zero when accounting for predicates off\n      if (NumActiveThreads &gt; 0) {\n        atomicAdd(&Counter, 1);\n      }\n    } else {\n      atomicAdd(&Counter, NumActiveThreads);\n    }\n  }\n}\n\nLUTHIER_EXPORT_HOOK_HANDLE(countInstructionsVector);\n\nLUTHIER_HOOK_ANNOTATE countInstructionsScalar() {\n  // Get the exec mask of the wavefront\n  unsigned long long int ExecMask = __builtin_amdgcn_read_exec();\n  // Overwrite the exec mask with one so that only a single thread is active\n  luthier::writeExec(1);\n  // Increment the counter by 1\n  atomicAdd(&Counter, 1);\n  // Restore the exec mask\n  luthier::writeExec(ExecMask);\n}\n\nLUTHIER_EXPORT_HOOK_HANDLE(countInstructionsScalar);\n\nstatic llvm::Error instrumentationLoop(InstrumentationTask &IT,\n                                       LiftedRepresentation &LR) {\n  // Create a constant bool indicating the CountWavefrontLevel value\n  auto *CountWavefrontLevelConstVal =\n      llvm::ConstantInt::getBool(LR.getContext(), CountWavefrontLevel);\n  unsigned int I = 0;\n  for (auto &[_, MF] : LR.functions()) {\n    for (auto &MBB : *MF) {\n      for (auto &MI : MBB) {\n        if (I &gt;= InstrBeginInterval && I &lt; InstrEndInterval) {\n          bool IsScalar =\n              llvm::SIInstrInfo::isSOP1(MI) || llvm::SIInstrInfo::isSOP2(MI) ||\n              llvm::SIInstrInfo::isSOPK(MI) || llvm::SIInstrInfo::isSOPC(MI) ||\n              llvm::SIInstrInfo::isSOPP(MI) || llvm::SIInstrInfo::isSMRD(MI);\n          bool IsLaneAccess =\n              MI.getOpcode() == llvm::AMDGPU::V_READFIRSTLANE_B32 ||\n              MI.getOpcode() == llvm::AMDGPU::V_READLANE_B32 ||\n              MI.getOpcode() == llvm::AMDGPU::V_WRITELANE_B32;\n          if (IsScalar || IsLaneAccess)\n            LUTHIER_RETURN_ON_ERROR(IT.insertHookBefore(\n                MI, LUTHIER_GET_HOOK_HANDLE(countInstructionsScalar)));\n          else\n            LUTHIER_RETURN_ON_ERROR(IT.insertHookBefore(\n                MI, LUTHIER_GET_HOOK_HANDLE(countInstructionsVector),\n                {CountWavefrontLevelConstVal}));\n        }\n        I++;\n      }\n    }\n  }\n  return llvm::Error::success();\n}\n\nstatic void\ninstrumentAllFunctionsOfLR(const hsa::LoadedCodeObjectKernel &Kernel) {\n  auto LR = lift(Kernel);\n  LUTHIER_REPORT_FATAL_ON_ERROR(LR.takeError());\n  LUTHIER_REPORT_FATAL_ON_ERROR(\n      instrumentAndLoad(Kernel, *LR, instrumentationLoop, \"instr_count\"));\n}\n\n/// ... Definition of Luthier HSA/ROCr callbacks goes here\nLuthier tools are written in HIP/C++. This example tool first calls the lift function on the kernel of interest to obtain an instance of LiftedRepresentation. It then uses the instrumentAndLoad function to instrument the kernel and load it into ROCr. The instrumentationLoop is a lambda function that allows direct modification of the LR (the one inside the instrumentAllFunctionsOfLR is immutable) and population of an InstrumentationTask by calling the insertHookBefore function, letting the tool know that we want to insert a call to an instrumentation function (also called hooks).\nSome special macros used in Luthier tools are as follows: 1. MARK_LUTHIER_DEVICE_MODULE has the following definition: c++     #define MARK_LUTHIER_DEVICE_MODULE \\     __attribute__((managed, used)) char __luthier_reserved = 0; This macro ensures the device module of the Luthier tool is easily identifiable by the Luthier runtime. Also the managed variable ensures that our device module will be loaded right before the first HIP kernel launch. In some instances where the target application directly uses the ROCr runtime, we enable eager loading in HIP using a special environment variable to ensure our tool device module is loaded right away. 2. The following macros are related to Luthier “hooks”: c++     #define LUTHIER_HOOK_ANNOTATE \\     __attribute__((device, used, annotate(\"luthier_hook\"))) extern \"C\" void     #define LUTHIER_EXPORT_HOOK_HANDLE(HookName) \\     __attribute__((global, used)) extern \"C\" void __luthier_hook_handle_##HookName(){};         #define LUTHIER_GET_HOOK_HANDLE(HookName)     \\     reinterpret_cast&lt;const void *&gt;(__luthier_hook_handle_##HookName) Hook is an special instrumentation function that can be called right before an instruction of a application. It will be inlined inside the call site while ensuring correct register and stack usage. A hook can also call other device functions and they don’t have to be inlined themselves. Hooks can take arguments to Register values and LLVM Constants (e.g. countInstructionsVector takes a bool argument which is setup inside instrumentationLoop).\nAs device functions in HIP don't get a handle accessible from the host code, the `LUTHIER_EXPORT_HOOK_HANDLE` macro creates a host-accessible dummy handle that can be used by the host logic using  `LUTHIER_GET_HOOK_HANDLE`. The `__attribute__(used)` ensures the compiler doesn't optimize these symbols away, as they are needed for Luthier's correct functionality.\n\nLuthier doesn’t allow usage of inline assembly inside its hooks or any of its called device functions as their register usage cannot be analyzed until the very last step of code generation; Instead, it introduces a new concept called a “Luthier Intrinsic”. Luthier intrinsics are meant to behave like LLVM intrinsics, as they end up translating to a sequence of low-level code. Luthier itself has a set of pre-implemented intrinsics, including luthier::readReg and luthier::writeReg, which read and write values to the registers. In this example, we use the luthier::writeExec intrinsic: ```c++ #define LUTHIER_INTRINSIC_ANNOTATE\nattribute((device, noinline, annotate(“luthier_intrinsic”)))\ntemplate  attribute((device, always_inline)) void doNotOptimize(T const &Value) { asm volatile(“” : : “X”(Value) : “memory”); }\nLUTHIER_INTRINSIC_ANNOTATE void writeExec(uint64_t Val) { doNotOptimize(Val); } ``TheLUTHIER_INTRINSIC_ANNOTATEmacro annotates any Luthier intrinsic device functions used in hooks. Since HIP doesn't allow calls toexterndevice functions (due to lack of support for device functions in the ROCr loader), we need to emit a body for it and ensure the compiler doesn't optimize any of the arguments away and the intrinsic calls away. ThedoNotOptimize` template device function is used for exactly this purpose, which is borrowed from Google Benchmark Framework.\n\nAs mentioned earlier, Luthier does not rely on inserting calls to a pre-compiled instrumentation device function. Luthier instead, embeds a pre-processed LLVM IR bitcode that it embeds inside the device code object. At tool compile time, Luthier utilizes a custom compiler plugin, implemented as follows:\n//===-- EmbedInstrumentationModuleBitcodePass.cpp -------------------------===//\n//\n//===----------------------------------------------------------------------===//\n///\n/// \\file\n/// This file implements the \\c luthier::EmbedInstrumentationModuleBitcode pass,\n/// used by Luthier tools to preprocess instrumentation modules and embedding\n/// them inside device code objects.\n//===----------------------------------------------------------------------===//\n\n#include \"EmbedInstrumentationModuleBitcodePass.hpp\"\n\n#include \"llvm/Passes/PassPlugin.h\"\n#include &lt;llvm/ADT/StringExtras.h&gt;\n#include &lt;llvm/Analysis/ValueTracking.h&gt;\n#include &lt;llvm/Bitcode/BitcodeWriterPass.h&gt;\n#include &lt;llvm/Demangle/Demangle.h&gt;\n#include &lt;llvm/IR/Module.h&gt;\n#include &lt;llvm/Passes/PassBuilder.h&gt;\n#include &lt;llvm/Support/AMDGPUAddrSpace.h&gt;\n#include &lt;llvm/Support/raw_ostream.h&gt;\n#include &lt;llvm/Transforms/Utils/Cloning.h&gt;\n#include &lt;llvm/Transforms/Utils/ModuleUtils.h&gt;\n\n#undef DEBUG_TYPE\n#define DEBUG_TYPE \"luthier-embed-optimized-bitcode-pass\"\n\nnamespace luthier {\n\n// TODO: Import these variables as well as the static functions from\n//  Luthier proper once the separate compilation issue is resolved\n\nstatic constexpr const char *ReservedManagedVar = \"__luthier_reserved\";\n\nstatic constexpr const char *HookAttribute = \"luthier_hook\";\n\nstatic constexpr const char *IntrinsicAttribute = \"luthier_intrinsic\";\n\nstatic constexpr const char *HipCUIDPrefix = \"__hip_cuid_\";\n\n/// Builds a \\c llvm::CallInst invoking the intrinsic indicated by\n/// \\p IntrinsicName at the instruction position indicated by the \\p Builder\n/// with the given \\p ReturnType and \\p Args\n/// \\tparam IArgs Arguments passed to the intrinsic; Can be either a scalar\n/// or a reference to a \\c llvm::Value\n/// \\param M the instrumentation module where the intrinsic will be inserted to\n/// \\param Builder the instruction builder used to build the call instruction\n/// \\param IntrinsicName the name of the intrinsic\n/// \\param ReturnType the return type of the intrinsic call instruction\n/// \\param Args the arguments to the intrinsic function\n/// \\return a \\c llvm::CallInst to the intrinsic function\nllvm::CallInst *insertCallToIntrinsic(llvm::Module &M,\n                                      llvm::IRBuilderBase &Builder,\n                                      llvm::StringRef IntrinsicName,\n                                      llvm::Type &ReturnType) {\n  auto &LLVMContext = Builder.getContext();\n  /// Construct the intrinsic's LLVM function type and its argument value\n  /// list\n  auto *IntrinsicFuncType = llvm::FunctionType::get(&ReturnType, false);\n  // Format the readReg intrinsic function name\n  std::string FormattedIntrinsicName{IntrinsicName};\n  llvm::raw_string_ostream IntrinsicNameOS(FormattedIntrinsicName);\n  // Format the intrinsic function name\n  IntrinsicNameOS &lt;&lt; \".\";\n  IntrinsicFuncType-&gt;getReturnType()-&gt;print(IntrinsicNameOS);\n  // Create the intrinsic function in the module, or get it if it already\n  // exists\n  auto ReadRegFunc = M.getOrInsertFunction(\n      FormattedIntrinsicName, IntrinsicFuncType,\n      llvm::AttributeList().addFnAttribute(LLVMContext, IntrinsicAttribute,\n                                           IntrinsicName));\n\n  return Builder.CreateCall(ReadRegFunc);\n}\n\n/// Given a function's mangled name \\p MangledFuncName,\n/// partially demangles it and returns the base function name with its\n/// namespace prefix \\n\n/// For example given a demangled function name int a::b::c&lt;int&gt;(), this\n/// method returns a::b::c\n/// \\param MangledFuncName the mangled function name\n/// \\return the name of the function with its namespace prefix\nstatic std::string\ngetDemangledFunctionNameWithNamespace(llvm::StringRef MangledFuncName) {\n  // Get the name of the function, without its template arguments\n  llvm::ItaniumPartialDemangler Demangler;\n  // Ensure successful partial demangle operation\n  if (Demangler.partialDemangle(MangledFuncName.data()))\n    llvm::report_fatal_error(\"Failed to demangle the intrinsic name \" +\n                             MangledFuncName + \".\");\n  // Output string\n  std::string Out;\n  // Output string's ostream\n  llvm::raw_string_ostream OS(Out);\n\n  size_t BufferSize;\n  char *FuncNamespaceBegin =\n      Demangler.getFunctionDeclContextName(nullptr, &BufferSize);\n  if (strlen(FuncNamespaceBegin) != 0) {\n    OS &lt;&lt; FuncNamespaceBegin;\n    OS &lt;&lt; \"::\";\n  }\n  char *FuncNameBase = Demangler.getFunctionBaseName(nullptr, &BufferSize);\n  OS &lt;&lt; FuncNameBase;\n  return Out;\n}\n\n/// Groups the set of annotated values in \\p M into instrumentation\n/// hooks and intrinsics of instrumentation hooks \\n\n/// \\note This function should get updated as Luthier's programming model\n/// gets updated\n/// \\param [in] M Module to inspect\n/// \\param [out] Hooks a list of hook functions found in \\p M\n/// \\param [out] Intrinsics a list of intrinsics found in \\p M\n/// \\return any \\c llvm::Error encountered during the process\nstatic llvm::Error\ngetAnnotatedValues(const llvm::Module &M,\n                   llvm::SmallVectorImpl&lt;llvm::Function *&gt; &Hooks,\n                   llvm::SmallVectorImpl&lt;llvm::Function *&gt; &Intrinsics) {\n  const llvm::GlobalVariable *V =\n      M.getGlobalVariable(\"llvm.global.annotations\");\n  if (V == nullptr)\n    return llvm::Error::success();\n  const llvm::ConstantArray *CA = cast&lt;llvm::ConstantArray&gt;(V-&gt;getOperand(0));\n  for (llvm::Value *Op : CA-&gt;operands()) {\n    auto *CS = cast&lt;llvm::ConstantStruct&gt;(Op);\n    // The first field of the struct contains a pointer to the annotated\n    // variable.\n    llvm::Value *AnnotatedVal = CS-&gt;getOperand(0)-&gt;stripPointerCasts();\n    if (auto *Func = llvm::dyn_cast&lt;llvm::Function&gt;(AnnotatedVal)) {\n      // The second field contains a pointer to a global annotation string.\n      auto *GV =\n          cast&lt;llvm::GlobalVariable&gt;(CS-&gt;getOperand(1)-&gt;stripPointerCasts());\n      llvm::StringRef Content;\n      llvm::getConstantStringInfo(GV, Content);\n      if (Content == HookAttribute) {\n        Hooks.push_back(Func);\n        LLVM_DEBUG(llvm::dbgs() &lt;&lt; \"Found hook \" &lt;&lt; Func-&gt;getName() &lt;&lt; \".\\n\");\n      } else if (Content == IntrinsicAttribute) {\n        Intrinsics.push_back(Func);\n        LLVM_DEBUG(llvm::dbgs()\n                   &lt;&lt; \"Found intrinsic \" &lt;&lt; Func-&gt;getName() &lt;&lt; \".\\n\");\n      }\n    }\n  }\n  return llvm::Error::success();\n}\n\nllvm::PreservedAnalyses\nEmbedInstrumentationModuleBitcodePass::run(llvm::Module &M,\n                                           llvm::ModuleAnalysisManager &AM) {\n  if (M.getGlobalVariable(\"llvm.embedded.module\", /*AllowInternal=*/true))\n    llvm::report_fatal_error(\n        \"Attempted to embed bitcode twice. Are you passing -fembed-bitcode?\",\n        /*gen_crash_diag=*/false);\n\n  llvm::Triple T(M.getTargetTriple());\n  // Only operate on the AMD GCN code objects\n  if (T.getArch() != llvm::Triple::ArchType::amdgcn)\n    return llvm::PreservedAnalyses::all();\n\n  // Clone the module in order to preprocess it + not interfere with normal\n  // HIP compilation\n  auto ClonedModule = llvm::CloneModule(M);\n\n  // Extract all the hooks and intrinsics\n  llvm::SmallVector&lt;llvm::Function *, 4&gt; Hooks;\n  llvm::SmallVector&lt;llvm::Function *, 4&gt; Intrinsics;\n  if (auto Err = getAnnotatedValues(*ClonedModule, Hooks, Intrinsics))\n    llvm::report_fatal_error(std::move(Err), true);\n\n  // Remove the annotations variable from the Module now that it is processed\n  auto AnnotationGV =\n      ClonedModule-&gt;getGlobalVariable(\"llvm.global.annotations\");\n  if (AnnotationGV) {\n    AnnotationGV-&gt;dropAllReferences();\n    AnnotationGV-&gt;eraseFromParent();\n  }\n\n  // Remove the llvm.used and llvm.compiler.use variable list\n  for (const auto &VarName : {\"llvm.compiler.used\", \"llvm.used\"}) {\n    auto LLVMUsedVar = ClonedModule-&gt;getGlobalVariable(VarName);\n    if (LLVMUsedVar != nullptr) {\n      LLVMUsedVar-&gt;dropAllReferences();\n      LLVMUsedVar-&gt;eraseFromParent();\n    }\n  }\n\n  // Give each Hook function a \"hook\" attribute\n  for (auto Hook : Hooks) {\n    // TODO: remove the always inline attribute once Hooks support the anyreg\n    // calling convention\n    Hook-&gt;addFnAttr(HookAttribute);\n    Hook-&gt;removeFnAttr(llvm::Attribute::OptimizeNone);\n    Hook-&gt;removeFnAttr(llvm::Attribute::NoInline);\n    Hook-&gt;addFnAttr(llvm::Attribute::AlwaysInline);\n  }\n  // Remove the body of each intrinsic function and make them extern\n  // Also demangle the name and format it similar to LLVM intrinsics\n  for (auto Intrinsic : Intrinsics) {\n    Intrinsic-&gt;deleteBody();\n    Intrinsic-&gt;setComdat(nullptr);\n    llvm::StringRef MangledIntrinsicName = Intrinsic-&gt;getName();\n    // Format the intrinsic name\n    std::string FormattedIntrinsicName;\n    llvm::raw_string_ostream FINOS(FormattedIntrinsicName);\n    std::string DemangledIntrinsicName =\n        getDemangledFunctionNameWithNamespace(MangledIntrinsicName);\n    FINOS &lt;&lt; DemangledIntrinsicName;\n    // Add the output type if it's not void\n    auto *ReturnType = Intrinsic-&gt;getReturnType();\n    if (!ReturnType-&gt;isVoidTy()) {\n      FINOS &lt;&lt; \".\";\n      ReturnType-&gt;print(FINOS);\n    }\n    // Add the argument types\n    for (const auto &Arg : Intrinsic-&gt;args()) {\n      FINOS &lt;&lt; \".\";\n      Arg.getType()-&gt;print(FINOS);\n    }\n    Intrinsic-&gt;addFnAttr(IntrinsicAttribute, DemangledIntrinsicName);\n    Intrinsic-&gt;setName(FormattedIntrinsicName);\n  }\n\n  // Remove all kernels that are meant to serve as a host handle\n  for (auto &F : llvm::make_early_inc_range(ClonedModule-&gt;functions())) {\n\n    if (F.getCallingConv() == llvm::CallingConv::AMDGPU_KERNEL) {\n      F.dropAllReferences();\n      F.eraseFromParent();\n    }\n  }\n\n  // Convert all global variables to extern, remove any managed variable\n  // initializers\n  // Remove any unnecessary variables (e.g. \"llvm.metadata\")\n  // Extract the CUID for identification\n  for (auto &GV : llvm::make_early_inc_range(ClonedModule-&gt;globals())) {\n    auto GVName = GV.getName();\n    if (GVName.ends_with(\".managed\") || GVName == ReservedManagedVar ||\n        GV.getSection() == \"llvm.metadata\") {\n      GV.dropAllReferences();\n      GV.eraseFromParent();\n    } else if (!GVName.starts_with(HipCUIDPrefix)) {\n      GV.setInitializer(nullptr);\n      GV.setLinkage(llvm::GlobalValue::ExternalLinkage);\n      GV.setVisibility(llvm::GlobalValue::DefaultVisibility);\n      GV.setDSOLocal(false);\n    }\n  }\n\n  LLVM_DEBUG(llvm::dbgs() &lt;&lt; \"Embedded Module \" &lt;&lt; ClonedModule-&gt;getName()\n                          &lt;&lt; \" dump: \");\n  LLVM_DEBUG(ClonedModule-&gt;print(llvm::dbgs(), nullptr));\n\n  llvm::SmallVector&lt;char&gt; Data;\n  llvm::raw_svector_ostream OS(Data);\n  auto PA = llvm::BitcodeWriterPass(OS).run(*ClonedModule, AM);\n\n  llvm::embedBufferInModule(\n      M, llvm::MemoryBufferRef(llvm::toStringRef(Data), \"ModuleData\"),\n      \".llvmbc\");\n\n  return PA;\n}\n} // namespace luthier\n\nllvm::PassPluginLibraryInfo getEmbedLuthierBitcodePassPluginInfo() {\n  const auto Callback = [](llvm::PassBuilder &PB) {\n    PB.registerOptimizerLastEPCallback(\n        [](llvm::ModulePassManager &MPM, llvm::OptimizationLevel Opt) {\n          MPM.addPass(luthier::EmbedInstrumentationModuleBitcodePass());\n        });\n  };\n\n  return {LLVM_PLUGIN_API_VERSION, \"pre-process-and-embed-luthier-bitcode\",\n          LLVM_VERSION_STRING, Callback};\n}\n\n#ifndef LLVM_LUTHIER_TOOL_COMPILE_PLUGIN_LINK_INTO_TOOLS\nextern \"C\" LLVM_ATTRIBUTE_WEAK ::llvm::PassPluginLibraryInfo\nllvmGetPassPluginInfo() {\n  return getEmbedLuthierBitcodePassPluginInfo();\n}\n#endif\nThe compiler plugin peforms the following actions: 1. It only operates on AMDGPU code objects and not the host executable. 2. It clones the device code’s llvm::Module. It doesn’t interfer with the original compilation as the next steps will cause Clang/LLVM to be unhappy. 3. Using annotations done in the tool’s source code, we identify the hooks and intrinsics inside the device module, and then remove the llvm::GlobalVariable that holds the annotated values. 4. Removes the llvm.compiler.use and llvm.used global variables, as they don’t matter in the instrumentation process. 5. Gives each hook device function a “luthier_hook” attribute so that they are easily identified later on. They are also given forced inlined attributes, as hooks are always meant to be inlined at the instrumentation point. 6. Removes the body of all Luthier intrinsic functions, and re-format their CXX Itanium mangled names to look similar to LLVM intrinsics. 7. Removes the “dummy” hook handles defined using the LUTHIER_EXPORT_HOOK_HANDLE macro. 8. Makes all the global variables extern, as the non-cloned module will be the one defining them. 9. Finally, the cloned module will be embedded inside a non-loadable section of the device code object called .llvmbc.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Final Project- Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions"
    ]
  },
  {
    "objectID": "blogs/matin/2024-12-12-Final-Project.html#generating-instrumented-code",
    "href": "blogs/matin/2024-12-12-Final-Project.html#generating-instrumented-code",
    "title": "Final Project- Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions",
    "section": "Generating Instrumented Code",
    "text": "Generating Instrumented Code\nAt runtime, calling the function luthier::instrumentAndLoad will generate a newly instrumented code object and loads it into the ROCm runtime for execution. It first clones the LiftedRepresentation to allow it to be writable (the original copy is cached by Luthier) and then runs the passed lambda (called the mutator) on the LiftedRepresentation. The mutator allows the LiftedRepresentation to be directly modified using LLVM CodeGen APIs, or one can also use the InstrumentationTask to insert calls to hooks before instructions.\nAfter the mutator is done executing, Luthier’s instrumentation code generation executes, which is as follows:\n  for (auto &[LCOHandle, LCOModule] : LR) {\n    hsa::LoadedCodeObject LCO(LCOHandle);\n    auto Agent = LCO.getAgent();\n    LUTHIER_RETURN_ON_ERROR(Agent.takeError());\n\n    auto &TM = *LR.getTM(LCOHandle);\n    // Load the bitcode of the instrumentation module into the\n    // Lifted Representation's context\n    std::unique_ptr&lt;llvm::Module&gt; IModule;\n    LUTHIER_RETURN_ON_ERROR(Task.getModule()\n                                .readBitcodeIntoContext(LR.getContext(), *Agent)\n                                .moveInto(IModule));\n    // Instantiate the Module PM and analysis in charge of running the\n    // IR pipeline for the instrumentation module\n    // We keep them here because we will need the analysis done at the IR\n    // stage at the code generation stage, which for now we have to use\n    // the legacy pass manager for\n    llvm::LoopAnalysisManager ILAM;\n    llvm::FunctionAnalysisManager IFAM;\n    llvm::CGSCCAnalysisManager ICGAM;\n    llvm::ModuleAnalysisManager IMAM;\n    llvm::ModulePassManager IPM;\n\n    // Instantiate the Legacy PM for running the modified codegen pipeline\n    // on the instrumentation module and MMI\n    // We allocate this on the heap to have the most control over its lifetime,\n    // as if it goes out of scope it will also delete the instrumentation\n    // MMI\n    auto LegacyIPM = new llvm::legacy::PassManager();\n    // Instrumentation module MMI wrapper pass, which will house the final\n    // generate instrumented code\n    auto *IMMIWP = new llvm::MachineModuleInfoWrapperPass(&TM);\n\n    // Create a module analysis manager for the target code\n    llvm::ModuleAnalysisManager TargetMAM;\n    // Create a new Module pass manager, in charge of running the entire\n    // pipeline\n    llvm::ModulePassManager TargetMPM;\n    // Add the pass instrumentation analysis as it is required by the new PM\n    TargetMAM.registerPass(\n        [&]() { return llvm::PassInstrumentationAnalysis(); });\n    // Add the MMI Analysis pass, pointing to the target app's lifted MMI\n    TargetMAM.registerPass(\n        [&]() { return llvm::MachineModuleAnalysis(LCOModule.second); });\n    // Add the instrumentation PM analysis\n    TargetMAM.registerPass([&]() {\n      return IModulePMAnalysis(*IModule, IPM, IMAM, ILAM, IFAM, ICGAM);\n    });\n    // Add the LR Analysis pass\n    TargetMAM.registerPass([&]() { return LiftedRepresentationAnalysis(LR); });\n    // Add the LCO Analysis pass\n    TargetMAM.registerPass([&]() { return LoadedCodeObjectAnalysis(LCO); });\n    // Add the LR Register Liveness pass\n    TargetMAM.registerPass([&]() { return LRRegLivenessAnalysis(); });\n    // Add the LR Callgraph analysis pass\n    TargetMAM.registerPass([&]() { return LRCallGraphAnalysis(); });\n    // Add the MMI-wide Slot indexes analysis pass\n    TargetMAM.registerPass([&]() { return MMISlotIndexesAnalysis(); });\n    // Add the State Value Array storage and load analysis pass\n    TargetMAM.registerPass(\n        [&]() { return LRStateValueStorageAndLoadLocationsAnalysis(); });\n    // Add the Function Preamble Descriptor Analysis pass\n    TargetMAM.registerPass(\n        [&]() { return FunctionPreambleDescriptorAnalysis(); });\n    // Add the IR pipeline for the instrumentation module\n    TargetMPM.addPass(\n        RunIRPassesOnIModulePass(Task, IntrinsicsProcessors, TM, *IModule));\n    // Add the MIR pipeline for the instrumentation module\n    TargetMPM.addPass(\n        RunMIRPassesOnIModulePass(TM, *IModule, *IMMIWP, *LegacyIPM));\n    // Add the kernel pre-amble emission pass\n    TargetMPM.addPass(PrePostAmbleEmitter());\n    // Add the lifted representation patching pass\n    TargetMPM.addPass(\n        PatchLiftedRepresentationPass(*IModule, IMMIWP-&gt;getMMI()));\n\n    TargetMPM.run(LCOModule.first, TargetMAM);\n    // TODO: remove this once the new MMI makes it to LLVM master\n    delete LegacyIPM;\n  };\n\nCreate an instrumentation llvm::Module (i.e. IModule) and read the bitcode we embedded in the previous step into it.\nCreate two separate pass managers: One in charge of managing the target application’s MIR and analysis passes on it, and one in charge of generating IR/MIR for the instrumentation logic and their analysis passes.\nAdd the following analysis to the Target App PM:\n\nLRRegLivenessAnalysis: Which analyzes the register liveness of the application using data flow analysis covered in class, though in some cases this is not enough to ensure liveness for VGPRs. I’m currently working on creating an alternate CFG to run liveness analysis on for VGPRs that also take into account changes in the EXEC mask value.\nLRCallGraphAnalysis, which naively recovers the call graph of the target application. This is used in conjunction with the register liveness analysis, ensuring correct register reuse in callee instrumentation points.\nMMISlotIndexesAnalysis assigns a slot index to each instruction and basic block inside the target application. It is used with the next analysis, LRStateValueStorageAndLoadLocationsAnalysis.\nLRStateValueStorageAndLoadLocationsAnalysis attempts to find a place to store the SVA (defined previously) with unused/dead registers. Luthier has many ways to store the SVA, depending on the target of choice:\n\nAn Unused VGPR.\n(If target supports using AGPRs as operands in vector instructions, post-gfx90A) an unused AGPR.\n(pre-gfx908) two unused AGPR, with one serving as a spill spot for the app’s live VGPR.\n(pre-gfx908) one unused AGPR, with 3 unused SGPRs, with the AGPR holding the SVA, two SGPRs holding the correct flat scratch register value, and a third SGPR to point to the bottom of the instrumentation stack.\n(post MI300, architected flat scratch) single SGPR that points to the bottom of the instrumentation stack.\n(pre-MI300, absolute flat scratch) 3 unused SGPRs, with two SGPRs holding the correct flat scratch register value, and a third SGPR to point to the bottom of the instrumentation stack. Each storage scheme has to have code for loading, and storing the SVA, as well as code to move itself to another storage scheme, and has enough registers/resources to do so without clobbering any of the application registers (especially the SCC bit). To minimize additional instructions injected inside the target application, this analysis first attempts to find a fixed storage for the SVA. In extreme cases where the attempt is not successful, it will then figure out where to store the SVA at each slot index of the application using the register liveness analysis. If, at any point in this analysis, no suitable SVA storage is found, instrumentation fails, as without an SVA there is no way to recover the instrumentation stack. Besides storing the SVA, this pass also decides where the SVA will be loaded at each “injected payload” (defined later in IModuleIRGeneratorPass). The SVA will be then kept at a fixed location and will not be spilled.\n\nFunctionPreambleDescriptorAnalysis, LiftedRepresentationAnalysis, and LoadedCodeObjectAnalysis are storage for the preamble descriptor, the LiftedRepresentation and the LoadedCodeObject being operated on. The FunctionPreambleDescriptor aggregates information for the required resources for instrumentation function; For example, if usage of stack is detected in the instrumentation code, the preamble descriptor will be signaled to emit code that sets up the SVA and access to instrumentation stack.\n\nRuns IR passes on the instrumentation Module, which runs the following passes on the IModule:\n\nIModuleIRGeneratorPass takes the InstrumentationTask description and peforms the following tasks for each instrumentation point (target application’s instruction):\n\nCreates a new llvm::Function with the C calling convention with no input or output arguments and a Naked attribute to prevent a frame to emitted for them (we will emit a custom frame ourselves). We call these functions an “Injected Payload”.\nInside each injected payload it inserts llvm::CallInsts to the hooks with the specified arguments; Register values are done with calls to luthier::ReadReg intrinsics; For constant values, the llvm::Constants passed to the instrumentation task will be directly used.\n\nThe normal LLVM IR optimization pipeline.\nProcessIntrinsicsAtIRLevelPass applies the IR lowering callback for each Luthier intrinsic. The callback:\n\nReplaces each call to a Luthier intrinsic with a call to a “dummy inline assembly” inside the IR\nEnforces the type of register (i.e. S/A/V) its inputs and outputs are required to be.\nCan also analyse the arguments passed to it, and based on them request access to a physical register. As inline assembly remains unchanged during both ISEL and CodeGen pipelines, we use the inline assembly string as a placeholder to identify it later done the code generation pipeline.\n\n\nRuns the Code Gen Passes on the instrumentation Module:\n\nRun the LLVM ISEL passes on the IModule to generate the IMMI, the instrumentation module’s MachineModuleInfo which will house the MIR of the IModule.\nPhysicalRegAccessVirtualizationPass essentially generates valid MIR code to enforce register constraints:\n\nEnsures the SVA is kept in a single place by declaring it as a live-in at the entry block, and adding the SVA VGPR as an implicit operand to all return instructions in all return blocks. They count as a valid def and use, respectively.\nThe set of registers live at the insertion point, as well as registers accessed by the injected payload will be divided into 32-bit registers; For example, if s[0:1] is live, then we divide it into s0 and s1. If s0_lo is live, then we set s0 to be live.\nIf registers s0, s1, s2, s3, FS_LO, FS_HI, s32 are live-ins, then they are copied into lanes of the SVA on entry in the InjectedPayloadPEIPass, and then copied back to their original location at the end of all exit blocks. These registers always have a fixed spill slot, as the injected payload expects that these registers are always free for its frame usage.\nAll other live-in registers and accessed physical registers will be copied to a new virtual register in the entry block, and then copied back from the virtual register to their original place in all return blocks. This basically allows the register allocator to spill the registers whenever it deems it necessary, reducing the number of times the register is spilled.\nAs accessed physical registers are now in virtual registers, an llvm::SSAUpdator is used for each physical register to manifest new virtual register values for them inside each llvm::MachineBasicBlock. That way, intrinsics that requested access to these physical registers can get access to their virtual registers.\n\nIntrinsicMIRLoweringPass runs the MIR lowering stage of Luthier intrinsics. In this stage, each intrinsic will run a callback with the information saved from its IR lowering stage that will result in a sequence of MI instructions in valid SSA form that operate on their input/output registers. These callbacks can request access to the virtual register associated with the physical registers they have requested before. They can also signal IntrinsicMIRLoweringPass that a physical register’s virtual register is written to and requires an update to retain the SSA form of the instrumentation MIR.\nNormal LLVM Machine Passes are run, with the InjectedPayloadPEIPass run right before the function prologue/epilogue insertion pass. This pass will analyse each injected payload after register allocation/rewrite to detect explicit uses of the SVA register. If an explicit use is detected, then it will emit code to load and store the SVA on entry and exit respectively. If stack usage in the injected payload is detected the FunctionPreambleDescriptor will be signaled that the instrumented kernel requires stack usage and must be setup in the beginning of the kernel.\n\nRuns the PrePostAmbleEmitter pass, which takes the information gathered in FunctionPreambleDescriptor and LRStateValueStorageAndLoadLocationsAnalysis to do the following on every kernel:\n\nDetermines whether SVA setup is required; If not, it won’t do anything to the code.\nIf SVA setup is required, it will see if this was due to requiring access to scratch; If so, it forcably enables it, emits code that sets up scratch access, and then saves it in fixed lanes of the SVA. It then proceeds to restore the original SGPR kernel arguments back to their original place.\n\nRuns the PatchLiftedRepresentationPass, which will “patch” the final instrumentation logic before their instrumentation point, copying over the generated instructions into the target application. Note that we have to keep the “target” application and “instrumentation” application instructions separate, as there is no good way of enforcing immutability of the “target” application instructions.\n\nThe final result is an instrumented llvm::MachineModuleInfo which can then be passed to LLVM’s AsmPrinter pass to generate a relocatable file in memory. The relocatable is then linked to an executable using AMD CoMGR, which can then be loaded and run inside the ROCm runtime.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Final Project- Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions"
    ]
  },
  {
    "objectID": "blogs/matin/2024-12-12-Final-Project.html#new-feature-lowering-amdgpu-intrinsics-that-depends-on-hidden-kernel-arguments",
    "href": "blogs/matin/2024-12-12-Final-Project.html#new-feature-lowering-amdgpu-intrinsics-that-depends-on-hidden-kernel-arguments",
    "title": "Final Project- Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions",
    "section": "New Feature: Lowering AMDGPU Intrinsics That Depends On Hidden Kernel Arguments",
    "text": "New Feature: Lowering AMDGPU Intrinsics That Depends On Hidden Kernel Arguments\nLet’s say I want to access the blockDim of a kernel inside a hook. On NVIDIA GPUs, accessing this property is very easy; There are dedicated registers the hardware can query to obtain these values; On AMD GPUs, however, there are no dedicated registers for blockDim; Instead, they are either passed as arguments to the SGPRs or as “hidden arguments” in the kernel argument buffer, as stated by the AMDGPU backend docs.\nThis can be easily observed using Compiler Explorer; The following (very incorrect) HIP code:\n__global__ void square(int* array, int n) {\n    int tid = blockDim.x;\n    if (tid &lt; n)\n        array[tid] = array[tid] * array[tid];\n}\nwill result in the following LLVM IR:\ndefine protected amdgpu_kernel void @square(int*, int)(ptr addrspace(1) nocapture %array.coerce, i32 %n) local_unnamed_addr {\nentry:\n  %0 = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()\n  %1 = getelementptr inbounds i16, ptr addrspace(4) %0, i64 6\n  %2 = load i16, ptr addrspace(4) %1, align 4\n  %conv.i.i = zext i16 %2 to i32\n  %cmp = icmp slt i32 %conv.i.i, %n\n  br i1 %cmp, label %if.then, label %if.end\n\nif.then:\n  %idxprom = zext i16 %2 to i64\n  %arrayidx = getelementptr inbounds i32, ptr addrspace(1) %array.coerce, i64 %idxprom\n  %3 = load i32, ptr addrspace(1) %arrayidx, align 4\n  %mul = mul nsw i32 %3, %3\n  store i32 %mul, ptr addrspace(1) %arrayidx, align 4\n  br label %if.end\n\nif.end:\n  ret void\n}\n\ndeclare align 4 ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr() #1\nIn the first 3 lines, we can see a call to llvm.amdgcn.implicitarg.ptr. This call will result in the pointer to the beginning of the hidden argument to be moved to value %0. Note that if we leave this LLVM intrinsic alone it will have undefined behavior according to the AMDGPU docs. If we need to find a way to replace it with a Luthier intrinsic so that we can lower it correctly ourselves, we should be in the clear.\nAnother point of note is the constant 6 offset in the second instruction %1 = getelementptr inbounds i16, ptr addrspace(4) %0, i64 6; We can tell from this that hidden kernel arguments have “fixed” offset for each entry, and I confirmed this by looking at the AMDGPUMetadataStreamer code:\n  auto *Int64Ty = Type::getInt64Ty(Func.getContext());\n  auto *Int32Ty = Type::getInt32Ty(Func.getContext());\n  auto *Int16Ty = Type::getInt16Ty(Func.getContext());\n\n  Offset = alignTo(Offset, ST.getAlignmentForImplicitArgPtr());\n  emitKernelArg(DL, Int32Ty, Align(4), \"hidden_block_count_x\", Offset, Args);\n  emitKernelArg(DL, Int32Ty, Align(4), \"hidden_block_count_y\", Offset, Args);\n  emitKernelArg(DL, Int32Ty, Align(4), \"hidden_block_count_z\", Offset, Args);\n\n  emitKernelArg(DL, Int16Ty, Align(2), \"hidden_group_size_x\", Offset, Args);\n  emitKernelArg(DL, Int16Ty, Align(2), \"hidden_group_size_y\", Offset, Args);\n  emitKernelArg(DL, Int16Ty, Align(2), \"hidden_group_size_z\", Offset, Args);\n\n  emitKernelArg(DL, Int16Ty, Align(2), \"hidden_remainder_x\", Offset, Args);\n  emitKernelArg(DL, Int16Ty, Align(2), \"hidden_remainder_y\", Offset, Args);\n  emitKernelArg(DL, Int16Ty, Align(2), \"hidden_remainder_z\", Offset, Args);\nAn offset of 6 i16 is 3 i32s, which indeed means we’re trying to access the hidden_group_size_x argument.\n\nImplementation\nFirst I had to create a Luthier intrinsic, called luthier::implicitArgPtr. The frontend would look like the following:\n/// \\return the address of the implicit argument segment\nLUTHIER_INTRINSIC_ANNOTATE uint32_t* implicitArgPtr() {\n  uint32_t* Out;\n  doNotOptimize(Out);\n  return Out;\n}\nThis intrinsic, when called, will return a pointer to the beginning of the hidden argument segment.\nThe next step was to modify the Luthier compiler plugin to replace all uses of llvm.amdgcn.implicitarg.ptr with luthier::implicitArgPtr, which was done by adding the following code snippet right before embedding the modified bitcode:\n  auto *Int32Type = llvm::Type::getInt32Ty(LLVMCtx);\n  auto *Int32Ptr =\n      llvm::PointerType::get(Int32Type, llvm::AMDGPUAS::CONSTANT_ADDRESS);\n  // Replace llvm.amdgcn.workgroup.id intrinsics with the luthier-equivalent\n  // Remove all kernels that are meant to serve as a host handle\n  for (auto &F : llvm::make_early_inc_range(ClonedModule-&gt;functions())) {\n    for (const auto &[LLVMName, LuthierName, ReturnType] :\n         std::initializer_list&lt;\n             std::tuple&lt;const char *, const char *, llvm::Type *&gt;&gt;{\n             {\"llvm.amdgcn.workgroup.id.x\", \"luthier::workgroupIdX\", Int32Type},\n             {\"llvm.amdgcn.workgroup.idx.y\", \"luthier::workgroupIdY\",\n              Int32Type},\n             {\"llvm.amdgcn.workgroup.idx.z\", \"luthier::workgroupIdZ\",\n              Int32Type},\n             {\"llvm.amdgcn.implicitarg.ptr\", \"luthier::implicitArgPtr\",\n              Int32Ptr}}) {\n      if (F.getName().starts_with(LLVMName)) {\n        for (auto *User : llvm::make_early_inc_range(F.users())) {\n          auto *CallInst = llvm::dyn_cast&lt;llvm::CallInst&gt;(User);\n          llvm::IRBuilder&lt;&gt; Builder(CallInst);\n          auto *LuthierIntrinsicCall = insertCallToIntrinsic(\n              *ClonedModule, Builder, LuthierName, *ReturnType);\n          CallInst-&gt;replaceAllUsesWith(LuthierIntrinsicCall);\n          CallInst-&gt;eraseFromParent();\n        }\n        F.dropAllReferences();\n        F.eraseFromParent();\n      }\n    }\n  }\n(Note that I plan to do a similar thing for other special intrinsics).\nNow that we’ve generated valid instrumentation IR, it was time to implement a way to lower this intrinsic. To do that, I need to add an additional feature to the Luthier intrinsic lowering mechanism: Accessing kernel arguments, which meant the MIR lowering callbacks had to be modified with an additional lambda:\nnamespace luthier {\n\n/// \\brief a set of kernel arguments Luthier's intrinsic lowering mechanism\n/// can ensure access to\n/// \\details these values are only available to the kernel as \"arguments\"\n/// as they come either preloaded in S/VGPRs or they are passed as \"hidden\"\n/// arguments in the kernel argument buffer. As these values (or the way to\n/// access them) are stored in GPRs they can be overwritten the moment they\n/// are unused by the instrumented app. To ensure access to these values\n/// in instrumentation routines, Luthier must emit a prologue on top of the\n/// kernel's original prologue to save these values in an unused register,\n/// or spill them to the top of the instrumentation stack's buffer to be\n/// loaded when necessary\nenum KernelArgumentType {\n  /// Wavefront's private segment buffer\n  WAVEFRONT_PRIVATE_SEGMENT_BUFFER = 0,\n  /// Enum marking the beginning of kernel arguments always passed on SGPRs\n  ALWAYS_IN_SGPR_BEGIN = WAVEFRONT_PRIVATE_SEGMENT_BUFFER,\n  /// 64-bit address of the kernel's argument buffer\n  KERNARG_SEGMENT_PTR = 1,\n  /// 32-bit offset from the beginning of the kernel's argument buffer where\n  /// the kernel's hidden arguments starts\n  HIDDEN_KERNARG_OFFSET = 2,\n  /// 32-bit offset from the beginning of the kernel's argument buffer where\n  /// the instrumentation-passed (i.e. user) argument buffer starts\n  USER_KERNARG_OFFSET = 3,\n  /// 64-bit Dispatch ID of the kernel\n  DISPATCH_ID = 4,\n  /// 64-bit flat scratch base address of the wavefront\n  FLAT_SCRATCH = 5,\n  /// 32-bit private segment wave offset\n  PRIVATE_SEGMENT_WAVE_BYTE_OFFSET = 6,\n  /// Enum marking the end of kernel arguments always passed on SGPRs\n  ALWAYS_IN_SGPR_END = PRIVATE_SEGMENT_WAVE_BYTE_OFFSET,\n  /// 64-bit address of the dispatch packet of the kernel being executed\n  DISPATCH_PTR = 7,\n  /// Enum marking the beginning of kernel arguments that can either be passed\n  /// on SGPRs or hidden kernel arguments\n  EITHER_IN_SGPR_OR_HIDDEN_BEGIN = DISPATCH_PTR,\n  /// 64-bit address of the HSA queue used to launch the kernel\n  QUEUE_PTR = 8,\n  /// Size of a work-item's private segment\n  WORK_ITEM_PRIVATE_SEGMENT_SIZE = 9,\n  /// Enum marking the end of kernel arguments that are either passed on the\n  /// SGPRs or hidden kernel arguments\n  EITHER_IN_SGPR_OR_HIDDEN_END = WORK_ITEM_PRIVATE_SEGMENT_SIZE,\n  /// Dispatch workgroup work-item count for the x dimension\n  BLOCK_COUNT_X = 10,\n  /// Enum marking the beginning of hidden-only kernel arguments\n  HIDDEN_BEGIN = BLOCK_COUNT_X,\n  /// Dispatch workgroup work-item count for the y dimension\n  BLOCK_COUNT_Y = 11,\n  /// Dispatch workgroup work-item count for the z dimension\n  BLOCK_COUNT_Z = 12,\n  GROUP_SIZE_X = 13,\n  GROUP_SIZE_Y = 14,\n  GROUP_SIZE_Z = 15,\n  REMAINDER_X = 16,\n  REMAINDER_Y = 17,\n  REMAINDER_Z = 18,\n  GLOBAL_OFFSET_X = 19,\n  GLOBAL_OFFSET_Y = 20,\n  GLOBAL_OFFSET_Z = 21,\n  PRINT_BUFFER = 22,\n  HOSTCALL_BUFFER = 23,\n  DEFAULT_QUEUE = 24,\n  COMPLETION_ACTION = 25,\n  MULTIGRID_SYNC = 26,\n  GRID_DIMS = 27,\n  HEAP_V1 = 28,\n  DYNAMIC_LDS_SIZE = 29,\n  PRIVATE_BASE = 30,\n  SHARED_BASE = 31,\n  HIDDEN_END = SHARED_BASE,\n  WORK_ITEM_X = 32,\n  WORK_ITEM_Y = 33,\n  WORK_ITEM_Z = 34\n};\n\n/// \\brief describes a function type used for each intrinsic to generate\n/// &lt;tt&gt;llvm::MachineInstr&lt;/tt&gt;s in place of its IR calls.\n/// The MIR processor takes in the\n/// \\c IntrinsicIRLoweringInfo generated by its \\c IntrinsicIRProcessorFunc as\n/// well as the lowered registers and their inline assembly flags for\n/// its used/defined values. A lambda which will create an\n/// \\c llvm::MachineInstr at the place of emission given an instruction opcode\n/// is also passed to this function\ntypedef std::function&lt;llvm::Error(\n    const IntrinsicIRLoweringInfo &,\n    llvm::ArrayRef&lt;std::pair&lt;llvm::InlineAsm::Flag, llvm::Register&gt;&gt;,\n    const std::function&lt;llvm::MachineInstrBuilder(int)&gt; &,\n    const std::function&lt;llvm::Register(const llvm::TargetRegisterClass *)&gt; &,\n    const std::function&lt;llvm::Register(KernelArgumentType)&gt; &,\n    const llvm::MachineFunction &,\n    const std::function&lt;llvm::Register(llvm::MCRegister)&gt; &,\n    llvm::DenseMap&lt;llvm::MCRegister, llvm::Register&gt; &)&gt;\n    IntrinsicMIRProcessorFunc;\n\n/// \\brief Used internally by \\c luthier::CodeGenerator to keep track of\n/// registered intrinsics and how to process them\nstruct IntrinsicProcessor {\n  IntrinsicIRProcessorFunc IRProcessor{};\n  IntrinsicMIRProcessorFunc MIRProcessor{};\n};\nThe new argument to the IntrinsicMIRProcessorFunc is a function which will return a virtual register that contains the value of that “kernel argument”. That function is implemented inside IntrinsicMIRLoweringPass as follows:\nauto SVAAccessorBuilder = [&](KernelArgumentType KA) {\n          auto LaneId =\n              stateValueArray::getKernelArgumentLaneIdStoreSlotBeginForWave64(\n                  KA);\n          LUTHIER_REPORT_FATAL_ON_ERROR(LaneId.takeError());\n          auto ArgSize =\n              stateValueArray::getKernelArgumentStoreSlotSizeForWave64(KA);\n          LUTHIER_REPORT_FATAL_ON_ERROR(ArgSize.takeError());\n\n          llvm::SmallVector&lt;llvm::Register, 2&gt; Out;\n\n          for (unsigned short i = 0; i &lt; *ArgSize; i++) {\n            Out.push_back(\n                MRI.createVirtualRegister(&llvm::AMDGPU::SGPR_32RegClass));\n            llvm::BuildMI(MBB, MI, llvm::MIMetadata(MI),\n                          TII-&gt;get(llvm::AMDGPU::V_READLANE_B32), Out.back())\n                .addReg(SVAVGPR, 0)\n                .addImm(*LaneId + i);\n          }\n          // Add the requested kernarg\n          auto TargetMF = TargetMI.getParent()-&gt;getParent();\n          if (TargetMF-&gt;getFunction().getCallingConv() ==\n              llvm::CallingConv::AMDGPU_KERNEL) {\n            PreambleDescriptor.Kernels[TargetMF]\n                .RequestedKernelArguments.insert(KA);\n          } else {\n            PreambleDescriptor.DeviceFunctions[TargetMF]\n                .RequestedKernelArguments.insert(KA);\n          }\n\n          // Emit a reg sequence if the arg size was greater than 1\n          if (*ArgSize &gt; 1) {\n            // First create a reg sequence MI\n            auto Builder = MIBuilder(llvm::AMDGPU::REG_SEQUENCE);\n\n            auto MergedReg = MRI.createVirtualRegister(\n                llvm::SIRegisterInfo::getSGPRClassForBitWidth(*ArgSize * 32));\n            Builder.addReg(MergedReg, llvm::RegState::Define);\n\n            // Split the src reg into 32-bit regs, and merge them in the\n            for (const auto &[SubIdx, Reg] : llvm::enumerate(Out)) {\n              Builder.addReg(Reg).addImm(\n                  llvm::SIRegisterInfo::getSubRegFromChannel(SubIdx));\n            }\n            return MergedReg;\n          } else {\n            return Out[0];\n          }\n        };\nEach kernel argument will be stored in fixed lane(s) of the SVA. Kernel argument address is saved in lanes 15 and 16, and the offset for the hidden kernel arg is located in lane 17.\nWith the MIR lowering callback now modified, we can implement a lowering mechanism for the luthier::implicitArgPtr:\nnamespace luthier {\n\nllvm::Expected&lt;IntrinsicIRLoweringInfo&gt;\nimplicitArgPtrIRProcessor(const llvm::Function &Intrinsic,\n                          const llvm::CallInst &User,\n                          const llvm::GCNTargetMachine &TM) {\n  // The user must not have any operands\n  LUTHIER_RETURN_ON_ERROR(\n      LUTHIER_ERROR_CHECK(User.arg_size() == 0,\n                          \"Expected no operands to be passed to the \"\n                          \"luthier::implicitArgPtr intrinsic '{0}', got {1}.\",\n                          User, User.arg_size()));\n\n  luthier::IntrinsicIRLoweringInfo Out;\n  // The kernarg hidden address will be returned in an SGPR\n  Out.setReturnValueInfo(&User, \"s\");\n\n  return Out;\n}\n\nllvm::Error implicitArgPtrMIRProcessor(\n    const IntrinsicIRLoweringInfo &IRLoweringInfo,\n    llvm::ArrayRef&lt;std::pair&lt;llvm::InlineAsm::Flag, llvm::Register&gt;&gt; Args,\n    const std::function&lt;llvm::MachineInstrBuilder(int)&gt; &MIBuilder,\n    const std::function&lt;llvm::Register(const llvm::TargetRegisterClass *)&gt;\n        &VirtRegBuilder,\n    const std::function&lt;llvm::Register(KernelArgumentType)&gt; &KernArgAccessor,\n    const llvm::MachineFunction &MF,\n    const std::function&lt;llvm::Register(llvm::MCRegister)&gt; &PhysRegAccessor,\n    llvm::DenseMap&lt;llvm::MCRegister, llvm::Register&gt; &PhysRegsToBeOverwritten) {\n  // There should be only a single virtual register involved in the operation\n  LUTHIER_RETURN_ON_ERROR(\n      LUTHIER_ERROR_CHECK(Args.size() == 1,\n                          \"Number of virtual register arguments \"\n                          \"involved in the MIR lowering stage of \"\n                          \"luthier::implicitArgPtr is {0} instead of 1.\",\n                          Args.size()));\n  LUTHIER_RETURN_ON_ERROR(LUTHIER_ERROR_CHECK(\n      Args[0].first.isRegDefKind(),\n      \"The register argument of luthier::implicitArgPtr is not a definition.\"));\n  llvm::Register Output = Args[0].second;\n  // Get the kernel argument\n  llvm::Register KernArgSGPR = KernArgAccessor(KERNARG_SEGMENT_PTR);\n  // Get the offset of the hidden arg\n  llvm::Register HiddenOffsetSGPR = KernArgAccessor(HIDDEN_KERNARG_OFFSET);\n\n    // The lower part of the hidden argument address\n  llvm::Register FirstAddSGPR = VirtRegBuilder(&llvm::AMDGPU::SGPR_32RegClass);\n\n    // The upper part of the hidden argument address\n  llvm::Register SecondAddSGPR = VirtRegBuilder(&llvm::AMDGPU::SGPR_32RegClass);\n\n    // Add the hidden argument offset to the kernel argument address's\n    // lower register\n  MIBuilder(llvm::AMDGPU::S_ADD_U32)\n      .addReg(FirstAddSGPR, llvm::RegState::Define)\n      .addReg(KernArgSGPR, llvm::RegState::Kill,\n              llvm::SIRegisterInfo::getSubRegFromChannel(0))\n      .addReg(HiddenOffsetSGPR, llvm::RegState::Kill);\n\n    // Add the carry to the upper register fo the kernel argument address\n  MIBuilder(llvm::AMDGPU::S_ADDC_U32)\n      .addReg(SecondAddSGPR, llvm::RegState::Define)\n      .addReg(KernArgSGPR, llvm::RegState::Kill,\n              llvm::SIRegisterInfo::getSubRegFromChannel(1))\n      .addImm(0);\n\n  // Do a reg sequence copy to the output to return it\n  (void)MIBuilder(llvm::AMDGPU::REG_SEQUENCE)\n      .addReg(Output, llvm::RegState::Define)\n      .addReg(SecondAddSGPR)\n      .addImm(llvm::SIRegisterInfo::getSubRegFromChannel(1))\n      .addReg(FirstAddSGPR)\n      .addImm(llvm::SIRegisterInfo::getSubRegFromChannel(0));\n\n  return llvm::Error::success();\n}\n\n} // namespace luthier\nFinally, we have to add support to the PrePostAmbleEmitter.cpp, so that whenever we need access to the hidden kernel argument offset, we extract it from the metadata and hardcode it into the kernel pre-amble. The kernel argument buffer address is different as it is an SGPR argument:\n        // If access to kernarg buffer was requested, enable it\n        if (SVAInfo.RequestedKernelArguments.contains(KERNARG_SEGMENT_PTR)) {\n          enableKernArg(MFI, TRI);\n        }\n        // Omitted code for handling scratch\n        // Emit code to store the rest of the requested SGPR kernel arguments\n        for (const auto &[KernArg, PreloadValue] :\n             {std::pair{KERNARG_SEGMENT_PTR,\n                        llvm::AMDGPUFunctionArgInfo::KERNARG_SEGMENT_PTR},\n              {DISPATCH_ID, llvm::AMDGPUFunctionArgInfo::DISPATCH_ID},\n              {DISPATCH_PTR, llvm::AMDGPUFunctionArgInfo::DISPATCH_PTR},\n              {QUEUE_PTR, llvm::AMDGPUFunctionArgInfo::QUEUE_PTR}}) {\n          if (SVAInfo.RequestedKernelArguments.contains(KernArg)) {\n            auto StoreSlotBegin =\n                stateValueArray::getKernelArgumentLaneIdStoreSlotBeginForWave64(\n                    KernArg);\n            if (auto Err = StoreSlotBegin.takeError()) {\n              TargetModule.getContext().emitError(toString(std::move(Err)));\n              return llvm::PreservedAnalyses::all();\n            }\n            auto StoreSlotSize =\n                stateValueArray::getKernelArgumentStoreSlotSizeForWave64(\n                    KernArg);\n            if (auto Err = StoreSlotSize.takeError()) {\n              TargetModule.getContext().emitError(toString(std::move(Err)));\n              return llvm::PreservedAnalyses::all();\n            }\n            if (auto Err = emitCodeToStoreSGPRKernelArg(\n                    *EntryInstr, getArgReg(*MF, PreloadValue), SVSStorageReg,\n                    *StoreSlotBegin, *StoreSlotSize,\n                    OriginalSGPRArgLocs.contains(PreloadValue))) {\n              TargetModule.getContext().emitError(toString(std::move(Err)));\n              return llvm::PreservedAnalyses::all();\n            }\n          }\n        }\n// Add code for storring the hidden kernarg offset\n        if (SVAInfo.RequestedKernelArguments.contains(HIDDEN_KERNARG_OFFSET)) {\n          llvm::outs() &lt;&lt; \"emitting code to store the hidden arg offset.\\n\";\n          auto &KernArgs =\n              llvm::dyn_cast&lt;hsa::LoadedCodeObjectKernel&gt;(FuncSymbol)\n                  -&gt;getKernelMetadata()\n                  .Args;\n\n          LUTHIER_REPORT_FATAL_ON_ERROR(LUTHIER_ERROR_CHECK(\n              KernArgs.has_value(), \"Attempted to access the hidden arguments \"\n                                    \"of a kernel without any arguments.\"));\n          uint32_t HiddenOffset = [&]() {\n            for (const auto &Arg : *KernArgs) {\n              if (Arg.ValueKind &gt;= hsa::md::ValueKind::HiddenArgKindBegin &&\n                  Arg.ValueKind &lt;= hsa::md::ValueKind::HiddenArgKindEnd) {\n                return Arg.Offset;\n              }\n            }\n            return uint32_t{0};\n          }();\n          auto StoreLane =\n              stateValueArray::getKernelArgumentLaneIdStoreSlotBeginForWave64(\n                  HIDDEN_KERNARG_OFFSET);\n          LUTHIER_REPORT_FATAL_ON_ERROR(StoreLane.takeError());\n\n          llvm::BuildMI(*EntryInstr-&gt;getParent(), EntryInstr, llvm::DebugLoc(),\n                        TII-&gt;get(llvm::AMDGPU::V_WRITELANE_B32), SVSStorageReg)\n              .addImm(HiddenOffset)\n              .addImm(*StoreLane)\n              .addReg(SVSStorageReg);\n\n          EntryInstr-&gt;getMF()-&gt;print(llvm::outs());\n        }\nThat’s it!",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Final Project- Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions"
    ]
  },
  {
    "objectID": "blogs/matin/2024-12-12-Final-Project.html#results",
    "href": "blogs/matin/2024-12-12-Final-Project.html#results",
    "title": "Final Project- Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions",
    "section": "Results",
    "text": "Results\nFor validating the implementation, I wrote a tool which instrumented the last instruction of the first basic block to copy over the gridDim into a managed variables as follows\nMARK_LUTHIER_DEVICE_MODULE\n\n/// Where we will write the grid dim\n__attribute__((managed)) dim3 GridDim{0, 0, 0};\n\n/// Copy over the grid dim and block dim\nLUTHIER_HOOK_ANNOTATE accessKernelArg() { GridDim = gridDim;}\n\nLUTHIER_EXPORT_HOOK_HANDLE(accessKernelArg);\n\n/// Instruments the last instruction of the first basic block\nstatic llvm::Error instrumentationLoop(InstrumentationTask &IT,\n                                       LiftedRepresentation &LR) {\n  for (auto &[_, MF] : LR.functions()) {\n    LUTHIER_RETURN_ON_ERROR(IT.insertHookBefore(\n        MF-&gt;begin()-&gt;back(), LUTHIER_GET_HOOK_HANDLE(accessKernelArg)));\n  }\n  return llvm::Error::success();\n}\n\nstatic void\ninstrumentAllFunctionsOfLR(const hsa::LoadedCodeObjectKernel &Kernel) {\n  auto LR = lift(Kernel);\n  LUTHIER_REPORT_FATAL_ON_ERROR(LR.takeError());\n\n  LUTHIER_REPORT_FATAL_ON_ERROR(\n      instrumentAndLoad(Kernel, *LR, instrumentationLoop, \"kernarg_access\"));\n}\nOn three separate benchmarks in the HecBench this tool will return the correct gridDim:\n\n# affine-hip\n\nLD_PRELOAD=./examples/KernelArgumentIntrinsic/libKernelArgumentIntrinsic.so ./main data/CT-MONO2-16-brain.raw result.raw 1000000\n\nKernel 15529 - affine(unsigned short const*, unsigned short*): Total Grid Size Dims (HSA): (512, 512, 1) Workgroup Dims (HSA): (16, 16, 1) : GridDim (HIP): (32, 32, 1).\n\n# asta-hip\n\nLD_PRELOAD=./examples/KernelArgumentIntrinsic/libKernelArgumentIntrinsic.so ./main\n\nKernel 35 - PTTWAC_soa_asta(int, int, int, float*, int*, int*): Total Grid Size Dims (HSA): (1024, 1, 1) Workgroup Dims (HSA): (64, 1, 1) : GridDim (HIP): (16, 1, 1).\n\n# complex-hip\nLD_PRELOAD=./examples/KernelArgumentIntrinsic/libKernelArgumentIntrinsic.so ./main 10000000 1000\n\nKernel 0 - complex_float(char*, int): Total Grid Size Dims: (10000128, 1, 1) Workgroup Dims: (256, 1, 1) Kernel block count: (39063, 1, 1).\n...\n\nKernel 3001 - complex_double(char*, int): Total Grid Size Dims (HSA): (10000128, 1, 1) Workgroup Dims (HSA): (256, 1, 1) : GridDim (HIP): (39063, 1, 1).\n...\nKernel 4001 - ref_complex_double(char*, int): Total Grid Size Dims (HSA): (10000128, 1, 1) Workgroup Dims (HSA): (256, 1, 1) : GridDim (HIP): (39063, 1, 1).\nSeems like it works! Though more testing is needed to handle more corner cases, I’m able to extract the correct grid dim and block dim of the launched kernel from inside the kernel itself, which is enough to prove that this was a success.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Final Project- Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions"
    ]
  },
  {
    "objectID": "blogs/matin/2024-12-12-Final-Project.html#conclusion-and-future-work",
    "href": "blogs/matin/2024-12-12-Final-Project.html#conclusion-and-future-work",
    "title": "Final Project- Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions",
    "section": "Conclusion and Future Work",
    "text": "Conclusion and Future Work\nThough the code is successfully lowered to correct code, there is still work that needs to be done: 1. If a kernel doesn’t require access to hidden arguments, it might not be setup by the ROCCLR low-level runtime; If this is indeed the case, Luthier instead needs to setup the kernel arguments. This will be extra important for things like host call buffers, which don’t get passed to kernels by default. 2. I setup the hidden kernel argument offset by writing an immediate value to the SVA lane; For kernels with a lot of explicit arguments this offset must be written to an SGPR first before the WRITELANE instruction. 3. Since hooks potentially can be called many many times inside a kernel, reading from the kernel argument memory many times is not efficient. Instead of replacing the implicitarg LLVM intrinsic, we need to look at the offsets that they are loading from, and use that to detect the exact kernel arguments they are accessing. This way, we can only do this once in the pre-amble and store it in the SVA, so that we can access it later. 4. I need to test more hidden kernel arguments.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Final Project- Lowering AMDGPU LLVM Intrinsics In Instrumentation Functions"
    ]
  },
  {
    "objectID": "blogs/matin/2024-10-18-HW4.html",
    "href": "blogs/matin/2024-10-18-HW4.html",
    "title": "Homework 4: Dominance Analysis",
    "section": "",
    "text": "For this homework I implemented an algorithm that given a BRIL function, it can find: 1. For each basic block in the function, it found the blocks that dominated it (and vice-versa) 2. The dominator frontier for each basic block. 3. The dominator tree by calculating the immediate dominator relation.\nThe implementation can be found here.\nTo ensure the code is implemented correctly, I initially relied on the turnt tests already included under the test/ folder. On top of that, to ensure my dominator calculations were correct, I implemented a breadth-first search path finding algorithm on the predecessor graph; More specifically, for each block (except entry), I calculated all the paths from said block to the entry block. Then I would iterate over the calculated dominator and verify if all paths of execution indeed contain the dominated block, which is the exact definition for the dominance relation.\nThe test would only print out in case of failure so that it wouldn’t interfere with Turnt’s testing.\nFor frontier and tree calculations since all I needed to do was to follow a definition, I only relied on the unit tests.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Homework 4: Dominance Analysis"
    ]
  },
  {
    "objectID": "blogs/matin/2024-10-18-HW4.html#sample-output",
    "href": "blogs/matin/2024-10-18-HW4.html#sample-output",
    "title": "Homework 4: Dominance Analysis",
    "section": "Sample output",
    "text": "Sample output\nbril2json &lt; ../../../benchmarks/float/cordic.bril | python3 ../../dom_matin.py dom\n{\n  \"b1\": [\n    \"b1\"\n  ]\n}\n{\n  \"b1\": [\n    \"b1\"\n  ],\n  \"else.104\": [\n    \"b1\",\n    \"else.104\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"else.111\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"else.118\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.118\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"else.125\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.118\",\n    \"else.125\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"else.132\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.118\",\n    \"else.125\",\n    \"else.132\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"else.139\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.118\",\n    \"else.125\",\n    \"else.132\",\n    \"else.139\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"else.31\": [\n    \"b1\",\n    \"else.31\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"else.39\": [\n    \"b1\",\n    \"else.39\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"else.46\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"else.53\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"else.60\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"else.60\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"else.67\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"else.60\",\n    \"else.67\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"else.74\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"else.60\",\n    \"else.67\",\n    \"else.74\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"else.81\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"else.60\",\n    \"else.67\",\n    \"else.74\",\n    \"else.81\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"else.97\": [\n    \"b1\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"endif.104\": [\n    \"b1\",\n    \"else.31\",\n    \"else.97\",\n    \"endif.104\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"endif.111\": [\n    \"b1\",\n    \"else.104\",\n    \"else.31\",\n    \"else.97\",\n    \"endif.111\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"endif.118\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.31\",\n    \"else.97\",\n    \"endif.118\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"endif.125\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.118\",\n    \"else.31\",\n    \"else.97\",\n    \"endif.125\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"endif.132\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.118\",\n    \"else.125\",\n    \"else.31\",\n    \"else.97\",\n    \"endif.132\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"endif.139\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.118\",\n    \"else.125\",\n    \"else.132\",\n    \"else.31\",\n    \"else.97\",\n    \"endif.139\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"endif.31\": [\n    \"b1\",\n    \"endif.31\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"endif.39\": [\n    \"b1\",\n    \"endif.39\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"endif.46\": [\n    \"b1\",\n    \"else.39\",\n    \"endif.46\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"endif.53\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"endif.53\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"endif.60\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"endif.60\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"endif.67\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"else.60\",\n    \"endif.67\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"endif.74\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"else.60\",\n    \"else.67\",\n    \"endif.74\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"endif.81\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"else.60\",\n    \"else.67\",\n    \"else.74\",\n    \"endif.81\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"endif.97\": [\n    \"b1\",\n    \"else.31\",\n    \"endif.97\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"for.body.12\": [\n    \"b1\",\n    \"for.body.12\",\n    \"for.cond.12\"\n  ],\n  \"for.cond.12\": [\n    \"b1\",\n    \"for.cond.12\"\n  ],\n  \"for.end.12\": [\n    \"b1\",\n    \"for.cond.12\",\n    \"for.end.12\"\n  ],\n  \"then.104\": [\n    \"b1\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.104\"\n  ],\n  \"then.111\": [\n    \"b1\",\n    \"else.104\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.111\"\n  ],\n  \"then.118\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.118\"\n  ],\n  \"then.125\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.118\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.125\"\n  ],\n  \"then.132\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.118\",\n    \"else.125\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.132\"\n  ],\n  \"then.139\": [\n    \"b1\",\n    \"else.104\",\n    \"else.111\",\n    \"else.118\",\n    \"else.125\",\n    \"else.132\",\n    \"else.31\",\n    \"else.97\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.139\"\n  ],\n  \"then.31\": [\n    \"b1\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\"\n  ],\n  \"then.39\": [\n    \"b1\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\",\n    \"then.39\"\n  ],\n  \"then.46\": [\n    \"b1\",\n    \"else.39\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\",\n    \"then.46\"\n  ],\n  \"then.53\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\",\n    \"then.53\"\n  ],\n  \"then.60\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\",\n    \"then.60\"\n  ],\n  \"then.67\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"else.60\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\",\n    \"then.67\"\n  ],\n  \"then.74\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"else.60\",\n    \"else.67\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\",\n    \"then.74\"\n  ],\n  \"then.81\": [\n    \"b1\",\n    \"else.39\",\n    \"else.46\",\n    \"else.53\",\n    \"else.60\",\n    \"else.67\",\n    \"else.74\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.31\",\n    \"then.81\"\n  ],\n  \"then.97\": [\n    \"b1\",\n    \"else.31\",\n    \"for.body.12\",\n    \"for.cond.12\",\n    \"then.97\"\n  ]\n}\nbril2json &lt; ../../../benchmarks/float/cordic.bril | python3 ../../dom_matin.py front\n{\n  \"b1\": []\n}\n{\n  \"b1\": [],\n  \"else.104\": [\n    \"endif.104\"\n  ],\n  \"else.111\": [\n    \"endif.111\"\n  ],\n  \"else.118\": [\n    \"endif.118\"\n  ],\n  \"else.125\": [\n    \"endif.125\"\n  ],\n  \"else.132\": [\n    \"endif.132\"\n  ],\n  \"else.139\": [\n    \"endif.139\"\n  ],\n  \"else.31\": [\n    \"endif.31\"\n  ],\n  \"else.39\": [\n    \"endif.39\"\n  ],\n  \"else.46\": [\n    \"endif.46\"\n  ],\n  \"else.53\": [\n    \"endif.53\"\n  ],\n  \"else.60\": [\n    \"endif.60\"\n  ],\n  \"else.67\": [\n    \"endif.67\"\n  ],\n  \"else.74\": [\n    \"endif.74\"\n  ],\n  \"else.81\": [\n    \"endif.81\"\n  ],\n  \"else.97\": [\n    \"endif.97\"\n  ],\n  \"endif.104\": [\n    \"endif.97\"\n  ],\n  \"endif.111\": [\n    \"endif.104\"\n  ],\n  \"endif.118\": [\n    \"endif.111\"\n  ],\n  \"endif.125\": [\n    \"endif.118\"\n  ],\n  \"endif.132\": [\n    \"endif.125\"\n  ],\n  \"endif.139\": [\n    \"endif.132\"\n  ],\n  \"endif.31\": [\n    \"for.cond.12\"\n  ],\n  \"endif.39\": [\n    \"endif.31\"\n  ],\n  \"endif.46\": [\n    \"endif.39\"\n  ],\n  \"endif.53\": [\n    \"endif.46\"\n  ],\n  \"endif.60\": [\n    \"endif.53\"\n  ],\n  \"endif.67\": [\n    \"endif.60\"\n  ],\n  \"endif.74\": [\n    \"endif.67\"\n  ],\n  \"endif.81\": [\n    \"endif.74\"\n  ],\n  \"endif.97\": [\n    \"endif.31\"\n  ],\n  \"for.body.12\": [\n    \"for.cond.12\"\n  ],\n  \"for.cond.12\": [\n    \"for.cond.12\"\n  ],\n  \"for.end.12\": [],\n  \"then.104\": [\n    \"endif.104\"\n  ],\n  \"then.111\": [\n    \"endif.111\"\n  ],\n  \"then.118\": [\n    \"endif.118\"\n  ],\n  \"then.125\": [\n    \"endif.125\"\n  ],\n  \"then.132\": [\n    \"endif.132\"\n  ],\n  \"then.139\": [\n    \"endif.139\"\n  ],\n  \"then.31\": [\n    \"endif.31\"\n  ],\n  \"then.39\": [\n    \"endif.39\"\n  ],\n  \"then.46\": [\n    \"endif.46\"\n  ],\n  \"then.53\": [\n    \"endif.53\"\n  ],\n  \"then.60\": [\n    \"endif.60\"\n  ],\n  \"then.67\": [\n    \"endif.67\"\n  ],\n  \"then.74\": [\n    \"endif.74\"\n  ],\n  \"then.81\": [\n    \"endif.81\"\n  ],\n  \"then.97\": [\n    \"endif.97\"\n  ]\n}\nbril2json &lt; ../../../benchmarks/float/cordic.bril | python3 ../../dom_matin.py tree\n{\n  \"b1\": []\n}\n{\n  \"b1\": [\n    \"for.cond.12\"\n  ],\n  \"else.104\": [\n    \"else.111\",\n    \"endif.111\",\n    \"then.111\"\n  ],\n  \"else.111\": [\n    \"else.118\",\n    \"endif.118\",\n    \"then.118\"\n  ],\n  \"else.118\": [\n    \"else.125\",\n    \"endif.125\",\n    \"then.125\"\n  ],\n  \"else.125\": [\n    \"else.132\",\n    \"endif.132\",\n    \"then.132\"\n  ],\n  \"else.132\": [\n    \"else.139\",\n    \"endif.139\",\n    \"then.139\"\n  ],\n  \"else.139\": [],\n  \"else.31\": [\n    \"else.97\",\n    \"endif.97\",\n    \"then.97\"\n  ],\n  \"else.39\": [\n    \"else.46\",\n    \"endif.46\",\n    \"then.46\"\n  ],\n  \"else.46\": [\n    \"else.53\",\n    \"endif.53\",\n    \"then.53\"\n  ],\n  \"else.53\": [\n    \"else.60\",\n    \"endif.60\",\n    \"then.60\"\n  ],\n  \"else.60\": [\n    \"else.67\",\n    \"endif.67\",\n    \"then.67\"\n  ],\n  \"else.67\": [\n    \"else.74\",\n    \"endif.74\",\n    \"then.74\"\n  ],\n  \"else.74\": [\n    \"else.81\",\n    \"endif.81\",\n    \"then.81\"\n  ],\n  \"else.81\": [],\n  \"else.97\": [\n    \"else.104\",\n    \"endif.104\",\n    \"then.104\"\n  ],\n  \"endif.104\": [],\n  \"endif.111\": [],\n  \"endif.118\": [],\n  \"endif.125\": [],\n  \"endif.132\": [],\n  \"endif.139\": [],\n  \"endif.31\": [],\n  \"endif.39\": [],\n  \"endif.46\": [],\n  \"endif.53\": [],\n  \"endif.60\": [],\n  \"endif.67\": [],\n  \"endif.74\": [],\n  \"endif.81\": [],\n  \"endif.97\": [],\n  \"for.body.12\": [\n    \"else.31\",\n    \"endif.31\",\n    \"then.31\"\n  ],\n  \"for.cond.12\": [\n    \"for.body.12\",\n    \"for.end.12\"\n  ],\n  \"for.end.12\": [],\n  \"then.104\": [],\n  \"then.111\": [],\n  \"then.118\": [],\n  \"then.125\": [],\n  \"then.132\": [],\n  \"then.139\": [],\n  \"then.31\": [\n    \"else.39\",\n    \"endif.39\",\n    \"then.39\"\n  ],\n  \"then.39\": [],\n  \"then.46\": [],\n  \"then.53\": [],\n  \"then.60\": [],\n  \"then.67\": [],\n  \"then.74\": [],\n  \"then.81\": [],\n  \"then.97\": []\n}\n bril2json &lt; ../dom/loopcond.bril | python3 ../../dom_matin.py dom\n{\n  \"body\": [\n    \"body\",\n    \"entry\",\n    \"loop\"\n  ],\n  \"endif\": [\n    \"body\",\n    \"endif\",\n    \"entry\",\n    \"loop\"\n  ],\n  \"entry\": [\n    \"entry\"\n  ],\n  \"exit\": [\n    \"entry\",\n    \"exit\",\n    \"loop\"\n  ],\n  \"loop\": [\n    \"entry\",\n    \"loop\"\n  ],\n  \"then\": [\n    \"body\",\n    \"entry\",\n    \"loop\",\n    \"then\"\n  ]\n}\nbril2json &lt; ../dom/loopcond.bril | python3 ../../dom_matin.py front\n{\n  \"body\": [\n    \"loop\"\n  ],\n  \"endif\": [\n    \"loop\"\n  ],\n  \"entry\": [],\n  \"exit\": [],\n  \"loop\": [\n    \"loop\"\n  ],\n  \"then\": [\n    \"endif\"\n  ]\n}\nbril2json &lt; ../dom/loopcond.bril | python3 ../../dom_matin.py tree\n{\n  \"body\": [\n    \"endif\",\n    \"then\"\n  ],\n  \"endif\": [],\n  \"entry\": [\n    \"loop\"\n  ],\n  \"exit\": [],\n  \"loop\": [\n    \"body\",\n    \"exit\"\n  ],\n  \"then\": []\n}",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Homework 4: Dominance Analysis"
    ]
  },
  {
    "objectID": "blogs/matin/2024-10-18-HW4.html#challenges",
    "href": "blogs/matin/2024-10-18-HW4.html#challenges",
    "title": "Homework 4: Dominance Analysis",
    "section": "Challenges",
    "text": "Challenges\nThe main challenge in this homework was making sure I got the dominated/dominator relations right and correctly keep track of them. One example of this issue was when calculating the dominator tree, where my initial implementation only relied on the dominator relation. After debugging I realized I wasn’t following the definition correctly, and I needed to use the dominated map for the second part of the definition, c dom b to make my life easier.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Homework 4: Dominance Analysis"
    ]
  },
  {
    "objectID": "blogs/matin/2024-09-20-HW1.html",
    "href": "blogs/matin/2024-09-20-HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "After looking at the example benchmarks and the BRIL documentation, I noticed that the example benchmarks had a pseudo random number generator, and BRIL had a floating point extension. Inspired by the other scientific-based floating-point benchmarks, I decided on writing a benchmark to calculate PI using Monte Carlo (explained here). What could possibly go wrong?\n\n\nProbably the main issue I encountered was that there was no way to convert floats to ints, meaning no matter how many random integers I was generating, I couldn’t use them to generate numbers inside a unit square (1x1). To get around this issue, I tried sampling integers inside a non-unit square instead (e.g. square is 10x10, and points come from an interval of [-10, 10] on both x and y). All I had to do was make sure the pseudo random number generator outputs had a lower bound that I specified instead of zero. Again, what could possibly go wrong?\n\n\n\nAfter some debugging, I noticed that either the random number generator didn’t work as advertised (I put print statements in other benchmarks, and the values certainly weren’t greater than zero), or I was using it wrong.\nTo get around both of these issues, instead I opted in to discretize the unit square into a set of grid points, and then query whether they are inside the unit circle or not. Increasing the number of points leads to the output of the BRIL program converging to PI, so it seems to work. The benchmark and its test files are added under the benchmarks folder.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Homework 1"
    ]
  },
  {
    "objectID": "blogs/matin/2024-09-20-HW1.html#problem-1-bril-doesnt-seem-to-convert-float-to-int-or-vice-versa",
    "href": "blogs/matin/2024-09-20-HW1.html#problem-1-bril-doesnt-seem-to-convert-float-to-int-or-vice-versa",
    "title": "Homework 1",
    "section": "",
    "text": "Probably the main issue I encountered was that there was no way to convert floats to ints, meaning no matter how many random integers I was generating, I couldn’t use them to generate numbers inside a unit square (1x1). To get around this issue, I tried sampling integers inside a non-unit square instead (e.g. square is 10x10, and points come from an interval of [-10, 10] on both x and y). All I had to do was make sure the pseudo random number generator outputs had a lower bound that I specified instead of zero. Again, what could possibly go wrong?",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Homework 1"
    ]
  },
  {
    "objectID": "blogs/matin/2024-09-20-HW1.html#problem-2-the-number-generator-didnt-have-a-lower-bound-or-plain-just-didnt-work-as-advertised.",
    "href": "blogs/matin/2024-09-20-HW1.html#problem-2-the-number-generator-didnt-have-a-lower-bound-or-plain-just-didnt-work-as-advertised.",
    "title": "Homework 1",
    "section": "",
    "text": "After some debugging, I noticed that either the random number generator didn’t work as advertised (I put print statements in other benchmarks, and the values certainly weren’t greater than zero), or I was using it wrong.\nTo get around both of these issues, instead I opted in to discretize the unit square into a set of grid points, and then query whether they are inside the unit circle or not. Increasing the number of points leads to the output of the BRIL program converging to PI, so it seems to work. The benchmark and its test files are added under the benchmarks folder.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Homework 1"
    ]
  },
  {
    "objectID": "blogs/rohit/2024-09-28-Rohit-HW2.html",
    "href": "blogs/rohit/2024-09-28-Rohit-HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Definition: An instruction is dead if that instruction writes a variable v and no path within the block starting at that instruction reaches a use of v in the same block or reaches the exit of the block\n\nThe code should remove the instructions that are reassigned without being used.\nIf the value in being used in between the reassigning, then the instruction should be retained.\n\n\n\n\n\n\n\nThis function generates control flow blocks from a list of instructions.\nParameters: instrs - a list of instructions.\nProcess:\n\nIt iterates through each instruction.\nIf the instruction contains an operation (op), it adds it to the current block.\nIf the operation is a terminator (br, jmp, ret), it yields the current block and starts a new one.\nIf the instruction does not contain an operation, it yields the current block and starts a new one.\nAt the end, if there are any remaining instructions in the current block, it yields that block.\n\nReturns: Yields blocks of instructions.\n\n\n\n\n\nThis function removes instructions that are redefined without being used in a function.\nParameters: func - a dictionary representing a function with a list of instructions.\nProcess:\n\nIt repeatedly splits the function’s instructions into blocks using myCFG.\nFor each block, it tracks the last definition of each variable and identifies instructions to drop.\nIt removes instructions that are redefined without being used.\nIt updates the function’s instructions with the optimized blocks.\nThe loop continues until no more changes are made.\n\nReturns: None (modifies the function in place).\n\n\n\n\n\nThis is the main function that orchestrates the optimization process.\nProcess:\n\nIt reads a JSON object from standard input.\nFor each function in the JSON object, it applies remove_reassigned to optimize the instructions.\nIt writes the optimized JSON object to standard output.\n\n\n\n\n\n\n\n\n\n\n    @main {\n    a: int = const 100;\n    a: int = const 42;\n    b: int = const 100;\n    b: int = const 42;\n    }\n\n\n\n    @main {\n    a: int = const 42;\n    b: int = const 42;\n    }\n\n\n\n\n\n\n    @main {\n    a: int = const 100;\n    print a;\n    a: int = const 42;\n    b: int = const 100;\n    b: int = const 42;\n    }\n\n\n\n    @main {\n    a: int = const 100;\n    print a;\n    a: int = const 42;\n    b: int = const 42;\n    }",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Homework 2"
    ]
  },
  {
    "objectID": "blogs/rohit/2024-09-28-Rohit-HW2.html#part-1-trivial-dead-code-elimination-which-you-delete-instructions-that-are-never-used-before-they-are-reassigned.",
    "href": "blogs/rohit/2024-09-28-Rohit-HW2.html#part-1-trivial-dead-code-elimination-which-you-delete-instructions-that-are-never-used-before-they-are-reassigned.",
    "title": "Homework 2",
    "section": "",
    "text": "Definition: An instruction is dead if that instruction writes a variable v and no path within the block starting at that instruction reaches a use of v in the same block or reaches the exit of the block\n\nThe code should remove the instructions that are reassigned without being used.\nIf the value in being used in between the reassigning, then the instruction should be retained.\n\n\n\n\n\n\n\nThis function generates control flow blocks from a list of instructions.\nParameters: instrs - a list of instructions.\nProcess:\n\nIt iterates through each instruction.\nIf the instruction contains an operation (op), it adds it to the current block.\nIf the operation is a terminator (br, jmp, ret), it yields the current block and starts a new one.\nIf the instruction does not contain an operation, it yields the current block and starts a new one.\nAt the end, if there are any remaining instructions in the current block, it yields that block.\n\nReturns: Yields blocks of instructions.\n\n\n\n\n\nThis function removes instructions that are redefined without being used in a function.\nParameters: func - a dictionary representing a function with a list of instructions.\nProcess:\n\nIt repeatedly splits the function’s instructions into blocks using myCFG.\nFor each block, it tracks the last definition of each variable and identifies instructions to drop.\nIt removes instructions that are redefined without being used.\nIt updates the function’s instructions with the optimized blocks.\nThe loop continues until no more changes are made.\n\nReturns: None (modifies the function in place).\n\n\n\n\n\nThis is the main function that orchestrates the optimization process.\nProcess:\n\nIt reads a JSON object from standard input.\nFor each function in the JSON object, it applies remove_reassigned to optimize the instructions.\nIt writes the optimized JSON object to standard output.\n\n\n\n\n\n\n\n\n\n\n    @main {\n    a: int = const 100;\n    a: int = const 42;\n    b: int = const 100;\n    b: int = const 42;\n    }\n\n\n\n    @main {\n    a: int = const 42;\n    b: int = const 42;\n    }\n\n\n\n\n\n\n    @main {\n    a: int = const 100;\n    print a;\n    a: int = const 42;\n    b: int = const 100;\n    b: int = const 42;\n    }\n\n\n\n    @main {\n    a: int = const 100;\n    print a;\n    a: int = const 42;\n    b: int = const 42;\n    }",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Homework 2"
    ]
  },
  {
    "objectID": "blogs/rohit/2024-09-28-Rohit-HW2.html#part-2-implement-local-value-numbering",
    "href": "blogs/rohit/2024-09-28-Rohit-HW2.html#part-2-implement-local-value-numbering",
    "title": "Homework 2",
    "section": "PART 2: Implement local value numbering",
    "text": "PART 2: Implement local value numbering\n\nAbout local value numbering:\nDefinition: Local Value Numbering (LVN) is a compiler optimization technique used to eliminate redundant calculations by assigning unique numbers to equivalent expressions. This helps in identifying and reusing previously computed values, thus improving the efficiency of the code.\n\n\nImplementation:\n\ninit(self)\n\nThis is the constructor for the ImprovedLVN class. It initializes several dictionaries and counters used in the Local Value Numbering (LVN) process.\nself.var2num: Maps variables to their value numbers.\nself.value2num: Maps operations to value numbers.\nself.num2var: Maps value numbers to variable names.\nself.next_vn: Counter for the next value number.\nself.last_computed: Maps operations to their last computed variable.\n\n\n\nfresh_value_number(self)\n\nThis function generates a fresh value number.\nReturns: A new value number and increments the counter.\n\n\n\ncanonicalize(self, value)\n\nThis function canonicalizes commutative operations like addition and multiplication to ensure consistent ordering of arguments.\nParameters: value - a Value namedtuple representing an operation and its arguments.\nReturns: A canonicalized Value namedtuple.\n\n\n\nget_lvn_var(self, vn)\n\nThis function generates a variable name for a given value number.\nParameters: vn - a value number.\nReturns: A string representing the LVN variable name.\n\n\n\nprocess_block(self, block)\n\nThis function performs Local Value Numbering (LVN) on a single block of instructions.\nParameters: block - a list of instructions.\nProcess:\n\nIterates through each instruction in the block.\nIf the instruction has a destination (‘dest’), it processes the instruction:\n\nFetches value numbers for arguments.\nCreates a canonicalized value for the current instruction.\nChecks if the value is already computed:\n\nIf yes, it uses the previous result.\nIf no, it assigns a fresh value number and updates mappings.\n\n\nAdds the processed instruction to the new block.\n\nReturns: A new block of optimized instructions.\n\n\n\nrun_lvn(self, bril_program)\n\nThis function runs LVN on the entire program.\nParameters: bril_program - a dictionary representing the BRIL program.\nProcess:\n\nIterates through each function in the program.\nApplies process_block to the instructions of each function.\n\nReturns: The optimized BRIL program.\n\n\n\n\nExample:\n\nwith a b no permutted\n\nInput bril:\n    @main {\n        a: int = const 4;\n        b: int = const 2;\n        sum1: int = div a b;\n        sum2: int = div a b;\n        prod: int = mul sum1 sum2;\n        sum2: int = div a b;\n        print prod;\n    }\n\n\nOutput bril:\n    @main {\n    a: int = const 4;\n    b: int = const 2;\n    sum1: int = div a b;\n    sum2 = id sum1;\n    prod: int = mul sum1 sum1;\n    sum2 = id sum1;\n    print prod;\n    }\n\n\n\npairing with dead code analysis:\n@main {\n  a: int = const 4;\n  b: int = const 2;\n  sum1: int = div a b;\n  prod: int = mul sum1 sum1;\n  sum2 = id sum1;\n  print prod;\n}\n\n\n\n\nwith a b with permutted\n\nInput bril:\n    @main {\n    a: int = const 4;\n    b: int = const 2;\n    sum1: int = add a b;\n    sum2: int = add b a;\n    prod: int = mul sum1 sum2;\n    sum2: int = add b a;\n    print prod;\n    }\n\n\n\nOutput bril:\n    @main {\n    a: int = const 4;\n    b: int = const 2;\n    sum1: int = add a b;\n    sum2 = id sum1;\n    prod: int = mul sum1 sum1;\n    sum2 = id sum1;\n    print prod;\n    }\n\n\n\npairing with dead code analysis:\n    @main {\n    a: int = const 4;\n    b: int = const 2;\n    sum1: int = div a b;\n    prod: int = mul sum1 sum1;\n    sum2 = id sum1;\n    print prod;\n    }\n\nfull code: https://github.com/gurusamyanandakuma-r/bril/tree/main/HW/HW2_Rohit",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Homework 2"
    ]
  },
  {
    "objectID": "blogs/rohit/2024-10-18-Rohit-HW4.html",
    "href": "blogs/rohit/2024-10-18-Rohit-HW4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Definition: Dominance analysis is a fundamental concept in compiler optimization that helps us understand control flow relationships between basic blocks in a program. Let’s break down the key components and their implementations.\n\n\n\nA block D dominates block B if every path from the program entry to B must go through D\nProperties:\n\nEntry block dominates all blocks\nEvery block dominates itself\nIf A dominates B and B dominates C, then A dominates C (transitive)\n\nApplications:\n\nUsed in SSA form construction\nHelps identify natural loops\nCritical for many optimizations\n\n\n\n\n\n\nA compact representation of dominance relationships\nProperties:\n\nEach block (except entry) has exactly one immediate dominator\nForms a tree structure where parent dominates all its children\nImmediate dominator is the closest dominator in the path from entry\n\nBenefits:\n\nEfficient dominance queries\nSimplified analysis of nested structures\nQuick ancestor-descendant checks\n\n\n\n\n\n\nThe dominance frontier of block X contains blocks where X’s dominance stops\nKey aspects:\n\nCritical for SSA form construction\nIdentifies where to place φ-functions\nHelps determine variable live ranges\n\nUsage:\n\nControl dependence analysis\nData flow optimization\nLoop optimization\n\n\n\n\n\n\n\n\n\nInitialize with a stack.\nDefine methods for:\n\nReversing the graph.\nTraversing the graph in post-order.\nMerging sets.\nCalculating dominators.\nFinding dominance frontiers.\nBuilding a dominator tree.\n\n\n\n\n\n\nCreate a reversed graph dictionary.\n\n\n\n\n\nPerform post-order traversal on the graph starting from a given node.\n\n\n\n\n\nMerge multiple sets, finding the intersection.\n\n\n\n\n\nReverse the graph.\nTraverse the graph in post-order.\nInitialize dominator sets.\nIteratively update dominator sets until convergence.\n\n\n\n\n\nReverse the dominator sets.\nIdentify and return the dominance frontiers for each block.\n\n\n\n\n\nReverse the dominator sets.\nCreate strict dominator sets.\nCreate double strict dominator sets.\nBuild and return the dominator tree.\n\n\n\n\n\nInitialize DominanceProcessor.\nFor each function in BRIL:\n\nMap blocks and add entry/terminators.\nCalculate dominators.\nBased on analysis type, find frontiers or build tree.\nPrint results in JSON format.\n\n\n\n\n\n\nLoad BRIL from input.\nCall process_dominance with the specified analysis type.\n\n\n\n\n\n\n\n```{turnt.toml} [envs.DOM] command = “bril2json &lt; {filename} | python3 HW4.py DOM” output.”DOM.json” = “-”\n[envs.FRONT]\ncommand = \"bril2json &lt; {filename} | python3 HW4.py FRONT\"\noutput.\"FRONT.json\" = \"-\"\n\n[envs.TREE]\ncommand = \"bril2json &lt; {filename} | python3 HW4.py TREE\"\noutput.\"TREE.json\" = \"-\"\n#### Example 1: Simple Code: Absolute Value\n##### Input bril:\n\n\n\n\n\n\n```{bril}\n    @main(x: int) {\n    zero: int = const 0;\n    is_negative: bool = lt x zero;\n    br is_negative .negative .positive;\n    .negative:\n    result: int = sub zero x;\n    jmp .end;\n    .positive:\n    result: int = id x;\n    .end:\n    print result;\n    }\n\n\n    {\n    \"BLOCK1\": [\n        \"BLOCK1\"\n    ],\n    \"end\": [\n        \"BLOCK1\",\n        \"end\"\n    ],\n    \"negative\": [\n        \"BLOCK1\",\n        \"negative\"\n    ],\n    \"positive\": [\n        \"BLOCK1\",\n        \"positive\"\n    ]\n    }\n\n\n\n\n{\n  \"BLOCK1\": []\n}\n{\n  \"BLOCK1\": [],\n  \"body\": [\n    \"loop\"\n  ],\n  \"done\": [],\n  \"loop\": [\n    \"loop\"\n  ],\n  \"loop_end\": [\n    \"loop\"\n  ]\n}\n{\n  \"BLOCK1\": [],\n  \"done\": [],\n  \"swap\": [\n    \"done\"\n  ]\n}\n{\n  \"BLOCK1\": [],\n  \"bodyi\": [\n    \"loopi\"\n  ],\n  \"bodyj\": [\n    \"loopj\"\n  ],\n  \"donei\": [],\n  \"donej\": [\n    \"loopi\"\n  ],\n  \"loop_endj\": [\n    \"loopj\"\n  ],\n  \"loopi\": [\n    \"loopi\"\n  ],\n  \"loopi_end\": [\n    \"loopi\"\n  ],\n  \"loopj\": [\n    \"loopi\",\n    \"loopj\"\n  ]\n}\n\n\n\n\n    {\n    \"BLOCK1\": [\n        \"end\",\n        \"negative\",\n        \"positive\"\n    ],\n    \"end\": [],\n    \"negative\": [],\n    \"positive\": []\n    }\n\n\n\n\n\n\n\n        \n    @pack(size: int, n1: int, n2: int, n3: int, n4: int, n5: int) : ptr&lt;int&gt; {\n        one: int = const 1;\n        i: int = const 0;\n        array: ptr&lt;int&gt; = alloc size;\n    # Pack data into array manually. Cannot use loop because of the different var name.     \n        loc: ptr&lt;int&gt; = ptradd array i;\n        store loc n1;\n        i: int = add i one;\n        loc: ptr&lt;int&gt; = ptradd array i;\n        store loc n2;\n        i: int = add i one;        \n        loc: ptr&lt;int&gt; = ptradd array i;\n        store loc n3;\n        i: int = add i one;        \n        loc: ptr&lt;int&gt; = ptradd array i;\n        store loc n4;\n        i: int = add i one;        \n        loc: ptr&lt;int&gt; = ptradd array i;\n        store loc n5;\n        ret array;\n    }\n\n    @print_array(array: ptr&lt;int&gt;, size: int) {\n        i: int = const 0;\n        one: int = const 1;\n    .loop:\n        cond: bool = lt i size;\n        br cond .body .done;\n    .body:\n        loc: ptr&lt;int&gt; = ptradd array i;\n        val: int = load loc;\n        print val;\n    .loop_end:\n        i: int = add i one;\n        jmp .loop;\n    .done:\n        ret;\n    }\n\n    @swap_cond(array: ptr&lt;int&gt;, j: int) {\n        one: int = const 1;\n        j_add_1: int = add j one;\n        loc: ptr&lt;int&gt; = ptradd array j;\n        loc_next: ptr&lt;int&gt; = ptradd array j_add_1;\n        elem_a: int = load loc;\n        elem_b: int = load loc_next;\n        \n        cond: bool = gt elem_a elem_b;\n        br cond .swap .done;\n    .swap:\n        store loc elem_b;\n        store loc_next elem_a;\n    .done:\n        ret;\n    }\n\n    # ARGS: 5 3 10 1 9 7\n    @main(size: int, n1: int, n2: int, n3: int, n4: int, n5: int) {\n    # Pack the input elements into an array with a starting pointer\n        array: ptr&lt;int&gt; = call @pack size n1 n2 n3 n4 n5;\n\n    # Bubble Sort\n    one: int = const 1;\n    i: int = const 0;\n    j: int = const 0;\n    sizei: int = sub size one;\n    .loopi:\n        condi: bool = lt i sizei;\n        br condi .bodyi .donei;\n    .bodyi:\n        sizej: int = sub size i;\n        sizej: int = sub sizej one;\n    .loopj:\n        condj: bool = lt j sizej;\n        br condj .bodyj .donej;\n    .bodyj:\n        call @swap_cond array j;\n    .loop_endj:\n        j: int = add j one;\n        jmp .loopj;\n    .donej:\n        j: int = const 0;\n    .loopi_end:\n        i: int = add i one;\n        jmp .loopi;\n    .donei:\n\n    # Print array\n        call @print_array array size;\n\n        free array;\n    }\n\n\n\n    {\n    \"BLOCK1\": [\n        \"BLOCK1\"\n    ]\n    }\n    {\n    \"BLOCK1\": [\n        \"BLOCK1\"\n    ],\n    \"body\": [\n        \"BLOCK1\",\n        \"body\",\n        \"loop\"\n    ],\n    \"done\": [\n        \"BLOCK1\",\n        \"done\",\n        \"loop\"\n    ],\n    \"loop\": [\n        \"BLOCK1\",\n        \"loop\"\n    ],\n    \"loop_end\": [\n        \"BLOCK1\",\n        \"body\",\n        \"loop\",\n        \"loop_end\"\n    ]\n    }\n    {\n    \"BLOCK1\": [\n        \"BLOCK1\"\n    ],\n    \"done\": [\n        \"BLOCK1\",\n        \"done\"\n    ],\n    \"swap\": [\n        \"BLOCK1\",\n        \"swap\"\n    ]\n    }\n    {\n    \"BLOCK1\": [\n        \"BLOCK1\"\n    ],\n    \"bodyi\": [\n        \"BLOCK1\",\n        \"bodyi\",\n        \"loopi\"\n    ],\n    \"bodyj\": [\n        \"BLOCK1\",\n        \"bodyi\",\n        \"bodyj\",\n        \"loopi\",\n        \"loopj\"\n    ],\n    \"donei\": [\n        \"BLOCK1\",\n        \"donei\",\n        \"loopi\"\n    ],\n    \"donej\": [\n        \"BLOCK1\",\n        \"bodyi\",\n        \"donej\",\n        \"loopi\",\n        \"loopj\"\n    ],\n    \"loop_endj\": [\n        \"BLOCK1\",\n        \"bodyi\",\n        \"bodyj\",\n        \"loop_endj\",\n        \"loopi\",\n        \"loopj\"\n    ],\n    \"loopi\": [\n        \"BLOCK1\",\n        \"loopi\"\n    ],\n    \"loopi_end\": [\n        \"BLOCK1\",\n        \"bodyi\",\n        \"donej\",\n        \"loopi\",\n        \"loopi_end\",\n        \"loopj\"\n    ],\n    \"loopj\": [\n        \"BLOCK1\",\n        \"bodyi\",\n        \"loopi\",\n        \"loopj\"\n    ]\n    }\n\n\n\n\n        {\n    \"BLOCK1\": []\n    }\n    {\n    \"BLOCK1\": [],\n    \"body\": [\n        \"loop\"\n    ],\n    \"done\": [],\n    \"loop\": [\n        \"loop\"\n    ],\n    \"loop_end\": [\n        \"loop\"\n    ]\n    }\n    {\n    \"BLOCK1\": [],\n    \"done\": [],\n    \"swap\": [\n        \"done\"\n    ]\n    }\n    {\n    \"BLOCK1\": [],\n    \"bodyi\": [\n        \"loopi\"\n    ],\n    \"bodyj\": [\n        \"loopj\"\n    ],\n    \"donei\": [],\n    \"donej\": [\n        \"loopi\"\n    ],\n    \"loop_endj\": [\n        \"loopj\"\n    ],\n    \"loopi\": [\n        \"loopi\"\n    ],\n    \"loopi_end\": [\n        \"loopi\"\n    ],\n    \"loopj\": [\n        \"loopi\",\n        \"loopj\"\n    ]\n    }\n\n\n\n\n        {\n    \"BLOCK1\": []\n    }\n    {\n    \"BLOCK1\": [\n        \"loop\"\n    ],\n    \"body\": [\n        \"loop_end\"\n    ],\n    \"done\": [],\n    \"loop\": [\n        \"body\",\n        \"done\"\n    ],\n    \"loop_end\": []\n    }\n    {\n    \"BLOCK1\": [\n        \"done\",\n        \"swap\"\n    ],\n    \"done\": [],\n    \"swap\": []\n    }\n    {\n    \"BLOCK1\": [\n        \"loopi\"\n    ],\n    \"bodyi\": [\n        \"loopj\"\n    ],\n    \"bodyj\": [\n        \"loop_endj\"\n    ],\n    \"donei\": [],\n    \"donej\": [\n        \"loopi_end\"\n    ],\n    \"loop_endj\": [],\n    \"loopi\": [\n        \"bodyi\",\n        \"donei\"\n    ],\n    \"loopi_end\": [],\n    \"loopj\": [\n        \"bodyj\",\n        \"donej\"\n    ]\n    }\n\n\n\n\n\n\n\n\n\n\n\nInput: dominators (dictionary), block_A (string), block_B (string)\nOutput: Boolean indicating if block_A dominates block_B\nLogic:\n\nIf block_B exists in dominators dictionary:\n\nCheck if block_A is in the list of dominators for block_B\nReturn True if it is, else return False\n\n\n\n\n\n\n\nLoad the dominator sets from a JSON file ex1.DOM.json\n\nOpen the file and parse JSON data into dominator_data dictionary\n\nDisplay the available blocks\n\nRetrieve keys from dominator_data and store in available_blocks\nPrint each block with its index number\n\nGet user input for block_A and block_B\n\nPrompt user to enter index for block_A and block_B\nConvert user input to integer and adjust for zero-based indexing\nUse these indices to select block_A and block_B from available_blocks\n\nCheck if block_A dominates block_B\n\nCall check_dominance function with dominator_data, block_A, block_B\nPrint result indicating if block_A dominates block_B\n\nExecute main function if the script is run directly\n\n\n\n\n\n\n\n\n    python3 checkDom.py ex1.DOM.json\n\n\n\n\n\n    {\n    \"BLOCK1\": [\n        \"BLOCK1\"\n    ],\n    \"end\": [\n        \"BLOCK1\",\n        \"end\"\n    ],\n    \"negative\": [\n        \"BLOCK1\",\n        \"negative\"\n    ],\n    \"positive\": [\n        \"BLOCK1\",\n        \"positive\"\n    ]\n    }\n\n\n\n\n    Available blocks:\n    1. BLOCK1\n    2. end\n    3. negative\n    4. positive\n\n    Enter the number for block A: 1\n    Enter the number for block B: 2\n\n    BLOCK1 dominates end\n\n\n\n\n\nThe dominator finding algorithm was complex and slow, and constructing the dominator tree required extensive testing and debugging to ensure accuracy.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Homework 4"
    ]
  },
  {
    "objectID": "blogs/rohit/2024-10-18-Rohit-HW4.html#dominance",
    "href": "blogs/rohit/2024-10-18-Rohit-HW4.html#dominance",
    "title": "Homework 4",
    "section": "",
    "text": "Definition: Dominance analysis is a fundamental concept in compiler optimization that helps us understand control flow relationships between basic blocks in a program. Let’s break down the key components and their implementations.\n\n\n\nA block D dominates block B if every path from the program entry to B must go through D\nProperties:\n\nEntry block dominates all blocks\nEvery block dominates itself\nIf A dominates B and B dominates C, then A dominates C (transitive)\n\nApplications:\n\nUsed in SSA form construction\nHelps identify natural loops\nCritical for many optimizations\n\n\n\n\n\n\nA compact representation of dominance relationships\nProperties:\n\nEach block (except entry) has exactly one immediate dominator\nForms a tree structure where parent dominates all its children\nImmediate dominator is the closest dominator in the path from entry\n\nBenefits:\n\nEfficient dominance queries\nSimplified analysis of nested structures\nQuick ancestor-descendant checks\n\n\n\n\n\n\nThe dominance frontier of block X contains blocks where X’s dominance stops\nKey aspects:\n\nCritical for SSA form construction\nIdentifies where to place φ-functions\nHelps determine variable live ranges\n\nUsage:\n\nControl dependence analysis\nData flow optimization\nLoop optimization\n\n\n\n\n\n\n\n\n\nInitialize with a stack.\nDefine methods for:\n\nReversing the graph.\nTraversing the graph in post-order.\nMerging sets.\nCalculating dominators.\nFinding dominance frontiers.\nBuilding a dominator tree.\n\n\n\n\n\n\nCreate a reversed graph dictionary.\n\n\n\n\n\nPerform post-order traversal on the graph starting from a given node.\n\n\n\n\n\nMerge multiple sets, finding the intersection.\n\n\n\n\n\nReverse the graph.\nTraverse the graph in post-order.\nInitialize dominator sets.\nIteratively update dominator sets until convergence.\n\n\n\n\n\nReverse the dominator sets.\nIdentify and return the dominance frontiers for each block.\n\n\n\n\n\nReverse the dominator sets.\nCreate strict dominator sets.\nCreate double strict dominator sets.\nBuild and return the dominator tree.\n\n\n\n\n\nInitialize DominanceProcessor.\nFor each function in BRIL:\n\nMap blocks and add entry/terminators.\nCalculate dominators.\nBased on analysis type, find frontiers or build tree.\nPrint results in JSON format.\n\n\n\n\n\n\nLoad BRIL from input.\nCall process_dominance with the specified analysis type.\n\n\n\n\n\n\n\n```{turnt.toml} [envs.DOM] command = “bril2json &lt; {filename} | python3 HW4.py DOM” output.”DOM.json” = “-”\n[envs.FRONT]\ncommand = \"bril2json &lt; {filename} | python3 HW4.py FRONT\"\noutput.\"FRONT.json\" = \"-\"\n\n[envs.TREE]\ncommand = \"bril2json &lt; {filename} | python3 HW4.py TREE\"\noutput.\"TREE.json\" = \"-\"\n#### Example 1: Simple Code: Absolute Value\n##### Input bril:\n\n\n\n\n\n\n```{bril}\n    @main(x: int) {\n    zero: int = const 0;\n    is_negative: bool = lt x zero;\n    br is_negative .negative .positive;\n    .negative:\n    result: int = sub zero x;\n    jmp .end;\n    .positive:\n    result: int = id x;\n    .end:\n    print result;\n    }\n\n\n    {\n    \"BLOCK1\": [\n        \"BLOCK1\"\n    ],\n    \"end\": [\n        \"BLOCK1\",\n        \"end\"\n    ],\n    \"negative\": [\n        \"BLOCK1\",\n        \"negative\"\n    ],\n    \"positive\": [\n        \"BLOCK1\",\n        \"positive\"\n    ]\n    }\n\n\n\n\n{\n  \"BLOCK1\": []\n}\n{\n  \"BLOCK1\": [],\n  \"body\": [\n    \"loop\"\n  ],\n  \"done\": [],\n  \"loop\": [\n    \"loop\"\n  ],\n  \"loop_end\": [\n    \"loop\"\n  ]\n}\n{\n  \"BLOCK1\": [],\n  \"done\": [],\n  \"swap\": [\n    \"done\"\n  ]\n}\n{\n  \"BLOCK1\": [],\n  \"bodyi\": [\n    \"loopi\"\n  ],\n  \"bodyj\": [\n    \"loopj\"\n  ],\n  \"donei\": [],\n  \"donej\": [\n    \"loopi\"\n  ],\n  \"loop_endj\": [\n    \"loopj\"\n  ],\n  \"loopi\": [\n    \"loopi\"\n  ],\n  \"loopi_end\": [\n    \"loopi\"\n  ],\n  \"loopj\": [\n    \"loopi\",\n    \"loopj\"\n  ]\n}\n\n\n\n\n    {\n    \"BLOCK1\": [\n        \"end\",\n        \"negative\",\n        \"positive\"\n    ],\n    \"end\": [],\n    \"negative\": [],\n    \"positive\": []\n    }\n\n\n\n\n\n\n\n        \n    @pack(size: int, n1: int, n2: int, n3: int, n4: int, n5: int) : ptr&lt;int&gt; {\n        one: int = const 1;\n        i: int = const 0;\n        array: ptr&lt;int&gt; = alloc size;\n    # Pack data into array manually. Cannot use loop because of the different var name.     \n        loc: ptr&lt;int&gt; = ptradd array i;\n        store loc n1;\n        i: int = add i one;\n        loc: ptr&lt;int&gt; = ptradd array i;\n        store loc n2;\n        i: int = add i one;        \n        loc: ptr&lt;int&gt; = ptradd array i;\n        store loc n3;\n        i: int = add i one;        \n        loc: ptr&lt;int&gt; = ptradd array i;\n        store loc n4;\n        i: int = add i one;        \n        loc: ptr&lt;int&gt; = ptradd array i;\n        store loc n5;\n        ret array;\n    }\n\n    @print_array(array: ptr&lt;int&gt;, size: int) {\n        i: int = const 0;\n        one: int = const 1;\n    .loop:\n        cond: bool = lt i size;\n        br cond .body .done;\n    .body:\n        loc: ptr&lt;int&gt; = ptradd array i;\n        val: int = load loc;\n        print val;\n    .loop_end:\n        i: int = add i one;\n        jmp .loop;\n    .done:\n        ret;\n    }\n\n    @swap_cond(array: ptr&lt;int&gt;, j: int) {\n        one: int = const 1;\n        j_add_1: int = add j one;\n        loc: ptr&lt;int&gt; = ptradd array j;\n        loc_next: ptr&lt;int&gt; = ptradd array j_add_1;\n        elem_a: int = load loc;\n        elem_b: int = load loc_next;\n        \n        cond: bool = gt elem_a elem_b;\n        br cond .swap .done;\n    .swap:\n        store loc elem_b;\n        store loc_next elem_a;\n    .done:\n        ret;\n    }\n\n    # ARGS: 5 3 10 1 9 7\n    @main(size: int, n1: int, n2: int, n3: int, n4: int, n5: int) {\n    # Pack the input elements into an array with a starting pointer\n        array: ptr&lt;int&gt; = call @pack size n1 n2 n3 n4 n5;\n\n    # Bubble Sort\n    one: int = const 1;\n    i: int = const 0;\n    j: int = const 0;\n    sizei: int = sub size one;\n    .loopi:\n        condi: bool = lt i sizei;\n        br condi .bodyi .donei;\n    .bodyi:\n        sizej: int = sub size i;\n        sizej: int = sub sizej one;\n    .loopj:\n        condj: bool = lt j sizej;\n        br condj .bodyj .donej;\n    .bodyj:\n        call @swap_cond array j;\n    .loop_endj:\n        j: int = add j one;\n        jmp .loopj;\n    .donej:\n        j: int = const 0;\n    .loopi_end:\n        i: int = add i one;\n        jmp .loopi;\n    .donei:\n\n    # Print array\n        call @print_array array size;\n\n        free array;\n    }\n\n\n\n    {\n    \"BLOCK1\": [\n        \"BLOCK1\"\n    ]\n    }\n    {\n    \"BLOCK1\": [\n        \"BLOCK1\"\n    ],\n    \"body\": [\n        \"BLOCK1\",\n        \"body\",\n        \"loop\"\n    ],\n    \"done\": [\n        \"BLOCK1\",\n        \"done\",\n        \"loop\"\n    ],\n    \"loop\": [\n        \"BLOCK1\",\n        \"loop\"\n    ],\n    \"loop_end\": [\n        \"BLOCK1\",\n        \"body\",\n        \"loop\",\n        \"loop_end\"\n    ]\n    }\n    {\n    \"BLOCK1\": [\n        \"BLOCK1\"\n    ],\n    \"done\": [\n        \"BLOCK1\",\n        \"done\"\n    ],\n    \"swap\": [\n        \"BLOCK1\",\n        \"swap\"\n    ]\n    }\n    {\n    \"BLOCK1\": [\n        \"BLOCK1\"\n    ],\n    \"bodyi\": [\n        \"BLOCK1\",\n        \"bodyi\",\n        \"loopi\"\n    ],\n    \"bodyj\": [\n        \"BLOCK1\",\n        \"bodyi\",\n        \"bodyj\",\n        \"loopi\",\n        \"loopj\"\n    ],\n    \"donei\": [\n        \"BLOCK1\",\n        \"donei\",\n        \"loopi\"\n    ],\n    \"donej\": [\n        \"BLOCK1\",\n        \"bodyi\",\n        \"donej\",\n        \"loopi\",\n        \"loopj\"\n    ],\n    \"loop_endj\": [\n        \"BLOCK1\",\n        \"bodyi\",\n        \"bodyj\",\n        \"loop_endj\",\n        \"loopi\",\n        \"loopj\"\n    ],\n    \"loopi\": [\n        \"BLOCK1\",\n        \"loopi\"\n    ],\n    \"loopi_end\": [\n        \"BLOCK1\",\n        \"bodyi\",\n        \"donej\",\n        \"loopi\",\n        \"loopi_end\",\n        \"loopj\"\n    ],\n    \"loopj\": [\n        \"BLOCK1\",\n        \"bodyi\",\n        \"loopi\",\n        \"loopj\"\n    ]\n    }\n\n\n\n\n        {\n    \"BLOCK1\": []\n    }\n    {\n    \"BLOCK1\": [],\n    \"body\": [\n        \"loop\"\n    ],\n    \"done\": [],\n    \"loop\": [\n        \"loop\"\n    ],\n    \"loop_end\": [\n        \"loop\"\n    ]\n    }\n    {\n    \"BLOCK1\": [],\n    \"done\": [],\n    \"swap\": [\n        \"done\"\n    ]\n    }\n    {\n    \"BLOCK1\": [],\n    \"bodyi\": [\n        \"loopi\"\n    ],\n    \"bodyj\": [\n        \"loopj\"\n    ],\n    \"donei\": [],\n    \"donej\": [\n        \"loopi\"\n    ],\n    \"loop_endj\": [\n        \"loopj\"\n    ],\n    \"loopi\": [\n        \"loopi\"\n    ],\n    \"loopi_end\": [\n        \"loopi\"\n    ],\n    \"loopj\": [\n        \"loopi\",\n        \"loopj\"\n    ]\n    }\n\n\n\n\n        {\n    \"BLOCK1\": []\n    }\n    {\n    \"BLOCK1\": [\n        \"loop\"\n    ],\n    \"body\": [\n        \"loop_end\"\n    ],\n    \"done\": [],\n    \"loop\": [\n        \"body\",\n        \"done\"\n    ],\n    \"loop_end\": []\n    }\n    {\n    \"BLOCK1\": [\n        \"done\",\n        \"swap\"\n    ],\n    \"done\": [],\n    \"swap\": []\n    }\n    {\n    \"BLOCK1\": [\n        \"loopi\"\n    ],\n    \"bodyi\": [\n        \"loopj\"\n    ],\n    \"bodyj\": [\n        \"loop_endj\"\n    ],\n    \"donei\": [],\n    \"donej\": [\n        \"loopi_end\"\n    ],\n    \"loop_endj\": [],\n    \"loopi\": [\n        \"bodyi\",\n        \"donei\"\n    ],\n    \"loopi_end\": [],\n    \"loopj\": [\n        \"bodyj\",\n        \"donej\"\n    ]\n    }\n\n\n\n\n\n\n\n\n\n\n\nInput: dominators (dictionary), block_A (string), block_B (string)\nOutput: Boolean indicating if block_A dominates block_B\nLogic:\n\nIf block_B exists in dominators dictionary:\n\nCheck if block_A is in the list of dominators for block_B\nReturn True if it is, else return False\n\n\n\n\n\n\n\nLoad the dominator sets from a JSON file ex1.DOM.json\n\nOpen the file and parse JSON data into dominator_data dictionary\n\nDisplay the available blocks\n\nRetrieve keys from dominator_data and store in available_blocks\nPrint each block with its index number\n\nGet user input for block_A and block_B\n\nPrompt user to enter index for block_A and block_B\nConvert user input to integer and adjust for zero-based indexing\nUse these indices to select block_A and block_B from available_blocks\n\nCheck if block_A dominates block_B\n\nCall check_dominance function with dominator_data, block_A, block_B\nPrint result indicating if block_A dominates block_B\n\nExecute main function if the script is run directly\n\n\n\n\n\n\n\n\n    python3 checkDom.py ex1.DOM.json\n\n\n\n\n\n    {\n    \"BLOCK1\": [\n        \"BLOCK1\"\n    ],\n    \"end\": [\n        \"BLOCK1\",\n        \"end\"\n    ],\n    \"negative\": [\n        \"BLOCK1\",\n        \"negative\"\n    ],\n    \"positive\": [\n        \"BLOCK1\",\n        \"positive\"\n    ]\n    }\n\n\n\n\n    Available blocks:\n    1. BLOCK1\n    2. end\n    3. negative\n    4. positive\n\n    Enter the number for block A: 1\n    Enter the number for block B: 2\n\n    BLOCK1 dominates end\n\n\n\n\n\nThe dominator finding algorithm was complex and slow, and constructing the dominator tree required extensive testing and debugging to ensure accuracy.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Homework 4"
    ]
  },
  {
    "objectID": "blogs/rohit/2024-10-17-Rohit-HW3.html",
    "href": "blogs/rohit/2024-10-17-Rohit-HW3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Definition: Data Flow Analysis (DFA) is a technique used in compiler optimization to analyze how values flow through program variables during execution.\n\nThe code should remove the instructions that are reassigned without being used.\nIf the value in being used in between the reassigning, then the instruction should be retained.\n\n\n\n\n\n\nFlows in direction of program execution\nPropagates information about variable definitions forward\nInitial state flows from start to end\n\n\n\n\n\nInformation flows from top to bottom\nComputes expressions already evaluated at each point\nPropagates forward through control flow graph\n\n\n\n\n\n\n\n\nInformation flows opposite to program execution\nStarts from variable uses and works backwards\nDetermines if variable value will be needed later\n\n\n\n\n\nFlows from bottom to top\nWorks backwards from expression uses\nDetermines what will definitely be computed later\n\n\n\n\n\n\nForward analyses use information from predecessor nodes\nBackward analyses use information from successor nodes\nDirection affects how we iterate through the control flow graph\nInitialization points differ (entry vs exit nodes)\n\n\n\n\n\n\n\n\nImport necessary modules (sys, json, etc.)\nDefine Analysis named tuple with attributes: forward, init, merge, transfer\n\n\n\n\n\nDefine union function: Merge multiple sets into one\nDefine intersection function: Find common elements among sets\n\n\n\n\n\nDetermine edges based on analysis direction (forward/backward)\nInitialize in_ and out sets for each block\nAdd all blocks to worklist\nWhile worklist is not empty:\n\nPop a block, calculate in_ value by merging predecessor/successor out values\nUpdate out value using transfer function\nIf out value changes, re-add successors/predecessors to worklist\n\n\n\n\n\n\nFormat sets and dictionaries into readable strings\n\n\n\n\n\nFor each function in the BRIL program:\n\nConvert instructions into blocks\nAdd terminators to blocks\nExecute worklist algorithm\nPrint in_ and out values for each block\n\n\n\n\n\n\nDefine functions to generate and kill variables/expressions for different analyses (liveness, reaching defs, very busy expressions, available expressions)\n\n\n\n\n\nDefine attributes and transfer functions for each analysis type (LIVENESS, VERYBUSY, REACHING, AVAILABLE)\n\n\n\n\n\nLoad BRIL program from input\nRun the specified data flow analysis\n\nFull code: https://github.com/gurusamyanandakuma-r/bril/blob/main/HW/HW3_Rohit/dfa.py\n\n\n\n\n\n\n```{turnt.toml} [envs.LIVENESS] command = “bril2json &lt; {filename} | python3 dfa.py LIVENESS” output.”LIVENESS.out” = “-”\n[envs.VERYBUSY]\ncommand = \"bril2json &lt; {filename} | python3 dfa.py VERYBUSY\"\noutput.\"VERYBUSY.out\" = \"-\"\n\n[envs.REACHING]\ncommand = \"bril2json &lt; {filename} | python3 dfa.py REACHING\"\noutput.\"REACHING.out\" = \"-\"\n\n[envs.AVAILABLE]\ncommand = \"bril2json &lt; {filename} | python3 dfa.py AVAILABLE\"\noutput.\"AVAILABLE.out\" = \"-\"\n#### Example 1: Without branches (one block code)\n##### Input bril:\n\n\n\n\n\n\n```{bril}\n    @main {\n\n        divisor: int = const 7;\n        divident: int = const 42;\n        quotient: int = div divident divisor;\n        product: int = mul quotient divisor;\n        remainder: int = sub divident product;  \n\n        print remainder; \n\n    }\n\n\n    BLOCK1:\n        IN:  NULL\n        OUT: NULL\n\n\n\n    BLOCK1:\n        IN:  ('div', ('divident', 'divisor')), ('mul', ('quotient', 'divisor')), ('sub', ('divident', 'product'))\n        OUT: NULL\n\n\n\n    BLOCK1:\n        IN:  NULL\n        OUT: ('divident', 'BLOCK1'), ('divisor', 'BLOCK1'), ('product', 'BLOCK1'), ('quotient', 'BLOCK1'), ('remainder', 'BLOCK1')\n\n\n\n    BLOCK1:\n        IN:  NULL\n        OUT: ('div', ('divident', 'divisor')), ('mul', ('quotient', 'divisor')), ('sub', ('divident', 'product'))\n\n\n\n\n\n\n    @main {\n    x: int = const 3;\n    y: int = const 4;\n    \n    .header:\n        cond: bool = lt x y;   # Check if x &lt; y\n        br cond .then .else;   # Branch based on condition\n\n    .then:\n        result: int = add x y;  # result = x + y\n        jmp .end;\n\n    .else:\n        result: int = sub x y;  # result = x - y\n        jmp .end;\n\n    .end:\n        print result;           # Print the result\n    }\n\n\n\n    BLOCK1:\n        IN:  NULL\n        OUT: x, y\n    header:\n        IN:  x, y\n        OUT: x, y\n    then:\n        IN:  x, y\n        OUT: result\n    else:\n        IN:  x, y\n        OUT: result\n    end:\n        IN:  result\n        OUT: NULL\n\n\n\n    BLOCK1:\n        IN:  ('lt', ('x', 'y'))\n        OUT: ('lt', ('x', 'y'))\n    header:\n        IN:  ('lt', ('x', 'y'))\n        OUT: NULL\n    then:\n        IN:  ('add', ('x', 'y'))\n        OUT: NULL\n    else:\n        IN:  ('sub', ('x', 'y'))\n        OUT: NULL\n    end:\n        IN:  NULL\n        OUT: NULL\n\n\n\n    BLOCK1:\n        IN:  NULL\n        OUT: ('x', 'BLOCK1'), ('y', 'BLOCK1')\n    header:\n        IN:  ('x', 'BLOCK1'), ('y', 'BLOCK1')\n        OUT: ('cond', 'header'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n    then:\n        IN:  ('cond', 'header'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n        OUT: ('cond', 'header'), ('result', 'then'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n    else:\n        IN:  ('cond', 'header'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n        OUT: ('cond', 'header'), ('result', 'else'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n    end:\n        IN:  ('cond', 'header'), ('result', 'else'), ('result', 'then'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n        OUT: ('cond', 'header'), ('result', 'else'), ('result', 'then'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n\n\n\n    BLOCK1:\n        IN:  NULL\n        OUT: NULL\n    header:\n        IN:  NULL\n        OUT: ('lt', ('x', 'y'))\n    then:\n        IN:  ('lt', ('x', 'y'))\n        OUT: ('add', ('x', 'y')), ('lt', ('x', 'y'))\n    else:\n        IN:  ('lt', ('x', 'y'))\n        OUT: ('lt', ('x', 'y')), ('sub', ('x', 'y'))\n    end:\n        IN:  ('lt', ('x', 'y'))\n        OUT: ('lt', ('x', 'y'))\n\n\n\n\n\n\nBalancing the forward and backward data flow analysis while managing the worklist efficiently wass tough. Each direction affects the edges and node values differently, which can lead to complicated logic.\n\n\n\n\nDeveloped code that handles liveness, availability, reaching definitions, and very busy expressions using a worklist algorithm. This ensures effective forward and backward analysis by generating and killing sets for each analysis type.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Homework 3"
    ]
  },
  {
    "objectID": "blogs/rohit/2024-10-17-Rohit-HW3.html#data-flow-analysis",
    "href": "blogs/rohit/2024-10-17-Rohit-HW3.html#data-flow-analysis",
    "title": "Homework 3",
    "section": "",
    "text": "Definition: Data Flow Analysis (DFA) is a technique used in compiler optimization to analyze how values flow through program variables during execution.\n\nThe code should remove the instructions that are reassigned without being used.\nIf the value in being used in between the reassigning, then the instruction should be retained.\n\n\n\n\n\n\nFlows in direction of program execution\nPropagates information about variable definitions forward\nInitial state flows from start to end\n\n\n\n\n\nInformation flows from top to bottom\nComputes expressions already evaluated at each point\nPropagates forward through control flow graph\n\n\n\n\n\n\n\n\nInformation flows opposite to program execution\nStarts from variable uses and works backwards\nDetermines if variable value will be needed later\n\n\n\n\n\nFlows from bottom to top\nWorks backwards from expression uses\nDetermines what will definitely be computed later\n\n\n\n\n\n\nForward analyses use information from predecessor nodes\nBackward analyses use information from successor nodes\nDirection affects how we iterate through the control flow graph\nInitialization points differ (entry vs exit nodes)\n\n\n\n\n\n\n\n\nImport necessary modules (sys, json, etc.)\nDefine Analysis named tuple with attributes: forward, init, merge, transfer\n\n\n\n\n\nDefine union function: Merge multiple sets into one\nDefine intersection function: Find common elements among sets\n\n\n\n\n\nDetermine edges based on analysis direction (forward/backward)\nInitialize in_ and out sets for each block\nAdd all blocks to worklist\nWhile worklist is not empty:\n\nPop a block, calculate in_ value by merging predecessor/successor out values\nUpdate out value using transfer function\nIf out value changes, re-add successors/predecessors to worklist\n\n\n\n\n\n\nFormat sets and dictionaries into readable strings\n\n\n\n\n\nFor each function in the BRIL program:\n\nConvert instructions into blocks\nAdd terminators to blocks\nExecute worklist algorithm\nPrint in_ and out values for each block\n\n\n\n\n\n\nDefine functions to generate and kill variables/expressions for different analyses (liveness, reaching defs, very busy expressions, available expressions)\n\n\n\n\n\nDefine attributes and transfer functions for each analysis type (LIVENESS, VERYBUSY, REACHING, AVAILABLE)\n\n\n\n\n\nLoad BRIL program from input\nRun the specified data flow analysis\n\nFull code: https://github.com/gurusamyanandakuma-r/bril/blob/main/HW/HW3_Rohit/dfa.py\n\n\n\n\n\n\n```{turnt.toml} [envs.LIVENESS] command = “bril2json &lt; {filename} | python3 dfa.py LIVENESS” output.”LIVENESS.out” = “-”\n[envs.VERYBUSY]\ncommand = \"bril2json &lt; {filename} | python3 dfa.py VERYBUSY\"\noutput.\"VERYBUSY.out\" = \"-\"\n\n[envs.REACHING]\ncommand = \"bril2json &lt; {filename} | python3 dfa.py REACHING\"\noutput.\"REACHING.out\" = \"-\"\n\n[envs.AVAILABLE]\ncommand = \"bril2json &lt; {filename} | python3 dfa.py AVAILABLE\"\noutput.\"AVAILABLE.out\" = \"-\"\n#### Example 1: Without branches (one block code)\n##### Input bril:\n\n\n\n\n\n\n```{bril}\n    @main {\n\n        divisor: int = const 7;\n        divident: int = const 42;\n        quotient: int = div divident divisor;\n        product: int = mul quotient divisor;\n        remainder: int = sub divident product;  \n\n        print remainder; \n\n    }\n\n\n    BLOCK1:\n        IN:  NULL\n        OUT: NULL\n\n\n\n    BLOCK1:\n        IN:  ('div', ('divident', 'divisor')), ('mul', ('quotient', 'divisor')), ('sub', ('divident', 'product'))\n        OUT: NULL\n\n\n\n    BLOCK1:\n        IN:  NULL\n        OUT: ('divident', 'BLOCK1'), ('divisor', 'BLOCK1'), ('product', 'BLOCK1'), ('quotient', 'BLOCK1'), ('remainder', 'BLOCK1')\n\n\n\n    BLOCK1:\n        IN:  NULL\n        OUT: ('div', ('divident', 'divisor')), ('mul', ('quotient', 'divisor')), ('sub', ('divident', 'product'))\n\n\n\n\n\n\n    @main {\n    x: int = const 3;\n    y: int = const 4;\n    \n    .header:\n        cond: bool = lt x y;   # Check if x &lt; y\n        br cond .then .else;   # Branch based on condition\n\n    .then:\n        result: int = add x y;  # result = x + y\n        jmp .end;\n\n    .else:\n        result: int = sub x y;  # result = x - y\n        jmp .end;\n\n    .end:\n        print result;           # Print the result\n    }\n\n\n\n    BLOCK1:\n        IN:  NULL\n        OUT: x, y\n    header:\n        IN:  x, y\n        OUT: x, y\n    then:\n        IN:  x, y\n        OUT: result\n    else:\n        IN:  x, y\n        OUT: result\n    end:\n        IN:  result\n        OUT: NULL\n\n\n\n    BLOCK1:\n        IN:  ('lt', ('x', 'y'))\n        OUT: ('lt', ('x', 'y'))\n    header:\n        IN:  ('lt', ('x', 'y'))\n        OUT: NULL\n    then:\n        IN:  ('add', ('x', 'y'))\n        OUT: NULL\n    else:\n        IN:  ('sub', ('x', 'y'))\n        OUT: NULL\n    end:\n        IN:  NULL\n        OUT: NULL\n\n\n\n    BLOCK1:\n        IN:  NULL\n        OUT: ('x', 'BLOCK1'), ('y', 'BLOCK1')\n    header:\n        IN:  ('x', 'BLOCK1'), ('y', 'BLOCK1')\n        OUT: ('cond', 'header'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n    then:\n        IN:  ('cond', 'header'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n        OUT: ('cond', 'header'), ('result', 'then'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n    else:\n        IN:  ('cond', 'header'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n        OUT: ('cond', 'header'), ('result', 'else'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n    end:\n        IN:  ('cond', 'header'), ('result', 'else'), ('result', 'then'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n        OUT: ('cond', 'header'), ('result', 'else'), ('result', 'then'), ('x', 'BLOCK1'), ('y', 'BLOCK1')\n\n\n\n    BLOCK1:\n        IN:  NULL\n        OUT: NULL\n    header:\n        IN:  NULL\n        OUT: ('lt', ('x', 'y'))\n    then:\n        IN:  ('lt', ('x', 'y'))\n        OUT: ('add', ('x', 'y')), ('lt', ('x', 'y'))\n    else:\n        IN:  ('lt', ('x', 'y'))\n        OUT: ('lt', ('x', 'y')), ('sub', ('x', 'y'))\n    end:\n        IN:  ('lt', ('x', 'y'))\n        OUT: ('lt', ('x', 'y'))\n\n\n\n\n\n\nBalancing the forward and backward data flow analysis while managing the worklist efficiently wass tough. Each direction affects the edges and node values differently, which can lead to complicated logic.\n\n\n\n\nDeveloped code that handles liveness, availability, reaching definitions, and very busy expressions using a worklist algorithm. This ensures effective forward and backward analysis by generating and killing sets for each analysis type.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Homework 3"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html",
    "title": "Paper Presentation",
    "section": "",
    "text": "Context: GPU compilers optimize programs for GPU hardware.\nProblem: Manual creation of heuristic rules is labor-intensive and requires experts.\nSolution Introduced: A new framework uses off-policy deep reinforcement learning to automate heuristic rule generation.\nObjective: Enhance frame rates of graphics applications.\nResilience: The new heuristics remain effective through compiler updates, reducing the need for frequent retraining.\nResults: The framework matches or exceeds traditional heuristics in 98% of benchmarks, improving frame rates by 1.6% to 15.8%.\n\n\n\n\n\nLet’s break down the concepts of “black box” and “glass box” paradigms in compiler autotuning frameworks:\n\n“Black Box” Paradigm:\n\nDefinition: In this approach, optimization is done from the outside. The framework uses pragmas (directives) and optimization flags to tweak the code for better performance.\nLearning Agent: The agent responsible for learning and improving the performance (like a human expert or a computational model) is separate from the compiler itself.\nLimitations: When new code is introduced, this external learning agent might not be available or integrated with the compiler, which can limit how well-tuned the optimizations are.\n\n“Glass Box” Paradigm:\n\nDefinition: This approach integrates the learning agent directly into the compiler. The agent has visibility into the internal workings of the compiler and can interact with the code more deeply.\nAdvantages: Because the learning agent is part of the compiler, it can continue to optimize new code introduced during production. This means a well-trained solution remains effective and adaptable without needing external adjustments.\n\n\n\n\n\n\n\nOptimal Heuristic Settings: Finding the best optimization settings for compilers could theoretically be done by trial-and-error across all possibilities. However, this is impractical due to the extensive time and computational resources required, especially for complex graphics benchmarks.\nChallenges: Graphics benchmarks have long runtimes. Production compilers are frequently updated, making manual tuning continuously laborious.\nStandard Approaches:\n\nExpert-Driven Autotuning: Heuristics (optimization rules) are hand-tuned by experts over selected benchmarks. This requires significant expertise and might miss complex patterns.\nMachine Learning-Based Autotuning: Machine learning can automatically learn complex, non-linear optimization functions in high-dimensional spaces.\n\nIdeal for Compiler Autotuning: It can handle the non-linear nature and local minima challenges of optimizing heuristics across different hardware.\nBenefits of ML-Based Autotuning: ML models can generalize better to new programs and can be re-tuned more quickly for new hardware with a well-labeled dataset.\n“Glass Box” Framework: In this approach, the ML models that are learned are integrated within the compiler itself, allowing them to control the optimizations directly.\n\n\n\n\n\nShaders and Their Role:\n\nShaders: Programs in graphics applications that render frames by projecting 3D shapes onto a 2D display and determining each pixel’s color and opacity.\nLanguages and Resources: Written in high-level languages, using resources like multi-dimensional arrays.\n\nCompilation Process:\n\nFront-End Compiler: Converts shaders into device-independent byte code.\nDriver Translation: Translates byte code into machine-dependent byte code.\nBack-End Compiler: Converts byte code into the instruction set architecture (ISA) specific to the GPU.\n\nExecution: ISA and dynamic resource information are combined and sent to the GPU for execution. Multiple shader pipelines can run asynchronously.\nChallenges and Heuristics:\n\nDynamic Information: Efforts to use dynamic information for optimization often fail as the back-end compiler lacks resource knowledge during compile time.\nSimplified Heuristics: Leads to assumptions that speeding up individual programs will improve the entire application.\n\n\n\n\n\nFocus on Supervised Learning: Most machine learning (ML)-based autotuning frameworks use supervised learning to create predictive models based on labeled data.\nPerformance and Training: Supervised learning has achieved top performance in compiler autotuning by using inputs derived from the program code and hardware characteristics, and labels based on performance measurements like execution times or frame rates.\nQuality of Training Data: The success of supervised learning depends heavily on the quality of the training dataset. Poor-quality or corrupted data can lead to models that don’t perform well in real-world scenarios.\nChallenges in GPU Compiler Autotuning: For real-world graphics applications, obtaining accurate performance labels is challenging due to the non-deterministic and multi-threaded nature of GPU executions, making performance measurement computationally intensive and often impractical.\n\n\n\n\n\nReinforcement Learning Basics: Unlike supervised learning (SL), RL doesn’t need pre-determined labels. Instead, it uses a reward signal from the environment.\nPolicy: The decision function that maps states to actions. In deep reinforcement learning (DRL), this is modeled using a deep neural network (DNN).\n\nObjective: To learn a policy (π) that maximizes cumulative expected rewards over a series of states, known as a trajectory (τ).\n\nTraining Process: RL involves trial-and-error, with reward signals guiding the updates to parameters. The “state, action, reward” tuples are generated through interactions between the policy and the environment, forming a feedback loop.\nApplications: RL is typically used for sequential decision-making problems but can also achieve high performance in tasks like classification or detection. Unlike SL, which trains on large static datasets, RL collects data dynamically throughout the training process.\nCompiler Autotuning with DRL:\n\nPrevious works have applied DRL to compiler autotuning.\nStates: Derived from characteristics of program code.\nActions: Applied code optimizations or heuristic settings.\nRewards: Based on performance measurements.\n\n\n\n\n\nWhat is Q-Learning?: Q-Learning is a reinforcement learning (RL) strategy that helps an agent learn how to act optimally by evaluating the quality of different actions in different states, known as state-action pairs.\nQ-Table: In discrete action spaces, the evaluations (or values) of state-action pairs are stored in a Q-table. Each entry represents the expected cumulative reward (discounted by a factor γ) for taking a certain action in a certain state and following a specific policy π.\nOptimal Policy: The goal is to find the optimal policy (π*) that maximizes expected cumulative reward over time. This is achieved by updating the Q-table through trial-and-error interactions with the environment. When following the optimal policy, the agent chooses actions that maximize the expected reward from an initial state s0.\nConvergence: Q-Learning is proven to converge to the optimal values of state-action pairs with probability 1, given that all state-action pairs are sampled repeatedly. Over time, the Q-table reflects the best possible actions for every state.\nExpected Cumulative Discounted Reward: The values in the Q-table, denoted as Q*(s, a), represent the expected cumulative reward, discounted by time, when following the optimal policy through a trajectory τ of size T.\n\n\n\n\n\n\nOn-Policy DRL:\n- Standard DRL algorithms require frequent interaction with a stationary environment, using collected data only once to update internal parameters, making them sample inefficient. - Research shows that on-policy algorithms are more simulation-bound compared to off-policy counterparts. - Relying solely on on-policy DRL can bottleneck GPU compiler autotuning due to long graphic benchmark runtimes and frequent compiler updates.\nOff-Policy DRL:\n- Off-policy DRL learns from previously collected data without needing frequent interaction with the environment, suitable for GPU compiler auto-tuning. - This approach allows decoupling data collection and model training, leveraging existing performance automation workflows to prepare offline datasets. - However, off-policy methods face instability without corrective feedback and require careful hyperparameter tuning. - To address these issues, a Q-learning-based strategy with a controlled feedback loop between data collection and model training is proposed.\n\n\n\nThe objective here will be to identify those heuristic settings that achieve the highest possible frame rate improvements for shaders, while deploying stable compiler heuristics in light of frequent code changes.\n\nEnvironment: Non-stationary system: static hardware with evolving compiler revisions over time.\nStates: Extracted from the IR of the shader, which changes according to the compiler updates.\nActions: Heuristic settings used at compile time. Optimal actions result in maximum increases in frame rates.\nRewards: Observed frame rate improvements when performing an action on states.\nPolicy: A DNN-based decision function which is trained to maximize the expected frame rates in a non-stationary environment.\n\n\n\n\nThe author propose a Q-learning-based off-policy RL strategy to generate GPU compiler heuristics.\n\nTraining Objective: Optimize a Q-table to maximize expected frame rate improvements for heuristic actions applied to given states.\n\n• Inference Objective: Fit a DNN-based decision policy to approximate the optimal policy by minimizing divergence between them. - Integration: The trained inference model will serve as a heuristic decision function integrated into the compiler. - Policy Differentiation: During training, iterate on the decision policy while the behavior policy is frozen at inference time for stable deployment.\n\n\n\nThe framework addresses the high cost of GPU graphics benchmarks with a robust and generalizable auto-tuning pipeline comprising three modules in a feedback loop: continuous integration, data collection, and model training.\n\nContinuous Integration (CI): Deploys the behavior policy as a heuristic in the latest production compiler.\nData Collection: Gathers performance metrics and IRs from the graphics benchmarks.\nModel Training: Updates Q-table and trains the decision policy to approximate the optimal policy for maximum expected frame rates across the application suite. Starting with a randomly initialized decision policy, the framework iteratively refines the model to generate stable and efficient GPU compiler heuristics.\n\n\n\n\nContinuous integration means developers’ source code revisions are incorporated into a shared mainline many times a day. For the generated code, this tempo translates to significant changes in a production compiler. Unlike traditional ML-based autotuning based on frozen snapshots of compilers, our approach integrates the behavior policy into a continuously updated compiler.\n\n\n\nThe compiler inference engine applies the behavior policy as a heuristic to the IR before machine-dependent optimizations.\n\nState Analysis: The state, derived from the incoming IR, is analyzed by the model to determine the optimal heuristic setting.\nContinuous Integration: The policy of behavior is compiled into the most recent compiler for each run of performance.\nData Collection: Every trial collects the static features of the IR, applied heuristic settings, and observed frame rates from the benchmark suite.\n\nStatic features extracted pre-inference are used to build the state to apply heuristics in a time and memory-efficient and reproducible manner.\n\n\n\nA stable policy generalizing well relies on a meaningful reward signal that encourages desired behavior. In a multi-threaded environment, the performance of one optimized shader sometimes degrades in others due to shared resource competition. The reward signal is defined by the normalized change in frame rate compared to the global default compiler actions. Given a baseline frame rate F0​ and observed frame rate F, the reward is the relative speed-up, F/F0​. But as compilers improve, the performance measurements from the past become outdated.\n\n\n\nRepresentation of source code for machine learning can be static or dynamic. Dynamic techniques depend on performance counters, which provide a compact summary at runtime, but are expensive to collect and unavailable at compile time. Static techniques, like those in natural language processing, directly extract features from the source code, which is resource-intensive. We extract the features at compile time from the compiler IR in the form of fast, machine-independent features, such as total instructions, basic blocks, and memory operations to meet the constraints of production compilers. This yields a fixed-length feature vector of 44 attributes, consuming merely 176 bytes at 32-bit floating point precision.\n\n\n\n\nOff-Policy DRL Algorithms: Reuse data collected asynchronously, allowing training without bottlenecks from data collection. This is crucial in production systems, where the compiler and data continuously evolve. We implement a Q-learning strategy for stable heuristics in GPU compiler autotuning.\n\n\n\n\n\nQ-Table and Probabilistic Policy:\n\nThe Q-table represents the expected reward for each state-action pair over a set of applications. It simplifies to the expected performance of applying a heuristic action given a shader’s state, modeled as a single time-step Markov Decision Process (MDP).\nThe optimal policy selects the action that maximizes the probability of achieving the best performance for a given state. Alternatively, the policy can be derived using the Boltzmann softmax operator to balance exploration and exploitation, controlled by the temperature parameter.\n\nQ-Table Updates and Staleness Discounting:\n\nAs the compiler evolves, older performance measurements may become stale. Q-table updates include a discount factor, diminishing the weight of older data based on elapsed time.\nFor existing state-action pairs, the Q-value is updated as a weighted average of the prior value and the newly observed reward, modulated by a learning rate. For unseen states, the Q-value initializes to the observed reward.\n\nHyperparameter Tuning and Policy Behavior:\n\nThe temperature parameter in the softmax operator adjusts the trade-off between exploration and exploitation.\nThe learning rate and discount factor are tunable hyperparameters, ensuring adaptability to changing compiler behavior and iterative data collection.\n\n\n\n\n\n\nDynamic Q-Table and Generalization Issue: Compiler updates change the intermediate representation (IR), creating a non-stationary environment (EEE) and expanding the Q-table with new state-action pairs. This makes the Q-table too large and impractical while lacking the ability to generalize to unseen states.\nModel-Based Policy Approximation and Iterative Learning: To address these issues, train decision policy to approximates the optimal decision policy by minimizing the KL divergence between the empirical Q-table policy and the model’s output. This model generalizes to new states and is periodically deployed to guide compiler decisions, with new performance data updating the Q-table in an iterative DRL feedback loop.\n\n\n    Algorithm 1: RL-based GPU Compiler Autotuning\n    Result: (N)\n    Randomly initialize ;\n    Initialize an empty Q-table;\n    while i &lt; numIterations do\n        Copy learned parameters of decision policy to behaviour policy (i);\n        Integrate behaviour policy into the latest compiler;\n        Get states and performance measurements over A;\n        Update (or expand) the Q-table based on the observed performance measurements;\n        Convert the empirical Q-table to probabilities optimal decision policy(sa);\n        Train decision policy(a|s) to approximate optimal decision policy(sa);\n    end\n\n\n\n\n\n\nOff-Policy DRL Algorithms: Reuse data collected asynchronously, allowing training without bottlenecks from data collection. This is crucial in production systems, where the compiler and data continuously evolve. We implement a Q-learning strategy for stable heuristics in GPU compiler autotuning.\n\n\n\n\n\nQ-Table and Probabilistic Policy:\n\nThe Q-table represents the expected reward for each state-action pair over a set of applications. It simplifies to the expected performance of applying a heuristic action given a shader’s state, modeled as a single time-step Markov Decision Process (MDP).\nThe optimal policy selects the action that maximizes the probability of achieving the best performance for a given state. Alternatively, the policy can be derived using the Boltzmann softmax operator to balance exploration and exploitation, controlled by the temperature parameter.\n\nQ-Table Updates and Staleness Discounting:\n\nAs the compiler evolves, older performance measurements may become stale. Q-table updates include a discount factor, diminishing the weight of older data based on elapsed time.\nFor existing state-action pairs, the Q-value is updated as a weighted average of the prior value and the newly observed reward, modulated by a learning rate. For unseen states, the Q-value initializes to the observed reward.\n\nHyperparameter Tuning and Policy Behavior:\n\nThe temperature parameter in the softmax operator adjusts the trade-off between exploration and exploitation.\nThe learning rate and discount factor are tunable hyperparameters, ensuring adaptability to changing compiler behavior and iterative data collection. ### Adapting to Production Compiler Development\n\n\n\nDynamic Environment: Production-level compilers often change rapidly, resulting in a dynamic and non-stationary environment, with constantly evolving conditions and code.\nNon-Stationary Environment: Defined by target hardware and continuously updated production compilers, leading to variability and complexity.\nImpact on IR Instructions: Percentage change in intermediate representation (IR) instructions for a shader can vary by up to 50% over a year, indicating significant code structure changes.\nGrowth of Q-Table: The rapid pace of software development leads to an increase in the state space, causing the Q-table to grow. To manage growth and memory overhead, a DNN decision policy (πθ) is trained to approximate the optimal policy (π∗).\nPolicy Deployment: The trained DNN policy (πθ) is periodically deployed as a practical policy (πβ), maintaining an efficient and updated policy without excessive memory requirements of a growing Q-table.\n\n\n\n\nWavefront Sizes: AMD RDNA™ graphics architecture supports two wavefront sizes: wave32 (32 work-items) and wave64 (64 work-items). The optimal wavefront size depends on dynamic shader properties such as divergence and memory access patterns.\nBandwidth Considerations:\n\nWave64: Can offer better performance if there is sufficient bandwidth, allowing for more simultaneous memory accesses without causing cache misses.\nWave32: Reduces bandwidth strain by spreading out memory accesses over time, beneficial when multiple shaders are running concurrently.\n\nDynamic Environment: System bandwidth and other resources are dynamic and shared among concurrently running shaders. The best wavefront size depends on the real-time execution environment, which the compiler doesn’t know at compile time.\n\nRL-Based Autotuning Framework: Improves frame rates by selecting the optimal wavefront size for each shader at compile time.\nState Representation: The state of each shader is represented as a fixed-length vector of static features from the compiler’s intermediate representation (IR).\nAction Space: Limited to two options: wave32 and wave64.\nReward Signal: Based on changes in frame rate compared to default behavior.\n\nPolicy and Implementation: The decision policy (πθ) is a lightweight, 3-layer feed-forward classifier with less than 20 KB of learnable parameters. The learned parameters are periodically integrated into the compiler as the behavior policy (πβ).\nExperimental Results: The RL framework was tested on the AMD Radeon™ 6800 XT and achieved frame rates matching or surpassing 98% of graphics benchmarks. Performance improvements ranged from an average increase of 1.6% to a maximum of 15.8%. The model converged in only 45 iterations per benchmark application. Experiments were conducted using over 150 graphics benchmarks, each with an average of 230 unique shaders.\n\n\n\n\n\nGeneralization: The model needs to generalize well to handle new code generated by the frequently updated compiler IR. A well-trained model should perform effectively without needing frequent updates to its learned parameters.\nStability Metric: Stability is measured using the inverse of the rate of statistically significant regressions. This metric shows how often the model’s performance does not decline significantly. A 1-tailed t-test with a p-value of 5% is used to determine statistical significance.\nShader Wavefront Size Selection: The histogram in Figure 8 illustrates changes in frame rate when the RL-based compiler heuristic is used for selecting shader wavefront size on the AMD Radeon™ RX 5700 XT, demonstrating how the RL-based method impacts performance across various graphics applications.\nStability Over Time: Stability is tracked over a year of production compiler updates without retraining the deployed model. A value of 100% indicates no statistically significant performance regressions in benchmarks when using the DNN heuristic compared to the default compiler behavior.\n\n\n\n\n\nTransfer Learning: Instead of starting from scratch, the researchers use transfer learning to optimize the wavefront execution mode for a different GPU (the AMD Radeon™ RX 5700 XT), utilizing the final Q-table and behavior policy (πβ) from previous experiments as the starting point.\nEfficiency and Performance: With transfer learning, the model needed only 10 iterations over the set of graphics benchmarks to match or surpass previous frame rates in 94.4% of the benchmarks. Improvements included increases of up to 10.3% and an average improvement of 1.5%.\nBenchmarking: Experiments were conducted using the AMD production graphics compiler on over 270 graphics benchmarks, each with an average of 230 unique shaders. Figure 8 in the document provides a histogram of these results, showing the distribution of observed changes in frame rates.\n\n\n\n\n\n\nFramework Development: The authors developed a GPU compiler autotuning framework using off-policy deep reinforcement learning (DRL) to generate heuristics that improve frame rates in graphics applications.\nContinuous Integration and Q-Learning: The framework combines continuous integration (CI) with Q-Learning to find optimal heuristic settings that maximize frame rate improvements across various graphics benchmarks.\nDeployment: By considering the rapid changes in software development, the trained models can be deployed as stable heuristics in evolving production compilers.\nGeneralized Gains: The framework demonstrates generalized performance gains across a large suite of graphics benchmarks and different GPUs.\n\n\n\n\n\nExplore Static Counters and Dynamic Properties: Investigate the relationship between the set of static counters and the dynamic properties that the neural network has learned to account for.\nExtend to Continuous Action Spaces: Aim to extend the framework to domains with continuous action spaces using techniques from deep Q-Learning.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#abstract",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#abstract",
    "title": "Paper Presentation",
    "section": "",
    "text": "Context: GPU compilers optimize programs for GPU hardware.\nProblem: Manual creation of heuristic rules is labor-intensive and requires experts.\nSolution Introduced: A new framework uses off-policy deep reinforcement learning to automate heuristic rule generation.\nObjective: Enhance frame rates of graphics applications.\nResilience: The new heuristics remain effective through compiler updates, reducing the need for frequent retraining.\nResults: The framework matches or exceeds traditional heuristics in 98% of benchmarks, improving frame rates by 1.6% to 15.8%.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#introduction",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#introduction",
    "title": "Paper Presentation",
    "section": "",
    "text": "Let’s break down the concepts of “black box” and “glass box” paradigms in compiler autotuning frameworks:\n\n“Black Box” Paradigm:\n\nDefinition: In this approach, optimization is done from the outside. The framework uses pragmas (directives) and optimization flags to tweak the code for better performance.\nLearning Agent: The agent responsible for learning and improving the performance (like a human expert or a computational model) is separate from the compiler itself.\nLimitations: When new code is introduced, this external learning agent might not be available or integrated with the compiler, which can limit how well-tuned the optimizations are.\n\n“Glass Box” Paradigm:\n\nDefinition: This approach integrates the learning agent directly into the compiler. The agent has visibility into the internal workings of the compiler and can interact with the code more deeply.\nAdvantages: Because the learning agent is part of the compiler, it can continue to optimize new code introduced during production. This means a well-trained solution remains effective and adaptable without needing external adjustments.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#background",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#background",
    "title": "Paper Presentation",
    "section": "",
    "text": "Optimal Heuristic Settings: Finding the best optimization settings for compilers could theoretically be done by trial-and-error across all possibilities. However, this is impractical due to the extensive time and computational resources required, especially for complex graphics benchmarks.\nChallenges: Graphics benchmarks have long runtimes. Production compilers are frequently updated, making manual tuning continuously laborious.\nStandard Approaches:\n\nExpert-Driven Autotuning: Heuristics (optimization rules) are hand-tuned by experts over selected benchmarks. This requires significant expertise and might miss complex patterns.\nMachine Learning-Based Autotuning: Machine learning can automatically learn complex, non-linear optimization functions in high-dimensional spaces.\n\nIdeal for Compiler Autotuning: It can handle the non-linear nature and local minima challenges of optimizing heuristics across different hardware.\nBenefits of ML-Based Autotuning: ML models can generalize better to new programs and can be re-tuned more quickly for new hardware with a well-labeled dataset.\n“Glass Box” Framework: In this approach, the ML models that are learned are integrated within the compiler itself, allowing them to control the optimizations directly.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#production-gpu-compiler-autotuning",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#production-gpu-compiler-autotuning",
    "title": "Paper Presentation",
    "section": "",
    "text": "Shaders and Their Role:\n\nShaders: Programs in graphics applications that render frames by projecting 3D shapes onto a 2D display and determining each pixel’s color and opacity.\nLanguages and Resources: Written in high-level languages, using resources like multi-dimensional arrays.\n\nCompilation Process:\n\nFront-End Compiler: Converts shaders into device-independent byte code.\nDriver Translation: Translates byte code into machine-dependent byte code.\nBack-End Compiler: Converts byte code into the instruction set architecture (ISA) specific to the GPU.\n\nExecution: ISA and dynamic resource information are combined and sent to the GPU for execution. Multiple shader pipelines can run asynchronously.\nChallenges and Heuristics:\n\nDynamic Information: Efforts to use dynamic information for optimization often fail as the back-end compiler lacks resource knowledge during compile time.\nSimplified Heuristics: Leads to assumptions that speeding up individual programs will improve the entire application.\n\n\n\n\n\nFocus on Supervised Learning: Most machine learning (ML)-based autotuning frameworks use supervised learning to create predictive models based on labeled data.\nPerformance and Training: Supervised learning has achieved top performance in compiler autotuning by using inputs derived from the program code and hardware characteristics, and labels based on performance measurements like execution times or frame rates.\nQuality of Training Data: The success of supervised learning depends heavily on the quality of the training dataset. Poor-quality or corrupted data can lead to models that don’t perform well in real-world scenarios.\nChallenges in GPU Compiler Autotuning: For real-world graphics applications, obtaining accurate performance labels is challenging due to the non-deterministic and multi-threaded nature of GPU executions, making performance measurement computationally intensive and often impractical.\n\n\n\n\n\nReinforcement Learning Basics: Unlike supervised learning (SL), RL doesn’t need pre-determined labels. Instead, it uses a reward signal from the environment.\nPolicy: The decision function that maps states to actions. In deep reinforcement learning (DRL), this is modeled using a deep neural network (DNN).\n\nObjective: To learn a policy (π) that maximizes cumulative expected rewards over a series of states, known as a trajectory (τ).\n\nTraining Process: RL involves trial-and-error, with reward signals guiding the updates to parameters. The “state, action, reward” tuples are generated through interactions between the policy and the environment, forming a feedback loop.\nApplications: RL is typically used for sequential decision-making problems but can also achieve high performance in tasks like classification or detection. Unlike SL, which trains on large static datasets, RL collects data dynamically throughout the training process.\nCompiler Autotuning with DRL:\n\nPrevious works have applied DRL to compiler autotuning.\nStates: Derived from characteristics of program code.\nActions: Applied code optimizations or heuristic settings.\nRewards: Based on performance measurements.\n\n\n\n\n\nWhat is Q-Learning?: Q-Learning is a reinforcement learning (RL) strategy that helps an agent learn how to act optimally by evaluating the quality of different actions in different states, known as state-action pairs.\nQ-Table: In discrete action spaces, the evaluations (or values) of state-action pairs are stored in a Q-table. Each entry represents the expected cumulative reward (discounted by a factor γ) for taking a certain action in a certain state and following a specific policy π.\nOptimal Policy: The goal is to find the optimal policy (π*) that maximizes expected cumulative reward over time. This is achieved by updating the Q-table through trial-and-error interactions with the environment. When following the optimal policy, the agent chooses actions that maximize the expected reward from an initial state s0.\nConvergence: Q-Learning is proven to converge to the optimal values of state-action pairs with probability 1, given that all state-action pairs are sampled repeatedly. Over time, the Q-table reflects the best possible actions for every state.\nExpected Cumulative Discounted Reward: The values in the Q-table, denoted as Q*(s, a), represent the expected cumulative reward, discounted by time, when following the optimal policy through a trajectory τ of size T.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#on-policy-vs.-off-policy-drl-for-gpu-compiler-autotuning",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#on-policy-vs.-off-policy-drl-for-gpu-compiler-autotuning",
    "title": "Paper Presentation",
    "section": "",
    "text": "On-Policy DRL:\n- Standard DRL algorithms require frequent interaction with a stationary environment, using collected data only once to update internal parameters, making them sample inefficient. - Research shows that on-policy algorithms are more simulation-bound compared to off-policy counterparts. - Relying solely on on-policy DRL can bottleneck GPU compiler autotuning due to long graphic benchmark runtimes and frequent compiler updates.\nOff-Policy DRL:\n- Off-policy DRL learns from previously collected data without needing frequent interaction with the environment, suitable for GPU compiler auto-tuning. - This approach allows decoupling data collection and model training, leveraging existing performance automation workflows to prepare offline datasets. - However, off-policy methods face instability without corrective feedback and require careful hyperparameter tuning. - To address these issues, a Q-learning-based strategy with a controlled feedback loop between data collection and model training is proposed.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#problem-formulation-for-rl-based-gpu-compiler-autotuning",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#problem-formulation-for-rl-based-gpu-compiler-autotuning",
    "title": "Paper Presentation",
    "section": "",
    "text": "The objective here will be to identify those heuristic settings that achieve the highest possible frame rate improvements for shaders, while deploying stable compiler heuristics in light of frequent code changes.\n\nEnvironment: Non-stationary system: static hardware with evolving compiler revisions over time.\nStates: Extracted from the IR of the shader, which changes according to the compiler updates.\nActions: Heuristic settings used at compile time. Optimal actions result in maximum increases in frame rates.\nRewards: Observed frame rate improvements when performing an action on states.\nPolicy: A DNN-based decision function which is trained to maximize the expected frame rates in a non-stationary environment.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#solution-overview",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#solution-overview",
    "title": "Paper Presentation",
    "section": "",
    "text": "The author propose a Q-learning-based off-policy RL strategy to generate GPU compiler heuristics.\n\nTraining Objective: Optimize a Q-table to maximize expected frame rate improvements for heuristic actions applied to given states.\n\n• Inference Objective: Fit a DNN-based decision policy to approximate the optimal policy by minimizing divergence between them. - Integration: The trained inference model will serve as a heuristic decision function integrated into the compiler. - Policy Differentiation: During training, iterate on the decision policy while the behavior policy is frozen at inference time for stable deployment.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#rl-based-gpu-compiler-auto-tuning",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#rl-based-gpu-compiler-auto-tuning",
    "title": "Paper Presentation",
    "section": "",
    "text": "The framework addresses the high cost of GPU graphics benchmarks with a robust and generalizable auto-tuning pipeline comprising three modules in a feedback loop: continuous integration, data collection, and model training.\n\nContinuous Integration (CI): Deploys the behavior policy as a heuristic in the latest production compiler.\nData Collection: Gathers performance metrics and IRs from the graphics benchmarks.\nModel Training: Updates Q-table and trains the decision policy to approximate the optimal policy for maximum expected frame rates across the application suite. Starting with a randomly initialized decision policy, the framework iteratively refines the model to generate stable and efficient GPU compiler heuristics.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#continuous-integration",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#continuous-integration",
    "title": "Paper Presentation",
    "section": "",
    "text": "Continuous integration means developers’ source code revisions are incorporated into a shared mainline many times a day. For the generated code, this tempo translates to significant changes in a production compiler. Unlike traditional ML-based autotuning based on frozen snapshots of compilers, our approach integrates the behavior policy into a continuously updated compiler.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#inference-engine",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#inference-engine",
    "title": "Paper Presentation",
    "section": "",
    "text": "The compiler inference engine applies the behavior policy as a heuristic to the IR before machine-dependent optimizations.\n\nState Analysis: The state, derived from the incoming IR, is analyzed by the model to determine the optimal heuristic setting.\nContinuous Integration: The policy of behavior is compiled into the most recent compiler for each run of performance.\nData Collection: Every trial collects the static features of the IR, applied heuristic settings, and observed frame rates from the benchmark suite.\n\nStatic features extracted pre-inference are used to build the state to apply heuristics in a time and memory-efficient and reproducible manner.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#robust-reward-signals",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#robust-reward-signals",
    "title": "Paper Presentation",
    "section": "",
    "text": "A stable policy generalizing well relies on a meaningful reward signal that encourages desired behavior. In a multi-threaded environment, the performance of one optimized shader sometimes degrades in others due to shared resource competition. The reward signal is defined by the normalized change in frame rate compared to the global default compiler actions. Given a baseline frame rate F0​ and observed frame rate F, the reward is the relative speed-up, F/F0​. But as compilers improve, the performance measurements from the past become outdated.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#data-collection",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#data-collection",
    "title": "Paper Presentation",
    "section": "",
    "text": "Representation of source code for machine learning can be static or dynamic. Dynamic techniques depend on performance counters, which provide a compact summary at runtime, but are expensive to collect and unavailable at compile time. Static techniques, like those in natural language processing, directly extract features from the source code, which is resource-intensive. We extract the features at compile time from the compiler IR in the form of fast, machine-independent features, such as total instructions, basic blocks, and memory operations to meet the constraints of production compilers. This yields a fixed-length feature vector of 44 attributes, consuming merely 176 bytes at 32-bit floating point precision.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#model-training",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#model-training",
    "title": "Paper Presentation",
    "section": "",
    "text": "Off-Policy DRL Algorithms: Reuse data collected asynchronously, allowing training without bottlenecks from data collection. This is crucial in production systems, where the compiler and data continuously evolve. We implement a Q-learning strategy for stable heuristics in GPU compiler autotuning.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#q-learning-for-gpu-compiler-autotuning",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#q-learning-for-gpu-compiler-autotuning",
    "title": "Paper Presentation",
    "section": "",
    "text": "Q-Table and Probabilistic Policy:\n\nThe Q-table represents the expected reward for each state-action pair over a set of applications. It simplifies to the expected performance of applying a heuristic action given a shader’s state, modeled as a single time-step Markov Decision Process (MDP).\nThe optimal policy selects the action that maximizes the probability of achieving the best performance for a given state. Alternatively, the policy can be derived using the Boltzmann softmax operator to balance exploration and exploitation, controlled by the temperature parameter.\n\nQ-Table Updates and Staleness Discounting:\n\nAs the compiler evolves, older performance measurements may become stale. Q-table updates include a discount factor, diminishing the weight of older data based on elapsed time.\nFor existing state-action pairs, the Q-value is updated as a weighted average of the prior value and the newly observed reward, modulated by a learning rate. For unseen states, the Q-value initializes to the observed reward.\n\nHyperparameter Tuning and Policy Behavior:\n\nThe temperature parameter in the softmax operator adjusts the trade-off between exploration and exploitation.\nThe learning rate and discount factor are tunable hyperparameters, ensuring adaptability to changing compiler behavior and iterative data collection.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#approximate-optimal-policy",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#approximate-optimal-policy",
    "title": "Paper Presentation",
    "section": "",
    "text": "Dynamic Q-Table and Generalization Issue: Compiler updates change the intermediate representation (IR), creating a non-stationary environment (EEE) and expanding the Q-table with new state-action pairs. This makes the Q-table too large and impractical while lacking the ability to generalize to unseen states.\nModel-Based Policy Approximation and Iterative Learning: To address these issues, train decision policy to approximates the optimal decision policy by minimizing the KL divergence between the empirical Q-table policy and the model’s output. This model generalizes to new states and is periodically deployed to guide compiler decisions, with new performance data updating the Q-table in an iterative DRL feedback loop.\n\n\n    Algorithm 1: RL-based GPU Compiler Autotuning\n    Result: (N)\n    Randomly initialize ;\n    Initialize an empty Q-table;\n    while i &lt; numIterations do\n        Copy learned parameters of decision policy to behaviour policy (i);\n        Integrate behaviour policy into the latest compiler;\n        Get states and performance measurements over A;\n        Update (or expand) the Q-table based on the observed performance measurements;\n        Convert the empirical Q-table to probabilities optimal decision policy(sa);\n        Train decision policy(a|s) to approximate optimal decision policy(sa);\n    end",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#model-training-1",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#model-training-1",
    "title": "Paper Presentation",
    "section": "",
    "text": "Off-Policy DRL Algorithms: Reuse data collected asynchronously, allowing training without bottlenecks from data collection. This is crucial in production systems, where the compiler and data continuously evolve. We implement a Q-learning strategy for stable heuristics in GPU compiler autotuning.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#q-learning-for-gpu-compiler-autotuning-1",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#q-learning-for-gpu-compiler-autotuning-1",
    "title": "Paper Presentation",
    "section": "",
    "text": "Q-Table and Probabilistic Policy:\n\nThe Q-table represents the expected reward for each state-action pair over a set of applications. It simplifies to the expected performance of applying a heuristic action given a shader’s state, modeled as a single time-step Markov Decision Process (MDP).\nThe optimal policy selects the action that maximizes the probability of achieving the best performance for a given state. Alternatively, the policy can be derived using the Boltzmann softmax operator to balance exploration and exploitation, controlled by the temperature parameter.\n\nQ-Table Updates and Staleness Discounting:\n\nAs the compiler evolves, older performance measurements may become stale. Q-table updates include a discount factor, diminishing the weight of older data based on elapsed time.\nFor existing state-action pairs, the Q-value is updated as a weighted average of the prior value and the newly observed reward, modulated by a learning rate. For unseen states, the Q-value initializes to the observed reward.\n\nHyperparameter Tuning and Policy Behavior:\n\nThe temperature parameter in the softmax operator adjusts the trade-off between exploration and exploitation.\nThe learning rate and discount factor are tunable hyperparameters, ensuring adaptability to changing compiler behavior and iterative data collection. ### Adapting to Production Compiler Development\n\n\n\nDynamic Environment: Production-level compilers often change rapidly, resulting in a dynamic and non-stationary environment, with constantly evolving conditions and code.\nNon-Stationary Environment: Defined by target hardware and continuously updated production compilers, leading to variability and complexity.\nImpact on IR Instructions: Percentage change in intermediate representation (IR) instructions for a shader can vary by up to 50% over a year, indicating significant code structure changes.\nGrowth of Q-Table: The rapid pace of software development leads to an increase in the state space, causing the Q-table to grow. To manage growth and memory overhead, a DNN decision policy (πθ) is trained to approximate the optimal policy (π∗).\nPolicy Deployment: The trained DNN policy (πθ) is periodically deployed as a practical policy (πβ), maintaining an efficient and updated policy without excessive memory requirements of a growing Q-table.\n\n\n\n\nWavefront Sizes: AMD RDNA™ graphics architecture supports two wavefront sizes: wave32 (32 work-items) and wave64 (64 work-items). The optimal wavefront size depends on dynamic shader properties such as divergence and memory access patterns.\nBandwidth Considerations:\n\nWave64: Can offer better performance if there is sufficient bandwidth, allowing for more simultaneous memory accesses without causing cache misses.\nWave32: Reduces bandwidth strain by spreading out memory accesses over time, beneficial when multiple shaders are running concurrently.\n\nDynamic Environment: System bandwidth and other resources are dynamic and shared among concurrently running shaders. The best wavefront size depends on the real-time execution environment, which the compiler doesn’t know at compile time.\n\nRL-Based Autotuning Framework: Improves frame rates by selecting the optimal wavefront size for each shader at compile time.\nState Representation: The state of each shader is represented as a fixed-length vector of static features from the compiler’s intermediate representation (IR).\nAction Space: Limited to two options: wave32 and wave64.\nReward Signal: Based on changes in frame rate compared to default behavior.\n\nPolicy and Implementation: The decision policy (πθ) is a lightweight, 3-layer feed-forward classifier with less than 20 KB of learnable parameters. The learned parameters are periodically integrated into the compiler as the behavior policy (πβ).\nExperimental Results: The RL framework was tested on the AMD Radeon™ 6800 XT and achieved frame rates matching or surpassing 98% of graphics benchmarks. Performance improvements ranged from an average increase of 1.6% to a maximum of 15.8%. The model converged in only 45 iterations per benchmark application. Experiments were conducted using over 150 graphics benchmarks, each with an average of 230 unique shaders.\n\n\n\n\n\nGeneralization: The model needs to generalize well to handle new code generated by the frequently updated compiler IR. A well-trained model should perform effectively without needing frequent updates to its learned parameters.\nStability Metric: Stability is measured using the inverse of the rate of statistically significant regressions. This metric shows how often the model’s performance does not decline significantly. A 1-tailed t-test with a p-value of 5% is used to determine statistical significance.\nShader Wavefront Size Selection: The histogram in Figure 8 illustrates changes in frame rate when the RL-based compiler heuristic is used for selecting shader wavefront size on the AMD Radeon™ RX 5700 XT, demonstrating how the RL-based method impacts performance across various graphics applications.\nStability Over Time: Stability is tracked over a year of production compiler updates without retraining the deployed model. A value of 100% indicates no statistically significant performance regressions in benchmarks when using the DNN heuristic compared to the default compiler behavior.\n\n\n\n\n\nTransfer Learning: Instead of starting from scratch, the researchers use transfer learning to optimize the wavefront execution mode for a different GPU (the AMD Radeon™ RX 5700 XT), utilizing the final Q-table and behavior policy (πβ) from previous experiments as the starting point.\nEfficiency and Performance: With transfer learning, the model needed only 10 iterations over the set of graphics benchmarks to match or surpass previous frame rates in 94.4% of the benchmarks. Improvements included increases of up to 10.3% and an average improvement of 1.5%.\nBenchmarking: Experiments were conducted using the AMD production graphics compiler on over 270 graphics benchmarks, each with an average of 230 unique shaders. Figure 8 in the document provides a histogram of these results, showing the distribution of observed changes in frame rates.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#conclusion",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#conclusion",
    "title": "Paper Presentation",
    "section": "",
    "text": "Framework Development: The authors developed a GPU compiler autotuning framework using off-policy deep reinforcement learning (DRL) to generate heuristics that improve frame rates in graphics applications.\nContinuous Integration and Q-Learning: The framework combines continuous integration (CI) with Q-Learning to find optimal heuristic settings that maximize frame rate improvements across various graphics benchmarks.\nDeployment: By considering the rapid changes in software development, the trained models can be deployed as stable heuristics in evolving production compilers.\nGeneralized Gains: The framework demonstrates generalized performance gains across a large suite of graphics benchmarks and different GPUs.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#future-work",
    "href": "blogs/rohit/paper_presentation_GPU_compiler_heuristics.html#future-work",
    "title": "Paper Presentation",
    "section": "",
    "text": "Explore Static Counters and Dynamic Properties: Investigate the relationship between the set of static counters and the dynamic properties that the neural network has learned to account for.\nExtend to Continuous Action Spaces: Aim to extend the framework to domains with continuous action spaces using techniques from deep Q-Learning.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-11-01-Sharmila-HW05.html",
    "href": "blogs/sharmila/2024-11-01-Sharmila-HW05.html",
    "title": "Compiler Homework 05 - LLVM Pass",
    "section": "",
    "text": "In this blog, I’ll walk through the process of creating an LLVM pass to detect floating-point divisions within a C++ program. This project involves using the LLVM framework to analyze the Intermediate Representation (IR) code generated from a C++ program and detecting instances of floating-point division operations\n\nImplementation:\nFor each instruction in the basic block, we check if it’s a BinaryOperator with the opcode Instruction::FDiv, which represents a floating-point division in LLVM IR.\n\n\nTesting :\nI tested my pass using the below example:\n#include &lt;iostream&gt;\n#include &lt;stdio.h&gt;\n\ndouble divide(double a, double b) {\n    return a / b;\n}\n\nint main() {\n    double result1 = divide(10.0, 2.0);\n    double result2 = divide(19.0, 3.0);\n\n    if (result1 &gt; result2) {\n        double result3 = divide(result1, result2);  // Division in if branch\n    } else {\n        double result4 = divide(result2, result1);  // Division in else branch\n    }\n\n    return 0;\n}\nTo look at LLVM IR of this example I used “clang -emit-llvm -S -o - test.cpp”\nAnalyzing Function: divided\nFloating point division detected in Basic Block: %0\n  Instruction:   %7 = fdiv double %5, %6\nAnalyzing Function: main\nNo floating point divisions found in function.\nAnalyzing Function: _GLOBAL__sub_I_test.cpp\nNo floating point divisions found in function.\n; ModuleID = '../test.cpp'\nsource_filename = \"../test.cpp\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n%\"class.std::ios_base::Init\" = type { i8 }\n\n@_ZStL8__ioinit = internal global %\"class.std::ios_base::Init\" zeroinitializer, align 1\n@__dso_handle = external hidden global i8\n@llvm.global_ctors = appending global [1 x { i32, void ()*, i8* }] [{ i32, void ()*, i8* } { i32 65535, void ()* @_GLOBAL__sub_I_test.cpp, i8* null }]\n\n; Function Attrs: noinline uwtable\ndefine internal void @__cxx_global_var_init() #0 section \".text.startup\" {\n  call void @_ZNSt8ios_base4InitC1Ev(%\"class.std::ios_base::Init\"* @_ZStL8__ioinit)\n  %1 = call i32 @__cxa_atexit(void (i8*)* bitcast (void (%\"class.std::ios_base::Init\"*)* @_ZNSt8ios_base4InitD1Ev to void (i8*)*), i8* getelementptr inbounds (%\"class.std::ios_base::Init\", %\"class.std::ios_base::Init\"* @_ZStL8__ioinit, i32 0, i32 0), i8* @__dso_handle) #3\n  ret void\n}\n\ndeclare dso_local void @_ZNSt8ios_base4InitC1Ev(%\"class.std::ios_base::Init\"*) unnamed_addr #1\n\n; Function Attrs: nounwind\ndeclare dso_local void @_ZNSt8ios_base4InitD1Ev(%\"class.std::ios_base::Init\"*) unnamed_addr #2\n\n; Function Attrs: nounwind\ndeclare dso_local i32 @__cxa_atexit(void (i8*)*, i8*, i8*) #3\n\n; Function Attrs: noinline nounwind optnone uwtable\ndefine dso_local double @_Z6dividedd(double %0, double %1) #4 {\n  %3 = alloca double, align 8\n  %4 = alloca double, align 8\n  store double %0, double* %3, align 8\n  store double %1, double* %4, align 8\n  %5 = load double, double* %3, align 8\n  %6 = load double, double* %4, align 8\n  %7 = fdiv double %5, %6\n  ret double %7\n}\n\n; Function Attrs: noinline norecurse nounwind optnone uwtable\ndefine dso_local i32 @main() #5 {\n  %1 = alloca i32, align 4\n  %2 = alloca double, align 8\n  %3 = alloca double, align 8\n  %4 = alloca double, align 8\n  %5 = alloca double, align 8\n  store i32 0, i32* %1, align 4\n  %6 = call double @_Z6dividedd(double 1.000000e+01, double 2.000000e+00)\n  store double %6, double* %2, align 8\n  %7 = call double @_Z6dividedd(double 1.900000e+01, double 3.000000e+00)\n  store double %7, double* %3, align 8\n  %8 = load double, double* %2, align 8\n  %9 = load double, double* %3, align 8\n  %10 = fcmp ogt double %8, %9\n  br i1 %10, label %11, label %15\n\n11:                                               ; preds = %0\n  %12 = load double, double* %2, align 8\n  %13 = load double, double* %3, align 8\n  %14 = call double @_Z6dividedd(double %12, double %13)\n  store double %14, double* %4, align 8\n  br label %19\n\n15:                                               ; preds = %0\n  %16 = load double, double* %3, align 8\n  %17 = load double, double* %2, align 8\n  %18 = call double @_Z6dividedd(double %16, double %17)\n  store double %18, double* %5, align 8\n  br label %19\n\n19:                                               ; preds = %15, %11\n  ret i32 0\n}\n\n; Function Attrs: noinline uwtable\ndefine internal void @_GLOBAL__sub_I_test.cpp() #0 section \".text.startup\" {\n  call void @__cxx_global_var_init()\n  ret void\n}\n\nattributes #0 = { noinline uwtable \"correctly-rounded-divide-sqrt-fp-math\"=\"false\" \"disable-tail-calls\"=\"false\" \"frame-pointer\"=\"all\" \"less-precise-fpmad\"=\"false\" \"min-legal-vector-width\"=\"0\" \"no-infs-fp-math\"=\"false\" \"no-jump-tables\"=\"false\" \"no-nans-fp-math\"=\"false\" \"no-signed-zeros-fp-math\"=\"false\" \"no-trapping-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" }\nattributes #1 = { \"correctly-rounded-divide-sqrt-fp-math\"=\"false\" \"disable-tail-calls\"=\"false\" \"frame-pointer\"=\"all\" \"less-precise-fpmad\"=\"false\" \"no-infs-fp-math\"=\"false\" \"no-nans-fp-math\"=\"false\" \"no-signed-zeros-fp-math\"=\"false\" \"no-trapping-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" }\nattributes #2 = { nounwind \"correctly-rounded-divide-sqrt-fp-math\"=\"false\" \"disable-tail-calls\"=\"false\" \"frame-pointer\"=\"all\" \"less-precise-fpmad\"=\"false\" \"no-infs-fp-math\"=\"false\" \"no-nans-fp-math\"=\"false\" \"no-signed-zeros-fp-math\"=\"false\" \"no-trapping-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" }\nattributes #3 = { nounwind }\nattributes #4 = { noinline nounwind optnone uwtable \"correctly-rounded-divide-sqrt-fp-math\"=\"false\" \"disable-tail-calls\"=\"false\" \"frame-pointer\"=\"all\" \"less-precise-fpmad\"=\"false\" \"min-legal-vector-width\"=\"0\" \"no-infs-fp-math\"=\"false\" \"no-jump-tables\"=\"false\" \"no-nans-fp-math\"=\"false\" \"no-signed-zeros-fp-math\"=\"false\" \"no-trapping-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" }\nattributes #5 = { noinline norecurse nounwind optnone uwtable \"correctly-rounded-divide-sqrt-fp-math\"=\"false\" \"disable-tail-calls\"=\"false\" \"frame-pointer\"=\"all\" \"less-precise-fpmad\"=\"false\" \"min-legal-vector-width\"=\"0\" \"no-infs-fp-math\"=\"false\" \"no-jump-tables\"=\"false\" \"no-nans-fp-math\"=\"false\" \"no-signed-zeros-fp-math\"=\"false\" \"no-trapping-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" }\n\n!llvm.module.flags = !{!0}\n!llvm.ident = !{!1}\n\n!0 = !{i32 1, !\"wchar_size\", i32 4}\n!1 = !{!\"clang version 10.0.0-4ubuntu1 \"}\nTo run this pass I used “clang -fpass-plugin=Homework5/build/FloatDivPass.so test.cpp”\nNo floating point divisions found in function.\nAnalyzing Function: divided\nFloating point division detected in Basic Block: %0\n  Instruction:   %7 = fdiv double %5, %6\nAnalyzing Function: main\nNo floating point divisions found in function.\nAnalyzing Function: _GLOBAL__sub_I_test.cpp\nNo floating point divisions found in function.\n\n\nChallenges Faced:\nInitially, I faced many challenges while setting up the llvm and during build and cmake stage. Then, I fond it difficult in llvm pass code. Because even cout can’t be mentioned as it is instead errs(). Even while testing I was able to see the output of the test file but not the LLVM pass.\n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 05 - LLVM Pass"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-10-15-Sharmila-HW03.html",
    "href": "blogs/sharmila/2024-10-15-Sharmila-HW03.html",
    "title": "Compiler Homework 03 - Data Flow Analysis",
    "section": "",
    "text": "In this homework the task is to implement a data flow analysis. The challenge was to make it generic enough to support multiple types of analysis, such as liveness analysis, reaching definitions, available expressions, and very busy expressions.. In this blog post, I’ll share my experience, the code I developed, and some of the challenges I faced along the way.\n\n\nThe four types of data flow analysis that I implemented in this project:\n\nLiveness Analysis (Backward) Liveness analysis determines which variables are “live” at each point in the program. A variable is considered live if its value may be read in the future before it’s overwritten. This is a backward analysis because it propagates information from the end of the program towards the beginning.\n\nDirection: Backward\nInitial State: Empty set\nMerge Operation: Union\n\nVery Busy Expressions Analysis (Backward) Very Busy Expressions (VBE) analysis identifies expressions that are “very busy” at each program point. An expression is very busy if it will be evaluated again before any of its operands are overwrittern. This is also a backward analysis.\n\nDirection: Backward\nInitial State: Empty set\nMerge Operation: Intersection\n\nReaching Definitions Analysis (Forward) Reaching Definitions analysis determines which definitions of variables may reach each point in the program. A definition reaches a point if there is a path from the definition to the point without any other overwrite or interven of the same variable. This is a forward analysis.\n\nDirection: Forward\nInitial State: Empty set\nMerge Operation: Union\n\nAvailable Expressions Analysis (Forward) Available Expressions analysis determines which expressions are already computed and not modified at each point in the program. An expression is available at a point if it has been computed on every path to that point and none of its operands have been redefined since its last computation. This is a forward analysis.\n\nDirection: Forward\nInitial State: Empty set (or all expressions for some formulations)\nMerge Operation: Intersection\n\n\n\n\n\nI decided to take on the challenge of creating a generic data flow analysis framework. Here’s a overview of the key components of my analysis:\n\nA df_worklist function that implements the worklist algorithm for data flow analysis.\nA run_df function that applies the analysis to each function in the input program.\nSeveral helper functions for set operations and formatting output.\nSpecific analysis implementations for liveness, very busy expressions (VBE), reaching definitions, and available expressions.\n\nThe core of the implementation is the df_worklist function, which can perform both forward and backward analysis based on the configuration provided:\ndef df_worklist(blocks, analysis):\n    # ... (implementation details)\nThis function takes a set of basic blocks and an analysis configuration, and returns the in and out sets for each block.\nThe analysis configuration is defined using a named tuple:\nAnalysis = namedtuple('Analysis', ['forward', 'init', 'merge', 'transfer'])\nThis allows us to easily specify different analyses by providing the appropriate parameters.\n\n\n\nI tested my implementation with two examples to ensure it was working correctly.\n\nsimple.bril: In order to understand that my analysis works or not I took a simple example to test and assured that the code works for all 4 types\n\n    @main {\n    a: int = const 5;\n    b: int = const 10;\n    \n    sum: int = add a b;\n    print sum;\n    }\nAnd the output are:\na. Liveness:\n    b1:\n    in:  ∅\n    out: ∅\n\nb. Very Busy Expression:\n    b1:\n    in:  ('add', ('a', 'b'))\n    out: ∅\nc. Reaching Definition:\n    b1:\n    in:  ∅\n    out: ('a', 'b1'), ('b', 'b1'), ('sum', 'b1')\nd. Available Expression:\n    b1:\n    in:  ∅\n    out: ('add', ('a', 'b'))\n\nAdding Square of Even and Odd numbers: This is another example which has loop and check whether my analysis works or not for this example.\n\n    @main {\n    sum_even: int = const 0;      \n    sum_odd: int = const 0;       \n    i: int = const 1;             \n    limit: int = const 10;        \n    one: int = const 1;           \n    two: int = const 2;           \n\n    .loop:\n    square: int = mul i i;        \n    half: int = div i two;        \n    check: int = mul half two;    \n    is_even: bool = eq check i;   \n    br is_even .even_case .odd_case; \n\n    .even_case:\n    sum_even: int = add sum_even square; \n    jmp .increment;\n\n    .odd_case:\n    sum_odd: int = add sum_odd square;   \n\n    .increment:\n    i: int = add i one;           \n    cond: bool = le i limit;      \n    br cond .loop .exit;          \n\n    .exit:\n    print sum_even;               \n    print sum_odd;                \n    }\nAnd the output are:\na. Liveness:\n        b1:\n        in:  ∅\n        out: i, limit, one, sum_even, sum_odd, two\n        \n        loop:\n        in:  i, limit, one, sum_even, sum_odd, two\n        out: i, limit, one, square, sum_even, sum_odd, two\n        \n        even_case:\n        in:  i, limit, one, square, sum_even, sum_odd, two\n        out: i, limit, one, sum_even, sum_odd, two\n        \n        odd_case:\n        in:  i, limit, one, square, sum_even, sum_odd, two\n        out: i, limit, one, sum_even, sum_odd, two\n        \n        increment:\n        in:  i, limit, one, sum_even, sum_odd, two\n        out: i, limit, one, sum_even, sum_odd, two\n        \n        exit:\n        in:  sum_even, sum_odd\n        out: ∅\n\nb. Very Busy Expression:\n        b1:\n        in:  ('add', ('i', 'one')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('le', ('i', 'limit')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        out: ('add', ('i', 'one')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('le', ('i', 'limit')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        \n        loop:\n        in:  ('add', ('i', 'one')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('le', ('i', 'limit')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        out: ('add', ('i', 'one')), ('le', ('i', 'limit'))\n        \n        even_case:\n        in:  ('add', ('i', 'one')), ('add', ('sum_even', 'square')), ('le', ('i', 'limit'))\n        out: ('add', ('i', 'one')), ('le', ('i', 'limit'))\n        \n        odd_case:\n        in:  ('add', ('i', 'one')), ('add', ('sum_odd', 'square')), ('le', ('i', 'limit'))\n        out: ('add', ('i', 'one')), ('le', ('i', 'limit'))\n        \n        increment:\n        in:  ('add', ('i', 'one')), ('le', ('i', 'limit'))\n        out: ∅\n        \n        exit:\n        in:  ∅\n        out: ∅\n   \nc. Reaching Definition:\n        b1:\n        in:  ∅\n        out: ('i', 'b1'), ('limit', 'b1'), ('one', 'b1'), ('sum_even', 'b1'), ('sum_odd', 'b1'), ('two', 'b1')\n\n        loop:\n        in:  ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        out: ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n\n        even_case:\n        in:  ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        out: ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        \n        odd_case:\n        in:  ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        out: ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        \n        increment:\n        in:  ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        out: ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        \n        exit:\n        in:  ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        out: ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n\nd. Available Expression:\n        b1:\n        in:  ∅\n        out: ∅\n        \n        loop:\n        in:  ∅\n        out: ('div', ('i', 'two')), ('eq', ('check', 'i')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        \n        even_case:\n        in:  ('div', ('i', 'two')), ('eq', ('check', 'i')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        out: ('add', ('sum_even', 'square')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        \n        odd_case:\n        in:  ('div', ('i', 'two')), ('eq', ('check', 'i')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        out: ('add', ('sum_odd', 'square')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        \n        increment:\n        in:  ('div', ('i', 'two')), ('eq', ('check', 'i')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        out: ('add', ('i', 'one')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('le', ('i', 'limit')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        \n        exit:\n        in:  ('add', ('i', 'one')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('le', ('i', 'limit')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        out: ('add', ('i', 'one')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('le', ('i', 'limit')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))    \n\n\n\n\nOne of the most significant challenges I encountered was correctly implementing both forward and backward analysis within the same framework. This took a considerable amount of time to debug and get right.\nThe most tricky part was getting the initialization and update of the in_ and out dictionaries correct.\nInitially, I made the mistake of not properly distinguishing between in_ and out for forward and backward analyses, which led to incorrect results. After careful debugging and reexamining the theory behind data flow analysis, I was able to correct this issue.\n\n\n\nImplementing a generic data flow analysis framework was a challenging. It deepened my understanding of how different analyses work. The main steps in this assignment are:\n\nFigure out the thing you want to know at the entry and exit of a block.\nwrite an equation for every block relting to the entry and exit.\nAdd equalities according to edges in the CFGs.\nFinally, solve the system of equations.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 03 - Data Flow Analysis"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-10-15-Sharmila-HW03.html#introduction-to-data-flow-analysis-types",
    "href": "blogs/sharmila/2024-10-15-Sharmila-HW03.html#introduction-to-data-flow-analysis-types",
    "title": "Compiler Homework 03 - Data Flow Analysis",
    "section": "",
    "text": "The four types of data flow analysis that I implemented in this project:\n\nLiveness Analysis (Backward) Liveness analysis determines which variables are “live” at each point in the program. A variable is considered live if its value may be read in the future before it’s overwritten. This is a backward analysis because it propagates information from the end of the program towards the beginning.\n\nDirection: Backward\nInitial State: Empty set\nMerge Operation: Union\n\nVery Busy Expressions Analysis (Backward) Very Busy Expressions (VBE) analysis identifies expressions that are “very busy” at each program point. An expression is very busy if it will be evaluated again before any of its operands are overwrittern. This is also a backward analysis.\n\nDirection: Backward\nInitial State: Empty set\nMerge Operation: Intersection\n\nReaching Definitions Analysis (Forward) Reaching Definitions analysis determines which definitions of variables may reach each point in the program. A definition reaches a point if there is a path from the definition to the point without any other overwrite or interven of the same variable. This is a forward analysis.\n\nDirection: Forward\nInitial State: Empty set\nMerge Operation: Union\n\nAvailable Expressions Analysis (Forward) Available Expressions analysis determines which expressions are already computed and not modified at each point in the program. An expression is available at a point if it has been computed on every path to that point and none of its operands have been redefined since its last computation. This is a forward analysis.\n\nDirection: Forward\nInitial State: Empty set (or all expressions for some formulations)\nMerge Operation: Intersection",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 03 - Data Flow Analysis"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-10-15-Sharmila-HW03.html#the-implementation",
    "href": "blogs/sharmila/2024-10-15-Sharmila-HW03.html#the-implementation",
    "title": "Compiler Homework 03 - Data Flow Analysis",
    "section": "",
    "text": "I decided to take on the challenge of creating a generic data flow analysis framework. Here’s a overview of the key components of my analysis:\n\nA df_worklist function that implements the worklist algorithm for data flow analysis.\nA run_df function that applies the analysis to each function in the input program.\nSeveral helper functions for set operations and formatting output.\nSpecific analysis implementations for liveness, very busy expressions (VBE), reaching definitions, and available expressions.\n\nThe core of the implementation is the df_worklist function, which can perform both forward and backward analysis based on the configuration provided:\ndef df_worklist(blocks, analysis):\n    # ... (implementation details)\nThis function takes a set of basic blocks and an analysis configuration, and returns the in and out sets for each block.\nThe analysis configuration is defined using a named tuple:\nAnalysis = namedtuple('Analysis', ['forward', 'init', 'merge', 'transfer'])\nThis allows us to easily specify different analyses by providing the appropriate parameters.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 03 - Data Flow Analysis"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-10-15-Sharmila-HW03.html#testing-the-implementation",
    "href": "blogs/sharmila/2024-10-15-Sharmila-HW03.html#testing-the-implementation",
    "title": "Compiler Homework 03 - Data Flow Analysis",
    "section": "",
    "text": "I tested my implementation with two examples to ensure it was working correctly.\n\nsimple.bril: In order to understand that my analysis works or not I took a simple example to test and assured that the code works for all 4 types\n\n    @main {\n    a: int = const 5;\n    b: int = const 10;\n    \n    sum: int = add a b;\n    print sum;\n    }\nAnd the output are:\na. Liveness:\n    b1:\n    in:  ∅\n    out: ∅\n\nb. Very Busy Expression:\n    b1:\n    in:  ('add', ('a', 'b'))\n    out: ∅\nc. Reaching Definition:\n    b1:\n    in:  ∅\n    out: ('a', 'b1'), ('b', 'b1'), ('sum', 'b1')\nd. Available Expression:\n    b1:\n    in:  ∅\n    out: ('add', ('a', 'b'))\n\nAdding Square of Even and Odd numbers: This is another example which has loop and check whether my analysis works or not for this example.\n\n    @main {\n    sum_even: int = const 0;      \n    sum_odd: int = const 0;       \n    i: int = const 1;             \n    limit: int = const 10;        \n    one: int = const 1;           \n    two: int = const 2;           \n\n    .loop:\n    square: int = mul i i;        \n    half: int = div i two;        \n    check: int = mul half two;    \n    is_even: bool = eq check i;   \n    br is_even .even_case .odd_case; \n\n    .even_case:\n    sum_even: int = add sum_even square; \n    jmp .increment;\n\n    .odd_case:\n    sum_odd: int = add sum_odd square;   \n\n    .increment:\n    i: int = add i one;           \n    cond: bool = le i limit;      \n    br cond .loop .exit;          \n\n    .exit:\n    print sum_even;               \n    print sum_odd;                \n    }\nAnd the output are:\na. Liveness:\n        b1:\n        in:  ∅\n        out: i, limit, one, sum_even, sum_odd, two\n        \n        loop:\n        in:  i, limit, one, sum_even, sum_odd, two\n        out: i, limit, one, square, sum_even, sum_odd, two\n        \n        even_case:\n        in:  i, limit, one, square, sum_even, sum_odd, two\n        out: i, limit, one, sum_even, sum_odd, two\n        \n        odd_case:\n        in:  i, limit, one, square, sum_even, sum_odd, two\n        out: i, limit, one, sum_even, sum_odd, two\n        \n        increment:\n        in:  i, limit, one, sum_even, sum_odd, two\n        out: i, limit, one, sum_even, sum_odd, two\n        \n        exit:\n        in:  sum_even, sum_odd\n        out: ∅\n\nb. Very Busy Expression:\n        b1:\n        in:  ('add', ('i', 'one')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('le', ('i', 'limit')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        out: ('add', ('i', 'one')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('le', ('i', 'limit')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        \n        loop:\n        in:  ('add', ('i', 'one')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('le', ('i', 'limit')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        out: ('add', ('i', 'one')), ('le', ('i', 'limit'))\n        \n        even_case:\n        in:  ('add', ('i', 'one')), ('add', ('sum_even', 'square')), ('le', ('i', 'limit'))\n        out: ('add', ('i', 'one')), ('le', ('i', 'limit'))\n        \n        odd_case:\n        in:  ('add', ('i', 'one')), ('add', ('sum_odd', 'square')), ('le', ('i', 'limit'))\n        out: ('add', ('i', 'one')), ('le', ('i', 'limit'))\n        \n        increment:\n        in:  ('add', ('i', 'one')), ('le', ('i', 'limit'))\n        out: ∅\n        \n        exit:\n        in:  ∅\n        out: ∅\n   \nc. Reaching Definition:\n        b1:\n        in:  ∅\n        out: ('i', 'b1'), ('limit', 'b1'), ('one', 'b1'), ('sum_even', 'b1'), ('sum_odd', 'b1'), ('two', 'b1')\n\n        loop:\n        in:  ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        out: ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n\n        even_case:\n        in:  ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        out: ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        \n        odd_case:\n        in:  ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        out: ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        \n        increment:\n        in:  ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'b1'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        out: ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        \n        exit:\n        in:  ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n        out: ('check', 'loop'), ('cond', 'increment'), ('half', 'loop'), ('i', 'increment'), ('is_even', 'loop'), ('limit', 'b1'), ('one', 'b1'), ('square', 'loop'), ('sum_even', 'b1'), ('sum_even', 'even_case'), ('sum_odd', 'b1'), ('sum_odd', 'odd_case'), ('two', 'b1')\n\nd. Available Expression:\n        b1:\n        in:  ∅\n        out: ∅\n        \n        loop:\n        in:  ∅\n        out: ('div', ('i', 'two')), ('eq', ('check', 'i')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        \n        even_case:\n        in:  ('div', ('i', 'two')), ('eq', ('check', 'i')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        out: ('add', ('sum_even', 'square')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        \n        odd_case:\n        in:  ('div', ('i', 'two')), ('eq', ('check', 'i')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        out: ('add', ('sum_odd', 'square')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        \n        increment:\n        in:  ('div', ('i', 'two')), ('eq', ('check', 'i')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        out: ('add', ('i', 'one')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('le', ('i', 'limit')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        \n        exit:\n        in:  ('add', ('i', 'one')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('le', ('i', 'limit')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))\n        out: ('add', ('i', 'one')), ('div', ('i', 'two')), ('eq', ('check', 'i')), ('le', ('i', 'limit')), ('mul', ('half', 'two')), ('mul', ('i', 'i'))",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 03 - Data Flow Analysis"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-10-15-Sharmila-HW03.html#challenges-faced-forward-vs.-backward-analysis",
    "href": "blogs/sharmila/2024-10-15-Sharmila-HW03.html#challenges-faced-forward-vs.-backward-analysis",
    "title": "Compiler Homework 03 - Data Flow Analysis",
    "section": "",
    "text": "One of the most significant challenges I encountered was correctly implementing both forward and backward analysis within the same framework. This took a considerable amount of time to debug and get right.\nThe most tricky part was getting the initialization and update of the in_ and out dictionaries correct.\nInitially, I made the mistake of not properly distinguishing between in_ and out for forward and backward analyses, which led to incorrect results. After careful debugging and reexamining the theory behind data flow analysis, I was able to correct this issue.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 03 - Data Flow Analysis"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-10-15-Sharmila-HW03.html#conclusion",
    "href": "blogs/sharmila/2024-10-15-Sharmila-HW03.html#conclusion",
    "title": "Compiler Homework 03 - Data Flow Analysis",
    "section": "",
    "text": "Implementing a generic data flow analysis framework was a challenging. It deepened my understanding of how different analyses work. The main steps in this assignment are:\n\nFigure out the thing you want to know at the entry and exit of a block.\nwrite an equation for every block relting to the entry and exit.\nAdd equalities according to edges in the CFGs.\nFinally, solve the system of equations.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 03 - Data Flow Analysis"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-10-18-Sharmila-HW04.html",
    "href": "blogs/sharmila/2024-10-18-Sharmila-HW04.html",
    "title": "Compiler Homework 04 - Implementing Dominance Utilities",
    "section": "",
    "text": "In this homework blog, the task involves three main components: finding dominators for a function, constructing the dominance tree, and computing the dominance frontier. This blog post will walk through the implementation process, challenges faced, and testing my implementation.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 04 -  Implementing Dominance Utilities"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-10-18-Sharmila-HW04.html#implementation-process",
    "href": "blogs/sharmila/2024-10-18-Sharmila-HW04.html#implementation-process",
    "title": "Compiler Homework 04 - Implementing Dominance Utilities",
    "section": "Implementation Process",
    "text": "Implementation Process\n\n1. Finding Dominators\nThe first step was to implement a function to find dominators for a given control flow graph (CFG). I used the iterative algorithm, which involves:\n\nInitializing all nodes as dominators for each node.\nIteratively refining the dominator sets based on the predecessors of each node.\nContinuing until no changes occur in the dominator sets.\n\n\n\n2. Constructing the Dominance Tree\nWith the dominator information in hand, constructing the dominance tree was the next challenge, implemented by:\n\nInverting the dominator relation to get a map of dominated nodes.\nIdentifying the strictly dominated nodes (excluding self-domination).\nBuilding the tree structure by finding the immediate dominator for each node.\n\n\n\n3. Computing the Dominance Frontier\nThe final piece of the puzzle was computing the dominance frontier. Implemented this by:\n\nFinding all successors of dominated blocks for each node.\nIdentifying which of these successors are not strictly dominated by the current node.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 04 -  Implementing Dominance Utilities"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-10-18-Sharmila-HW04.html#testing",
    "href": "blogs/sharmila/2024-10-18-Sharmila-HW04.html#testing",
    "title": "Compiler Homework 04 - Implementing Dominance Utilities",
    "section": "Testing",
    "text": "Testing\nTo ensure the correctness of the implementation, I tested the code with 2 examples:\n\nExample 1: Sum of Two Numbers\n\n@main {\n a: int = const 10;\n b: int = const 3;\n sum: int = add a b;\n print sum;\n}\nOutput: Dominators:\n{\n  \"Main1\": [\"Main1\"]\n}\nDominance Tree:\n{\n  \"Main1\": []\n}\nDominance Frontier:\n{\n  \"Main1\": []\n}\n\nExample 1: Fibonacci Sequence\n\n@main(n: int) {\n zero: int = const 0;\n one: int = const 1;\n is_base_case: bool = le n one;\n br is_base_case .base_case .recursive_case;\n\n.base_case:\n result: int = id n;\n jmp .end;\n\n.recursive_case:\n n_minus_one: int = sub n one;\n n_minus_two: int = sub n two;\n \n fib_n_minus_one: int = call @fibonacci n_minus_one;\n fib_n_minus_two: int = call @fibonacci n_minus_two;\n \n result: int = add fib_n_minus_one fib_n_minus_two;\n\n.end:\n print result;\n}\n\n@fibonacci(n: int): int {\n zero: int = const 0;\n one: int = const 1;\n is_base_case: bool = le n one;\n br is_base_case .base_case .recursive_case;\n\n.base_case:\n ret n;\n\n.recursive_case:\n n_minus_one: int = sub n one;\n n_minus_two: int = sub n two;\n \n fib_n_minus_one: int = call @fibonacci n_minus_one;\n fib_n_minus_two: int = call @fibonacci n_minus_two;\n \n result: int = add fib_n_minus_one fib_n_minus_two;\n ret result;\n}\nOutput: Dominators:\n{\n  \"Main1\": [\"Main1\"],\n  \"base_case\": [\"Main1\",\"base_case\"],\n  \"end\": [\"Main1\",\"end\"],\n  \"recursive_case\": [\"Main1\",\"recursive_case\"]\n}\nDominance Tree:\n{\n  \"Main1\": [\"base_case\",\"end\",\"recursive_case\"],\n  \"base_case\": [],\n  \"end\": [],\n  \"recursive_case\": []\n}\nDominance Frontier:\n{\n  \"Main1\": [],\n  \"base_case\": [\"end\"],\n  \"end\": [],\n  \"recursive_case\": [\"end\"]\n}\nThe code correctly identifies dominators, constructs accurate dominance trees, and computes the correct dominance frontiers. By these output it is understood that Block A(Main1 block here) dominates the other blocks.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 04 -  Implementing Dominance Utilities"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-10-18-Sharmila-HW04.html#challenges-faced",
    "href": "blogs/sharmila/2024-10-18-Sharmila-HW04.html#challenges-faced",
    "title": "Compiler Homework 04 - Implementing Dominance Utilities",
    "section": "Challenges Faced",
    "text": "Challenges Faced\nComplexity of the dominator finding algorithm, which is slow for large tree. Additionally, constructing dominator tree was difficult required extra testing and debugging to ensure correctness.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 04 -  Implementing Dominance Utilities"
    ]
  },
  {
    "objectID": "blogs/Sana/10-18-2024-HW4-SanaTaghipourAnvari.html",
    "href": "blogs/Sana/10-18-2024-HW4-SanaTaghipourAnvari.html",
    "title": "Homework4 - dominance",
    "section": "",
    "text": "Link for the code: cfgdominance",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework4 - dominance"
    ]
  },
  {
    "objectID": "blogs/Sana/10-18-2024-HW4-SanaTaghipourAnvari.html#explanation-of-the-code",
    "href": "blogs/Sana/10-18-2024-HW4-SanaTaghipourAnvari.html#explanation-of-the-code",
    "title": "Homework4 - dominance",
    "section": "Explanation of the code",
    "text": "Explanation of the code\nThis implementation performs dominator analysis on a control flow graph (CFG). It includes several key functions:\n\nbuild_cfg: Constructs a Control Flow Graph from a given function’s instructions.\ncompute_dominators: Calculates the set of dominators for each node in the CFG.\nbuild_dominator_tree: Constructs the dominator tree by finding the immediate dominator for each node.\ncompute_dominance_frontier: Computes the dominance frontier for each node in the CFG.\ntest_dominance: Verifies the correctness of the computed dominators.\n\nThe main function reads a JSON input file containing function definitions, applies these analyses to each function, and outputs the results. Input:\nmain {\n  entry:\n    x: int = const 0;\n    jmp L1;\n\n  L1:\n    y: int = const 1;\n    br y L2 L3;\n\n  L2:\n    z: int = add x y;\n    jmp L4;\n\n  L3:\n    w: int = sub x y;\n    jmp L4;\n\n  L4:\n    ret z;\n}\nControl Flow Graph:\n          [ Block 0 (Entry) ]\n                   |\n                   v\n             [ Block 1 (L1) ]\n              /             \\\n             v               v\n    [ Block 2 (L2) ]     [ Block 3 (L3) ]\n             \\               /\n              \\             /\n               v           v\n             [ Block 4 (L4) ]\n                   |\n                [ Exit ]\n\nimport json\nfrom collections import defaultdict\n\ndef build_cfg(func):\n    cfg = defaultdict(list)\n    blocks = []\n    current_block = []\n    label_to_block = {}\n\n    for instr in func['instrs']:\n        if isinstance(instr, dict) and 'label' in instr:\n            if current_block:\n                blocks.append(current_block)\n            current_block = [instr]\n            label_to_block[instr['label']] = len(blocks)\n        else:\n            current_block.append(instr)\n\n        if isinstance(instr, dict) and instr.get('op') in ['jmp', 'br']:\n            blocks.append(current_block)\n            current_block = []\n\n    if current_block:\n        blocks.append(current_block)\n\n    for i, block in enumerate(blocks):\n        last_instr = block[-1]\n        if isinstance(last_instr, dict):\n            if last_instr.get('op') in ['jmp', 'br']:\n                for label in last_instr['labels']:\n                    if label in label_to_block:\n                        cfg[i].append(label_to_block[label])\n            elif last_instr.get('op') != 'ret' and i + 1 &lt; len(blocks):\n                cfg[i].append(i + 1)\n        cfg[i] \n\n    return cfg\n\ndef compute_dominators(cfg):\n    entry = 0\n    all_nodes = set(cfg.keys())\n    dom = {node: all_nodes.copy() for node in all_nodes}\n    dom[entry] = {entry}\n\n    changed = True\n    while changed:\n        changed = False\n        for node in cfg:\n            predecessors = get_predecessors(cfg, node)\n            if predecessors:\n                new_dom = set.intersection(*(dom[pred] for pred in predecessors))\n                new_dom.add(node)\n            else:\n                new_dom = {node}\n            if new_dom != dom[node]:\n                dom[node] = new_dom\n                changed = True\n\n    return dom\n\ndef get_predecessors(cfg, node):\n    return [pred for pred, succs in cfg.items() if node in succs]\n\ndef build_dominator_tree(dom):\n    idom = {}\n    for node in dom:\n        if node == 0:  \n            idom[node] = None\n        else:\n            # Sort dominators by the size of their dominator sets in descending order\n            sorted_doms = sorted(dom[node] - {node}, key=lambda x: len(dom[x]), reverse=True)\n            idom[node] = next(d for d in sorted_doms if d != node)\n    return idom\n\ndef compute_dominance_frontier(cfg, dom):\n    df = defaultdict(set)\n    for node in cfg:\n        for succ in cfg[node]:\n            runner = node\n            while runner not in dom[succ]:\n                df[runner].add(succ)\n                runner = next(d for d in sorted(dom[runner] - {runner}) if d != runner)\n    return df\n\ndef test_dominance(cfg, dom):\n    entry = 0\n    for node in cfg:\n        if node == entry:\n            continue\n        for d in dom[node] - {node}:\n            if d == entry:\n                continue\n            #Temporarily remove dominator d from the CFG\n            modified_cfg = remove_node_from_cfg(cfg, d)\n            # Check if node is still reachable from entry\n            if is_reachable(modified_cfg, entry, node):\n                #if node is still reachable without d, then d does not dominate node\n                return False\n    return True\n\n\ndef remove_node_from_cfg(cfg, node_to_remove):\n    modified_cfg = {node: succs.copy() for node, succs in cfg.items() if node != node_to_remove}\n    for succs in modified_cfg.values():\n        if node_to_remove in succs:\n            succs.remove(node_to_remove)\n    return modified_cfg\n\ndef is_reachable(cfg, start, target):\n    visited = set()\n    stack = [start]\n    while stack:\n        node = stack.pop()\n        if node == target:\n            return True\n        if node not in visited:\n            visited.add(node)\n            successors = cfg.get(node, [])\n            stack.extend(successors)\n    return False\n\n\ndef main(json_input):\n    data = json.loads(json_input)\n\n    for func in data['functions']:\n        print(f\"Function: {func['name']}\")\n\n        cfg = build_cfg(func)\n        print(\"\\nControl Flow Graph:\")\n        print(json.dumps(cfg, indent=2))\n\n        dom = compute_dominators(cfg)\n        print(\"\\nDominators:\")\n        print(json.dumps({k: sorted(list(v)) for k, v in dom.items()}, indent=2))\n\n        idom = build_dominator_tree(dom)\n        print(\"\\nImmediate Dominators (Dominator Tree):\")\n        print(json.dumps(idom, indent=2))\n\n        df = compute_dominance_frontier(cfg, dom)\n        print(\"\\nDominance Frontier:\")\n        print(json.dumps({k: sorted(list(v)) for k, v in df.items()}, indent=2))\n\n        test_result = test_dominance(cfg, dom)\n        print(\"\\nDominance Test Result:\")\n        print(test_result)\n\nif __name__ == \"__main__\":\n    input_json = '''\n    {\n      \"functions\": [\n        {\n          \"name\": \"main\",\n          \"instrs\": [\n            { \"label\": \"entry\" },\n            { \"op\": \"const\", \"dest\": \"x\", \"type\": \"int\", \"value\": 0 },\n            { \"op\": \"jmp\", \"labels\": [\"L1\"] },\n            \n            { \"label\": \"L1\" },\n            { \"op\": \"const\", \"dest\": \"y\", \"type\": \"int\", \"value\": 1 },\n            { \"op\": \"br\", \"args\": [\"y\"], \"labels\": [\"L2\", \"L3\"] },\n\n            { \"label\": \"L2\" },\n            { \"op\": \"add\", \"args\": [\"x\", \"y\"], \"dest\": \"z\", \"type\": \"int\" },\n            { \"op\": \"jmp\", \"labels\": [\"L4\"] },\n\n            { \"label\": \"L3\" },\n            { \"op\": \"sub\", \"args\": [\"x\", \"y\"], \"dest\": \"w\", \"type\": \"int\" },\n            { \"op\": \"jmp\", \"labels\": [\"L4\"] },\n\n            { \"label\": \"L4\" },\n            { \"op\": \"ret\", \"args\": [\"z\"] }\n          ]\n        }\n      ]\n    }\n    '''\n    main(input_json)\n\nFunction: main\n\nControl Flow Graph:\n{\n  \"0\": [\n    1\n  ],\n  \"1\": [\n    2,\n    3\n  ],\n  \"2\": [\n    4\n  ],\n  \"3\": [\n    4\n  ],\n  \"4\": []\n}\n\nDominators:\n{\n  \"0\": [\n    0\n  ],\n  \"1\": [\n    0,\n    1\n  ],\n  \"2\": [\n    0,\n    1,\n    2\n  ],\n  \"3\": [\n    0,\n    1,\n    3\n  ],\n  \"4\": [\n    0,\n    1,\n    4\n  ]\n}\n\nImmediate Dominators (Dominator Tree):\n{\n  \"0\": null,\n  \"1\": 0,\n  \"2\": 1,\n  \"3\": 1,\n  \"4\": 1\n}\n\nDominance Frontier:\n{\n  \"2\": [\n    4\n  ],\n  \"3\": [\n    4\n  ]\n}\n\nDominance Test Result:\nTrue",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework4 - dominance"
    ]
  },
  {
    "objectID": "blogs/Sana/10-18-2024-HW4-SanaTaghipourAnvari.html#how-we-testing-our-implementation",
    "href": "blogs/Sana/10-18-2024-HW4-SanaTaghipourAnvari.html#how-we-testing-our-implementation",
    "title": "Homework4 - dominance",
    "section": "How we testing our implementation",
    "text": "How we testing our implementation\nThe test_dominance function verifies the correctness of the computed dominators. It does this by:\n\nIt iterates through all nodes in the CFG except the entry node.\nFor each node, it considers all of its computed dominators (excluding the node itself and the entry node).\nFor each dominator, it temporarily removes that node from the CFG using the remove_node_from_cfg function.\nIt then checks if the original node is still reachable from the entry node in this modified CFG using the is_reachable function.\nIf the node is still reachable after removing a supposed dominator, it means that dominator wasn’t actually necessary to reach the node, so the test fails.\nIf all nodes pass this test for all their dominators, the function returns True. Also, in the code directory I tested this code with two different inputs and both passed the test function.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework4 - dominance"
    ]
  },
  {
    "objectID": "blogs/Sana/10-18-2024-HW4-SanaTaghipourAnvari.html#hardest-part-of-the-task-and-how-we-addressed-it",
    "href": "blogs/Sana/10-18-2024-HW4-SanaTaghipourAnvari.html#hardest-part-of-the-task-and-how-we-addressed-it",
    "title": "Homework4 - dominance",
    "section": "Hardest Part of the Task and How We Addressed It",
    "text": "Hardest Part of the Task and How We Addressed It\nThe hardest part of this task was likely the correct implementation of the dominator tree construction, specifically in the build_dominator_tree function. This is challenging because:\n\nIt requires finding the immediate dominator for each node, which is the closest strict dominator in the dominator graph.\nThe naive approach of simply choosing any dominator can lead to incorrect results in complex control flow structures.\n\nWe addressed this challenge by:\n\nSorting the dominators of each node based on the size of their own dominator sets, in descending order.\nThis sorting ensures that we consider closer dominators first, as nodes closer in the dominator tree will have larger dominator sets.\nWe then select the first dominator from this sorted list (excluding the node itself) as the immediate dominator.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework4 - dominance"
    ]
  },
  {
    "objectID": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html",
    "href": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html",
    "title": "LLMs for Code Optimization, A Promising Start or Overhyped Solution?",
    "section": "",
    "text": "Link for the Paper: LLMs",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "LLMs for Code Optimization, A Promising Start or Overhyped Solution?"
    ]
  },
  {
    "objectID": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#introduction",
    "href": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#introduction",
    "title": "LLMs for Code Optimization, A Promising Start or Overhyped Solution?",
    "section": "Introduction",
    "text": "Introduction\nThe paper “Large Language Models for Compiler Optimization” by Cummins et al. introduces a novel application of Large Language Models (LLMs) for optimizing low-level compiler assembly code (LLVM IR). Compiler optimization has traditionally relied on complex rule-based systems developed over decades, but this work explores the potential for LLMs to perform these tasks using a purely data-driven approach. The authors trained a 7-billion parameter transformer model from scratch, specifically tailored to generate optimization strategies for LLVM IR, marking the first time LLMs have been applied directly to compiler optimization.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "LLMs for Code Optimization, A Promising Start or Overhyped Solution?"
    ]
  },
  {
    "objectID": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#background",
    "href": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#background",
    "title": "LLMs for Code Optimization, A Promising Start or Overhyped Solution?",
    "section": "Background",
    "text": "Background\nTo fully appreciate the contributions of this paper, it’s important to understand the context of compiler optimization and the role of LLVM:\n\nCompiler Optimization:\n\nCompilers like LLVM use a series of transformation passes to optimize code for performance and size.\nThe pass ordering problem, where the goal is to select the sequence of optimization passes that yields the best result, is crucial because different orders can significantly impact performance.\n\nLLVM IR:\n\nLLVM IR (Intermediate Representation) is a low-level, platform-independent assembly-like code that allows for fine-grained control of compiler optimizations.\nOptimizing at the level of IR is challenging as it requires a deep understanding of control flow, data flow, and the underlying hardware.\n\nTraditional Approaches:\n\nPrevious works on compiler optimization using machine learning relied on hand-crafted features, reinforcement learning, or graph neural networks, all of which require extensive manual engineering and multiple compilation attempts.\n\n\nThe key question addressed by this paper is: Can an LLM, trained directly on LLVM IR, learn to optimize code effectively without these manual features or iterative compilation?",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "LLMs for Code Optimization, A Promising Start or Overhyped Solution?"
    ]
  },
  {
    "objectID": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#main-contributions",
    "href": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#main-contributions",
    "title": "LLMs for Code Optimization, A Promising Start or Overhyped Solution?",
    "section": "Main Contributions",
    "text": "Main Contributions\nThe authors’ work offers several innovative contributions:\n\nFirst Application of LLMs for Code Optimization\n\nUnlike previous machine learning models, this LLM is trained on raw, unoptimized LLVM IR code, directly predicting the sequence of optimization passes.\nThe model takes as input the unoptimized LLVM IR and outputs an ordered list of optimization passes, bypassing the need for multiple compilations.\n\nAuxiliary Learning Tasks for Enhanced Understanding\n\nThe model is trained not only to generate pass lists but also to predict the instruction counts before and after optimization, and to generate the optimized code itself.\nThese auxiliary tasks force the model to develop a deeper understanding of code semantics, and improve its ability to make effective optimization decisions.\n\nImproved Performance Over Traditional Baselines\n\nThe LLM outperforms state-of-the-art machine learning approaches like AutoPhase and Coreset-NVP, achieving a 3.0% reduction in instruction count without invoking the compiler even once, also it’s compared to the 5.0% reduction achieved by an autotuner that required millions of compilations.\n\nEvaluation on a Diverse Set of Benchmarks\n\nThe model’s performance was evaluated on a variety of datasets including AI-SOCO, ExeBench, and YARPGen, demonstrating its robustness across different domains.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "LLMs for Code Optimization, A Promising Start or Overhyped Solution?"
    ]
  },
  {
    "objectID": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#merits-and-shortcomings",
    "href": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#merits-and-shortcomings",
    "title": "LLMs for Code Optimization, A Promising Start or Overhyped Solution?",
    "section": "Merits and Shortcomings",
    "text": "Merits and Shortcomings\n\nMerits\n\nReduced Compilation Overhead:\n\nThe LLM generates effective pass lists without requiring iterative compilations and this makes this approach far more efficient than autotuning approaches that rely on exhaustive search.\n\nGeneralization Across Different Code Bases:\n\nThe model generalizes well to unseen programs, effectively optimizes code from diverse sources without relying on specific handcrafted features.\n\nDemonstrated Code Understanding:\n\nThe auxiliary learning tasks show that the LLM can develop a sophisticated understanding of LLVM IR, even generating optimized code with almost high accuracy.\n\n\n\n\nShortcomings\n\nContext Window Limitations:\n\nThe fixed sequence length (2k tokens) limits the size of LLVM IR that can be processed, restricting the model’s ability to optimize larger functions or entire modules.\n\nArithmetic Reasoning Challenges:\n\nThe model struggles with complex arithmetic reasoning, such as constant folding and data flow analysis, which are crucial for certain compiler optimizations.\n\nInference Speed and Resource Requirements:\n\nWhile faster than autotuning, the LLM inference is still significantly slower than traditional compiler heuristics, and the model requires substantial GPU resources.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "LLMs for Code Optimization, A Promising Start or Overhyped Solution?"
    ]
  },
  {
    "objectID": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#historical-context-and-connections",
    "href": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#historical-context-and-connections",
    "title": "LLMs for Code Optimization, A Promising Start or Overhyped Solution?",
    "section": "Historical Context and Connections",
    "text": "Historical Context and Connections\nThis paper represents a significant departure from traditional approaches in compiler optimization, which have relied heavily on handcrafted heuristics and rule-based systems for decades. By applying LLMs to this problem, the authors open up new avenues for leveraging data-driven methods in compiler design. This work is part of a broader trend towards integrating machine learning into software engineering tasks, building on successes in code generation and analysis by models like Codex and Code Llama.\nThe use of LLMs for compiler optimization also connects to recent efforts in neural machine translation, where models are trained to translate code between different programming languages. However, this is the first instance of a model targeting LLVM IR, a more complex and lower-level code representation than typical source code.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "LLMs for Code Optimization, A Promising Start or Overhyped Solution?"
    ]
  },
  {
    "objectID": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#class-discussion-and-consensus",
    "href": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#class-discussion-and-consensus",
    "title": "LLMs for Code Optimization, A Promising Start or Overhyped Solution?",
    "section": "Class Discussion and Consensus",
    "text": "Class Discussion and Consensus\nDuring the in-class and online discussions, several key insights emerged:\n\nPotential for Hybrid Approaches:\n\nMany participants suggested integrating the LLM with traditional compiler heuristics or using it as a guidance system for existing optimizers rather than a complete replacement.\n\nLimitations of Current Model Size:\n\nThere was a consensus that scaling up the model and incorporating longer context windows could address some of the issues with handling larger code fragments.\n\nHigh Error Rate and Need for Improved Accuracy:\n\nWe discussed that the current error rate is high, indicating the need for better accuracy. Training a larger model with a bigger input dataset could help improve the model’s performance and reduce the error rate, especially in challenging optimization tasks.\n\nImplications for Future Compiler Design:\n\nThe class generally agreed that this approach could influence the design of future compilers, possibly shifting towards data-driven optimization frameworks that can adapt based on new code patterns.\n\n\nFinally, a bigger model (bigger than Llama2) and bigger data size should be used for training LLMs to give us better accuracy, and a model that people can use and get benefit from.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "LLMs for Code Optimization, A Promising Start or Overhyped Solution?"
    ]
  },
  {
    "objectID": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#conclusion",
    "href": "blogs/Sana/PaperPresentation-LLMs-for-Compiler-Optimization.html#conclusion",
    "title": "LLMs for Code Optimization, A Promising Start or Overhyped Solution?",
    "section": "Conclusion",
    "text": "Conclusion\nThe paper “Large Language Models for Compiler Optimization” presents an innovative use of LLMs, demonstrating their potential to replace traditional compiler optimization strategies with a purely data-driven approach. Despite its impressive results, the work highlights several challenges, particularly in arithmetic reasoning and sequence length limitations. Future research could address these issues by leveraging longer context windows, more powerful and accurate models, and hybrid techniques that combine LLMs with traditional compiler heuristics.\nOverall, this paper is an exciting first step in applying LLMs to compiler optimization, providing a glimpse into the future of compiler design and the potential role of AI in software engineering.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "LLMs for Code Optimization, A Promising Start or Overhyped Solution?"
    ]
  },
  {
    "objectID": "blogs/Sana/FinalProjectReport.html",
    "href": "blogs/Sana/FinalProjectReport.html",
    "title": "Data Layout Optimization and Loop Transformations",
    "section": "",
    "text": "Link for the project: BrilLayoutOptimizer",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Data Layout Optimization and Loop Transformations"
    ]
  },
  {
    "objectID": "blogs/Sana/FinalProjectReport.html#design-and-implementation",
    "href": "blogs/Sana/FinalProjectReport.html#design-and-implementation",
    "title": "Data Layout Optimization and Loop Transformations",
    "section": "Design and Implementation",
    "text": "Design and Implementation\n\nAnalysis Pass\nThe project employs a modular architecture with several key components:\n\nCFGBuilder: We implemented a CFGBuilder that constructs detailed control flow graphs of the program. This component breaks down the code into basic blocks, establishes relationships between these blocks, and computes dominator information essential for optimization decisions. The control flow information proves crucial for understanding program structure and ensuring safe transformations.\nSSAConverter: Our SSA (Static Single Assignment) Converter transforms the code into SSA form, this simplifies analysis and optimization by ensuring each variable is assigned exactly once. This component manages variable versioning, handles the insertion of phi nodes at control flow convergence points, and maintains accurate definition-use chains throughout the program.\nDataLayoutAnalyzer: The DataLayoutAnalyzer serves as our primary analysis engine, examining code to detect array access patterns and identify potential optimization opportunities. It works in conjunction with our dependency tracking system to ensure that all transformations preserve program semantics. This component provides the foundation for making informed optimization decisions.\nLayoutOptimizer: The LayoutOptimizer acts as the central coordinator, applying various transformation passes based on the analysis results. It manages the sequence of optimizations and ensures that transformations take into account system-specific parameters such as cache sizes and memory hierarchy.\n\n\n\n\nOptimization Techniques\nWe implemented several key optimization techniques to improve program performance:\n\nLoop Tiling: Loop tiling stands as one of our primary transformations, breaking large loops into cache-friendly blocks. The tile sizes are carefully calculated based on the target system’s cache parameters to maximize cache utilization while minimizing overhead.\nLoop Interchange: Loop interchange focuses on reordering nested loops to improve memory access patterns. We determine the optimal loop ordering that minimizes cache misses and maximizes spatial locality by analyzing array access patterns.\nLoop Fusion: Loop fusion combines adjacent compatible loops when possible, reducing loop overhead and improving cache utilization by processing related operations together. This transformation requires careful analysis of dependencies to ensure correctness.\nArray Padding: Our array padding implementation adjusts array dimensions to better align with cache line sizes, helping to avoid cache conflicts that can degrade performance. This transformation considers both the cache architecture and access patterns to determine optimal padding.\nLoop Unrolling: Loop unrolling reduces loop overhead and enables better instruction-level parallelism by replicating the loop body multiple times with adjusted indices, uses a configurable unroll factor to balance between performance gains and code size increase.\n\n\n\n\nTechnical Challenges\n\nSSA Implementation Complexities\nDuring implementation, we encountered several significant technical challenges. The SSA implementation proved particularly complex, especially in handling phi node placement and maintaining correct variable versioning across different scopes. We needed to carefully manage convergence points and ensure proper tracking of variable definitions throughout the program.\n\n\nLoop Tiling Difficulties\nLoop tiling presented its own set of challenges, particularly in determining optimal tile sizes and handling boundary conditions. We had to carefully balance cache utilization against the overhead of additional loop control structures and develop methods for transforming complex index expressions.\n\n\nAccess Pattern Analysis\nAccess pattern analysis required developing sophisticated detection methods that could reliably identify different access patterns while handling complex indexing expressions. We needed to strike a balance between analysis precision and computational complexity while ensuring our heuristics remained effective across a wide range of code patterns.\n\n\n\nHow the implementation is tested\nWe wrote an optimization checker code, which performs four main checks by comparing the input JSON against the optimized output JSON:\n\nLoop Unrolling Detection: For loop unrolling detection, the checker examines loop body instructions to identify the signs of unrolling. It specifically looks for patterns where loop variables appear multiple times with different offsets, such as expressions like (i + 0) and (i + 1) in consecutive instructions. This is implemented through the detect_loop_unrolling function, which recursively traverses the program’s JSON structure and analyzes each loop’s body to detect these characteristic patterns.\nLoop Tiling Detection: When checking for loop tiling transformations, the system searches for newly introduced loops with variables containing the _tile suffix in their names. The checker performs a string comparison between input and output program representations to identify these tiling-specific patterns. It verifies both the presence of tile loops and ensures they maintain the correct hierarchical structure with appropriate bounds modifications.\nArray Padding Detection: Array padding detection involves a more nuanced comparison of array allocations between input and output programs. The checker maintains lists of allocation patterns from both versions and compares their dimensions and sizes. It specifically looks for changes in array dimensions that would indicate padding has been applied to improve cache alignment.\nLoop Interchange Detection: For loop interchange verification, the checker performs a structural comparison of loop orderings in the AST. It extracts and stores the sequence of loop constructs from both input and output programs, then compares them to detect if their relative positions have changed. This includes analyzing both the loop structure itself and any modifications to array access patterns that would result from loop reordering.\n\nAll of these checks are coordinated through the compare_files function, which orchestrates the entire verification process and provides clear, formatted output indicating which optimizations were successfully applied to the code. The checker maintains a careful balance between detailed analysis and performance, ensuring accurate detection without becoming overly computationally expensive.\n\n\n\nDetailed Cache Model Implementation\nOur cache optimization system implements a sophisticated approach to memory hierarchy awareness, with the core implementation in the CacheInfo and LayoutOptimizer classes:\nclass CacheInfo:\n    def __init__(self):\n        self.l1_size = 0  \n        self.l2_size = 0\n        self.l3_size = 0\n        self.line_size = 0\n        self._detect_cache_sizes()\n    \n    def _detect_cache_sizes(self):\n        system = platform.system()\n        if system == \"Linux\":\n            self._detect_linux_cache()\n        elif system == \"Darwin\":\n            self._detect_darwin_cache()\n        else:\n            self._set_default_sizes()\nThe system automatically detects cache parameters or falls back to conservative defaults (32KB L1, 256KB L2, 8MB L3). These values inform our tile size calculations:\ndef _calculate_optimal_tile_size(self):\n    try:\n        cache_size = self.cache_info.l1_size\n        element_size = 4  # assuming 4-byte elements\n        target_size = cache_size // 3  # Use 1/3 of L1 cache\n        elements = target_size // element_size\n        elements_per_line = self.cache_info.line_size // element_size\n        \n        # Calculate base tile size as square root of elements\n        base_tile_size = int(elements ** 0.5)\n        tile_size = (base_tile_size // elements_per_line) * elements_per_line\n        \n        return max(elements_per_line, min(tile_size, 256))\n    except Exception as e:\n        return 32  # Conservative fallback\nPadding implementation aligns arrays with cache lines:\ndef _pad_allocation(self, alloc_instr: Dict) -&gt; Dict:\n    element_size = 4\n    elements_per_line = self.cache_line_size // element_size\n    padded_dim = ((last_dim + elements_per_line - 1) // \n                  elements_per_line) * elements_per_line\nThis cache-aware design significantly improves memory access patterns by:\n\nAligning data structures with cache line boundaries\nCalculating tile sizes based on actual cache parameters\nOptimizing spatial and temporal locality through padding\nAdapting transformations to the target architecture’s memory hierarchy\n\nThe system primarily optimizes for L1 cache but maintains awareness of the complete cache hierarchy for future extensions to multi-level optimization strategies.\n\nOverally what is happening?\nWhen the provided code runs with a specific cache size supplied by the user (for example: –cache-size 32768), it performs the following steps:\n\nThe CacheInfo class is initialized, and the user-supplied cache size (32768 bytes) is set as the L1 cache size:\n\nif args.cache_size:\n    cache_info.l1_size = args.cache_size\n\nThe input JSON (input.json) is parsed into a data structure representing the program (a list of functions with instructions).\nThe DataLayoutAnalyzer constructs the Control Flow Graph (CFG) with CFGBuilder, converts the CFG into Static Single Assignment (SSA) by using SSAConverter, analyzes array access patterns to classify them as row-major, column-major, strided, or random, and collects array and loop information.\nThe LayoutOptimizer uses the cache size to compute tile sizes for loop tiling:\n\ntile_size = (base_tile_size // elements_per_line) * elements_per_line\nHere, the tile size is computed based on L1 cache size divided by element size (assuming 4 bytes per element), leaving room for other data in the cache. Then applies optimization passes in this order: Fusion, Padding, Unrolling, Interchange, Tiling. Then teh code writes teh optimized output json file.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Data Layout Optimization and Loop Transformations"
    ]
  },
  {
    "objectID": "blogs/Sana/FinalProjectReport.html#results",
    "href": "blogs/Sana/FinalProjectReport.html#results",
    "title": "Data Layout Optimization and Loop Transformations",
    "section": "Results",
    "text": "Results\n\ninput 1:\n{\n  \"functions\": [\n    {\n      \"name\": \"matrix_operations\",\n      \"args\": [\n        {\n          \"name\": \"A\",\n          \"type\": {\n            \"ptr\": \"int\"\n          }\n        },\n        {\n          \"name\": \"B\",\n          \"type\": {\n            \"ptr\": \"int\"\n          }\n        },\n        {\n          \"name\": \"C\",\n          \"type\": {\n            \"ptr\": \"int\"\n          }\n        }\n      ],\n      \"instrs\": [\n        {\n          \"op\": \"const\",\n          \"dest\": \"size\",\n          \"type\": \"int\",\n          \"value\": 1024\n        },\n        {\n          \"op\": \"const\",\n          \"dest\": \"zero\",\n          \"type\": \"int\",\n          \"value\": 0\n        },\n        {\n          \"op\": \"loop\",\n          \"args\": [\"i\", \"0\", \"16\", \"1\"],\n          \"body\": {\n            \"instrs\": [\n              {\n                \"op\": \"load\",\n                \"dest\": \"val\",\n                \"args\": [\"A\", \"i\"],\n                \"type\": \"int\"\n              },\n              {\n                \"op\": \"store\",\n                \"args\": [\"B\", \"i\", \"val\"]\n              }\n            ]\n          }\n        },\n        {\n          \"op\": \"loop\",\n          \"args\": [\"j\", \"0\", \"32\", \"1\"],\n          \"body\": {\n            \"instrs\": [\n              {\n                \"op\": \"mul\",\n                \"dest\": \"idx\",\n                \"args\": [\"j\", \"4\"],\n                \"type\": \"int\"\n              },\n              {\n                \"op\": \"load\",\n                \"dest\": \"val\",\n                \"args\": [\"B\", \"idx\"],\n                \"type\": \"int\"\n              },\n              {\n                \"op\": \"add\",\n                \"dest\": \"new_val\",\n                \"args\": [\"val\", \"1\"],\n                \"type\": \"int\"\n              },\n              {\n                \"op\": \"store\",\n                \"args\": [\"C\", \"idx\", \"new_val\"]\n              }\n            ]\n          }\n        },\n        {\n          \"op\": \"loop\",\n          \"args\": [\"k\", \"0\", \"64\", \"1\"],\n          \"body\": {\n            \"instrs\": [\n              {\n                \"op\": \"mul\",\n                \"dest\": \"offset\",\n                \"args\": [\"k\", \"2\"],\n                \"type\": \"int\"\n              },\n              {\n                \"op\": \"load\",\n                \"dest\": \"val1\",\n                \"args\": [\"A\", \"offset\"],\n                \"type\": \"int\"\n              },\n              {\n                \"op\": \"load\",\n                \"dest\": \"val2\",\n                \"args\": [\"B\", \"offset\"],\n                \"type\": \"int\"\n              },\n              {\n                \"op\": \"add\",\n                \"dest\": \"sum\",\n                \"args\": [\"val1\", \"val2\"],\n                \"type\": \"int\"\n              },\n              {\n                \"op\": \"store\",\n                \"args\": [\"C\", \"offset\", \"sum\"]\n              }\n            ]\n          }\n        }\n      ]\n    }\n  ]\n}\nFrom an input with three independent loops performing strided array accesses (strides 1, 4, and 2) and array operations (copy, increment, sum), we see loop unrolling, tiling, and interchange being successfully applied, demonstrating the optimizer handles simple strided access patterns effectively.\n✓ Loop unrolling detected\n✓ Loop tiling detected \n✓ Loop interchange detected\n\n\ninput2:\n{\n  \"functions\": [\n    {\n      \"name\": \"vector_operations\",\n      \"args\": [\n        {\n          \"name\": \"input_array\",\n          \"type\": {\n            \"ptr\": \"int\"\n          }\n        }\n      ],\n      \"instrs\": [\n        {\n          \"op\": \"const\",\n          \"dest\": \"size\",\n          \"type\": \"int\",\n          \"value\": 1024\n        },\n        {\n          \"op\": \"alloc\",\n          \"dest\": \"output_array\",\n          \"type\": {\n            \"ptr\": \"int\",\n            \"size\": [1024]\n          }\n        },\n        {\n          \"op\": \"alloc\",\n          \"dest\": \"temp_array\",\n          \"type\": {\n            \"ptr\": \"int\",\n            \"size\": [1024]\n          }\n        },\n        {\n          \"op\": \"const\",\n          \"dest\": \"zero\",\n          \"type\": \"int\",\n          \"value\": 0\n        },\n        {\n          \"op\": \"const\",\n          \"dest\": \"one\",\n          \"type\": \"int\",\n          \"value\": 1\n        },\n        {\n          \"op\": \"const\",\n          \"dest\": \"threshold\",\n          \"type\": \"int\",\n          \"value\": 100\n        },\n        {\n          \"op\": \"const\",\n          \"dest\": \"scale\",\n          \"type\": \"int\",\n          \"value\": 2\n        },\n        {\n          \"op\": \"loop\",\n          \"args\": [\"i\", \"zero\", \"size\"],\n          \"body\": {\n            \"instrs\": [\n              {\n                \"op\": \"load\",\n                \"dest\": \"val\",\n                \"type\": \"int\",\n                \"args\": [\"input_array\", \"i\"]\n              },\n              {\n                \"op\": \"mul\",\n                \"dest\": \"val\",\n                \"type\": \"int\",\n                \"args\": [\"val\", \"scale\"]\n              },\n              {\n                \"op\": \"store\",\n                \"args\": [\"temp_array\", \"i\", \"val\"]\n              }\n            ]\n          }\n        },\n        {\n          \"op\": \"loop\",\n          \"args\": [\"i\", \"zero\", \"size\"],\n          \"body\": {\n            \"instrs\": [\n              {\n                \"op\": \"load\",\n                \"dest\": \"val\",\n                \"type\": \"int\",\n                \"args\": [\"temp_array\", \"i\"]\n              },\n              {\n                \"op\": \"lt\",\n                \"dest\": \"cond\",\n                \"type\": \"bool\",\n                \"args\": [\"val\", \"threshold\"]\n              },\n              {\n                \"op\": \"mul\",\n                \"dest\": \"val\",\n                \"type\": \"int\",\n                \"args\": [\"val\", \"scale\"]\n              },\n              {\n                \"op\": \"store\",\n                \"args\": [\"output_array\", \"i\", \"val\"]\n              }\n            ]\n          }\n        },\n        {\n          \"op\": \"loop\",\n          \"args\": [\"i\", \"zero\", \"size\", \"one\"],\n          \"body\": {\n            \"instrs\": [\n              {\n                \"op\": \"load\",\n                \"dest\": \"val\",\n                \"type\": \"int\",\n                \"args\": [\"output_array\", \"i\"]\n              },\n              {\n                \"op\": \"add\",\n                \"dest\": \"val\",\n                \"type\": \"int\",\n                \"args\": [\"val\", \"one\"]\n              },\n              {\n                \"op\": \"store\",\n                \"args\": [\"output_array\", \"i\", \"val\"]\n              }\n            ]\n          }\n        }\n      ]\n    }\n  ]\n}\nFrom a vector operations input with three sequential loops performing vector scaling, thresholding and increment operations using direct indexing, we see all major optimizations being applied including array padding for better cache alignment.\n✓ Loop unrolling detected\n✓ Loop tiling detected\n✓ Array padding detected \n✓ Loop interchange detected\n\n\ninput3:\n{\n  \"functions\": [{\n    \"name\": \"complex_matrix_ops\",\n    \"args\": [\n      {\"name\": \"matrix_a\", \"type\": {\"ptr\": \"int\"}},\n      {\"name\": \"matrix_b\", \"type\": {\"ptr\": \"int\"}},\n      {\"name\": \"vector_x\", \"type\": {\"ptr\": \"int\"}},\n      {\"name\": \"n\", \"type\": \"int\"}\n    ],\n    \"instrs\": [\n      {\n        \"op\": \"alloc\",\n        \"dest\": \"result_matrix\",\n        \"type\": {\n          \"ptr\": \"int\",\n          \"size\": [512, 512]\n        }\n      },\n      {\n        \"op\": \"alloc\",\n        \"dest\": \"temp_vector\",\n        \"type\": {\n          \"ptr\": \"int\",\n          \"size\": [512]\n        }\n      },\n      {\n        \"op\": \"const\",\n        \"dest\": \"alpha\",\n        \"type\": \"int\",\n        \"value\": 2\n      },\n      {\n        \"op\": \"const\",\n        \"dest\": \"beta\",\n        \"type\": \"int\",\n        \"value\": 3\n      },\n      {\n        \"op\": \"loop\",\n        \"args\": [\"i\", \"0\", \"512\"],\n        \"body\": {\n          \"instrs\": [\n            {\n              \"op\": \"mul\",\n              \"dest\": \"row_offset\",\n              \"type\": \"int\",\n              \"args\": [\"i\", \"512\"]\n            },\n            {\n              \"op\": \"loop\",\n              \"args\": [\"j\", \"0\", \"512\"],\n              \"body\": {\n                \"instrs\": [\n                  {\n                    \"op\": \"add\",\n                    \"dest\": \"idx\",\n                    \"type\": \"int\",\n                    \"args\": [\"row_offset\", \"j\"]\n                  },\n                  {\n                    \"op\": \"load\",\n                    \"dest\": \"a_val\",\n                    \"type\": \"int\",\n                    \"args\": [\"matrix_a\", \"idx\"]\n                  },\n                  {\n                    \"op\": \"load\",\n                    \"dest\": \"b_val\",\n                    \"type\": \"int\",\n                    \"args\": [\"matrix_b\", \"idx\"]\n                  },\n                  {\n                    \"op\": \"mul\",\n                    \"dest\": \"prod\",\n                    \"type\": \"int\",\n                    \"args\": [\"a_val\", \"b_val\"]\n                  },\n                  {\n                    \"op\": \"store\",\n                    \"args\": [\"result_matrix\", \"idx\", \"prod\"]\n                  }\n                ]\n              }\n            }\n          ]\n        }\n      },\n      {\n        \"op\": \"loop\",\n        \"args\": [\"i\", \"0\", \"512\"],\n        \"body\": {\n          \"instrs\": [\n            {\n              \"op\": \"const\",\n              \"dest\": \"sum\",\n              \"type\": \"int\",\n              \"value\": 0\n            },\n            {\n              \"op\": \"loop\",\n              \"args\": [\"k\", \"0\", \"512\"],\n              \"body\": {\n                \"instrs\": [\n                  {\n                    \"op\": \"mul\",\n                    \"dest\": \"idx1\",\n                    \"type\": \"int\",\n                    \"args\": [\"i\", \"512\"]\n                  },\n                  {\n                    \"op\": \"add\",\n                    \"dest\": \"idx1\",\n                    \"type\": \"int\",\n                    \"args\": [\"idx1\", \"k\"]\n                  },\n                  {\n                    \"op\": \"load\",\n                    \"dest\": \"matrix_val\",\n                    \"type\": \"int\",\n                    \"args\": [\"result_matrix\", \"idx1\"]\n                  },\n                  {\n                    \"op\": \"load\",\n                    \"dest\": \"vec_val\",\n                    \"type\": \"int\",\n                    \"args\": [\"vector_x\", \"k\"]\n                  },\n                  {\n                    \"op\": \"mul\",\n                    \"dest\": \"prod\",\n                    \"type\": \"int\",\n                    \"args\": [\"matrix_val\", \"vec_val\"]\n                  },\n                  {\n                    \"op\": \"add\",\n                    \"dest\": \"sum\",\n                    \"type\": \"int\",\n                    \"args\": [\"sum\", \"prod\"]\n                  }\n                ]\n              }\n            },\n            {\n              \"op\": \"store\",\n              \"args\": [\"temp_vector\", \"i\", \"sum\"]\n            }\n          ]\n        }\n      },\n      {\n        \"op\": \"loop\",\n        \"args\": [\"i\", \"0\", \"512\"],\n        \"body\": {\n          \"instrs\": [\n            {\n              \"op\": \"load\",\n              \"dest\": \"val\",\n              \"type\": \"int\",\n              \"args\": [\"temp_vector\", \"i\"]\n            },\n            {\n              \"op\": \"mul\",\n              \"dest\": \"scaled\",\n              \"type\": \"int\",\n              \"args\": [\"val\", \"alpha\"]\n            },\n            {\n              \"op\": \"load\",\n              \"dest\": \"x_val\",\n              \"type\": \"int\",\n              \"args\": [\"vector_x\", \"i\"]\n            },\n            {\n              \"op\": \"mul\",\n              \"dest\": \"beta_x\",\n              \"type\": \"int\",\n              \"args\": [\"x_val\", \"beta\"]\n            },\n            {\n              \"op\": \"add\",\n              \"dest\": \"result\",\n              \"type\": \"int\",\n              \"args\": [\"scaled\", \"beta_x\"]\n            },\n            {\n              \"op\": \"store\",\n              \"args\": [\"temp_vector\", \"i\", \"result\"]\n            }\n          ]\n        }\n      }\n    ]\n  }]\n}\nFrom a complex matrix operations input combining matrix multiplication, matrix-vector multiplication and vector scaling with true 2D indexing (i*512 + j), we see the success of loop unrolling with tiling for cache locality and array padding for alignment, showing the optimizer handles 2D access patterns well.\n✓ Loop unrolling detected\n✓ Loop tiling detected\n✓ Array padding detected\n✓ Loop interchange detected\n\n\ninput 4:\n{\n  \"functions\": [{\n    \"name\": \"matrix_operations\",\n    \"args\": [\n      {\"name\": \"n\", \"type\": \"int\"}\n    ],\n    \"instrs\": [\n      {\n        \"op\": \"alloc\",\n        \"dest\": \"matrix1\",\n        \"type\": {\n          \"ptr\": \"int\",\n          \"size\": [1023, 511]\n        }\n      },\n      {\n        \"op\": \"alloc\",\n        \"dest\": \"matrix2\",\n        \"type\": {\n          \"ptr\": \"int\", \n          \"size\": [511, 1023]\n        }\n      },\n      {\n        \"op\": \"alloc\",\n        \"dest\": \"result\",\n        \"type\": {\n          \"ptr\": \"int\",\n          \"size\": [1023, 1023]\n        }\n      },\n      {\n        \"op\": \"loop\",\n        \"args\": [\"i\", \"n\"],\n        \"body\": {\n          \"instrs\": [\n            {\n              \"op\": \"loop\",\n              \"args\": [\"j\", \"n\"],\n              \"body\": {\n                \"instrs\": [\n                  {\n                    \"op\": \"loop\",\n                    \"args\": [\"k\", \"n\"],\n                    \"body\": {\n                      \"instrs\": [\n                        {\n                          \"op\": \"load\",\n                          \"dest\": \"m1val\",\n                          \"args\": [\"matrix1\", \"i * 511 + k\"]\n                        },\n                        {\n                          \"op\": \"load\",\n                          \"dest\": \"m2val\",\n                          \"args\": [\"matrix2\", \"k * 1023 + j\"]\n                        },\n                        {\n                          \"op\": \"mul\",\n                          \"dest\": \"prod\",\n                          \"args\": [\"m1val\", \"m2val\"]\n                        },\n                        {\n                          \"op\": \"load\",\n                          \"dest\": \"current\",\n                          \"args\": [\"result\", \"i * 1023 + j\"]\n                        },\n                        {\n                          \"op\": \"add\",\n                          \"dest\": \"newval\",\n                          \"args\": [\"current\", \"prod\"]\n                        },\n                        {\n                          \"op\": \"store\",\n                          \"args\": [\"result\", \"i * 1023 + j\", \"newval\"]\n                        }\n                      ]\n                    }\n                  }\n                ]\n              }\n            }\n          ]\n        }\n      }\n    ]\n  }]\n}\nFrom a matrix multiplication input with three deeply nested loops doing matrix multiplication with non-power-of-2 dimensions (1023x511 * 511x1023 = 1023x1023) and complex index expressions combining loop variables (i511 + k, k1023 + j), we only see array padding being applied but no loop transformations, revealing that probably our optimizer’s IndexExpressionParser fails to properly analyze composite index expressions that combine multiple loop variables with non-power-of-2 strides, limiting its ability to apply important optimizations like tiling and interchange that would be beneficial for matrix multiplication.\n✗ No loop unrolling applied\n✗ No loop tiling applied\n✓ Array padding detected\n✗ No loop interchange applied\nMore inputs and results are in the code repo.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Data Layout Optimization and Loop Transformations"
    ]
  },
  {
    "objectID": "blogs/Sana/FinalProjectReport.html#limitations",
    "href": "blogs/Sana/FinalProjectReport.html#limitations",
    "title": "Data Layout Optimization and Loop Transformations",
    "section": "Limitations",
    "text": "Limitations\nOur implementation faces several key challenges in handling complex code patterns and optimizations. The parser struggles with complex index expressions (particularly nested arithmetic and composite indices like i*511 + k), while the static analysis approach has difficulty with dynamic array sizes and indirect memory accesses.\n\nLoop transformations, though effective for simple cases, have important constraints: tiling can increase code size significantly (which happened for above inputs), fusion only works with adjacent compatible loops, and interchange requires perfectly nested loops. The system particularly struggles with non-power-of-2 strides and complex alignment patterns that arise in matrix operations.\nMemory layout optimizations face similar challenges, balancing increased memory footprint from padding against performance gains. The implementation must work with fixed tile sizes and single-dimension padding, which may not be optimal across different architectures. When handling multiple loop variables or irregular strides, the system often fails to determine safe optimization strategies, particularly for loop tiling and interchange operations.\nThese limitations primarily impact complex numerical computations where precise optimization and accurate dependency analysis are crucial, indicating key areas for future improvement in both analysis and transformation capabilities.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Data Layout Optimization and Loop Transformations"
    ]
  },
  {
    "objectID": "blogs/Sana/FinalProjectReport.html#conclusion",
    "href": "blogs/Sana/FinalProjectReport.html#conclusion",
    "title": "Data Layout Optimization and Loop Transformations",
    "section": "Conclusion:",
    "text": "Conclusion:\nThe implemented data layout optimizer successfully handles certain optimization patterns but shows key limitations in complex matrix operations. The system demonstrates effectiveness with simple vector operations and basic matrix operations involving constant strides, successfully applying transformations like loop unrolling, tiling, and array padding. However, a significant limitation emerges in handling non-power-of-2 matrix dimensions and complex index expressions combining multiple loop variables (e.g., i*511 + k). This limitation primarily stems from the IndexExpressionParser’s restricted capability in analyzing complex composite expressions, resulting in missed optimization opportunities particularly in matrix multiplication scenarios.:w",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Data Layout Optimization and Loop Transformations"
    ]
  },
  {
    "objectID": "blogs/Sana/FinalProjectReport.html#future-work",
    "href": "blogs/Sana/FinalProjectReport.html#future-work",
    "title": "Data Layout Optimization and Loop Transformations",
    "section": "Future Work:",
    "text": "Future Work:\n\nEnhanced Index Expression Analysis:\n\nImplement a more sophisticated index expression parser that can handle composite expressions\nAdd support for analyzing non-power-of-2 stride patterns\nImprove detection of matrix multiplication patterns\n\n\n\nOptimization Coordination:\n\nDevelop a cost model to better decide when to apply each optimization\nImplement optimization ordering to handle cases where transformations interact\nAdd support for partial loop fusion in compatible cases\n\n\n\nCache Optimization:\n\nImprove tile size calculation based on actual matrix dimensions\nAdd multi-level cache analysis\nImplement more sophisticated padding strategies for irregular matrix sizes\n\n\n\nAdditional Features:\n\nAdd support for data layout transformations (row-major to column-major)\nImplement automatic vectorization hints\nAdd analysis and optimization for diagonal and block-diagonal matrix patterns\nSupport dynamic array sizes and bounds\n\nAlthough this project demonstrates effective implementation of core optimization techniques (loop tiling, fusion, interchange, and array padding) and shows promising results for basic matrix operations and vector computations, it needs significant improvements in handling complex index expressions, non-power-of-2 matrix dimensions, multi-level cache optimization strategies, and dynamic array size support to be truly competitive with state-of-the-art layout optimizers in the field of high-performance computing and compiler optimization.\nIt’s important to note that while our optimization passes are being applied successfully, we CANNOT guarantee they always improve code performance. Some optimizations like loop unrolling and tiling can sometimes degrade performance by increasing code size or adding overhead that outweighs the cache benefits. Future work should include comprehensive benchmarking with hardware performance counters and systematic testing across different matrix sizes to verify when these optimizations truly help versus when they might hurt performance, as our current implementation focuses on detecting and applying transformations but lacks concrete measurements to validate their effectiveness in improving cache behavior and overall program efficiency.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Data Layout Optimization and Loop Transformations"
    ]
  },
  {
    "objectID": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html",
    "href": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html",
    "title": "Homework1 - Sana",
    "section": "",
    "text": "I created a Brill benchmark that calculates and prints the sum of prime numbers and composite numbers up to a given input (in this case, 100). The algorithm used here is very similar to the algorithm used in sieve.bril code in the Brill benchmark folder which is designed to find and print all prime numbers up to a specified limit. It emphasizes the identification of primes through boolean flags and modular functions. Our implementation extends the algorithm to calculate the sum of primes and the sum of composites up to a specified limit. It uses an integer array to flag primes and composites and focuses on aggregating values rather than listing them.\n\n\n\nTo give a little explanation about what each part of the code does:\n\n\n@sumOfPrimes: This function calculates the sum of all prime numbers up to n. First, an array nums of size n is allocated, where each index represents whether a number is prime or composite (1 for prime, 0 for composite). Initially, all numbers are marked as prime. Starting from current = 2, the function marks the multiples of each number as composite (0), skipping prime numbers. The outer loop increments current, and the inner loop marks all multiples of current as composite. After marking non-primes, the function iterates over the array from 2 to n and sums the numbers where the value is 1 (they are prime).\n@sumOfComposites: This function calculates the sum of all composite numbers up to n. It works similarly to the sumOfPrimes function but focuses on composite numbers. The main difference is in the summing step: instead of summing primes, the function starts summing from 4 (since 1 is neither prime nor composite, and 2 and 3 are primes) and adds all numbers marked as 0 (composite).\n\n\n\n\nTo test the implementation, I used the following approach:\n\na) I ran the benchmark with different input values, starting with small numbers like 10, 20, and then larger numbers like 100.\nb) I verified the results manually for smaller inputs by calculating the expected sums of primes and composites.\nc) I used the Brill interpreter (brili) to run the benchmark and checked both the output and the number of dynamic instructions executed.\n\nQuantitative results for input 100:\n1060\n3889\n\n\n\nThe most challenging part was handling the inner and outer loops in the Bril format, especially managing control flow with branches (br) and jumps (jmp). In Bril, control flow is more explicit than in higher-level languages, and ensuring proper transitions between loop bodies and loop conditions while avoiding infinite loops or incorrect logic was complex.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework1 - Sana"
    ]
  },
  {
    "objectID": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html#explanation-of-the-code",
    "href": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html#explanation-of-the-code",
    "title": "Homework1 - Sana",
    "section": "",
    "text": "To give a little explanation about what each part of the code does:\n\n\n@sumOfPrimes: This function calculates the sum of all prime numbers up to n. First, an array nums of size n is allocated, where each index represents whether a number is prime or composite (1 for prime, 0 for composite). Initially, all numbers are marked as prime. Starting from current = 2, the function marks the multiples of each number as composite (0), skipping prime numbers. The outer loop increments current, and the inner loop marks all multiples of current as composite. After marking non-primes, the function iterates over the array from 2 to n and sums the numbers where the value is 1 (they are prime).\n@sumOfComposites: This function calculates the sum of all composite numbers up to n. It works similarly to the sumOfPrimes function but focuses on composite numbers. The main difference is in the summing step: instead of summing primes, the function starts summing from 4 (since 1 is neither prime nor composite, and 2 and 3 are primes) and adds all numbers marked as 0 (composite).",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework1 - Sana"
    ]
  },
  {
    "objectID": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html#how-the-implementation-is-tested",
    "href": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html#how-the-implementation-is-tested",
    "title": "Homework1 - Sana",
    "section": "",
    "text": "To test the implementation, I used the following approach:\n\na) I ran the benchmark with different input values, starting with small numbers like 10, 20, and then larger numbers like 100.\nb) I verified the results manually for smaller inputs by calculating the expected sums of primes and composites.\nc) I used the Brill interpreter (brili) to run the benchmark and checked both the output and the number of dynamic instructions executed.\n\nQuantitative results for input 100:\n1060\n3889",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework1 - Sana"
    ]
  },
  {
    "objectID": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html#challenges-faced",
    "href": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html#challenges-faced",
    "title": "Homework1 - Sana",
    "section": "",
    "text": "The most challenging part was handling the inner and outer loops in the Bril format, especially managing control flow with branches (br) and jumps (jmp). In Bril, control flow is more explicit than in higher-level languages, and ensuring proper transitions between loop bodies and loop conditions while avoiding infinite loops or incorrect logic was complex.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework1 - Sana"
    ]
  },
  {
    "objectID": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html#explanation-of-the-code-1",
    "href": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html#explanation-of-the-code-1",
    "title": "Homework1 - Sana",
    "section": "Explanation of the Code",
    "text": "Explanation of the Code\nThe code first iterates through each function in the program using the outer loop. Inside the function, the inner loop goes through each instruction. For each instruction, it increments the instruction count and checks if the instruction is a jump (br or jmp). If it is, a print instruction is added before the jump.\n\nimport json\nimport copy\n\nbril_json = \"\"\"\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v0\", \"value\": 1 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v1\", \"value\": 2 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"v2\", \"args\": [\"v0\", \"v1\"] },\n        { \"op\": \"br\", \"args\": [\"v2\"], \"labels\": [\"then\", \"else\"] },\n        { \"label\": \"then\" },\n        { \"op\": \"print\", \"args\": [\"v2\"] },\n        { \"op\": \"jmp\", \"labels\": [\"end\"] },\n        { \"label\": \"else\" },\n        { \"op\": \"print\", \"args\": [\"v1\"] },\n        { \"label\": \"end\" },\n        { \"op\": \"ret\" }\n      ]\n    }\n  ]\n}\n\"\"\"\n\nbril_program = json.loads(bril_json)\n\ndef process_bril_program(bril_program):\n    total_instructions = 0\n    for function in bril_program['functions']:\n        new_instrs = []\n        for instruction in function['instrs']:\n            total_instructions += 1\n            \n            # add a print instruction before a jump!\n            if instruction.get('op') in ['br', 'jmp']:\n                print_instr = {\n                    \"op\": \"print\",\n                    \"args\": [\"Jumping\"]\n                }\n                new_instrs.append(print_instr)\n            \n            new_instrs.append(instruction)\n        \n        function['instrs'] = new_instrs\n\n    return bril_program, total_instructions\n\nmodified_program, instruction_count = process_bril_program(bril_program)\n\nprint(\"Total number of instructions:\", instruction_count)\nprint(\"Modified Bril program:\")\nprint(json.dumps(modified_program, indent=2))\n\nTotal number of instructions: 11\nModified Bril program:\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"v0\",\n          \"value\": 1\n        },\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"v1\",\n          \"value\": 2\n        },\n        {\n          \"op\": \"add\",\n          \"type\": \"int\",\n          \"dest\": \"v2\",\n          \"args\": [\n            \"v0\",\n            \"v1\"\n          ]\n        },\n        {\n          \"op\": \"print\",\n          \"args\": [\n            \"Jumping\"\n          ]\n        },\n        {\n          \"op\": \"br\",\n          \"args\": [\n            \"v2\"\n          ],\n          \"labels\": [\n            \"then\",\n            \"else\"\n          ]\n        },\n        {\n          \"label\": \"then\"\n        },\n        {\n          \"op\": \"print\",\n          \"args\": [\n            \"v2\"\n          ]\n        },\n        {\n          \"op\": \"print\",\n          \"args\": [\n            \"Jumping\"\n          ]\n        },\n        {\n          \"op\": \"jmp\",\n          \"labels\": [\n            \"end\"\n          ]\n        },\n        {\n          \"label\": \"else\"\n        },\n        {\n          \"op\": \"print\",\n          \"args\": [\n            \"v1\"\n          ]\n        },\n        {\n          \"label\": \"end\"\n        },\n        {\n          \"op\": \"ret\"\n        }\n      ]\n    }\n  ]\n}",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework1 - Sana"
    ]
  },
  {
    "objectID": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html#how-the-implementation-is-tested-1",
    "href": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html#how-the-implementation-is-tested-1",
    "title": "Homework1 - Sana",
    "section": "How the Implementation is Tested",
    "text": "How the Implementation is Tested\nTo test the implementation, I created a small Bril program called test.json that contains various types of instructions, including constants, an add operation, a branch instruction (br), a jump instruction (jmp), and print statements. I used this file to verify that the transformation was applied correctly and that the total number of instructions was counted.\nTest Input (test.json):\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v0\", \"value\": 1 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v1\", \"value\": 2 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"v2\", \"args\": [\"v0\", \"v1\"] },\n        { \"op\": \"br\", \"args\": [\"v2\"], \"labels\": [\"then\", \"else\"] },\n        { \"label\": \"then\" },\n        { \"op\": \"print\", \"args\": [\"v2\"] },\n        { \"op\": \"jmp\", \"labels\": [\"end\"] },\n        { \"label\": \"else\" },\n        { \"op\": \"print\", \"args\": [\"v1\"] },\n        { \"label\": \"end\" },\n        { \"op\": \"ret\" }\n      ]\n    }\n  ]\n}\n\nOutput of The program is shown in previous section.\nQuantitative Results: The total number of instructions before the transformation was 11. After transformation, 2 print instructions were added (one before the br and another before the jmp), making the total number of instructions 13. (We can conclude this by counting the instructions in the modified Bril program)",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework1 - Sana"
    ]
  },
  {
    "objectID": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html#challenges-faced-1",
    "href": "blogs/Sana/09-20-2024-HW1-SanaTaghipourAnvari.html#challenges-faced-1",
    "title": "Homework1 - Sana",
    "section": "Challenges Faced",
    "text": "Challenges Faced\nThe most challenging part of the task was ensuring that the additional instructions were inserted in the correct place without modifying the logic of the Bril program. The Bril program must still execute in the intended order, and placing the print instructions incorrectly could have changed the control flow. To address this challenge, I carefully looped through the instruction list, checked for br and jmp operations, and inserted the print instructions before each of these jump-related instructions by also maintaining the program’s original flow and semantics.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework1 - Sana"
    ]
  },
  {
    "objectID": "blogs/samples/junk_dir/index.html",
    "href": "blogs/samples/junk_dir/index.html",
    "title": "sample blog with an image and a graph",
    "section": "",
    "text": "image of rabbits\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# example data\nx = np.arange(0.1, 4, 0.5)\ny = np.exp(-x)\n\nfig, ax = plt.subplots()\nax.errorbar(x, y, xerr=0.2, yerr=0.4)\n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Samples",
      "sample blog with an image and a graph"
    ]
  },
  {
    "objectID": "lectures/poly_final.html",
    "href": "lectures/poly_final.html",
    "title": "Introduction To Polyhedral Methods",
    "section": "",
    "text": "Many modern programs in fields like High-Performance Computing (HPC), GPU programming, scientific applications and machine learning are loop-intensive.\nTraditional compiler techniques encounter several challenges in these contexts:",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#what-are-polyhedral-methods",
    "href": "lectures/poly_final.html#what-are-polyhedral-methods",
    "title": "Introduction To Polyhedral Methods",
    "section": "",
    "text": "Many modern programs in fields like High-Performance Computing (HPC), GPU programming, scientific applications and machine learning are loop-intensive.\nTraditional compiler techniques encounter several challenges in these contexts:",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#challenges",
    "href": "lectures/poly_final.html#challenges",
    "title": "Introduction To Polyhedral Methods",
    "section": "challenges",
    "text": "challenges\n\nComplex Loop Dependencies:\n\nDependencies can exist both within a loop and across multiple loops, limiting optimization potential.\n\nMissed Parallelism:\n\nInsufficient analysis may prevent identification of potential parallel execution paths, underutilizing hardware capabilities.\n\nPoor Data Locality:\n\nInefficient usage of cache memory leads to performance bottlenecks.\n\n\nPolyhedral methods address these challenges by providing a precise mathematical framework for analyzing and optimizing loop nests.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#challenges-in-traditional-loop-optimization",
    "href": "lectures/poly_final.html#challenges-in-traditional-loop-optimization",
    "title": "Introduction To Polyhedral Methods",
    "section": "Challenges in Traditional Loop Optimization",
    "text": "Challenges in Traditional Loop Optimization\nfor (i = 0; i &lt; N; i++)  \n  for (j = 0; j &lt; N; j++)  \n    A[i][j] = A[i-1][j] + A[i][j-1];\n\nIteration Dependencies\n\n\\(A[i][j]\\) depends on \\(A[i-1][j]\\) and \\(A[i][j-1]\\)\nLimits parallel execution due to data dependencies.\n\nPerformance Bottleneck\n\nWithout optimization such as tiling, loop execution suffers from cache misses.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#improving-loop-performance-by-tiling",
    "href": "lectures/poly_final.html#improving-loop-performance-by-tiling",
    "title": "Introduction To Polyhedral Methods",
    "section": "Improving Loop Performance by tiling",
    "text": "Improving Loop Performance by tiling\n\nBenefits of Tiling\n\nImproved Cache Access Patterns\n\nReduces cache misses by increasing data locality.\n\nEnhanced Parallelism\n\nBreaking dependencies across tiles can allow independent execution of tiles on different processors.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#example-tiling-transformation",
    "href": "lectures/poly_final.html#example-tiling-transformation",
    "title": "Introduction To Polyhedral Methods",
    "section": "Example Tiling Transformation",
    "text": "Example Tiling Transformation\nTransform the inner loops by introducing tile sizes \\(T_i\\) and \\(T_j\\):\nfor (ii = 0; ii &lt; N; ii += T_i)\n  for (jj = 0; jj &lt; N; jj += T_j)\n    for (i = ii; i &lt; min(ii + T_i, N); i++)\n      for (j = jj; j &lt; min(jj + T_j, N); j++)\n        A[i][j] = A[i-1][j] + A[i][j-1];",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#limitations-of-polyhedral-methods",
    "href": "lectures/poly_final.html#limitations-of-polyhedral-methods",
    "title": "Introduction To Polyhedral Methods",
    "section": "Limitations of Polyhedral Methods",
    "text": "Limitations of Polyhedral Methods\n\nAffine Loops:\n\nOnly applicable to loops with linear bounds and affine array accesses.\nExample: Bounds and accesses must be simple linear expressions of loop indices.\nStatic Control Flow:\nMethods require loops without complex control flow such as recursion or unpredictable branches (virtual functions).",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#scops",
    "href": "lectures/poly_final.html#scops",
    "title": "Introduction To Polyhedral Methods",
    "section": "scops",
    "text": "scops\nRegions of code that can be handled in the polyhedral model are usually called Static Control Parts, abbreviated as SCoPs.\nUsually, SCoPs may only contain regular control flow free of exceptions and other constructs that may provoke changes in control flow such as conditional expressions dependent on data (read from memory) or side effects of function calls.\nloop free code can be a wrapped in for(i = 0; i &lt; 1; i++){}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#two-kinds-of-problems",
    "href": "lectures/poly_final.html#two-kinds-of-problems",
    "title": "Introduction To Polyhedral Methods",
    "section": "two kinds of problems",
    "text": "two kinds of problems\n\nPolyhedral Analysis\nGiven a transformation. - Does a transformation preserve the loop’s semantics? - What is the resulting loop structure after transformation?\n\n\nPolyhedral Scheduling\nFind the “best” transformation that optimizes a property such as parallelism or data locality.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#advantages-of-polyhedral-methods",
    "href": "lectures/poly_final.html#advantages-of-polyhedral-methods",
    "title": "Introduction To Polyhedral Methods",
    "section": "Advantages of Polyhedral Methods",
    "text": "Advantages of Polyhedral Methods\n\nMathematical Rigor\n\nAffine Constraints:\n\nPrecisely model and manipulate loops, ensuring correctness.\n\n\n\n\nAdvanced Optimizations Enabled\n\nLoop Tiling:\n\nBoosts data locality by breaking loops into smaller blocks.\n\nLoop Fusion:\n\nMinimizes overhead and improves cache performance by combining adjacent loops.\n\nSkewing and Parallelization:\n\nAddresses complex dependencies, allowing effective parallel execution.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#loop-execution",
    "href": "lectures/poly_final.html#loop-execution",
    "title": "Introduction To Polyhedral Methods",
    "section": "Loop Execution",
    "text": "Loop Execution\n\nStatements and Instances\n\nIn loops, statements can execute multiple times.\nEach execution of a statement is called an instance.\nPolyhedral methods track these instances explicitly.\n\nfor i in [1, 2, 3, 4]:\n  s: a[i] = 3  // Statement `s` executes four times (4 instances)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#program-representation",
    "href": "lectures/poly_final.html#program-representation",
    "title": "Introduction To Polyhedral Methods",
    "section": "Program Representation",
    "text": "Program Representation\n\nStatements and Index Sets\n\nDefine what computations to perform.\nSpecify the indices for which these computations are valid.\n\nSchedule\n\nspecifies the order in which the indices are executed.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#toy-example-changing-the-order-of-iterations",
    "href": "lectures/poly_final.html#toy-example-changing-the-order-of-iterations",
    "title": "Introduction To Polyhedral Methods",
    "section": "Toy Example: Changing the Order of Iterations",
    "text": "Toy Example: Changing the Order of Iterations\nProblem: Can we reverse this loop’s execution order?\nfor i = [1,2,3,4]\ns:   a[i] = a[i-1]\nOriginal schedule: i = 1, 2, 3, 4\nReversed schedule: i = 4, 3, 2, 1\nDo these loops produce the same result?\nfor i = [1,2,3,4]               for i = [4,3,2,1]\ns:   a[i] = a[i-1]               s:   a[i] = a[i-1]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#execution-trace-and-data-access",
    "href": "lectures/poly_final.html#execution-trace-and-data-access",
    "title": "Introduction To Polyhedral Methods",
    "section": "Execution Trace and Data Access",
    "text": "Execution Trace and Data Access\n\n\n\n\n\n\n\nOriginal Order\nReversed Order\n\n\n\n\nInstance s(1): Reads a[0], Writes a[1]\nInstance s(4): Reads a[3], Writes a[4]\n\n\nInstance s(2): Reads a[1], Writes a[2]\nInstance s(3): Reads a[2], Writes a[3]\n\n\nInstance s(3): Reads a[2], Writes a[3]\nInstance s(2): Reads a[1], Writes a[2]\n\n\nInstance s(4): Reads a[3], Writes a[4]\nInstance s(1): Reads a[0], Writes a[1]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#data-dependency",
    "href": "lectures/poly_final.html#data-dependency",
    "title": "Introduction To Polyhedral Methods",
    "section": "data dependency",
    "text": "data dependency\nThe data dependency arises from the use of \\(𝑎[𝑖−1]\\) which has to be computed before \\(a[i]\\)\nThis kind of data dependency is called read after write or raw dependency\n\ntransformation\nwe propose a transformation \\(i \\rightarrow 5 -i\\) Does this transformation preserve all the dependencies?\na transformation is a change to the schedule",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#schedules",
    "href": "lectures/poly_final.html#schedules",
    "title": "Introduction To Polyhedral Methods",
    "section": "Schedules:",
    "text": "Schedules:\nA schedule is a function:\n\\[s(\\text{iteration}) = \\text{execution order}(\\text{time})\\]\nExample:\nOriginal order: s(i) = i\nReversed order: s(i) = 5 - i\nProducers must execute before consumers for the transformation to be valid.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#schedule-validity",
    "href": "lectures/poly_final.html#schedule-validity",
    "title": "Introduction To Polyhedral Methods",
    "section": "schedule validity",
    "text": "schedule validity\nnot valid: if there is a pair \\(i\\),\\(j\\), \\(s[i]\\) produces a value, \\(s[j]\\) reads that value, and \\(s[j]\\) is first in the new schedule\nHow do we find such a pair?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#visualization",
    "href": "lectures/poly_final.html#visualization",
    "title": "Introduction To Polyhedral Methods",
    "section": "Visualization",
    "text": "Visualization\n\nGraphical Representation:\n\nNodes represent statements with iteration index.\nDirected edges indicate data dependencies.\nValid schedules respect edge directions.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#forward-order",
    "href": "lectures/poly_final.html#forward-order",
    "title": "Introduction To Polyhedral Methods",
    "section": "forward order",
    "text": "forward order\n\niteration_space = [1,2,3,4]\ndependences = [(1,2), (2,3), (3,4)]\nschedule = [1,2,3,4]\ndraw_1d_iteration_space_with_dependence_graph(iteration_space, dependences, schedule)\n\nSchedule:  [1, 2, 3, 4]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#backward-order",
    "href": "lectures/poly_final.html#backward-order",
    "title": "Introduction To Polyhedral Methods",
    "section": "backward order",
    "text": "backward order\n\niteration_space = [1,2,3,4]\ndependences = [(1,2), (2,3), (3,4)]\nschedule= [4,3,2,1]\ndraw_1d_iteration_space_with_dependence_graph(iteration_space, dependences, schedule)\n\nSchedule:  [4, 3, 2, 1]\n\n\n\n\n\n\n\n\n\na schedule is not valid if the arrows go in different directions",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#more-analytical-check-for-valid",
    "href": "lectures/poly_final.html#more-analytical-check-for-valid",
    "title": "Introduction To Polyhedral Methods",
    "section": "more analytical check for valid",
    "text": "more analytical check for valid\nThere are a number of tests for validity.\nMathematical Condition\n\nFind \\(i, j\\) where: \\[\\text{s}(i) &lt; \\text{s}(j) : i \\text{ executes first}\\]\n\n\\(i\\) reads a value.\n\\(j\\) writes that value.\n\n\nOne: an integer linear programming solver (ILP) to find an integer solution to the equations.\nNP-hard but efficient solvers exist for many of the small cases found in programs\nTwo: check for a cycle in the graph",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#analyzing-parallelism-in-nested-loops",
    "href": "lectures/poly_final.html#analyzing-parallelism-in-nested-loops",
    "title": "Introduction To Polyhedral Methods",
    "section": "Analyzing Parallelism in Nested Loops",
    "text": "Analyzing Parallelism in Nested Loops\n\nCode Example\nfor i in [1, 2, 3, 4]:\n    for j in [1, 2, 3, 4]:\n        a[i, j] = a[i, j-1] + a[i-1, j]\nCan this Loop Nest be Executed in Parallel?\ndata dependence is a mapping:\n\\(s(i) =&gt; s(i+1)\\) from instance to instance",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#horizontal-dependency",
    "href": "lectures/poly_final.html#horizontal-dependency",
    "title": "Introduction To Polyhedral Methods",
    "section": "Horizontal Dependency:",
    "text": "Horizontal Dependency:\nEach iteration (i, j) depends on (i, j-1) (the value to the left in the same row).\n1. For example, a[1, 2] = a[1, 1] + a[0, 2], which means a[1, 2] depends on the value of a[1, 1] from the same row. \n1. (1,1) -&gt; (1,2)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#vertical-dependency",
    "href": "lectures/poly_final.html#vertical-dependency",
    "title": "Introduction To Polyhedral Methods",
    "section": "Vertical Dependency:",
    "text": "Vertical Dependency:\nEach iteration (i, j) depends on (i-1, j) (the value from the previous row in the same column).\n1. For example, a[2, 1] = a[1, 1] + a[2, 0], meaning a[2, 1] depends on a[1, 1] from the row above\n1. (1,1) -&gt; (2,1)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#data-dependencies",
    "href": "lectures/poly_final.html#data-dependencies",
    "title": "Introduction To Polyhedral Methods",
    "section": "Data Dependencies",
    "text": "Data Dependencies\n\nTrue Dependency (Flow Dependency): When an instruction depends on the result of a previous instruction.\n\nExample: A = B + C followed by D = A + E\n\nAnti Dependency: Occurs when an instruction requires a variable that will be overwritten by a subsequent instruction.\nOutput Dependency: When two instructions write to the same location, creating a dependency on the order of execution.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#serial-schedule",
    "href": "lectures/poly_final.html#serial-schedule",
    "title": "Introduction To Polyhedral Methods",
    "section": "serial schedule",
    "text": "serial schedule\nIn a serial execution, iterations proceed row by row, strictly respecting both horizontal and vertical dependencies. This schedule respects all dependencies but executes sequentially, limiting parallelism.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#visualization-1",
    "href": "lectures/poly_final.html#visualization-1",
    "title": "Introduction To Polyhedral Methods",
    "section": "visualization",
    "text": "visualization\n\niteration_space = [(1,1),(1,2),(1,3),(1,4),\n                    (2,1),(2,2),(2,3),(2,4),\n                    (3,1),(3,2),(3,3),(3,4),\n                    (4,1),(4,2),(4,3),(4,4)]\ndependences = []\nfor i in range(1, 5):\n    for j in range(1, 5):\n        if j &gt; 1:  # Horizontal dependency\n            dependences.append(( (i, j-1),(i, j)))\n        if i &gt; 1:  # Vertical dependency\n            dependences.append(((i-1, j),(i, j) ))\n\nschedule = [(i, j) for i in range(1, 5) for j in range(1, 5)]\ndraw_2d_iteration_space_with_dependence_graph(iteration_space, dependences, schedule)\n\nSchedule:  [(1, 1), (1, 2), (1, 3), (1, 4), (2, 1), (2, 2), (2, 3), (2, 4), (3, 1), (3, 2), (3, 3), (3, 4), (4, 1), (4, 2), (4, 3), (4, 4)]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#skewing-the-loops-parallel-schedule-diagonal-parallelism",
    "href": "lectures/poly_final.html#skewing-the-loops-parallel-schedule-diagonal-parallelism",
    "title": "Introduction To Polyhedral Methods",
    "section": "skewing the loops Parallel Schedule: Diagonal Parallelism",
    "text": "skewing the loops Parallel Schedule: Diagonal Parallelism\nTo maximize parallel execution in matrix computations, it’s crucial to identify and execute independent iterations simultaneously. One efficient strategy is Diagonal Parallelism:\n\nDiagonal Organization: Process iterations by diagonals in the iteration space.\nDiagonal Key: Defined as i + j (sum of indices).\nParallel Execution: Iterations with the same diagonal key (i + j constant) can be executed in parallel since they are independent.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#example-diagonal-scheduling-code",
    "href": "lectures/poly_final.html#example-diagonal-scheduling-code",
    "title": "Introduction To Polyhedral Methods",
    "section": "Example: Diagonal Scheduling Code",
    "text": "Example: Diagonal Scheduling Code\n\n# Organize the iteration space by diagonals\ndiagonals = {}\nfor (i, j) in iteration_space:\n    diag_key = i + j  # Diagonal key: sum of indices\n    if diag_key not in diagonals:\n        diagonals[diag_key] = []\n    diagonals[diag_key].append((i, j))\n\n# Sort the diagonals by diagonal key\nsorted_diagonals = sorted(diagonals.keys())\n\n# Print the diagonal schedule\nprint(\"Diagonal Schedule:\")\nprint(diagonals)\n\nDiagonal Schedule:\n{2: [(1, 1)], 3: [(1, 2), (2, 1)], 4: [(1, 3), (2, 2), (3, 1)], 5: [(1, 4), (2, 3), (3, 2), (4, 1)], 6: [(2, 4), (3, 3), (4, 2)], 7: [(3, 4), (4, 3)], 8: [(4, 4)]}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#visualizing-the-parallel-schedule",
    "href": "lectures/poly_final.html#visualizing-the-parallel-schedule",
    "title": "Introduction To Polyhedral Methods",
    "section": "Visualizing the parallel schedule:",
    "text": "Visualizing the parallel schedule:\n\n\nSchedule:  [(1, 1), (1, 2), (2, 1), (1, 3), (2, 2), (3, 1), (1, 4), (2, 3), (3, 2), (4, 1), (2, 4), (3, 3), (4, 2), (3, 4), (4, 3), (4, 4)]\n\n\n\n\n\n\n\n\n\neach diagonal line can run in parallel",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#benefits-of-diagonal-scheduling",
    "href": "lectures/poly_final.html#benefits-of-diagonal-scheduling",
    "title": "Introduction To Polyhedral Methods",
    "section": "Benefits of Diagonal Scheduling",
    "text": "Benefits of Diagonal Scheduling\n\nMaximized Parallelism: All iterations of a diagonal can run concurrently.\nImproved Cache Locality: Access patterns aligned to diagonals enhance data locality.\nScalability: Easily scalable on systems with symmetric multi-processing capabilities.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#definitions",
    "href": "lectures/poly_final.html#definitions",
    "title": "Introduction To Polyhedral Methods",
    "section": "Definitions",
    "text": "Definitions",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#affine-functions-of-loop-indexes",
    "href": "lectures/poly_final.html#affine-functions-of-loop-indexes",
    "title": "Introduction To Polyhedral Methods",
    "section": "Affine Functions of Loop Indexes",
    "text": "Affine Functions of Loop Indexes\nAn affine function \\(f(\\vec{v})\\) is defined as:\n\\[f(\\vec{v}) = M_f \\vec{v} + \\vec{f}_0\\]\nwhere:\n\n\\(\\vec{v}\\) is a \\(d\\)-dimensional vector of loop indices:\n\\[\\vec{v} = \\begin{pmatrix} v_1 \\\\ \\vdots \\\\ v_d \\end{pmatrix}\\]\n\\(M_f\\) is an integer matrix with \\(k\\) rows and \\(d\\) columns:\n\\[M_f \\in \\mathbb{Z}^{k \\times d}\\]\n\\(\\vec{f}_0\\) is a \\(k\\)-dimensional integer translation vector:\n\\[\\vec{f}_0 \\in \\mathbb{Z}^k\\]\n\nNote: All components are integers \\(\\mathbb{Z}\\).",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#affine-functions",
    "href": "lectures/poly_final.html#affine-functions",
    "title": "Introduction To Polyhedral Methods",
    "section": "affine functions",
    "text": "affine functions\nAn affine function is a linear transformation followed by a translation. For example, each loop iteration \\(\\vec{v}\\) (e.g., \\((i, j)\\) in a 2D nested loop) can map iteration indices to dependence locations.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#loop-nest-types",
    "href": "lectures/poly_final.html#loop-nest-types",
    "title": "Introduction To Polyhedral Methods",
    "section": "Loop Nest Types:",
    "text": "Loop Nest Types:\nPerfect Loop Nest:\n\nDefinition: All statements are contained within the body of the innermost loop.\nExample:\n\n  for (int i = 0; i &lt; N; i++) {\n      for (int j = 0; j &lt; N; j++) {\n          a[i][j] = i + j;\n      }\n  }",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#imperfect-loop-nest",
    "href": "lectures/poly_final.html#imperfect-loop-nest",
    "title": "Introduction To Polyhedral Methods",
    "section": "Imperfect Loop Nest",
    "text": "Imperfect Loop Nest\n\nDefinition: There are statements located outside the innermost loop.\nExample:\n\n    for (int i = 0; i &lt; N; i++) {\n        b[i] = 0;\n        for (int j = 0; j &lt; N; j++) {\n            a[i][j] += i * j;\n        }\n    }",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#affine-loop-nest",
    "href": "lectures/poly_final.html#affine-loop-nest",
    "title": "Introduction To Polyhedral Methods",
    "section": "Affine Loop Nest:",
    "text": "Affine Loop Nest:\n\nCharacteristics:\n\nLoop bounds and array accesses are affine functions of outer loop variables and program parameters.\nProgram parameters (e.g., \\(N\\), \\(\\beta\\)) are symbolic constants representing problem size.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#domains-in-affine-loops",
    "href": "lectures/poly_final.html#domains-in-affine-loops",
    "title": "Introduction To Polyhedral Methods",
    "section": "domains in affine loops",
    "text": "domains in affine loops\nif we look at the domain or index set of an affine loop nest we have a convex polyhedron",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#understanding-polyhedrons-and-polytopes",
    "href": "lectures/poly_final.html#understanding-polyhedrons-and-polytopes",
    "title": "Introduction To Polyhedral Methods",
    "section": "Understanding Polyhedrons and Polytopes",
    "text": "Understanding Polyhedrons and Polytopes\n\nPolyhedron: Intersection of a finite number of planes.\nPolytope: A bounded polyhedron.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#polyhedron-representation",
    "href": "lectures/poly_final.html#polyhedron-representation",
    "title": "Introduction To Polyhedral Methods",
    "section": "Polyhedron Representation",
    "text": "Polyhedron Representation\n\nCompact representation of a polyhedron: \\[\n\\{\\vec{x} \\in \\mathbb{Z}^n \\mid A \\vec{x} + \\vec{b} \\geq \\vec{0}\\}\n\\]\n\n\\(A \\in \\mathbb{Z}^{m \\times n}\\)\n\\(\\vec{b} \\in \\mathbb{Z}^m\\)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#iteration-vector",
    "href": "lectures/poly_final.html#iteration-vector",
    "title": "Introduction To Polyhedral Methods",
    "section": "Iteration Vector",
    "text": "Iteration Vector\nThe iteration vector \\(\\vec{i}_S\\) for a statement \\(S\\) is a vector of loop indices from outermost to innermost. It represents a dynamic instance of \\(S\\) in a loop nest.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#domain-or-index-set",
    "href": "lectures/poly_final.html#domain-or-index-set",
    "title": "Introduction To Polyhedral Methods",
    "section": "Domain or Index Set",
    "text": "Domain or Index Set\nThe domain (or index set) of a statement \\(S\\) is the set of all its iteration vectors.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#compiler-notation",
    "href": "lectures/poly_final.html#compiler-notation",
    "title": "Introduction To Polyhedral Methods",
    "section": "compiler notation",
    "text": "compiler notation\nfor (i=0 ; i&lt;N ; i++)\n    for (j=0 ; j&lt;N ; j++) \n          s1: a(i, j) = a(i - 1, j) + 1; \n\njust the domain\n\\[\n\\begin{aligned}\ni & \\geq 0 \\\\\nj & \\geq 0 \\\\\n-i+N-1 & \\geq 0 \\\\\n-j+N-1 & \\geq 0\n\\end{aligned} \\quad \\quad \\mathcal{D}^{S_{1}}:\\left(\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n-1 & 0 & 1 & -1 \\\\\n0 & -1 & 1 & -1\n\\end{array}\\right)\\left(\\begin{array}{c}\ni \\\\\nj \\\\\nN \\\\\n1\n\\end{array}\\right) \\geq 0\n\\]\nWe use a matrix and a vector to store a domain",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#adding-in-the-dependence",
    "href": "lectures/poly_final.html#adding-in-the-dependence",
    "title": "Introduction To Polyhedral Methods",
    "section": "adding in the dependence",
    "text": "adding in the dependence\nwe have (\\(i_\\text{src}\\), \\(j_\\text{src}\\)), (\\(i_\\text{dest}\\), \\(j_\\text{dest})\\)\nboth pairs are in the iteration space and \\(i_{\\text{dest}} = i_{\\text{src}} + 1, \\quad j_{\\text{dest}} = j_{\\text{src}}\\).\n\\[\n\\quad \\quad \\left(\\begin{array}{cccc}\n1 & 0 & 0 & 0 & 0 &0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n-1 & 0 &0&0&  1 & -1 \\\\\n0 & -1 &0& 0& 1 & -1 \\\\\n0 & 0 & 1 & 0 & 0 & 0  \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\\n-0 & 0 &-1&0&  1 & -1 \\\\\n0 & 0 &0& -1 & 1 & -1 \\\\\n-1 & 0 & 1 &0 &0 &0 \\\\\n0 & -1 & 0 & 1 &0 &1\n\\end{array}\\right)\\left(\\begin{array}{c}\ni_\\text{src} \\\\\nj_\\text{src} \\\\\ni_\\text{dest}\\\\\nj_\\text{dest}\\\\\n1\\\\\nN\n\\end{array}\\right) \\geq 0\n\\]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#understanding-dependence-in-matrix-computation",
    "href": "lectures/poly_final.html#understanding-dependence-in-matrix-computation",
    "title": "Introduction To Polyhedral Methods",
    "section": "Understanding Dependence in Matrix Computation",
    "text": "Understanding Dependence in Matrix Computation\n\nOriginal Code\nfor (i = 0; i &lt; N; i++)\n  for (j = 0; j &lt; N; j++) \n     S1: A[i, j] = A[i, j] + u1[i] * v1[j] + u2[i] * v2[j];\n\nfor (k = 0; k &lt; N; k++)\n  for (l = 0; l &lt; N; l++)\n     S2: x[k] = x[k] + beta * A[l, k] * y[l];\n\nData Dependence: Occurs when multiple operations require reading from or writing to the same memory location.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#matrix-locations",
    "href": "lectures/poly_final.html#matrix-locations",
    "title": "Introduction To Polyhedral Methods",
    "section": "matrix locations",
    "text": "matrix locations\n\nMatrix Location Relation: A[i, j] potentially overlaps with A[l, k].\n\nwhen\n\n(0 i &lt; N)\n(0 j &lt; N)\n(0 k &lt; N)\n(0 l &lt; N)\nDependence Conditions:\n\nIf (i - l = 0), then the loop in S1 depends on the loop in S2.\nIf (j - k = 0), there is potential write-after-read or read-after-write dependencies.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#loop-interchange-example",
    "href": "lectures/poly_final.html#loop-interchange-example",
    "title": "Introduction To Polyhedral Methods",
    "section": "Loop Interchange Example",
    "text": "Loop Interchange Example\n\nOriginal Loop Structure\nfor (i = 0; i &lt; 9; i++)\n  for (j = i; j &lt; 7 && j &lt; i + 4; j++)\n     a[i,j] = 3;\n\n\nQuestion\n\nCan we reorder these loops?\nYes, we can reorder because there is no dependence.\n\nChallenge: Determine the new loop bounds after interchange.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#domain",
    "href": "lectures/poly_final.html#domain",
    "title": "Introduction To Polyhedral Methods",
    "section": "domain",
    "text": "domain",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#loop-bounds",
    "href": "lectures/poly_final.html#loop-bounds",
    "title": "Introduction To Polyhedral Methods",
    "section": "loop bounds",
    "text": "loop bounds\noriginal \n\nfor (i = 0, i &lt; 9; i++)\n  for (j = i; j &lt; 7 && j &lt; i+4; j++)\n     a[i,j] =  3\n\nreordered \n\nfor (j = 0; j &lt;=6; j++)\n for (i = max(j-3,0); i &lt;= j; i++)\n   a[i,j] = 3\nhow do we get the reordered bounds?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#fourier-motzkin-method-for-variable-elimination",
    "href": "lectures/poly_final.html#fourier-motzkin-method-for-variable-elimination",
    "title": "Introduction To Polyhedral Methods",
    "section": "Fourier-Motzkin Method for Variable Elimination",
    "text": "Fourier-Motzkin Method for Variable Elimination\n\nPurpose: Project a polyhedron onto a lower-dimensional space by eliminating variables from systems of linear inequalities.\nVariable Elimination: Simplifies a system of linear inequalities by removing variables, reducing the dimensionality of the polyhedron while maintaining its feasibility.\nEffective for systems in higher dimensions.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#steps-of-fourier-motzkin-elimination",
    "href": "lectures/poly_final.html#steps-of-fourier-motzkin-elimination",
    "title": "Introduction To Polyhedral Methods",
    "section": "Steps of Fourier-Motzkin Elimination",
    "text": "Steps of Fourier-Motzkin Elimination\n\nIdentify a Variable to Eliminate:\n\nChoose a variable (e.g., \\(i\\)) from the system to eliminate.\n\nRearrange Each Inequality:\n\nSolve each inequality for the chosen variable.\n\nEliminate the Variable:\n\nCombine pairs of inequalities to remove the chosen variable.\nProduce new inequalities in terms of the remaining variables.\n\nRepeat the Process:\n\nContinue eliminating variables until only the desired ones remain.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#back-to-this-case",
    "href": "lectures/poly_final.html#back-to-this-case",
    "title": "Introduction To Polyhedral Methods",
    "section": "back to this case",
    "text": "back to this case\nfor (i = 0, i &lt; 9; i++)\n  for (j = i; j &lt; 7 && j &lt; i+4; j++)\n     a[i,j] =  3\nin the new version j will be the outer loop, so cannot use i in the bounds",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#original-inequalities",
    "href": "lectures/poly_final.html#original-inequalities",
    "title": "Introduction To Polyhedral Methods",
    "section": "Original Inequalities",
    "text": "Original Inequalities\n\n\\(i \\ge 0\\)\n\\(i \\le 8\\) (since \\(i &lt; 9\\))\n\\(i \\le j\\) (since loop starts at \\(j = i\\))\n\\(j \\le 6\\)\n\\(j \\le i + 3\\)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#strategy-eliminate-i-and-determine-bounds-for-j",
    "href": "lectures/poly_final.html#strategy-eliminate-i-and-determine-bounds-for-j",
    "title": "Introduction To Polyhedral Methods",
    "section": "Strategy: Eliminate i and Determine Bounds for j",
    "text": "Strategy: Eliminate i and Determine Bounds for j\n\nTo determine bounds for \\(j\\), eliminate \\(i\\):\n\nFind all constraints involving \\(i\\): \\(L \\le c_1 \\cdot i\\) and \\(c_2 \\cdot i \\le U\\)\nFor each pair, derive: \\(c_2 \\cdot L \\le c_1 \\cdot U\\) along with constraints on \\(j\\) only\n\n\n\nDerived Constraints\n\nPair (1, 2): \\(0 \\le 8\\) — Ignore\nPair (1, 3): \\(0 \\le j\\)\nPair (2, 5): \\(j - 3 \\le j\\) — Ignore\nPair (3, 5): \\(j - 3 \\le 8\\) ⇒ \\(j \\le 11\\) — Ignore\nFrom inequality (4): \\(j \\le 6\\)\n\nBounds for \\(j\\): \\(0 \\le j \\le 6\\)\n\n\nSolving for i\n\nFor the inner loop:\n\n\\(i \\ge 0\\) and \\(i \\ge j - 3\\) ⇒ \\(i \\ge \\max(0, j-3)\\)\n\\(i \\le 8\\) and \\(i \\le j\\) ⇒ ((8, j) = j), since \\(j \\le 6\\)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#complexity-of-fourier-motzkin",
    "href": "lectures/poly_final.html#complexity-of-fourier-motzkin",
    "title": "Introduction To Polyhedral Methods",
    "section": "Complexity of Fourier-Motzkin",
    "text": "Complexity of Fourier-Motzkin\nFourier-Motzkin elimination is computationally expensive, especially when the number of variables is large.\nWorst-case complexity: The number of inequalities generated can grow exponentially with the number of variables, resulting in inefficient performance for high-dimensional systems. - Despite this, it is still a useful tool for small to medium-dimensional systems where the elimination process can be computationally feasible.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#how-much-of-a-limitation-is-affine-only",
    "href": "lectures/poly_final.html#how-much-of-a-limitation-is-affine-only",
    "title": "Introduction To Polyhedral Methods",
    "section": "How much of a limitation is affine only",
    "text": "How much of a limitation is affine only\n99% of hpc loops are affine: C. Bastoul, A. Cohen, S. Girbal, S. Sharma, and O. Temam. Putting polyhedral loop transformations to work. In LCPC, 2003.\nover 95% of loops in deep learning are affine:\nNorman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, RaminderBajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA). IEEE, 1–12.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#what-about-the-ilp-solver",
    "href": "lectures/poly_final.html#what-about-the-ilp-solver",
    "title": "Introduction To Polyhedral Methods",
    "section": "what about the ilp solver",
    "text": "what about the ilp solver\nilp is np-complete so it is slow, but often tractable for problems with up to several hundred variables.\nSome compiler writers feel that means poly methods can only do this for toy programs? - Is the complexity a barrier for real-world compiler applications?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#affine-functions-1",
    "href": "lectures/poly_final.html#affine-functions-1",
    "title": "Introduction To Polyhedral Methods",
    "section": "Affine Functions",
    "text": "Affine Functions\n\nOnly allow operations: addition, subtraction, multiplication by a constant.\nCannot multiply by unknowns or use quantifiers (e.g., ∀, ∃).\nCan reformulate some problems but requires breaking them into simpler parts.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#multiple-dimensions-and-data-dependencies",
    "href": "lectures/poly_final.html#multiple-dimensions-and-data-dependencies",
    "title": "Introduction To Polyhedral Methods",
    "section": "Multiple Dimensions and Data Dependencies",
    "text": "Multiple Dimensions and Data Dependencies\n// Consider the following nested loops with data dependency in matrix A:\nfor (i = 1; i &lt;= n; i++) {\n    for (j = i; j &lt;= n; j++) {\n        // S: Data dependency on A[i-1][j] and A[i][j-1]\n        A[i][j] = (A[i-1][j] + A[i][j-1]) * 0.5;\n    }\n}\nThe computation of \\(A[i][j]\\) depends on \\(A[i-1][j]\\) and \\(A[i][j-1]\\), creating a pattern of data dependencies.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#graph-with-tiles",
    "href": "lectures/poly_final.html#graph-with-tiles",
    "title": "Introduction To Polyhedral Methods",
    "section": "graph with tiles",
    "text": "graph with tiles\n\n\nData Dependence for A[i][j] = (A[i-1][j] + A[i][j-1]) * 0.5\n[((1, 1), (1, 2)), ((1, 2), (1, 3)), ((1, 3), (1, 4)), ((1, 4), (1, 5)), ((1, 5), (1, 6)), ((1, 6), (1, 7)), ((1, 7), (1, 8)), ((1, 2), (2, 2)), ((2, 1), (2, 2)), ((1, 3), (2, 3)), ((2, 2), (2, 3)), ((1, 4), (2, 4)), ((2, 3), (2, 4)), ((1, 5), (2, 5)), ((2, 4), (2, 5)), ((1, 6), (2, 6)), ((2, 5), (2, 6)), ((1, 7), (2, 7)), ((2, 6), (2, 7)), ((1, 8), (2, 8)), ((2, 7), (2, 8)), ((2, 3), (3, 3)), ((3, 2), (3, 3)), ((2, 4), (3, 4)), ((3, 3), (3, 4)), ((2, 5), (3, 5)), ((3, 4), (3, 5)), ((2, 6), (3, 6)), ((3, 5), (3, 6)), ((2, 7), (3, 7)), ((3, 6), (3, 7)), ((2, 8), (3, 8)), ((3, 7), (3, 8)), ((3, 4), (4, 4)), ((4, 3), (4, 4)), ((3, 5), (4, 5)), ((4, 4), (4, 5)), ((3, 6), (4, 6)), ((4, 5), (4, 6)), ((3, 7), (4, 7)), ((4, 6), (4, 7)), ((3, 8), (4, 8)), ((4, 7), (4, 8)), ((4, 5), (5, 5)), ((5, 4), (5, 5)), ((4, 6), (5, 6)), ((5, 5), (5, 6)), ((4, 7), (5, 7)), ((5, 6), (5, 7)), ((4, 8), (5, 8)), ((5, 7), (5, 8)), ((5, 6), (6, 6)), ((6, 5), (6, 6)), ((5, 7), (6, 7)), ((6, 6), (6, 7)), ((5, 8), (6, 8)), ((6, 7), (6, 8)), ((6, 7), (7, 7)), ((7, 6), (7, 7)), ((6, 8), (7, 8)), ((7, 7), (7, 8)), ((7, 8), (8, 8)), ((8, 7), (8, 8))]\n\n\n\n\n\n\n\n\n\nThe schedule traverses the matrix in an upward direction for each column (j), moving left to right across columns (i).\nadding tiles does not have a problem when dependence, but with-in each tile have to execute serially",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#formalizing-the-schedule-lexicographic-ordering",
    "href": "lectures/poly_final.html#formalizing-the-schedule-lexicographic-ordering",
    "title": "Introduction To Polyhedral Methods",
    "section": "Formalizing the Schedule: Lexicographic Ordering",
    "text": "Formalizing the Schedule: Lexicographic Ordering\n\nSchedule Mapping: \\(s(i,j) \\rightarrow (i,j)\\)\nStatements as Vectors: Considered as ‘times’ (e.g., hours, minutes, seconds)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#understanding-lexicographic-ordering",
    "href": "lectures/poly_final.html#understanding-lexicographic-ordering",
    "title": "Introduction To Polyhedral Methods",
    "section": "Understanding Lexicographic Ordering",
    "text": "Understanding Lexicographic Ordering\n\\[(i,j) \\gg (m,n) \\iff i &gt; m \\lor (i = m \\land j &gt; n)\\]\n\nCompare elements from left to right.\nContinue comparison on equal terms.\nLexicographic order generalizes alphabetical order (→).",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#checking-for-loop-interchange",
    "href": "lectures/poly_final.html#checking-for-loop-interchange",
    "title": "Introduction To Polyhedral Methods",
    "section": "Checking for loop interchange",
    "text": "Checking for loop interchange\nfor i in [1,2,3,4]                     for j in [1,2,3]\n  for j in [1,2,3]                       for i in [1,2,3,4]\ns:   a(i,j) = a(i-1,j+1)                   a(i,j) = a(i-1,j+1)  \n\n\nOriginal Schedule: \\(s(i, j) \\rightarrow (i,j)\\)\nAfter Interchange: \\(s(i,j) \\rightarrow (j,i)\\)\n\ndata flow\n        read write\ns(1,1)  a(0,2)  a(1,1)\ns(1,2)  a(0,3)  a(1,2)\ns(1,3)  a(0,4)  a(1,3)\ns(1,4)  a(0,5)  a(1,4)\ns(2,1)  a(1,2)  a(2,1)   s(1,2)-&gt; s(2,1)\ns(2,2)  a(1,3)  a(2,2)   s(1,3)-&gt; s(2.2)\n\\[\ns(i,j) \\rightarrow s(i+1, j-1)\n\\]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#constants",
    "href": "lectures/poly_final.html#constants",
    "title": "Introduction To Polyhedral Methods",
    "section": "constants:",
    "text": "constants:\nDoes there exist a statement s(i,j) and a statement \\(s(i',j')\\) where in the new schedule \\(s(i',j')\\) executes first and data flows backward in time \\[\n\\begin{align*}\n(i', j') \\gg (j,i)   &\\text{ $i',j'$ is first} \\\\\ni' = 1+ i            &\\text{ data\\  from \\ i+1 to $i'$}\\\\\nj' = -1 +j           &\\text{ data\\  from \\ j-1 to $j'$}\\\\\n1 \\le i \\le 4 \\\\\n1 \\le j \\le 3  \\\\\n1 \\le i' \\le 4 \\\\\n1 \\le j' \\leftrightarrows 3\n\\end{align*}\n\\]\nbecause of the lexicographic order we have two ilp problems one where \\(i'\\) is greater then j, and one where \\(i'\\) = j",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#solution",
    "href": "lectures/poly_final.html#solution",
    "title": "Introduction To Polyhedral Methods",
    "section": "solution",
    "text": "solution\nI ran it through:\nhttps://online-optimizer.appspot.com\nwhich gave me a solution:\ns(4,2) reads s(3,3) but s(4,2) executes first",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#classic-transformations",
    "href": "lectures/poly_final.html#classic-transformations",
    "title": "Introduction To Polyhedral Methods",
    "section": "Classic transformations",
    "text": "Classic transformations",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#loop-reversal",
    "href": "lectures/poly_final.html#loop-reversal",
    "title": "Introduction To Polyhedral Methods",
    "section": "loop reversal",
    "text": "loop reversal\ntransformation: [i] ==&gt; [-i]\nfor (int c0 = 0; c0 &lt; n; c0 += 1)\n  S(c0);\n\nfor (int c0 = -n + 1; c0 &lt;= 0; c0 += 1)\n  S(-c0);",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#loop-fusion",
    "href": "lectures/poly_final.html#loop-fusion",
    "title": "Introduction To Polyhedral Methods",
    "section": "loop fusion",
    "text": "loop fusion\ntransformation: [0, i] -&gt; [i,0]; [1, i] -&gt; [i, 1]\n{\n  for (int c1 = 0; c1 &lt;= n; c1 += 1)\n    S(c1);\n  for (int c1 = 0; c1 &lt;= n; c1 += 1)\n    T(c1);\n}\nfor (int c0 = 0; c0 &lt;= n; c0 += 1) {\n  S(c0);\n  T(c0);\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#loop-fission",
    "href": "lectures/poly_final.html#loop-fission",
    "title": "Introduction To Polyhedral Methods",
    "section": "loop fission",
    "text": "loop fission\ntransformation: [i, 0] -&gt; [0, i]; [i, 1] -&gt; [1, i]\nfor (int c0 = 0; c0 &lt;= n; c0 += 1) {\n  S(c0);\n  T(c0);\n}\n{\n  for (int c1 = 0; c1 &lt;= n; c1 += 1)\n    S(c1);\n  for (int c1 = 0; c1 &lt;= n; c1 += 1)\n    T(c1);\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#strip-mining",
    "href": "lectures/poly_final.html#strip-mining",
    "title": "Introduction To Polyhedral Methods",
    "section": "strip mining",
    "text": "strip mining\ntransformation: [i] -&gt; [floor(i/4), i % 4]\nfor (int c0 = 0; c0 &lt;= 1023; c0 += 1)\n  S(c0);\n\nfor (int c0 = 0; c0 &lt;= 255; c0 += 1)\n  for (int c1 = 0; c1 &lt;= 3; c1 += 1)\n    S(4 * c0 + c1);",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#loop-tiling",
    "href": "lectures/poly_final.html#loop-tiling",
    "title": "Introduction To Polyhedral Methods",
    "section": "loop tiling",
    "text": "loop tiling\ntransformation: [i,j] -&gt; [floor(i/4), i % 4, floor(j/4), j % 4]\nfor (int c0 = 0; c0 &lt;= 1023; c0 += 1)\n  for (int c1 = 0; c1 &lt;= 1023; c1 += 1)\n    S(c0, c1);\n\nfor (int c0 = 0; c0 &lt;= 255; c0 += 1)\n  for (int c1 = 0; c1 &lt;= 3; c1 += 1)\n    for (int c2 = 0; c2 &lt;= 255; c2 += 1)\n      for (int c3 = 0; c3 &lt;= 3; c3 += 1)\n        S(4 * c0 + c1, 4 * c2 + c3);",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#finding-transformations",
    "href": "lectures/poly_final.html#finding-transformations",
    "title": "Introduction To Polyhedral Methods",
    "section": "finding transformations:",
    "text": "finding transformations:\nfor i in [0, 1, 2, 3, 4, 5]:\n    P:  a(i) = input(i) + 1\nfor j in [0, 1, 2, 3, 4, 5]:\n    c:  b(j) = a(j) + 2\nwhat transformation will give the best locality?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#poly-form",
    "href": "lectures/poly_final.html#poly-form",
    "title": "Introduction To Polyhedral Methods",
    "section": "poly form",
    "text": "poly form\nEach loop can be represented by its iteration space in a polyhedral form.\nLoop 1: \\(𝑖 \\in [0,1,2,3,5]\\)\nLoop 2: \\(j \\in [0,1,2,3,4,5]\\)\nThe dependencies between the two loops are: \\(p(i)\\) must be computed before \\(c(i)\\)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#define-a-new-schedule",
    "href": "lectures/poly_final.html#define-a-new-schedule",
    "title": "Introduction To Polyhedral Methods",
    "section": "define a new schedule",
    "text": "define a new schedule\nWe introduce a schedule for each loop using affine transformations. (we can only handle affine transformations )\nFor the first loop: \\[\\theta_1(i) = \\alpha_1 i + \\beta_1\\]\nFor the second loop: \\[\\theta_2(j) = \\alpha_2 j + \\beta_2\\]\nThe goal is to reorder the iterations such that all dependencies are respected and memory locality is improved.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#solving",
    "href": "lectures/poly_final.html#solving",
    "title": "Introduction To Polyhedral Methods",
    "section": "solving",
    "text": "solving\nWe have to find four scalars\nFor all \\(i, j\\) such that \\(p(i)\\) sends data to \\(c(j)\\), the following constraint holds:\n\\[\\text{if } p(i) \\to c(j), \\quad \\text{then } \\theta_1(i) \\leq  \\theta_2(j).\\]\nFor all \\(0 \\leq i \\leq 5\\) and \\(0 \\leq j \\leq 5\\), and \\(i=j\\) the following condition holds: \\[\\theta_1(i) \\leq theta_2(j)\\]\nexpanding this out\n\\[\\alpha_1 i + \\beta_1 \\leq \\alpha_2 j + \\beta_2\\]\nlooks bad- non-linear constraint since there is a product, and the foralls do not work with solvers",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#magic",
    "href": "lectures/poly_final.html#magic",
    "title": "Introduction To Polyhedral Methods",
    "section": "magic",
    "text": "magic\nWe apply a theorem called the affine form of the Farkas lemma which turns this into a tractable problem\nAn affine form is non-negative over a polyhedron if and only if it can be be written as a non-negative combination of the constraints that form the polyhedron\nforall \\(0 \\le i \\le 5\\) and \\(0 \\le j \\le 5\\) and \\(i = j\\)\nsuch that \\(-\\alpha_1 * i + \\alpha_2  * j + (\\beta_1 - \\beta_2) \\ge 0\\) affine form\npolyhedron\n\n\\(i \\ge 0\\)\n\\(-i \\ge -5\\)\n\\(j \\ge 0\\)\n\\(-j \\ge -5\\)\n\\(j - i \\ge 0\\)\n\\(i=j \\ge 0\\)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#optimization-problem",
    "href": "lectures/poly_final.html#optimization-problem",
    "title": "Introduction To Polyhedral Methods",
    "section": "optimization problem",
    "text": "optimization problem\nfind a function that gives the locality\nminimize w: \\(w \\ge \\alpha_2 * i + \\beta_2 - \\alpha_1 * j - \\beta_1\\)\nw is a bound in time between producer and consumer, how long the location needs to stay in the local memory",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#parallelism",
    "href": "lectures/poly_final.html#parallelism",
    "title": "Introduction To Polyhedral Methods",
    "section": "parallelism",
    "text": "parallelism\nwe minimized the time between definition and use, for best locality, if we maximize that time we get the most parallel code",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/poly_final.html#objectives",
    "href": "lectures/poly_final.html#objectives",
    "title": "Introduction To Polyhedral Methods",
    "section": "objectives",
    "text": "objectives\nSince everything is affine it is hard to formulate a complex cost function",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Introduction To Polyhedral Methods"
    ]
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#what-are-polyhedral-methods",
    "href": "lectures/revealjs_poly_final.qmd.html#what-are-polyhedral-methods",
    "title": "Introduction To Polyhedral Methods",
    "section": "What Are Polyhedral Methods",
    "text": "What Are Polyhedral Methods\n\nMany modern programs in fields like High-Performance Computing (HPC), GPU programming, scientific applications and machine learning are loop-intensive.\nTraditional compiler techniques encounter several challenges in these contexts:"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#challenges",
    "href": "lectures/revealjs_poly_final.qmd.html#challenges",
    "title": "Introduction To Polyhedral Methods",
    "section": "challenges",
    "text": "challenges\n\nComplex Loop Dependencies:\n\nDependencies can exist both within a loop and across multiple loops, limiting optimization potential.\n\nMissed Parallelism:\n\nInsufficient analysis may prevent identification of potential parallel execution paths, underutilizing hardware capabilities.\n\nPoor Data Locality:\n\nInefficient usage of cache memory leads to performance bottlenecks.\n\n\nPolyhedral methods address these challenges by providing a precise mathematical framework for analyzing and optimizing loop nests."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#challenges-in-traditional-loop-optimization",
    "href": "lectures/revealjs_poly_final.qmd.html#challenges-in-traditional-loop-optimization",
    "title": "Introduction To Polyhedral Methods",
    "section": "Challenges in Traditional Loop Optimization",
    "text": "Challenges in Traditional Loop Optimization\nfor (i = 0; i &lt; N; i++)  \n  for (j = 0; j &lt; N; j++)  \n    A[i][j] = A[i-1][j] + A[i][j-1];\n\nIteration Dependencies\n\n\\(A[i][j]\\) depends on \\(A[i-1][j]\\) and \\(A[i][j-1]\\)\nLimits parallel execution due to data dependencies.\n\nPerformance Bottleneck\n\nWithout optimization such as tiling, loop execution suffers from cache misses."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#improving-loop-performance-by-tiling",
    "href": "lectures/revealjs_poly_final.qmd.html#improving-loop-performance-by-tiling",
    "title": "Introduction To Polyhedral Methods",
    "section": "Improving Loop Performance by tiling",
    "text": "Improving Loop Performance by tiling\nBenefits of Tiling\n\nImproved Cache Access Patterns\n\nReduces cache misses by increasing data locality.\n\nEnhanced Parallelism\n\nBreaking dependencies across tiles can allow independent execution of tiles on different processors."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#example-tiling-transformation",
    "href": "lectures/revealjs_poly_final.qmd.html#example-tiling-transformation",
    "title": "Introduction To Polyhedral Methods",
    "section": "Example Tiling Transformation",
    "text": "Example Tiling Transformation\nTransform the inner loops by introducing tile sizes \\(T_i\\) and \\(T_j\\):\nfor (ii = 0; ii &lt; N; ii += T_i)\n  for (jj = 0; jj &lt; N; jj += T_j)\n    for (i = ii; i &lt; min(ii + T_i, N); i++)\n      for (j = jj; j &lt; min(jj + T_j, N); j++)\n        A[i][j] = A[i-1][j] + A[i][j-1];"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#limitations-of-polyhedral-methods",
    "href": "lectures/revealjs_poly_final.qmd.html#limitations-of-polyhedral-methods",
    "title": "Introduction To Polyhedral Methods",
    "section": "Limitations of Polyhedral Methods",
    "text": "Limitations of Polyhedral Methods\n\nAffine Loops:\n\nOnly applicable to loops with linear bounds and affine array accesses.\nExample: Bounds and accesses must be simple linear expressions of loop indices.\nStatic Control Flow:\nMethods require loops without complex control flow such as recursion or unpredictable branches (virtual functions)."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#scops",
    "href": "lectures/revealjs_poly_final.qmd.html#scops",
    "title": "Introduction To Polyhedral Methods",
    "section": "scops",
    "text": "scops\nRegions of code that can be handled in the polyhedral model are usually called Static Control Parts, abbreviated as SCoPs.\nUsually, SCoPs may only contain regular control flow free of exceptions and other constructs that may provoke changes in control flow such as conditional expressions dependent on data (read from memory) or side effects of function calls.\nloop free code can be a wrapped in for(i = 0; i &lt; 1; i++){}"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#two-kinds-of-problems",
    "href": "lectures/revealjs_poly_final.qmd.html#two-kinds-of-problems",
    "title": "Introduction To Polyhedral Methods",
    "section": "two kinds of problems",
    "text": "two kinds of problems\nPolyhedral Analysis\nGiven a transformation. - Does a transformation preserve the loop’s semantics? - What is the resulting loop structure after transformation?\nPolyhedral Scheduling\nFind the “best” transformation that optimizes a property such as parallelism or data locality."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#advantages-of-polyhedral-methods",
    "href": "lectures/revealjs_poly_final.qmd.html#advantages-of-polyhedral-methods",
    "title": "Introduction To Polyhedral Methods",
    "section": "Advantages of Polyhedral Methods",
    "text": "Advantages of Polyhedral Methods\nMathematical Rigor\n\nAffine Constraints:\n\nPrecisely model and manipulate loops, ensuring correctness.\n\n\nAdvanced Optimizations Enabled\n\nLoop Tiling:\n\nBoosts data locality by breaking loops into smaller blocks.\n\nLoop Fusion:\n\nMinimizes overhead and improves cache performance by combining adjacent loops.\n\nSkewing and Parallelization:\n\nAddresses complex dependencies, allowing effective parallel execution."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#loop-execution",
    "href": "lectures/revealjs_poly_final.qmd.html#loop-execution",
    "title": "Introduction To Polyhedral Methods",
    "section": "Loop Execution",
    "text": "Loop Execution\nStatements and Instances\n\nIn loops, statements can execute multiple times.\nEach execution of a statement is called an instance.\nPolyhedral methods track these instances explicitly.\n\nfor i in [1, 2, 3, 4]:\n  s: a[i] = 3  // Statement `s` executes four times (4 instances)"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#program-representation",
    "href": "lectures/revealjs_poly_final.qmd.html#program-representation",
    "title": "Introduction To Polyhedral Methods",
    "section": "Program Representation",
    "text": "Program Representation\n\nStatements and Index Sets\n\nDefine what computations to perform.\nSpecify the indices for which these computations are valid.\n\nSchedule\n\nspecifies the order in which the indices are executed."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#toy-example-changing-the-order-of-iterations",
    "href": "lectures/revealjs_poly_final.qmd.html#toy-example-changing-the-order-of-iterations",
    "title": "Introduction To Polyhedral Methods",
    "section": "Toy Example: Changing the Order of Iterations",
    "text": "Toy Example: Changing the Order of Iterations\nProblem: Can we reverse this loop’s execution order?\nfor i = [1,2,3,4]\ns:   a[i] = a[i-1]\nOriginal schedule: i = 1, 2, 3, 4\nReversed schedule: i = 4, 3, 2, 1\nDo these loops produce the same result?\nfor i = [1,2,3,4]               for i = [4,3,2,1]\ns:   a[i] = a[i-1]               s:   a[i] = a[i-1]"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#execution-trace-and-data-access",
    "href": "lectures/revealjs_poly_final.qmd.html#execution-trace-and-data-access",
    "title": "Introduction To Polyhedral Methods",
    "section": "Execution Trace and Data Access",
    "text": "Execution Trace and Data Access\n\n\n\n\n\n\n\nOriginal Order\nReversed Order\n\n\n\n\nInstance s(1): Reads a[0], Writes a[1]\nInstance s(4): Reads a[3], Writes a[4]\n\n\nInstance s(2): Reads a[1], Writes a[2]\nInstance s(3): Reads a[2], Writes a[3]\n\n\nInstance s(3): Reads a[2], Writes a[3]\nInstance s(2): Reads a[1], Writes a[2]\n\n\nInstance s(4): Reads a[3], Writes a[4]\nInstance s(1): Reads a[0], Writes a[1]"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#data-dependency",
    "href": "lectures/revealjs_poly_final.qmd.html#data-dependency",
    "title": "Introduction To Polyhedral Methods",
    "section": "data dependency",
    "text": "data dependency\nThe data dependency arises from the use of \\(𝑎[𝑖−1]\\) which has to be computed before \\(a[i]\\)\nThis kind of data dependency is called read after write or raw dependency\ntransformation\nwe propose a transformation \\(i \\rightarrow 5 -i\\) Does this transformation preserve all the dependencies?\na transformation is a change to the schedule"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#schedules",
    "href": "lectures/revealjs_poly_final.qmd.html#schedules",
    "title": "Introduction To Polyhedral Methods",
    "section": "Schedules:",
    "text": "Schedules:\nA schedule is a function:\n\\[s(\\text{iteration}) = \\text{execution order}(\\text{time})\\]\nExample:\nOriginal order: s(i) = i\nReversed order: s(i) = 5 - i\nProducers must execute before consumers for the transformation to be valid."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#schedule-validity",
    "href": "lectures/revealjs_poly_final.qmd.html#schedule-validity",
    "title": "Introduction To Polyhedral Methods",
    "section": "schedule validity",
    "text": "schedule validity\nnot valid: if there is a pair \\(i\\),\\(j\\), \\(s[i]\\) produces a value, \\(s[j]\\) reads that value, and \\(s[j]\\) is first in the new schedule\nHow do we find such a pair?"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#visualization",
    "href": "lectures/revealjs_poly_final.qmd.html#visualization",
    "title": "Introduction To Polyhedral Methods",
    "section": "Visualization",
    "text": "Visualization\n\nGraphical Representation:\n\nNodes represent statements with iteration index.\nDirected edges indicate data dependencies.\nValid schedules respect edge directions."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#forward-order",
    "href": "lectures/revealjs_poly_final.qmd.html#forward-order",
    "title": "Introduction To Polyhedral Methods",
    "section": "forward order",
    "text": "forward order\n\n\nCode\niteration_space = [1,2,3,4]\ndependences = [(1,2), (2,3), (3,4)]\nschedule = [1,2,3,4]\ndraw_1d_iteration_space_with_dependence_graph(iteration_space, dependences, schedule)\n\n\nSchedule:  [1, 2, 3, 4]"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#backward-order",
    "href": "lectures/revealjs_poly_final.qmd.html#backward-order",
    "title": "Introduction To Polyhedral Methods",
    "section": "backward order",
    "text": "backward order\n\n\nCode\niteration_space = [1,2,3,4]\ndependences = [(1,2), (2,3), (3,4)]\nschedule= [4,3,2,1]\ndraw_1d_iteration_space_with_dependence_graph(iteration_space, dependences, schedule)\n\n\nSchedule:  [4, 3, 2, 1]\n\n\n\n\n\n\n\n\n\na schedule is not valid if the arrows go in different directions"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#more-analytical-check-for-valid",
    "href": "lectures/revealjs_poly_final.qmd.html#more-analytical-check-for-valid",
    "title": "Introduction To Polyhedral Methods",
    "section": "more analytical check for valid",
    "text": "more analytical check for valid\nThere are a number of tests for validity.\nMathematical Condition\n\nFind \\(i, j\\) where: \\[\\text{s}(i) &lt; \\text{s}(j) : i \\text{ executes first}\\]\n\n\\(i\\) reads a value.\n\\(j\\) writes that value.\n\n\nOne: an integer linear programming solver (ILP) to find an integer solution to the equations.\nNP-hard but efficient solvers exist for many of the small cases found in programs\nTwo: check for a cycle in the graph"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#analyzing-parallelism-in-nested-loops",
    "href": "lectures/revealjs_poly_final.qmd.html#analyzing-parallelism-in-nested-loops",
    "title": "Introduction To Polyhedral Methods",
    "section": "Analyzing Parallelism in Nested Loops",
    "text": "Analyzing Parallelism in Nested Loops\nCode Example\nfor i in [1, 2, 3, 4]:\n    for j in [1, 2, 3, 4]:\n        a[i, j] = a[i, j-1] + a[i-1, j]\nCan this Loop Nest be Executed in Parallel?\ndata dependence is a mapping:\n\\(s(i) =&gt; s(i+1)\\) from instance to instance"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#horizontal-dependency",
    "href": "lectures/revealjs_poly_final.qmd.html#horizontal-dependency",
    "title": "Introduction To Polyhedral Methods",
    "section": "Horizontal Dependency:",
    "text": "Horizontal Dependency:\nEach iteration (i, j) depends on (i, j-1) (the value to the left in the same row).\n1. For example, a[1, 2] = a[1, 1] + a[0, 2], which means a[1, 2] depends on the value of a[1, 1] from the same row. \n1. (1,1) -&gt; (1,2)"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#vertical-dependency",
    "href": "lectures/revealjs_poly_final.qmd.html#vertical-dependency",
    "title": "Introduction To Polyhedral Methods",
    "section": "Vertical Dependency:",
    "text": "Vertical Dependency:\nEach iteration (i, j) depends on (i-1, j) (the value from the previous row in the same column).\n1. For example, a[2, 1] = a[1, 1] + a[2, 0], meaning a[2, 1] depends on a[1, 1] from the row above\n1. (1,1) -&gt; (2,1)"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#data-dependencies",
    "href": "lectures/revealjs_poly_final.qmd.html#data-dependencies",
    "title": "Introduction To Polyhedral Methods",
    "section": "Data Dependencies",
    "text": "Data Dependencies\n\nTrue Dependency (Flow Dependency): When an instruction depends on the result of a previous instruction.\n\nExample: A = B + C followed by D = A + E\n\nAnti Dependency: Occurs when an instruction requires a variable that will be overwritten by a subsequent instruction.\nOutput Dependency: When two instructions write to the same location, creating a dependency on the order of execution."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#serial-schedule",
    "href": "lectures/revealjs_poly_final.qmd.html#serial-schedule",
    "title": "Introduction To Polyhedral Methods",
    "section": "serial schedule",
    "text": "serial schedule\nIn a serial execution, iterations proceed row by row, strictly respecting both horizontal and vertical dependencies. This schedule respects all dependencies but executes sequentially, limiting parallelism."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#visualization-1",
    "href": "lectures/revealjs_poly_final.qmd.html#visualization-1",
    "title": "Introduction To Polyhedral Methods",
    "section": "visualization",
    "text": "visualization\n\n\nCode\niteration_space = [(1,1),(1,2),(1,3),(1,4),\n                    (2,1),(2,2),(2,3),(2,4),\n                    (3,1),(3,2),(3,3),(3,4),\n                    (4,1),(4,2),(4,3),(4,4)]\ndependences = []\nfor i in range(1, 5):\n    for j in range(1, 5):\n        if j &gt; 1:  # Horizontal dependency\n            dependences.append(( (i, j-1),(i, j)))\n        if i &gt; 1:  # Vertical dependency\n            dependences.append(((i-1, j),(i, j) ))\n\nschedule = [(i, j) for i in range(1, 5) for j in range(1, 5)]\ndraw_2d_iteration_space_with_dependence_graph(iteration_space, dependences, schedule)\n\n\nSchedule:  [(1, 1), (1, 2), (1, 3), (1, 4), (2, 1), (2, 2), (2, 3), (2, 4), (3, 1), (3, 2), (3, 3), (3, 4), (4, 1), (4, 2), (4, 3), (4, 4)]"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#skewing-the-loops-parallel-schedule-diagonal-parallelism",
    "href": "lectures/revealjs_poly_final.qmd.html#skewing-the-loops-parallel-schedule-diagonal-parallelism",
    "title": "Introduction To Polyhedral Methods",
    "section": "skewing the loops Parallel Schedule: Diagonal Parallelism",
    "text": "skewing the loops Parallel Schedule: Diagonal Parallelism\nTo maximize parallel execution in matrix computations, it’s crucial to identify and execute independent iterations simultaneously. One efficient strategy is Diagonal Parallelism:\n\nDiagonal Organization: Process iterations by diagonals in the iteration space.\nDiagonal Key: Defined as i + j (sum of indices).\nParallel Execution: Iterations with the same diagonal key (i + j constant) can be executed in parallel since they are independent."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#example-diagonal-scheduling-code",
    "href": "lectures/revealjs_poly_final.qmd.html#example-diagonal-scheduling-code",
    "title": "Introduction To Polyhedral Methods",
    "section": "Example: Diagonal Scheduling Code",
    "text": "Example: Diagonal Scheduling Code\n\n\nCode\n# Organize the iteration space by diagonals\ndiagonals = {}\nfor (i, j) in iteration_space:\n    diag_key = i + j  # Diagonal key: sum of indices\n    if diag_key not in diagonals:\n        diagonals[diag_key] = []\n    diagonals[diag_key].append((i, j))\n\n# Sort the diagonals by diagonal key\nsorted_diagonals = sorted(diagonals.keys())\n\n# Print the diagonal schedule\nprint(\"Diagonal Schedule:\")\nprint(diagonals)\n\n\nDiagonal Schedule:\n{2: [(1, 1)], 3: [(1, 2), (2, 1)], 4: [(1, 3), (2, 2), (3, 1)], 5: [(1, 4), (2, 3), (3, 2), (4, 1)], 6: [(2, 4), (3, 3), (4, 2)], 7: [(3, 4), (4, 3)], 8: [(4, 4)]}"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#visualizing-the-parallel-schedule",
    "href": "lectures/revealjs_poly_final.qmd.html#visualizing-the-parallel-schedule",
    "title": "Introduction To Polyhedral Methods",
    "section": "Visualizing the parallel schedule:",
    "text": "Visualizing the parallel schedule:\n\n\nSchedule:  [(1, 1), (1, 2), (2, 1), (1, 3), (2, 2), (3, 1), (1, 4), (2, 3), (3, 2), (4, 1), (2, 4), (3, 3), (4, 2), (3, 4), (4, 3), (4, 4)]\n\n\n\n\n\n\n\n\n\neach diagonal line can run in parallel"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#benefits-of-diagonal-scheduling",
    "href": "lectures/revealjs_poly_final.qmd.html#benefits-of-diagonal-scheduling",
    "title": "Introduction To Polyhedral Methods",
    "section": "Benefits of Diagonal Scheduling",
    "text": "Benefits of Diagonal Scheduling\n\nMaximized Parallelism: All iterations of a diagonal can run concurrently.\nImproved Cache Locality: Access patterns aligned to diagonals enhance data locality.\nScalability: Easily scalable on systems with symmetric multi-processing capabilities."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#definitions",
    "href": "lectures/revealjs_poly_final.qmd.html#definitions",
    "title": "Introduction To Polyhedral Methods",
    "section": "Definitions",
    "text": "Definitions"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#affine-functions-of-loop-indexes",
    "href": "lectures/revealjs_poly_final.qmd.html#affine-functions-of-loop-indexes",
    "title": "Introduction To Polyhedral Methods",
    "section": "Affine Functions of Loop Indexes",
    "text": "Affine Functions of Loop Indexes\nAn affine function \\(f(\\vec{v})\\) is defined as:\n\\[f(\\vec{v}) = M_f \\vec{v} + \\vec{f}_0\\]\nwhere:\n\n\\(\\vec{v}\\) is a \\(d\\)-dimensional vector of loop indices:\n\\[\\vec{v} = \\begin{pmatrix} v_1 \\\\ \\vdots \\\\ v_d \\end{pmatrix}\\]\n\\(M_f\\) is an integer matrix with \\(k\\) rows and \\(d\\) columns:\n\\[M_f \\in \\mathbb{Z}^{k \\times d}\\]\n\\(\\vec{f}_0\\) is a \\(k\\)-dimensional integer translation vector:\n\\[\\vec{f}_0 \\in \\mathbb{Z}^k\\]\n\nNote: All components are integers \\(\\mathbb{Z}\\)."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#affine-functions",
    "href": "lectures/revealjs_poly_final.qmd.html#affine-functions",
    "title": "Introduction To Polyhedral Methods",
    "section": "affine functions",
    "text": "affine functions\nAn affine function is a linear transformation followed by a translation. For example, each loop iteration \\(\\vec{v}\\) (e.g., \\((i, j)\\) in a 2D nested loop) can map iteration indices to dependence locations."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#loop-nest-types",
    "href": "lectures/revealjs_poly_final.qmd.html#loop-nest-types",
    "title": "Introduction To Polyhedral Methods",
    "section": "Loop Nest Types:",
    "text": "Loop Nest Types:\nPerfect Loop Nest:\n\nDefinition: All statements are contained within the body of the innermost loop.\nExample:\n\n  for (int i = 0; i &lt; N; i++) {\n      for (int j = 0; j &lt; N; j++) {\n          a[i][j] = i + j;\n      }\n  }"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#imperfect-loop-nest",
    "href": "lectures/revealjs_poly_final.qmd.html#imperfect-loop-nest",
    "title": "Introduction To Polyhedral Methods",
    "section": "Imperfect Loop Nest",
    "text": "Imperfect Loop Nest\n\nDefinition: There are statements located outside the innermost loop.\nExample:\n\n    for (int i = 0; i &lt; N; i++) {\n        b[i] = 0;\n        for (int j = 0; j &lt; N; j++) {\n            a[i][j] += i * j;\n        }\n    }"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#affine-loop-nest",
    "href": "lectures/revealjs_poly_final.qmd.html#affine-loop-nest",
    "title": "Introduction To Polyhedral Methods",
    "section": "Affine Loop Nest:",
    "text": "Affine Loop Nest:\n\nCharacteristics:\n\nLoop bounds and array accesses are affine functions of outer loop variables and program parameters.\nProgram parameters (e.g., \\(N\\), \\(\\beta\\)) are symbolic constants representing problem size."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#domains-in-affine-loops",
    "href": "lectures/revealjs_poly_final.qmd.html#domains-in-affine-loops",
    "title": "Introduction To Polyhedral Methods",
    "section": "domains in affine loops",
    "text": "domains in affine loops\nif we look at the domain or index set of an affine loop nest we have a convex polyhedron"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#understanding-polyhedrons-and-polytopes",
    "href": "lectures/revealjs_poly_final.qmd.html#understanding-polyhedrons-and-polytopes",
    "title": "Introduction To Polyhedral Methods",
    "section": "Understanding Polyhedrons and Polytopes",
    "text": "Understanding Polyhedrons and Polytopes\n\nPolyhedron: Intersection of a finite number of planes.\nPolytope: A bounded polyhedron."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#polyhedron-representation",
    "href": "lectures/revealjs_poly_final.qmd.html#polyhedron-representation",
    "title": "Introduction To Polyhedral Methods",
    "section": "Polyhedron Representation",
    "text": "Polyhedron Representation\n\nCompact representation of a polyhedron: \\[\n\\{\\vec{x} \\in \\mathbb{Z}^n \\mid A \\vec{x} + \\vec{b} \\geq \\vec{0}\\}\n\\]\n\n\\(A \\in \\mathbb{Z}^{m \\times n}\\)\n\\(\\vec{b} \\in \\mathbb{Z}^m\\)"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#iteration-vector",
    "href": "lectures/revealjs_poly_final.qmd.html#iteration-vector",
    "title": "Introduction To Polyhedral Methods",
    "section": "Iteration Vector",
    "text": "Iteration Vector\nThe iteration vector \\(\\vec{i}_S\\) for a statement \\(S\\) is a vector of loop indices from outermost to innermost. It represents a dynamic instance of \\(S\\) in a loop nest."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#domain-or-index-set",
    "href": "lectures/revealjs_poly_final.qmd.html#domain-or-index-set",
    "title": "Introduction To Polyhedral Methods",
    "section": "Domain or Index Set",
    "text": "Domain or Index Set\nThe domain (or index set) of a statement \\(S\\) is the set of all its iteration vectors."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#compiler-notation",
    "href": "lectures/revealjs_poly_final.qmd.html#compiler-notation",
    "title": "Introduction To Polyhedral Methods",
    "section": "compiler notation",
    "text": "compiler notation\nfor (i=0 ; i&lt;N ; i++)\n    for (j=0 ; j&lt;N ; j++) \n          s1: a(i, j) = a(i - 1, j) + 1; \n\njust the domain\n\\[\n\\begin{aligned}\ni & \\geq 0 \\\\\nj & \\geq 0 \\\\\n-i+N-1 & \\geq 0 \\\\\n-j+N-1 & \\geq 0\n\\end{aligned} \\quad \\quad \\mathcal{D}^{S_{1}}:\\left(\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n-1 & 0 & 1 & -1 \\\\\n0 & -1 & 1 & -1\n\\end{array}\\right)\\left(\\begin{array}{c}\ni \\\\\nj \\\\\nN \\\\\n1\n\\end{array}\\right) \\geq 0\n\\]\nWe use a matrix and a vector to store a domain"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#adding-in-the-dependence",
    "href": "lectures/revealjs_poly_final.qmd.html#adding-in-the-dependence",
    "title": "Introduction To Polyhedral Methods",
    "section": "adding in the dependence",
    "text": "adding in the dependence\nwe have (\\(i_\\text{src}\\), \\(j_\\text{src}\\)), (\\(i_\\text{dest}\\), \\(j_\\text{dest})\\)\nboth pairs are in the iteration space and \\(i_{\\text{dest}} = i_{\\text{src}} + 1, \\quad j_{\\text{dest}} = j_{\\text{src}}\\).\n\\[\n\\quad \\quad \\left(\\begin{array}{cccc}\n1 & 0 & 0 & 0 & 0 &0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n-1 & 0 &0&0&  1 & -1 \\\\\n0 & -1 &0& 0& 1 & -1 \\\\\n0 & 0 & 1 & 0 & 0 & 0  \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\\n-0 & 0 &-1&0&  1 & -1 \\\\\n0 & 0 &0& -1 & 1 & -1 \\\\\n-1 & 0 & 1 &0 &0 &0 \\\\\n0 & -1 & 0 & 1 &0 &1\n\\end{array}\\right)\\left(\\begin{array}{c}\ni_\\text{src} \\\\\nj_\\text{src} \\\\\ni_\\text{dest}\\\\\nj_\\text{dest}\\\\\n1\\\\\nN\n\\end{array}\\right) \\geq 0\n\\]"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#understanding-dependence-in-matrix-computation",
    "href": "lectures/revealjs_poly_final.qmd.html#understanding-dependence-in-matrix-computation",
    "title": "Introduction To Polyhedral Methods",
    "section": "Understanding Dependence in Matrix Computation",
    "text": "Understanding Dependence in Matrix Computation\nOriginal Code\nfor (i = 0; i &lt; N; i++)\n  for (j = 0; j &lt; N; j++) \n     S1: A[i, j] = A[i, j] + u1[i] * v1[j] + u2[i] * v2[j];\n\nfor (k = 0; k &lt; N; k++)\n  for (l = 0; l &lt; N; l++)\n     S2: x[k] = x[k] + beta * A[l, k] * y[l];\n\nData Dependence: Occurs when multiple operations require reading from or writing to the same memory location."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#matrix-locations",
    "href": "lectures/revealjs_poly_final.qmd.html#matrix-locations",
    "title": "Introduction To Polyhedral Methods",
    "section": "matrix locations",
    "text": "matrix locations\n\nMatrix Location Relation: A[i, j] potentially overlaps with A[l, k].\n\nwhen\n\n(0 i &lt; N)\n(0 j &lt; N)\n(0 k &lt; N)\n(0 l &lt; N)\nDependence Conditions:\n\nIf (i - l = 0), then the loop in S1 depends on the loop in S2.\nIf (j - k = 0), there is potential write-after-read or read-after-write dependencies."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#loop-interchange-example",
    "href": "lectures/revealjs_poly_final.qmd.html#loop-interchange-example",
    "title": "Introduction To Polyhedral Methods",
    "section": "Loop Interchange Example",
    "text": "Loop Interchange Example\nOriginal Loop Structure\nfor (i = 0; i &lt; 9; i++)\n  for (j = i; j &lt; 7 && j &lt; i + 4; j++)\n     a[i,j] = 3;\nQuestion\n\nCan we reorder these loops?\nYes, we can reorder because there is no dependence.\n\nChallenge: Determine the new loop bounds after interchange."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#domain",
    "href": "lectures/revealjs_poly_final.qmd.html#domain",
    "title": "Introduction To Polyhedral Methods",
    "section": "domain",
    "text": "domain"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#loop-bounds",
    "href": "lectures/revealjs_poly_final.qmd.html#loop-bounds",
    "title": "Introduction To Polyhedral Methods",
    "section": "loop bounds",
    "text": "loop bounds\noriginal \n\nfor (i = 0, i &lt; 9; i++)\n  for (j = i; j &lt; 7 && j &lt; i+4; j++)\n     a[i,j] =  3\n\nreordered \n\nfor (j = 0; j &lt;=6; j++)\n for (i = max(j-3,0); i &lt;= j; i++)\n   a[i,j] = 3\nhow do we get the reordered bounds?"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#fourier-motzkin-method-for-variable-elimination",
    "href": "lectures/revealjs_poly_final.qmd.html#fourier-motzkin-method-for-variable-elimination",
    "title": "Introduction To Polyhedral Methods",
    "section": "Fourier-Motzkin Method for Variable Elimination",
    "text": "Fourier-Motzkin Method for Variable Elimination\n\nPurpose: Project a polyhedron onto a lower-dimensional space by eliminating variables from systems of linear inequalities.\nVariable Elimination: Simplifies a system of linear inequalities by removing variables, reducing the dimensionality of the polyhedron while maintaining its feasibility.\nEffective for systems in higher dimensions."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#steps-of-fourier-motzkin-elimination",
    "href": "lectures/revealjs_poly_final.qmd.html#steps-of-fourier-motzkin-elimination",
    "title": "Introduction To Polyhedral Methods",
    "section": "Steps of Fourier-Motzkin Elimination",
    "text": "Steps of Fourier-Motzkin Elimination\n\nIdentify a Variable to Eliminate:\n\nChoose a variable (e.g., \\(i\\)) from the system to eliminate.\n\nRearrange Each Inequality:\n\nSolve each inequality for the chosen variable.\n\nEliminate the Variable:\n\nCombine pairs of inequalities to remove the chosen variable.\nProduce new inequalities in terms of the remaining variables.\n\nRepeat the Process:\n\nContinue eliminating variables until only the desired ones remain."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#back-to-this-case",
    "href": "lectures/revealjs_poly_final.qmd.html#back-to-this-case",
    "title": "Introduction To Polyhedral Methods",
    "section": "back to this case",
    "text": "back to this case\nfor (i = 0, i &lt; 9; i++)\n  for (j = i; j &lt; 7 && j &lt; i+4; j++)\n     a[i,j] =  3\nin the new version j will be the outer loop, so cannot use i in the bounds"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#original-inequalities",
    "href": "lectures/revealjs_poly_final.qmd.html#original-inequalities",
    "title": "Introduction To Polyhedral Methods",
    "section": "Original Inequalities",
    "text": "Original Inequalities\n\n\\(i \\ge 0\\)\n\\(i \\le 8\\) (since \\(i &lt; 9\\))\n\\(i \\le j\\) (since loop starts at \\(j = i\\))\n\\(j \\le 6\\)\n\\(j \\le i + 3\\)"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#strategy-eliminate-i-and-determine-bounds-for-j",
    "href": "lectures/revealjs_poly_final.qmd.html#strategy-eliminate-i-and-determine-bounds-for-j",
    "title": "Introduction To Polyhedral Methods",
    "section": "Strategy: Eliminate i and Determine Bounds for j",
    "text": "Strategy: Eliminate i and Determine Bounds for j\n\nTo determine bounds for \\(j\\), eliminate \\(i\\):\n\nFind all constraints involving \\(i\\): \\(L \\le c_1 \\cdot i\\) and \\(c_2 \\cdot i \\le U\\)\nFor each pair, derive: \\(c_2 \\cdot L \\le c_1 \\cdot U\\) along with constraints on \\(j\\) only\n\n\nDerived Constraints\n\nPair (1, 2): \\(0 \\le 8\\) — Ignore\nPair (1, 3): \\(0 \\le j\\)\nPair (2, 5): \\(j - 3 \\le j\\) — Ignore\nPair (3, 5): \\(j - 3 \\le 8\\) ⇒ \\(j \\le 11\\) — Ignore\nFrom inequality (4): \\(j \\le 6\\)\n\nBounds for \\(j\\): \\(0 \\le j \\le 6\\)\nSolving for i\n\nFor the inner loop:\n\n\\(i \\ge 0\\) and \\(i \\ge j - 3\\) ⇒ \\(i \\ge \\max(0, j-3)\\)\n\\(i \\le 8\\) and \\(i \\le j\\) ⇒ ((8, j) = j), since \\(j \\le 6\\)"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#complexity-of-fourier-motzkin",
    "href": "lectures/revealjs_poly_final.qmd.html#complexity-of-fourier-motzkin",
    "title": "Introduction To Polyhedral Methods",
    "section": "Complexity of Fourier-Motzkin",
    "text": "Complexity of Fourier-Motzkin\nFourier-Motzkin elimination is computationally expensive, especially when the number of variables is large.\nWorst-case complexity: The number of inequalities generated can grow exponentially with the number of variables, resulting in inefficient performance for high-dimensional systems. - Despite this, it is still a useful tool for small to medium-dimensional systems where the elimination process can be computationally feasible."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#how-much-of-a-limitation-is-affine-only",
    "href": "lectures/revealjs_poly_final.qmd.html#how-much-of-a-limitation-is-affine-only",
    "title": "Introduction To Polyhedral Methods",
    "section": "How much of a limitation is affine only",
    "text": "How much of a limitation is affine only\n99% of hpc loops are affine: C. Bastoul, A. Cohen, S. Girbal, S. Sharma, and O. Temam. Putting polyhedral loop transformations to work. In LCPC, 2003.\nover 95% of loops in deep learning are affine:\nNorman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, RaminderBajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA). IEEE, 1–12."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#what-about-the-ilp-solver",
    "href": "lectures/revealjs_poly_final.qmd.html#what-about-the-ilp-solver",
    "title": "Introduction To Polyhedral Methods",
    "section": "what about the ilp solver",
    "text": "what about the ilp solver\nilp is np-complete so it is slow, but often tractable for problems with up to several hundred variables.\nSome compiler writers feel that means poly methods can only do this for toy programs? - Is the complexity a barrier for real-world compiler applications?"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#affine-functions-1",
    "href": "lectures/revealjs_poly_final.qmd.html#affine-functions-1",
    "title": "Introduction To Polyhedral Methods",
    "section": "Affine Functions",
    "text": "Affine Functions\n\nOnly allow operations: addition, subtraction, multiplication by a constant.\nCannot multiply by unknowns or use quantifiers (e.g., ∀, ∃).\nCan reformulate some problems but requires breaking them into simpler parts."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#multiple-dimensions-and-data-dependencies",
    "href": "lectures/revealjs_poly_final.qmd.html#multiple-dimensions-and-data-dependencies",
    "title": "Introduction To Polyhedral Methods",
    "section": "Multiple Dimensions and Data Dependencies",
    "text": "Multiple Dimensions and Data Dependencies\n// Consider the following nested loops with data dependency in matrix A:\nfor (i = 1; i &lt;= n; i++) {\n    for (j = i; j &lt;= n; j++) {\n        // S: Data dependency on A[i-1][j] and A[i][j-1]\n        A[i][j] = (A[i-1][j] + A[i][j-1]) * 0.5;\n    }\n}\nThe computation of \\(A[i][j]\\) depends on \\(A[i-1][j]\\) and \\(A[i][j-1]\\), creating a pattern of data dependencies."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#graph-with-tiles",
    "href": "lectures/revealjs_poly_final.qmd.html#graph-with-tiles",
    "title": "Introduction To Polyhedral Methods",
    "section": "graph with tiles",
    "text": "graph with tiles\n\n\nData Dependence for A[i][j] = (A[i-1][j] + A[i][j-1]) * 0.5\n[((1, 1), (1, 2)), ((1, 2), (1, 3)), ((1, 3), (1, 4)), ((1, 4), (1, 5)), ((1, 5), (1, 6)), ((1, 6), (1, 7)), ((1, 7), (1, 8)), ((1, 2), (2, 2)), ((2, 1), (2, 2)), ((1, 3), (2, 3)), ((2, 2), (2, 3)), ((1, 4), (2, 4)), ((2, 3), (2, 4)), ((1, 5), (2, 5)), ((2, 4), (2, 5)), ((1, 6), (2, 6)), ((2, 5), (2, 6)), ((1, 7), (2, 7)), ((2, 6), (2, 7)), ((1, 8), (2, 8)), ((2, 7), (2, 8)), ((2, 3), (3, 3)), ((3, 2), (3, 3)), ((2, 4), (3, 4)), ((3, 3), (3, 4)), ((2, 5), (3, 5)), ((3, 4), (3, 5)), ((2, 6), (3, 6)), ((3, 5), (3, 6)), ((2, 7), (3, 7)), ((3, 6), (3, 7)), ((2, 8), (3, 8)), ((3, 7), (3, 8)), ((3, 4), (4, 4)), ((4, 3), (4, 4)), ((3, 5), (4, 5)), ((4, 4), (4, 5)), ((3, 6), (4, 6)), ((4, 5), (4, 6)), ((3, 7), (4, 7)), ((4, 6), (4, 7)), ((3, 8), (4, 8)), ((4, 7), (4, 8)), ((4, 5), (5, 5)), ((5, 4), (5, 5)), ((4, 6), (5, 6)), ((5, 5), (5, 6)), ((4, 7), (5, 7)), ((5, 6), (5, 7)), ((4, 8), (5, 8)), ((5, 7), (5, 8)), ((5, 6), (6, 6)), ((6, 5), (6, 6)), ((5, 7), (6, 7)), ((6, 6), (6, 7)), ((5, 8), (6, 8)), ((6, 7), (6, 8)), ((6, 7), (7, 7)), ((7, 6), (7, 7)), ((6, 8), (7, 8)), ((7, 7), (7, 8)), ((7, 8), (8, 8)), ((8, 7), (8, 8))]\n\n\n\n\n\n\n\n\n\nThe schedule traverses the matrix in an upward direction for each column (j), moving left to right across columns (i).\nadding tiles does not have a problem when dependence, but with-in each tile have to execute serially"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#formalizing-the-schedule-lexicographic-ordering",
    "href": "lectures/revealjs_poly_final.qmd.html#formalizing-the-schedule-lexicographic-ordering",
    "title": "Introduction To Polyhedral Methods",
    "section": "Formalizing the Schedule: Lexicographic Ordering",
    "text": "Formalizing the Schedule: Lexicographic Ordering\n\nSchedule Mapping: \\(s(i,j) \\rightarrow (i,j)\\)\nStatements as Vectors: Considered as ‘times’ (e.g., hours, minutes, seconds)"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#understanding-lexicographic-ordering",
    "href": "lectures/revealjs_poly_final.qmd.html#understanding-lexicographic-ordering",
    "title": "Introduction To Polyhedral Methods",
    "section": "Understanding Lexicographic Ordering",
    "text": "Understanding Lexicographic Ordering\n\\[(i,j) \\gg (m,n) \\iff i &gt; m \\lor (i = m \\land j &gt; n)\\]\n\nCompare elements from left to right.\nContinue comparison on equal terms.\nLexicographic order generalizes alphabetical order (→)."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#checking-for-loop-interchange",
    "href": "lectures/revealjs_poly_final.qmd.html#checking-for-loop-interchange",
    "title": "Introduction To Polyhedral Methods",
    "section": "Checking for loop interchange",
    "text": "Checking for loop interchange\nfor i in [1,2,3,4]                     for j in [1,2,3]\n  for j in [1,2,3]                       for i in [1,2,3,4]\ns:   a(i,j) = a(i-1,j+1)                   a(i,j) = a(i-1,j+1)  \n\n\nOriginal Schedule: \\(s(i, j) \\rightarrow (i,j)\\)\nAfter Interchange: \\(s(i,j) \\rightarrow (j,i)\\)\n\ndata flow\n        read write\ns(1,1)  a(0,2)  a(1,1)\ns(1,2)  a(0,3)  a(1,2)\ns(1,3)  a(0,4)  a(1,3)\ns(1,4)  a(0,5)  a(1,4)\ns(2,1)  a(1,2)  a(2,1)   s(1,2)-&gt; s(2,1)\ns(2,2)  a(1,3)  a(2,2)   s(1,3)-&gt; s(2.2)\n\\[\ns(i,j) \\rightarrow s(i+1, j-1)\n\\]"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#constants",
    "href": "lectures/revealjs_poly_final.qmd.html#constants",
    "title": "Introduction To Polyhedral Methods",
    "section": "constants:",
    "text": "constants:\nDoes there exist a statement s(i,j) and a statement \\(s(i',j')\\) where in the new schedule \\(s(i',j')\\) executes first and data flows backward in time \\[\n\\begin{align*}\n(i', j') \\gg (j,i)   &\\text{ $i',j'$ is first} \\\\\ni' = 1+ i            &\\text{ data\\  from \\ i+1 to $i'$}\\\\\nj' = -1 +j           &\\text{ data\\  from \\ j-1 to $j'$}\\\\\n1 \\le i \\le 4 \\\\\n1 \\le j \\le 3  \\\\\n1 \\le i' \\le 4 \\\\\n1 \\le j' \\leftrightarrows 3\n\\end{align*}\n\\]\nbecause of the lexicographic order we have two ilp problems one where \\(i'\\) is greater then j, and one where \\(i'\\) = j"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#solution",
    "href": "lectures/revealjs_poly_final.qmd.html#solution",
    "title": "Introduction To Polyhedral Methods",
    "section": "solution",
    "text": "solution\nI ran it through:\nhttps://online-optimizer.appspot.com\nwhich gave me a solution:\ns(4,2) reads s(3,3) but s(4,2) executes first"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#classic-transformations",
    "href": "lectures/revealjs_poly_final.qmd.html#classic-transformations",
    "title": "Introduction To Polyhedral Methods",
    "section": "Classic transformations",
    "text": "Classic transformations"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#loop-reversal",
    "href": "lectures/revealjs_poly_final.qmd.html#loop-reversal",
    "title": "Introduction To Polyhedral Methods",
    "section": "loop reversal",
    "text": "loop reversal\ntransformation: [i] ==&gt; [-i]\nfor (int c0 = 0; c0 &lt; n; c0 += 1)\n  S(c0);\n\nfor (int c0 = -n + 1; c0 &lt;= 0; c0 += 1)\n  S(-c0);"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#loop-fusion",
    "href": "lectures/revealjs_poly_final.qmd.html#loop-fusion",
    "title": "Introduction To Polyhedral Methods",
    "section": "loop fusion",
    "text": "loop fusion\ntransformation: [0, i] -&gt; [i,0]; [1, i] -&gt; [i, 1]\n{\n  for (int c1 = 0; c1 &lt;= n; c1 += 1)\n    S(c1);\n  for (int c1 = 0; c1 &lt;= n; c1 += 1)\n    T(c1);\n}\nfor (int c0 = 0; c0 &lt;= n; c0 += 1) {\n  S(c0);\n  T(c0);\n}"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#loop-fission",
    "href": "lectures/revealjs_poly_final.qmd.html#loop-fission",
    "title": "Introduction To Polyhedral Methods",
    "section": "loop fission",
    "text": "loop fission\ntransformation: [i, 0] -&gt; [0, i]; [i, 1] -&gt; [1, i]\nfor (int c0 = 0; c0 &lt;= n; c0 += 1) {\n  S(c0);\n  T(c0);\n}\n{\n  for (int c1 = 0; c1 &lt;= n; c1 += 1)\n    S(c1);\n  for (int c1 = 0; c1 &lt;= n; c1 += 1)\n    T(c1);\n}"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#strip-mining",
    "href": "lectures/revealjs_poly_final.qmd.html#strip-mining",
    "title": "Introduction To Polyhedral Methods",
    "section": "strip mining",
    "text": "strip mining\ntransformation: [i] -&gt; [floor(i/4), i % 4]\nfor (int c0 = 0; c0 &lt;= 1023; c0 += 1)\n  S(c0);\n\nfor (int c0 = 0; c0 &lt;= 255; c0 += 1)\n  for (int c1 = 0; c1 &lt;= 3; c1 += 1)\n    S(4 * c0 + c1);"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#loop-tiling",
    "href": "lectures/revealjs_poly_final.qmd.html#loop-tiling",
    "title": "Introduction To Polyhedral Methods",
    "section": "loop tiling",
    "text": "loop tiling\ntransformation: [i,j] -&gt; [floor(i/4), i % 4, floor(j/4), j % 4]\nfor (int c0 = 0; c0 &lt;= 1023; c0 += 1)\n  for (int c1 = 0; c1 &lt;= 1023; c1 += 1)\n    S(c0, c1);\n\nfor (int c0 = 0; c0 &lt;= 255; c0 += 1)\n  for (int c1 = 0; c1 &lt;= 3; c1 += 1)\n    for (int c2 = 0; c2 &lt;= 255; c2 += 1)\n      for (int c3 = 0; c3 &lt;= 3; c3 += 1)\n        S(4 * c0 + c1, 4 * c2 + c3);"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#finding-transformations",
    "href": "lectures/revealjs_poly_final.qmd.html#finding-transformations",
    "title": "Introduction To Polyhedral Methods",
    "section": "finding transformations:",
    "text": "finding transformations:\nfor i in [0, 1, 2, 3, 4, 5]:\n    P:  a(i) = input(i) + 1\nfor j in [0, 1, 2, 3, 4, 5]:\n    c:  b(j) = a(j) + 2\nwhat transformation will give the best locality?"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#poly-form",
    "href": "lectures/revealjs_poly_final.qmd.html#poly-form",
    "title": "Introduction To Polyhedral Methods",
    "section": "poly form",
    "text": "poly form\nEach loop can be represented by its iteration space in a polyhedral form.\nLoop 1: \\(𝑖 \\in [0,1,2,3,5]\\)\nLoop 2: \\(j \\in [0,1,2,3,4,5]\\)\nThe dependencies between the two loops are: \\(p(i)\\) must be computed before \\(c(i)\\)"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#define-a-new-schedule",
    "href": "lectures/revealjs_poly_final.qmd.html#define-a-new-schedule",
    "title": "Introduction To Polyhedral Methods",
    "section": "define a new schedule",
    "text": "define a new schedule\nWe introduce a schedule for each loop using affine transformations. (we can only handle affine transformations )\nFor the first loop: \\[\\theta_1(i) = \\alpha_1 i + \\beta_1\\]\nFor the second loop: \\[\\theta_2(j) = \\alpha_2 j + \\beta_2\\]\nThe goal is to reorder the iterations such that all dependencies are respected and memory locality is improved."
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#solving",
    "href": "lectures/revealjs_poly_final.qmd.html#solving",
    "title": "Introduction To Polyhedral Methods",
    "section": "solving",
    "text": "solving\nWe have to find four scalars\nFor all \\(i, j\\) such that \\(p(i)\\) sends data to \\(c(j)\\), the following constraint holds:\n\\[\\text{if } p(i) \\to c(j), \\quad \\text{then } \\theta_1(i) \\leq  \\theta_2(j).\\]\nFor all \\(0 \\leq i \\leq 5\\) and \\(0 \\leq j \\leq 5\\), and \\(i=j\\) the following condition holds: \\[\\theta_1(i) \\leq theta_2(j)\\]\nexpanding this out\n\\[\\alpha_1 i + \\beta_1 \\leq \\alpha_2 j + \\beta_2\\]\nlooks bad- non-linear constraint since there is a product, and the foralls do not work with solvers"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#magic",
    "href": "lectures/revealjs_poly_final.qmd.html#magic",
    "title": "Introduction To Polyhedral Methods",
    "section": "magic",
    "text": "magic\nWe apply a theorem called the affine form of the Farkas lemma which turns this into a tractable problem\nAn affine form is non-negative over a polyhedron if and only if it can be be written as a non-negative combination of the constraints that form the polyhedron\nforall \\(0 \\le i \\le 5\\) and \\(0 \\le j \\le 5\\) and \\(i = j\\)\nsuch that \\(-\\alpha_1 * i + \\alpha_2  * j + (\\beta_1 - \\beta_2) \\ge 0\\) affine form\npolyhedron\n\n\\(i \\ge 0\\)\n\\(-i \\ge -5\\)\n\\(j \\ge 0\\)\n\\(-j \\ge -5\\)\n\\(j - i \\ge 0\\)\n\\(i=j \\ge 0\\)"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#optimization-problem",
    "href": "lectures/revealjs_poly_final.qmd.html#optimization-problem",
    "title": "Introduction To Polyhedral Methods",
    "section": "optimization problem",
    "text": "optimization problem\nfind a function that gives the locality\nminimize w: \\(w \\ge \\alpha_2 * i + \\beta_2 - \\alpha_1 * j - \\beta_1\\)\nw is a bound in time between producer and consumer, how long the location needs to stay in the local memory"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#parallelism",
    "href": "lectures/revealjs_poly_final.qmd.html#parallelism",
    "title": "Introduction To Polyhedral Methods",
    "section": "parallelism",
    "text": "parallelism\nwe minimized the time between definition and use, for best locality, if we maximize that time we get the most parallel code"
  },
  {
    "objectID": "lectures/revealjs_poly_final.qmd.html#objectives",
    "href": "lectures/revealjs_poly_final.qmd.html#objectives",
    "title": "Introduction To Polyhedral Methods",
    "section": "objectives",
    "text": "objectives\nSince everything is affine it is hard to formulate a complex cost function"
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html",
    "href": "lectures/03b_local_value_numbering.html",
    "title": "local value numbering",
    "section": "",
    "text": "slides from Phil Gibbons at CMU for more details and context on LVN\nValue numbering is a very powerful technique that removes redundancies, An instruction x + y is redundant inside a block if it has already been computed in the block, and no intervening operation redefines x or y. If the compiler finds a redundant expression, it can save that value at the first computation and replace any subsequent evaluations with references to the saved value.\n\nThe idea is simple - The algorithm executes the block, Each time it sees a new variable it gives it a value (represented as a number)\nEach time it sees an instruction it forms a hash of the op code and the value numbers of its operands and gives it a new value number.\nTwo instructions are redundant if they have same op code and operands, which means the same value number\n\n\\(e_i\\) and \\(e_j\\) have the same value number if and only if \\(e_i\\) and \\(e_j\\) are provably equal for all possible operands of the expressions.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#local-value-numbering-covers-lot-of-optimizations-that-look-different",
    "href": "lectures/03b_local_value_numbering.html#local-value-numbering-covers-lot-of-optimizations-that-look-different",
    "title": "local value numbering",
    "section": "local value numbering covers lot of optimizations that look different",
    "text": "local value numbering covers lot of optimizations that look different\ndead code elimination\n\nmain {\n    a: int = const 100;\n    a: int = const 42;\n    print a;\n\n}\n\ncopy propagation\n\nmain{\n    x: int = const 4;\n    copy1: int = id x;\n    copy2: int = id copy1;\n    print copy2;\n}\n\ncommon sub-expression elimination cse \n\nmain {\n    a: int = const 4;\n    b: int = const 2;\n    sum1: int = add a b;\n    sum2: int = add a b;\n    prod: int = mul sum1 sum2;\n    print prod;\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#variables-vis-values",
    "href": "lectures/03b_local_value_numbering.html#variables-vis-values",
    "title": "local value numbering",
    "section": "variables vis values",
    "text": "variables vis values\nWe want to stop thinking about variables and think about values. Two instructions are redundant if they compute the same value.\n\nfor example in a JIT compiler we want computation to be fast so we can get rid of all the variables\nb: int const 1;\nc: int cont 2;\na:  int b c;  \nbecomes:\n[  int const 1\n   int const 2 \n   int 0 1\n]\nless storage, args are just pointers, instructions are smaller. faster because any use points to the corresponding def without any searching.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#value-numbering-continued",
    "href": "lectures/03b_local_value_numbering.html#value-numbering-continued",
    "title": "local value numbering",
    "section": "value numbering continued",
    "text": "value numbering continued",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#use-of-turnt",
    "href": "lectures/03b_local_value_numbering.html#use-of-turnt",
    "title": "local value numbering",
    "section": "use of turnt ",
    "text": "use of turnt \nturnt\nthere is a directory   bril/examples/test/tdce   which has some test programs \nand a file turnt.toml  that contains  one line \ncommand = “bril2json &lt; {filename} | python3 ../../tdce.py {args} | bril2txt”\nto execute \n(.venv) (base) norm@norm-ubuntu:~/bril/examples/test/tdce$ turnt *.bril 1..8 ok 1 - combo.bril ok 2 - diamond.bril ok 3 - double.bril ok 4 - double-pass.bril ok 5 - reassign.bril ok 6 - reassign-dkp.bril ok 7 - simple.bril ok 8 - skipped.bril",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#redundancy-elimination",
    "href": "lectures/03b_local_value_numbering.html#redundancy-elimination",
    "title": "local value numbering",
    "section": "redundancy elimination",
    "text": "redundancy elimination\nan expression x+y is redundant if and only if\n\nalong every path from the entry it has been evaluated and\nits subexpressions x and y have not been redefined\n\nif the compiler can prove an expression is redundant it can\n\npreserve the earlier evaluation\nreplace the redundant expression with a use of the preserved value",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#key-idea",
    "href": "lectures/03b_local_value_numbering.html#key-idea",
    "title": "local value numbering",
    "section": "key idea",
    "text": "key idea\nassign a number (value number) to each expression\n\ntwo expressions have the same value number if they always have the same value\nuse hashing to make this efficient",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#pseudo-code",
    "href": "lectures/03b_local_value_numbering.html#pseudo-code",
    "title": "local value numbering",
    "section": "pseudo code",
    "text": "pseudo code\nwalk each block, assign a distinct value number to each value the block computes.\n\\(e_i\\) and \\(e_j\\) have the same value number if and only if \\(e_i\\) and \\(e_j\\) are provably equal for all possible operands of the expressions.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#pseudo-code-vn-version-1",
    "href": "lectures/03b_local_value_numbering.html#pseudo-code-vn-version-1",
    "title": "local value numbering",
    "section": "pseudo code vn version 1",
    "text": "pseudo code vn version 1\nwe have two tables - hash_table: expression to vn, variable holding the value variable to vn\nfor each instr in the block\n  v= [ value_number(a) for a in the args of the instr]\n  build temp inst hash = instr.op + v\n  if hash in hash_table:\n     get from table vn, cann_variable \n     replace instr with instr.dest = cann_variable\n     instr.dest = vn\n  else: \n    generate a new value number, add new entry to hash_table, new vn, instr.dest \nAn example\na add b, c \nb sub a, d \nc add b, c\nd sub a, d  // d id b\nitem   vn       hash \nb      0/4\nc      1/5\n                add12  2    a \na      2 \nd      3\n\n                sub23 4     b\n                add41 5     c\n                \nPseudo code (similar to an interpreter)\n\nhash table constants and expressions of value numbers to value numbers and to a variable holding the value\nreverse map from variables to value numbers\n\n\n\n\n  main {\n    a: int = const 4;\n    b: int = const 2;\n    sum1: int = add a b;\n    sum2: int = add a b;\n    prod: int = mult sum1 sum2;\n    print prod\n\n  }\n\n\n\n\nkey\nvalue\ncanonical name\n\n\n\n\nconst 4\n1\na\n\n\nconst 2\n2\nb\n\n\nadd 1 2\n3\nsum1\n\n\nmul 3 3\n4\nprod\n\n\n\n\n\n\nname\nvalue\n\n\n\n\na\n1\n\n\nb\n2\n\n\nsum1\n3\n\n\nsum2\n3\n\n\nprod\n4\n\n\n\n\n\nextensions:\n\na: int id b\n\na gets the value number of b. No copy required\nCommutative operations Commutative operations that differ only in the order of their operands, such as a × b and b × a, should receive the same value numbers. As lvn constructs a hash key for the right-hand side of the current operation, it can sort the operands using some convenient scheme, such as ordering them by value number. This simple action will ensure that commutative variants receive the same value number.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#extension",
    "href": "lectures/03b_local_value_numbering.html#extension",
    "title": "local value numbering",
    "section": "extension",
    "text": "extension\nconstant folding \n   a: int const 1;\n   b: int const 2;\n   c: add a b;\nConstant folding If all the operands of an operation have known constant values, lvn can perform the operation and fold the answer directly into the code. lvn can store information about constants in the hash table, including their value. Before hash-key formation, it can test the operands and, if possible, evaluate them. If lvn discovers a constant expression, it can replace the operation with an immediate load of the result. Subsequent copy folding will clean up the code.\nAlgebraic identities: lvn can apply algebraic identities to simplify the code. For example, x + 0 and x should receive the same value number. Unfortunately, lvn needs special-case code for each identity. A series of tests, one per identity, can easily become long enough to produce an unacceptable slowdown in the algorithm. To ameliorate this problem, lvn should organize the tests into operator-specific decision trees.\na +0, a-0, a1 a0, a-a",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#vn-version-2",
    "href": "lectures/03b_local_value_numbering.html#vn-version-2",
    "title": "local value numbering",
    "section": "vn version 2",
    "text": "vn version 2\nadd a bit indicating that a variable is a constant\nfor each instr in the block\n  v= [ value_number(a) for a in the args of the instr]\n  if all v's are constants, fold the operation \n  check for all the identities \n  build temp inst hash = instr.op + v\n  if hash in hash_table:\n     get from table vn, cann_variable \n     replace instr with instr.dest = cann_variable\n     instr.dest = vn\n  else: \n    generate a new value number, add new entry to hash_table, new vn, instr.dest",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#problem",
    "href": "lectures/03b_local_value_numbering.html#problem",
    "title": "local value numbering",
    "section": "problem:",
    "text": "problem:\na = x +y \nb = x + y\na = 17\nc = x +y \nkeep track of all variables that contain the value and select one\none option is to save the value, if x will be overwritten add a temp\nt = a+b\nx = t \nx = \n  = t",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#another-option-is-renaming",
    "href": "lectures/03b_local_value_numbering.html#another-option-is-renaming",
    "title": "local value numbering",
    "section": "another option is renaming",
    "text": "another option is renaming\n\n\na = x + y\nb = x + y\na = 17\nc = x +Y\n\na0 = x0 + y0\nb0 = x0+ y0\na1 = 17\nc0 = x0 +y0",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#indirect-assignments",
    "href": "lectures/03b_local_value_numbering.html#indirect-assignments",
    "title": "local value numbering",
    "section": "indirect assignments",
    "text": "indirect assignments\nassignments via a pointer, or to an array element\na = b[i]\n...       no change to i \nc = b[i]\n\n\na = b[i]\ni=\nc = b[i]\n\n\na = b[i]\nb[k] =\n   =b[i]\n   =b[k]\nindexed stores\nwhen we see an assignment a[i] = exp\nwe have 3 value numbers a, i, exp\ngive the array a new value number give the array[i] operation the value number of the exp",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#local-value-numbering.",
    "href": "lectures/03b_local_value_numbering.html#local-value-numbering.",
    "title": "local value numbering",
    "section": "Local value numbering.",
    "text": "Local value numbering.\nYou can see one implementation in lvn.py in the Bril repository. But seriously, don’t be tempted! You want to write your implementation without looking at mine!\nexamples",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#testing-your-optimizations",
    "href": "lectures/03b_local_value_numbering.html#testing-your-optimizations",
    "title": "local value numbering",
    "section": "Testing Your Optimizations",
    "text": "Testing Your Optimizations\nAs part of your tasks for this lesson, you will implement your first two optimizations. The two main things you want your optimizations to do are:\n\nNot break programs.\nMake programs faster, most of the time.\n\n\nAs with every task in this class, part of the work is checking that you have done what you set out to do — in this case, that your optimizations do those two things.\nThink carefully about how to make a convincing case for each of those criteria.\n\nOne tempting methodology might be to hand write a few small test-case Bril programs (or, worse, borrow the woefully inadequate ones sitting around in the Bril git repository), run them through your optimizations, and look at them to check whether they look right. This does not amount to convincing evidence (maybe you can think of a few specific reasons why).\n\nWhile there are many ways to be convincing, a pretty good way might be to run your optimization on *every single available Bril benchmark, systematically check that it still produces the right output for at least one input, and collect aggregate statistics about some metric you’re interested in. This is a nice way to check for unexpected behavior in programs that you didn’t carefully write yourself to test the cases you’re thinking of.\n\nIf this is the route you choose, you can do it however you like, There is a simple tool that you can consider using, called Brench. Brench is not very fancy; it does three things:\n\nIt makes it easy to run a long list of inputs through several different commands. (For example, you can run a long list of Bril benchmarks through an “interpret” command and an “optimize-and-then-interpret” command.)\nIt checks that all the commands agree on their output. (So, in our use case, it checks that optimizing the benchmark doesn’t change its output when interpreted.)\nIt can collect a statistic from each command for comparison. (Like the number of dynamic instructions the interpreter executed, which is a pretty good metric for standard optimizations.)\n\nThose three things are probably what you want to do to make a convincing case for an optimization’s correctness and effectiveness, whether or not you use Brench. It’s there if you want it, but feel free to go your own way!",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#homework-2",
    "href": "lectures/03b_local_value_numbering.html#homework-2",
    "title": "local value numbering",
    "section": "homework 2",
    "text": "homework 2\npart 1: Implement “trivial” dead code elimination in which you delete instructions that are never used before they are reassigned.\npart2: Implement local value numbering. Try pairing it with your dead code elimination code, in the write up be sure to include evidence that your implementation is correct and actually optimizes programs, you might want to use the Brench program, for extra points, extend your implementation to handle some of the tricker examples talked about in class.\nremember that the result is a blog post",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#crossing-blocks",
    "href": "lectures/03b_local_value_numbering.html#crossing-blocks",
    "title": "local value numbering",
    "section": "crossing blocks",
    "text": "crossing blocks\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA[\"m = a + b&lt;br&gt; n = a + b\"]\nB[\"p = c + d&lt;br&gt;r = c + d\"]\n\nC[\"q = a + b&lt;br&gt; r = c + d\"]\nD[\"e = b + 18&lt;br&gt; s = a + b &lt;br&gt; u = e + f\"]\nE[\"e = a + 17&lt;br&gt; t = c + d &lt;br&gt; u = e + f\"]\nF[\"v = a + b &lt;br&gt; w = c + d &lt;br&gt; x = e + f\"]\nG[\"y = a + b &lt;br&gt; z = c + d\"]\n\nstyle A fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle B fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle C fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle D fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle E fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle F fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle G fill:#ffffff,stroke:#000000,stroke-width:1px\nA--&gt; B\nA--&gt; C\nC --&gt; D\nC --&gt; E\nD--&gt; F\nE --&gt; F\nF--&gt; G\nB--&gt; G\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA[\"m = a + b&lt;br&gt; n = a + b\"]\nB[\"p = c + d&lt;br&gt;r = c + d\"]\n\nC[\"q = a + b&lt;br&gt; r = c + d\"]\nD[\"e = b + 18&lt;br&gt; s = a + b &lt;br&gt; u = e + f\"]\nE[\"e = a + 17&lt;br&gt; t = c + d &lt;br&gt; u = e + f\"]\nF[\"v = a + b &lt;br&gt; w = c + d &lt;br&gt; x = e + f\"]\nG[\"y = a + b &lt;br&gt; z = c + d\"]\n\nstyle A fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle B fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle C fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle D fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle E fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle F fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle G fill:#ffffff,stroke:#000000,stroke-width:1px\nA--&gt; B\nA--&gt; C\nC --&gt; D\nC --&gt; E\nD--&gt; F\nE --&gt; F\nF--&gt; G\nB--&gt; G",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#extended-basic-blocks",
    "href": "lectures/03b_local_value_numbering.html#extended-basic-blocks",
    "title": "local value numbering",
    "section": "extended basic blocks",
    "text": "extended basic blocks\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA--&gt;b\nA--&gt;c\nc--&gt;d\nc--&gt;e\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA--&gt;b\nA--&gt;c\nc--&gt;d\nc--&gt;e\n\n\n\n\n\n\nhow do we extend the hash tables over the boundary\nwe need to do value numbering over each path\nworklist = {entry block}\nstack = {}\nwhile worklist is not empty \n   remove a block b from the worklist \n   evn(b)\n\nevn(b, stack)\n   t = new table for b  \n   link t above stack  \n   lvn(b,t) \n   for each s successor of b,\n     if s has one pred then evn(s, t)\n     else add s to worklist \n   dealocate t",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#safety",
    "href": "lectures/03b_local_value_numbering.html#safety",
    "title": "local value numbering",
    "section": "safety",
    "text": "safety\nif the result of evaluating E1 cannot be distinguished from evaluating E the compiler is free to replace E with E1\nSome compilers assume it is ok if E1 produces less errors than E\nsome compilers assume that safety is only required for “standard conforming” code and undefined behavior for other code.\nWhy is value numbering safe?\n\nif an expression is in the hash table, it must have occurred at least one in the block\nAlgorithm modified the code but does not invalidate the table",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/03b_local_value_numbering.html#when-is-value-numbering-profitable",
    "href": "lectures/03b_local_value_numbering.html#when-is-value-numbering-profitable",
    "title": "local value numbering",
    "section": "when is value numbering profitable",
    "text": "when is value numbering profitable\nif reuse is cheaper then re-compute 1. does not cause a spill 1. if does not need a copy (does the copy take as long as the compute)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "local value numbering"
    ]
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#local-value-numbering-covers-lot-of-optimizations-that-look-different",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#local-value-numbering-covers-lot-of-optimizations-that-look-different",
    "title": "local value numbering",
    "section": "local value numbering covers lot of optimizations that look different",
    "text": "local value numbering covers lot of optimizations that look different\ndead code elimination\n\nmain {\n    a: int = const 100;\n    a: int = const 42;\n    print a;\n\n}\n\ncopy propagation\n\nmain{\n    x: int = const 4;\n    copy1: int = id x;\n    copy2: int = id copy1;\n    print copy2;\n}\n\ncommon sub-expression elimination cse \n\nmain {\n    a: int = const 4;\n    b: int = const 2;\n    sum1: int = add a b;\n    sum2: int = add a b;\n    prod: int = mul sum1 sum2;\n    print prod;\n}"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#variables-vis-values",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#variables-vis-values",
    "title": "local value numbering",
    "section": "variables vis values",
    "text": "variables vis values\nWe want to stop thinking about variables and think about values. Two instructions are redundant if they compute the same value."
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#value-numbering-continued",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#value-numbering-continued",
    "title": "local value numbering",
    "section": "value numbering continued",
    "text": "value numbering continued"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#use-of-turnt",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#use-of-turnt",
    "title": "local value numbering",
    "section": "use of turnt ",
    "text": "use of turnt \nturnt\nthere is a directory   bril/examples/test/tdce   which has some test programs \nand a file turnt.toml  that contains  one line \ncommand = “bril2json &lt; {filename} | python3 ../../tdce.py {args} | bril2txt”\nto execute \n(.venv) (base) norm@norm-ubuntu:~/bril/examples/test/tdce$ turnt *.bril 1..8 ok 1 - combo.bril ok 2 - diamond.bril ok 3 - double.bril ok 4 - double-pass.bril ok 5 - reassign.bril ok 6 - reassign-dkp.bril ok 7 - simple.bril ok 8 - skipped.bril"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#redundancy-elimination",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#redundancy-elimination",
    "title": "local value numbering",
    "section": "redundancy elimination",
    "text": "redundancy elimination\nan expression x+y is redundant if and only if\n\nalong every path from the entry it has been evaluated and\nits subexpressions x and y have not been redefined\n\nif the compiler can prove an expression is redundant it can\n\npreserve the earlier evaluation\nreplace the redundant expression with a use of the preserved value"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#key-idea",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#key-idea",
    "title": "local value numbering",
    "section": "key idea",
    "text": "key idea\nassign a number (value number) to each expression\n\ntwo expressions have the same value number if they always have the same value\nuse hashing to make this efficient"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#pseudo-code",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#pseudo-code",
    "title": "local value numbering",
    "section": "pseudo code",
    "text": "pseudo code\nwalk each block, assign a distinct value number to each value the block computes.\n\\(e_i\\) and \\(e_j\\) have the same value number if and only if \\(e_i\\) and \\(e_j\\) are provably equal for all possible operands of the expressions."
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#pseudo-code-vn-version-1",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#pseudo-code-vn-version-1",
    "title": "local value numbering",
    "section": "pseudo code vn version 1",
    "text": "pseudo code vn version 1\nwe have two tables - hash_table: expression to vn, variable holding the value variable to vn\nfor each instr in the block\n  v= [ value_number(a) for a in the args of the instr]\n  build temp inst hash = instr.op + v\n  if hash in hash_table:\n     get from table vn, cann_variable \n     replace instr with instr.dest = cann_variable\n     instr.dest = vn\n  else: \n    generate a new value number, add new entry to hash_table, new vn, instr.dest \nAn example\na add b, c \nb sub a, d \nc add b, c\nd sub a, d  // d id b\nitem   vn       hash \nb      0/4\nc      1/5\n                add12  2    a \na      2 \nd      3\n\n                sub23 4     b\n                add41 5     c\n                \nPseudo code (similar to an interpreter)\n\nhash table constants and expressions of value numbers to value numbers and to a variable holding the value\nreverse map from variables to value numbers"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#extension",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#extension",
    "title": "local value numbering",
    "section": "extension",
    "text": "extension\nconstant folding \n   a: int const 1;\n   b: int const 2;\n   c: add a b;\nConstant folding If all the operands of an operation have known constant values, lvn can perform the operation and fold the answer directly into the code. lvn can store information about constants in the hash table, including their value. Before hash-key formation, it can test the operands and, if possible, evaluate them. If lvn discovers a constant expression, it can replace the operation with an immediate load of the result. Subsequent copy folding will clean up the code.\nAlgebraic identities: lvn can apply algebraic identities to simplify the code. For example, x + 0 and x should receive the same value number. Unfortunately, lvn needs special-case code for each identity. A series of tests, one per identity, can easily become long enough to produce an unacceptable slowdown in the algorithm. To ameliorate this problem, lvn should organize the tests into operator-specific decision trees.\na +0, a-0, a1 a0, a-a"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#vn-version-2",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#vn-version-2",
    "title": "local value numbering",
    "section": "vn version 2",
    "text": "vn version 2\nadd a bit indicating that a variable is a constant\nfor each instr in the block\n  v= [ value_number(a) for a in the args of the instr]\n  if all v's are constants, fold the operation \n  check for all the identities \n  build temp inst hash = instr.op + v\n  if hash in hash_table:\n     get from table vn, cann_variable \n     replace instr with instr.dest = cann_variable\n     instr.dest = vn\n  else: \n    generate a new value number, add new entry to hash_table, new vn, instr.dest"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#problem",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#problem",
    "title": "local value numbering",
    "section": "problem:",
    "text": "problem:\na = x +y \nb = x + y\na = 17\nc = x +y \nkeep track of all variables that contain the value and select one\none option is to save the value, if x will be overwritten add a temp\nt = a+b\nx = t \nx = \n  = t"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#another-option-is-renaming",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#another-option-is-renaming",
    "title": "local value numbering",
    "section": "another option is renaming",
    "text": "another option is renaming\n\n\na = x + y\nb = x + y\na = 17\nc = x +Y\n\na0 = x0 + y0\nb0 = x0+ y0\na1 = 17\nc0 = x0 +y0"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#indirect-assignments",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#indirect-assignments",
    "title": "local value numbering",
    "section": "indirect assignments",
    "text": "indirect assignments\nassignments via a pointer, or to an array element\na = b[i]\n...       no change to i \nc = b[i]\n\n\na = b[i]\ni=\nc = b[i]\n\n\na = b[i]\nb[k] =\n   =b[i]\n   =b[k]\nindexed stores\nwhen we see an assignment a[i] = exp\nwe have 3 value numbers a, i, exp\ngive the array a new value number give the array[i] operation the value number of the exp"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#local-value-numbering.",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#local-value-numbering.",
    "title": "local value numbering",
    "section": "Local value numbering.",
    "text": "Local value numbering.\nYou can see one implementation in lvn.py in the Bril repository. But seriously, don’t be tempted! You want to write your implementation without looking at mine!\nexamples"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#testing-your-optimizations",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#testing-your-optimizations",
    "title": "local value numbering",
    "section": "Testing Your Optimizations",
    "text": "Testing Your Optimizations\nAs part of your tasks for this lesson, you will implement your first two optimizations. The two main things you want your optimizations to do are:\n\nNot break programs.\nMake programs faster, most of the time."
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#homework-2",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#homework-2",
    "title": "local value numbering",
    "section": "homework 2",
    "text": "homework 2\npart 1: Implement “trivial” dead code elimination in which you delete instructions that are never used before they are reassigned.\npart2: Implement local value numbering. Try pairing it with your dead code elimination code, in the write up be sure to include evidence that your implementation is correct and actually optimizes programs, you might want to use the Brench program, for extra points, extend your implementation to handle some of the tricker examples talked about in class.\nremember that the result is a blog post"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#crossing-blocks",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#crossing-blocks",
    "title": "local value numbering",
    "section": "crossing blocks",
    "text": "crossing blocks\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA[\"m = a + b&lt;br&gt; n = a + b\"]\nB[\"p = c + d&lt;br&gt;r = c + d\"]\n\nC[\"q = a + b&lt;br&gt; r = c + d\"]\nD[\"e = b + 18&lt;br&gt; s = a + b &lt;br&gt; u = e + f\"]\nE[\"e = a + 17&lt;br&gt; t = c + d &lt;br&gt; u = e + f\"]\nF[\"v = a + b &lt;br&gt; w = c + d &lt;br&gt; x = e + f\"]\nG[\"y = a + b &lt;br&gt; z = c + d\"]\n\nstyle A fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle B fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle C fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle D fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle E fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle F fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle G fill:#ffffff,stroke:#000000,stroke-width:1px\nA--&gt; B\nA--&gt; C\nC --&gt; D\nC --&gt; E\nD--&gt; F\nE --&gt; F\nF--&gt; G\nB--&gt; G\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA[\"m = a + b&lt;br&gt; n = a + b\"]\nB[\"p = c + d&lt;br&gt;r = c + d\"]\n\nC[\"q = a + b&lt;br&gt; r = c + d\"]\nD[\"e = b + 18&lt;br&gt; s = a + b &lt;br&gt; u = e + f\"]\nE[\"e = a + 17&lt;br&gt; t = c + d &lt;br&gt; u = e + f\"]\nF[\"v = a + b &lt;br&gt; w = c + d &lt;br&gt; x = e + f\"]\nG[\"y = a + b &lt;br&gt; z = c + d\"]\n\nstyle A fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle B fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle C fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle D fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle E fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle F fill:#ffffff,stroke:#000000,stroke-width:1px\nstyle G fill:#ffffff,stroke:#000000,stroke-width:1px\nA--&gt; B\nA--&gt; C\nC --&gt; D\nC --&gt; E\nD--&gt; F\nE --&gt; F\nF--&gt; G\nB--&gt; G"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#extended-basic-blocks",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#extended-basic-blocks",
    "title": "local value numbering",
    "section": "extended basic blocks",
    "text": "extended basic blocks\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA--&gt;b\nA--&gt;c\nc--&gt;d\nc--&gt;e\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA--&gt;b\nA--&gt;c\nc--&gt;d\nc--&gt;e\n\n\n\n\n\n\nhow do we extend the hash tables over the boundary\nwe need to do value numbering over each path\nworklist = {entry block}\nstack = {}\nwhile worklist is not empty \n   remove a block b from the worklist \n   evn(b)\n\nevn(b, stack)\n   t = new table for b  \n   link t above stack  \n   lvn(b,t) \n   for each s successor of b,\n     if s has one pred then evn(s, t)\n     else add s to worklist \n   dealocate t"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#safety",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#safety",
    "title": "local value numbering",
    "section": "safety",
    "text": "safety\nif the result of evaluating E1 cannot be distinguished from evaluating E the compiler is free to replace E with E1\nSome compilers assume it is ok if E1 produces less errors than E\nsome compilers assume that safety is only required for “standard conforming” code and undefined behavior for other code.\nWhy is value numbering safe?\n\nif an expression is in the hash table, it must have occurred at least one in the block\nAlgorithm modified the code but does not invalidate the table"
  },
  {
    "objectID": "lectures/revealjs_03b_local_value_numbering.qmd.html#when-is-value-numbering-profitable",
    "href": "lectures/revealjs_03b_local_value_numbering.qmd.html#when-is-value-numbering-profitable",
    "title": "local value numbering",
    "section": "when is value numbering profitable",
    "text": "when is value numbering profitable\nif reuse is cheaper then re-compute 1. does not cause a spill 1. if does not need a copy (does the copy take as long as the compute)"
  },
  {
    "objectID": "lectures/llvm.html",
    "href": "lectures/llvm.html",
    "title": "using llvm",
    "section": "",
    "text": "At the end of the course, you’ll do a language implementation research project. This is an open-ended and open-source project that can be on any topic that you can construe as being about compiler hacking. The final product is an experience report on the course blog where you rigorously evaluate the success of your implementation.\nYou can work individually or in groups of 2–3 people.\nProposal\nanswer these three questions\n\nWhat will you do?\nHow will you do it?\nHow will you empirically measure success?\n\nI will have feedback on how to approach your project.\nImplementation\nThe main phase, of course, is implementing the thing you said you would implement. I recommend you keep a “lab notebook” to log your thoughts, attempts, and frustrations—this will come in handy for the report you’ll write about the project.\nEvaluation\nA major part of your project is an empirical evaluation. To design your evaluation strategy, you will need to consider at least these things:\n\nWhere will you get the input code you’ll use in your evaluation?\nHow will you check the correctness of your implementation? If you’ve implemented an optimization, for example, “correctness” means that the transformed programs behave the same way as the original programs.\nHow will you measure the benefit (in performance, energy, complexity, etc.) of your implementation?\nHow will you present the data you collect from your empirical evaluation?\n\nOther questions may be relevant depending on the project you choose. Consider the SIGPLAN empirical evaluation guidelines when you design your methodology.\nExperience Report\nFor the main project deadline, you will write up the project’s outcomes in the form of a post on the course blog. Your writeup should answer these questions:\n\nWhat was the goal?\nWhat did you do? (Include both the design and the implementation.)\nWhat were the hardest parts to get right?\nWere you successful? (Report rigorously on your empirical evaluation.)\n\nTo submit your report, open a pull request in the course’s GitHub repository to add your post to the blog.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#remember-that-project-proposals-are-due-oct-22",
    "href": "lectures/llvm.html#remember-that-project-proposals-are-due-oct-22",
    "title": "using llvm",
    "section": "",
    "text": "At the end of the course, you’ll do a language implementation research project. This is an open-ended and open-source project that can be on any topic that you can construe as being about compiler hacking. The final product is an experience report on the course blog where you rigorously evaluate the success of your implementation.\nYou can work individually or in groups of 2–3 people.\nProposal\nanswer these three questions\n\nWhat will you do?\nHow will you do it?\nHow will you empirically measure success?\n\nI will have feedback on how to approach your project.\nImplementation\nThe main phase, of course, is implementing the thing you said you would implement. I recommend you keep a “lab notebook” to log your thoughts, attempts, and frustrations—this will come in handy for the report you’ll write about the project.\nEvaluation\nA major part of your project is an empirical evaluation. To design your evaluation strategy, you will need to consider at least these things:\n\nWhere will you get the input code you’ll use in your evaluation?\nHow will you check the correctness of your implementation? If you’ve implemented an optimization, for example, “correctness” means that the transformed programs behave the same way as the original programs.\nHow will you measure the benefit (in performance, energy, complexity, etc.) of your implementation?\nHow will you present the data you collect from your empirical evaluation?\n\nOther questions may be relevant depending on the project you choose. Consider the SIGPLAN empirical evaluation guidelines when you design your methodology.\nExperience Report\nFor the main project deadline, you will write up the project’s outcomes in the form of a post on the course blog. Your writeup should answer these questions:\n\nWhat was the goal?\nWhat did you do? (Include both the design and the implementation.)\nWhat were the hardest parts to get right?\nWere you successful? (Report rigorously on your empirical evaluation.)\n\nTo submit your report, open a pull request in the course’s GitHub repository to add your post to the blog.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#using-llvm",
    "href": "lectures/llvm.html#using-llvm",
    "title": "using llvm",
    "section": "Using LLVM",
    "text": "Using LLVM\nhandy links\nAdrians tutorial\nskeleton code\nllvm doc\nllvm programmers guide\ngoogle, github pilot and chatgpt are very useful.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#install-clang-and-cmake",
    "href": "lectures/llvm.html#install-clang-and-cmake",
    "title": "using llvm",
    "section": "install clang and cmake",
    "text": "install clang and cmake\nhow to install clang and cmake\nsudo apt install clang cmake",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#llvm-ir",
    "href": "lectures/llvm.html#llvm-ir",
    "title": "using llvm",
    "section": "llvm ir",
    "text": "llvm ir\nclang -emit-llvm -S -o - temp.c\n\n; ModuleID = 'temp.c'\nsource_filename = \"temp.c\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n; Function Attrs: noinline nounwind optnone uwtable\ndefine dso_local i32 @main(i32 noundef %0, i8** noundef %1) #0 {\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  ret i32 %6\n}\ncompared to bril\nmore complex types, variables are %n, assumes a stack, must be in ssa",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#using-copilot",
    "href": "lectures/llvm.html#using-copilot",
    "title": "using llvm",
    "section": "using copilot",
    "text": "using copilot\nwhat does the llvm alloca do\nIn LLVM, the alloca instruction is used to allocate memory on the stack. It stands for “allocate” and is similar to the alloca function in C. The memory allocated by alloca is automatically freed when the function returns, making it suitable for allocating temporary storage within a function.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#alloca",
    "href": "lectures/llvm.html#alloca",
    "title": "using llvm",
    "section": "alloca",
    "text": "alloca\nExplanation of alloca Instruction\nSyntax: alloca &lt;type&gt;, &lt;num_elements&gt;, &lt;alignment&gt;\n&lt;type&gt;: The type of the elements to be allocated.\n&lt;num_elements&gt;: The number of elements to allocate (optional, defaults to 1).\n&lt;alignment&gt;: The alignment of the allocated memory (optional).",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#example",
    "href": "lectures/llvm.html#example",
    "title": "using llvm",
    "section": "example",
    "text": "example\nExplanation:\n%3 = alloca i32, align 4:\n\nAllocates memory for a single i32 (32-bit integer) on the stack.\nThe allocated memory is aligned to a 4-byte boundary.\nThe result is a pointer to the allocated memory, stored in the variable %3.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#example-2",
    "href": "lectures/llvm.html#example-2",
    "title": "using llvm",
    "section": "example 2",
    "text": "example 2\n%4 = alloca i32, align 4:\n\nSimilar to the first instruction, this allocates memory for another i32 on the stack with 4-byte alignment.\nThe result is stored in the variable %4.\n\n\n%5 = alloca i8**, align 8:\n\nAllocates memory for a single i8** (pointer to a pointer to an 8-bit integer) on the stack.\nThe allocated memory is aligned to an 8-byte boundary.\nThe result is stored in the variable %5.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#allocate-usage",
    "href": "lectures/llvm.html#allocate-usage",
    "title": "using llvm",
    "section": "allocate usage",
    "text": "allocate usage\nUsage:\nTemporary Storage: alloca is typically used for temporary storage within a function, such as local variables.\nAutomatic Deallocation: The memory allocated by alloca is automatically deallocated when the function returns, so there is no need to manually free it.\nExample in Context:\nIn the provided LLVM IR code, the alloca instructions are used to allocate space for local variables %3, %4, and %5 on the stack. These variables are then used to store values and perform operations within the main function.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#allocate",
    "href": "lectures/llvm.html#allocate",
    "title": "using llvm",
    "section": "allocate",
    "text": "allocate\nIn this function:\n\n%3 is allocated for an i32 and initialized to 0.\n%4 is allocated for an i32 and initialized with the value of the first argument %0.\n%5 is allocated for an i8** and initialized with the value of the second argument %1.\nThe value stored in %4 is loaded into %6 and returned.\nThis demonstrates how alloca is used to manage local variables within an LLVM function.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#llvm-classes",
    "href": "lectures/llvm.html#llvm-classes",
    "title": "using llvm",
    "section": "llvm classes",
    "text": "llvm classes\nwhat is an llvm value\nIn LLVM, a Value is a fundamental class that represents a typed constant or variable. It is the base class for all values computed by a program that may be used as operands to other values. This includes constants, instructions, function arguments, and more.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#value-continued",
    "href": "lectures/llvm.html#value-continued",
    "title": "using llvm",
    "section": "value continued",
    "text": "value continued\nKey Points about llvm::Value:\nBase Class: llvm::Value is the base class for many other classes in LLVM, such as llvm::Instruction, llvm::Constant, llvm::Argument, and llvm::GlobalVariable.\nTyped: Every Value has a type, represented by the llvm::Type class. This type information is crucial for type checking and code generation.\nUse-Def Chain: Value objects maintain a list of uses, which are the places where the value is used. This is part of the use-def (use-definition) chain, which is important for optimizations and transformations.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#value-continued-1",
    "href": "lectures/llvm.html#value-continued-1",
    "title": "using llvm",
    "section": "value continued",
    "text": "value continued\nCommon Subclasses of llvm::Value:\nllvm::Instruction: Represents an individual instruction in the LLVM IR.\nllvm::Constant: Represents a constant value, such as an integer or floating-point constant.\nllvm::Argument: Represents an argument to a function.\nllvm::GlobalVariable: Represents a global variable.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#llvm-classes-llvm-is-c-but-does-not-use-standard-library",
    "href": "lectures/llvm.html#llvm-classes-llvm-is-c-but-does-not-use-standard-library",
    "title": "using llvm",
    "section": "llvm classes (llvm is c++ but does not use standard library)",
    "text": "llvm classes (llvm is c++ but does not use standard library)\n\nllvm does not use char* or std::string, it has something else called a StringRef.\nthere is no std::cout or std::cerr there are outs(), errs()\nlot of built in data structures\ncomplex class hierarchy",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#class-hierarchy",
    "href": "lectures/llvm.html#class-hierarchy",
    "title": "using llvm",
    "section": "class hierarchy",
    "text": "class hierarchy\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\nflowchart TD;\nValue --&gt; Argument ;\nValue --&gt; other[\"...\"];\nValue --&gt; User;\nUser --&gt; Constant\nUser--&gt; Operator\nUser--&gt; Instruction\nConstant --&gt; ConstantExpr\nConstant--&gt; ConstantData\nOperator--&gt; ConcreteOperator\nInstruction--&gt; UnaryInst\nConstantData --&gt; ConstantInt\nConstantData --&gt; UndefValue\nInstruction --&gt; BinaryOperator\nInstruction--&gt; CallBase\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\nflowchart TD;\nValue --&gt; Argument ;\nValue --&gt; other[\"...\"];\nValue --&gt; User;\nUser --&gt; Constant\nUser--&gt; Operator\nUser--&gt; Instruction\nConstant --&gt; ConstantExpr\nConstant--&gt; ConstantData\nOperator--&gt; ConcreteOperator\nInstruction--&gt; UnaryInst\nConstantData --&gt; ConstantInt\nConstantData --&gt; UndefValue\nInstruction --&gt; BinaryOperator\nInstruction--&gt; CallBase\n\n\n\n\n\n\nInstructions are a kind of Value, since everything is in SSA form, operands are pointers to instructions",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#plugins",
    "href": "lectures/llvm.html#plugins",
    "title": "using llvm",
    "section": "plugins",
    "text": "plugins\nAn LLVM plugin is a shared library that can add additional functionality to the LLVM infrastructure. Plugins can be used to add new passes, analyses, targets, and more.\nPlugins are dynamically loaded into LLVM. Once loaded, a plugin can register new command-line options, passes, etc., that are then available for use in that invocation of the tool.\nThe advantage for us is that using a plugin means you do not have to ever build llvm from source.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#pass-starter",
    "href": "lectures/llvm.html#pass-starter",
    "title": "using llvm",
    "section": "pass starter",
    "text": "pass starter\nThere is a cs6120 package that makes setting up the build process for plugins simpler\npass starter\nThis has branches\nmaster - prints names of functions\ncontainers - prints everything\nmutate - changes the code\nrtlib - easier way to insert code with needing irbuilder",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#using-llvm-branches",
    "href": "lectures/llvm.html#using-llvm-branches",
    "title": "using llvm",
    "section": "using llvm branches",
    "text": "using llvm branches\nTo clone a specific branch from a GitHub repository, you can use the git clone command with the -b option followed by the branch name and the repository URL. Here is the syntax:\ngit clone -b   \nto switch branches\ngit fetch –all\ngit checkout",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#using-the-master-branch",
    "href": "lectures/llvm.html#using-the-master-branch",
    "title": "using llvm",
    "section": "using the master branch",
    "text": "using the master branch\ngit clone https://github.com/sampsyo/llvm-pass-skeleton\nls gives\nCMakeLists.txt LICENSE README.md skeleton\nls skeleton CMakeLists.txt Skeleton.cpp",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#skeleton.cpp",
    "href": "lectures/llvm.html#skeleton.cpp",
    "title": "using llvm",
    "section": "Skeleton.cpp",
    "text": "Skeleton.cpp\n#include \"llvm/Pass.h\"\n#include \"llvm/Passes/PassBuilder.h\"\n#include \"llvm/Passes/PassPlugin.h\"\n#include \"llvm/Support/raw_ostream.h\"\n\nusing namespace llvm;\n\nnamespace {\n\nstruct SkeletonPass : public PassInfoMixin&lt;SkeletonPass&gt; {\n    PreservedAnalyses run(Module &M, ModuleAnalysisManager &AM) {\n        for (auto &F : M) {\n            errs() &lt;&lt; \"I saw a function called \" &lt;&lt; F.getName() &lt;&lt; \"!\\n\";\n        }\n        return PreservedAnalyses::all();\n    };\n};\n\n}\n\nextern \"C\" LLVM_ATTRIBUTE_WEAK ::llvm::PassPluginLibraryInfo\nllvmGetPassPluginInfo() {\n    return {\n        .APIVersion = LLVM_PLUGIN_API_VERSION,\n        .PluginName = \"Skeleton pass\",\n        .PluginVersion = \"v0.1\",\n        .RegisterPassBuilderCallbacks = [](PassBuilder &PB) {\n            PB.registerPipelineStartEPCallback(\n                [](ModulePassManager &MPM, OptimizationLevel Level) {\n                    MPM.addPass(SkeletonPass());\n                });\n        }\n    };\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#how-to-build-this",
    "href": "lectures/llvm.html#how-to-build-this",
    "title": "using llvm",
    "section": "how to build this",
    "text": "how to build this\n~/llvm/llvm-pass-skeleton$ mkdir build\ncd build \ncmake ..\nThis generates build/skeleton/SkeletonPass.so",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#how-to-run-a-plugin",
    "href": "lectures/llvm.html#how-to-run-a-plugin",
    "title": "using llvm",
    "section": "how to run a plugin",
    "text": "how to run a plugin\nto run this\nclang -fpass-plugin=llvm-pass-skeleton/build/skeleton/SkeletonPass.so a.cpp\ncreates a.out and prints out info from the pass\nto make this easier to use you might create a bash script or a makefile",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#look-at-the-post-on-containers",
    "href": "lectures/llvm.html#look-at-the-post-on-containers",
    "title": "using llvm",
    "section": "look at the post on containers",
    "text": "look at the post on containers",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#getting-more-info",
    "href": "lectures/llvm.html#getting-more-info",
    "title": "using llvm",
    "section": "getting more info",
    "text": "getting more info\nSome helpful llvm operations\nerrs() &lt;&lt; \"function Body:\\n\";\nerrs() &lt;&lt; F &lt;&lt; '\\n\"; '\noutput\nI saw a function called main!\nfunction Body:\n; Function Attrs: noinline nounwind optnone uwtable\ndefine dso_local i32 @main(i32 noundef %0, i8** noundef %1) #0 {\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  ret i32 %6\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#entering-containers",
    "href": "lectures/llvm.html#entering-containers",
    "title": "using llvm",
    "section": "entering containers",
    "text": "entering containers\nPreservedAnalyses run(Module &M, ModuleAnalysisManager &AM)\n        {\n            for (auto &F : M)\n            {\n                errs() &lt;&lt; \"I saw a function called \" &lt;&lt; F.getName() &lt;&lt; \"!\\n\";\n                for (auto &B : F)\n                {\n                    errs() &lt;&lt; \"starting block\\n\";\n                    errs() &lt;&lt; B &lt;&lt; \"\\n\";\n                    for (auto &I : B)\n                    {\n                        errs() &lt;&lt; \"instr:\" &lt;&lt; I &lt;&lt; \"\\n\";\n                    }\n                };\n            };\n            return PreservedAnalyses::all();\n        };\n\nI saw a function called main!\nstarting block\n\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  ret i32 %6\n\ninstr:  %3 = alloca i32, align 4\ninstr:  %4 = alloca i32, align 4\ninstr:  %5 = alloca i8**, align 8\ninstr:  store i32 0, i32* %3, align 4\ninstr:  store i32 %0, i32* %4, align 4\ninstr:  store i8** %1, i8*** %5, align 8\ninstr:  %6 = load i32, i32* %4, align 4\ninstr:  ret i32 %6",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#modify-the-program",
    "href": "lectures/llvm.html#modify-the-program",
    "title": "using llvm",
    "section": "modify the program",
    "text": "modify the program\nfind the binary operators and change them to multiplies\n#// temp.c\nint main(int argc, char **argv)\n{\n       return argc + 5;\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#find-all-the-binary-operators",
    "href": "lectures/llvm.html#find-all-the-binary-operators",
    "title": "using llvm",
    "section": "find all the binary operators",
    "text": "find all the binary operators\nhow do i find all the binary operators in a llvm plugin\nto find all the binary operators in an LLVM plugin, you can iterate over all instructions in each function and use the isa&lt;BinaryOperator&gt; function to check if an instruction is a binary operator. If it is, you can then cast it to a BinaryOperator using dyn_cast&lt;BinaryOperator&gt; and process it as needed.\n\nHere is an example of how you can modify your LLVM plugin to find and print all binary operators:",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#plugin",
    "href": "lectures/llvm.html#plugin",
    "title": "using llvm",
    "section": "plugin",
    "text": "plugin\nPreservedAnalyses run(Module &M, ModuleAnalysisManager &AM)\n    {\n        for (auto &F : M)\n        {\n            errs() &lt;&lt; \"I saw a function called \" &lt;&lt; F.getName() &lt;&lt; \"!\\n\";\n            for (auto &B : F)\n            {\n                for (auto &I : B)\n                {\n                    if (isa&lt;BinaryOperator&gt;(&I))\n                    {\n                        errs() &lt;&lt; \"instr:\" &lt;&lt; I &lt;&lt; \"\\n\";\n                    }\n                    auto *op = dyn_cast&lt;BinaryOperator&gt;(&I);\n                    errs() &lt;&lt; \"from_op:\" &lt;&lt; op &lt;&lt; \"\\n\";\n                }\n            };\n        };\n        return PreservedAnalyses::all();\n    }\n\noutput\nI saw a function called main!\nI saw a function called main!\nfrom_op:0x0\nfrom_op:0x0\nfrom_op:0x0\nfrom_op:0x0\nfrom_op:0x0\nfrom_op:0x0\nfrom_op:0x0\ninstr:  %7 = add nsw i32 %6, 5\nfrom_op:0xb58610\nfrom_op:0x0",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#getting-analysis-info",
    "href": "lectures/llvm.html#getting-analysis-info",
    "title": "using llvm",
    "section": "getting analysis info",
    "text": "getting analysis info\n   PreservedAnalyses run(Module &M, ModuleAnalysisManager &MAM) {\n        // Get the FunctionAnalysisManager.\n        FunctionAnalysisManager &FAM = MAM.getResult&lt;FunctionAnalysisManagerModuleProxy&gt;(M).getManager();\n\n        for (Function &F : M) {\n            // Skip external functions.\n            if (F.isDeclaration()) continue;\n\n            // Get the DominatorTree for the function.\n            DominatorTree &DT = FAM.getResult&lt;DominatorTreeAnalysis&gt;(F);\n\n            // Print the dominator tree.\n            errs() &lt;&lt; \"Dominator Tree for function: \" &lt;&lt; F.getName() &lt;&lt; \"\\n\";\n            DT.print(errs());\n        }",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#irbuilder",
    "href": "lectures/llvm.html#irbuilder",
    "title": "using llvm",
    "section": "irbuilder",
    "text": "irbuilder\nerrs() &lt;&lt; \"I saw a function called \" &lt;&lt; F.getName() &lt;&lt; \"!\\n\";\nfor (auto &B : F){\n    errs() &lt;&lt; B &lt;&lt; \"\\n\";\n    for (auto &I : B){\n        if (auto *op = dyn_cast&lt;BinaryOperator&gt;(&I)){\n            errs() &lt;&lt; \"old inst \" &lt;&lt; *op &lt;&lt; \"\\n\";\n            IRBuilder&lt;&gt; builder(op);\n            Value *left = op-&gt;getOperand(0);  // first operand\n            Value *right = op-&gt;getOperand(1); // second operad\n            Value *mul = builder.CreateMul(left, right);\n\n            errs() &lt;&lt; \"new inst:\" &lt;&lt; *mul &lt;&lt; \"\\n\";\n\n            errs() &lt;&lt; B &lt;&lt; \"\\n\";\n\n            // replace uses of op with mul\n            for (auto &U : op-&gt;uses()){\n                int num = U.getOperandNo(); // which argument\n                User *user = U.getUser();   // the instruction with the use\n                errs() &lt;&lt; \" user:\" &lt;&lt; *user &lt;&lt; \"   \";\n                user-&gt;setOperand(num, mul);\n                errs() &lt;&lt; *user &lt;&lt; \"\\n\";\n                }\n            }\n        }\n    }\n};\nreturn PreservedAnalyses::none();",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#output",
    "href": "lectures/llvm.html#output",
    "title": "using llvm",
    "section": "output",
    "text": "output\nI saw a function called main!\n\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  %7 = add nsw i32 %6, 5\n  ret i32 %7",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#output-2",
    "href": "lectures/llvm.html#output-2",
    "title": "using llvm",
    "section": "output 2",
    "text": "output 2\n\nold inst   %7 = add nsw i32 %6, 5\nnew inst:  %7 = mul i32 %6, 5\n\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  %7 = mul i32 %6, 5\n  %8 = add nsw i32 %6, 5\n  ret i32 %8\n\n user:  ret i32 %8     ret i32 %7",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#more-complex-transforms---for-instrumentation",
    "href": "lectures/llvm.html#more-complex-transforms---for-instrumentation",
    "title": "using llvm",
    "section": "more complex transforms - for instrumentation",
    "text": "more complex transforms - for instrumentation\ninstrumentation code in c not using builder\ninsert calls to functions and link them in\nusing IRBuilder is a mess, So I’m going to show a trick that makes it much simpler",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#chat-gpt",
    "href": "lectures/llvm.html#chat-gpt",
    "title": "using llvm",
    "section": "chat gpt",
    "text": "chat gpt\nhow do i write an instrumentation function in c for llvm, use a plugin pass to insert a call to the instrumentation routine\n\nWrite the Instrumentation Function in C: Create a C function that you want to call from your LLVM pass.\nCreate an LLVM Pass: Write an LLVM pass that inserts a call to the instrumentation function.\nBuild and Load the Pass: Compile the pass and load it using the opt tool or integrate it into your build system.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#instrumentation-routine",
    "href": "lectures/llvm.html#instrumentation-routine",
    "title": "using llvm",
    "section": "instrumentation routine",
    "text": "instrumentation routine\nvoid instrument_function(const char* bb_name) {\n    printf(\"Instrumentation function called for basic block: %s\\n\", bb_name);\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#pass",
    "href": "lectures/llvm.html#pass",
    "title": "using llvm",
    "section": "pass",
    "text": "pass\n   PreservedAnalyses run(Module &M, ModuleAnalysisManager &MAM) {\n        LLVMContext &Ctx = M.getContext();\n        IRBuilder&lt;&gt; Builder(Ctx);\n\n        // Declare the instrumentation function as an external function\n        FunctionType *FuncType = FunctionType::get(Type::getVoidTy(Ctx), Type::getInt8PtrTy(Cctx), false);\n        FunctionCallee InstrumentFunc =\n                  M.getOrInsertFunction(\"instrument_function\", FuncType);\n\n        for (Function &F : M) {\n\n            // Insert the call at the beginning of each basic block\n            for (BasicBlock &BB : F) {\n                Builder.SetInsertPoint(&BB, BB.begin());\n\n                // Create a global string for the basic block name\n                Value *BBName = Builder.CreateGlobalStringPtr(BB.getName());\n\n                // Create the call to the instrumentation function\n                Builder.CreateCall(InstrumentFunc, BBName);\n            }\n        }\n\n        return PreservedAnalyses::none();",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#starter-code",
    "href": "lectures/llvm.html#starter-code",
    "title": "using llvm",
    "section": "starter code",
    "text": "starter code\nrm -r llvm-pass-skeleton\ngit clone  -b rtlib  https://github.com/sampsyo/llvm-pass-skeleton.git\ncd llvm-pass-skeleton\nmkdir -p build \ncd build \ncmake ..\nmake",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#adrians-skeleton",
    "href": "lectures/llvm.html#adrians-skeleton",
    "title": "using llvm",
    "section": "Adrians skeleton",
    "text": "Adrians skeleton\nstruct SkeletonPass : public PassInfoMixin&lt;SkeletonPass&gt; {\n    PreservedAnalyses run(Module &M, ModuleAnalysisManager &AM) {\n        for (auto &F : M.functions()) {\n\n            // Get the function to call from our runtime library.\n            LLVMContext &Ctx = F.getContext();\n            std::vector&lt;Type*&gt; paramTypes = {Type::getInt32Ty(Ctx)};\n            Type *retType = Type::getVoidTy(Ctx);\n            FunctionType *logFuncType = FunctionType::get(retType, paramTypes, false);\n            FunctionCallee logFunc =\n                F.getParent()-&gt;getOrInsertFunction(\"logop\", logFuncType);\n\n            for (auto &B : F) {\n                for (auto &I : B) {\n                    if (auto *op = dyn_cast&lt;BinaryOperator&gt;(&I)) {\n                        // Insert *after* `op`.\n                        IRBuilder&lt;&gt; builder(op);\n                        builder.SetInsertPoint(&B, ++builder.GetInsertPoint());\n\n                        // Insert a call to our function.\n                        Value* args[] = {op};\n                        builder.CreateCall(logFunc, args);\n\n                        return PreservedAnalyses::none();\n                    }\n                }\n            }\n\n        }\n        return PreservedAnalyses::all();\n    }\n};",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#adrians-instrumentation-code",
    "href": "lectures/llvm.html#adrians-instrumentation-code",
    "title": "using llvm",
    "section": "Adrians instrumentation code",
    "text": "Adrians instrumentation code\n#include &lt;stdio.h&gt;\nvoid logop(int i) {\n    printf(\"computed: %i\\n\", i);\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#link-together",
    "href": "lectures/llvm.html#link-together",
    "title": "using llvm",
    "section": "link together",
    "text": "link together\ncd llvm-pass-skeleton\ncc -c rtlib.c --- generates rtlib.o \nclang  -fpass-plugin=build/skeleton/SkeletonPass.so -c test.cpp   -- generates test.o\ncc test.o rtlib.o  -- links it together \n./a.out     --   runs it",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/llvm.html#homework",
    "href": "lectures/llvm.html#homework",
    "title": "using llvm",
    "section": "Homework",
    "text": "Homework\nFollow the LLVM tutorial blog post far enough to implement a pass that changes program execution.\nThis is intentionally open-ended. You can be as ambitious or as unambitious as you want. An example of an unambitious but acceptable task would be to print out a message every time the program uses floating-point division.\nAn example of an ambitious task would be to implement an optimization on LLVM IR and make sure it speeds things up in actual wall-clock time execution.\nFind a real-ish C/C++ program somewhere and run your pass on it to observe the results.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "using llvm"
    ]
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#remember-that-project-proposals-are-due-oct-22",
    "href": "lectures/revealjs_llvm.qmd.html#remember-that-project-proposals-are-due-oct-22",
    "title": "using llvm",
    "section": "remember that project proposals are due oct 22!",
    "text": "remember that project proposals are due oct 22!\nAt the end of the course, you’ll do a language implementation research project. This is an open-ended and open-source project that can be on any topic that you can construe as being about compiler hacking. The final product is an experience report on the course blog where you rigorously evaluate the success of your implementation.\nYou can work individually or in groups of 2–3 people.\nProposal\nanswer these three questions\n\nWhat will you do?\nHow will you do it?\nHow will you empirically measure success?\n\nI will have feedback on how to approach your project.\nImplementation\nThe main phase, of course, is implementing the thing you said you would implement. I recommend you keep a “lab notebook” to log your thoughts, attempts, and frustrations—this will come in handy for the report you’ll write about the project.\nEvaluation\nA major part of your project is an empirical evaluation. To design your evaluation strategy, you will need to consider at least these things:\n\nWhere will you get the input code you’ll use in your evaluation?\nHow will you check the correctness of your implementation? If you’ve implemented an optimization, for example, “correctness” means that the transformed programs behave the same way as the original programs.\nHow will you measure the benefit (in performance, energy, complexity, etc.) of your implementation?\nHow will you present the data you collect from your empirical evaluation?\n\nOther questions may be relevant depending on the project you choose. Consider the SIGPLAN empirical evaluation guidelines when you design your methodology.\nExperience Report\nFor the main project deadline, you will write up the project’s outcomes in the form of a post on the course blog. Your writeup should answer these questions:\n\nWhat was the goal?\nWhat did you do? (Include both the design and the implementation.)\nWhat were the hardest parts to get right?\nWere you successful? (Report rigorously on your empirical evaluation.)\n\nTo submit your report, open a pull request in the course’s GitHub repository to add your post to the blog."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#using-llvm",
    "href": "lectures/revealjs_llvm.qmd.html#using-llvm",
    "title": "using llvm",
    "section": "Using LLVM",
    "text": "Using LLVM\nhandy links\nAdrians tutorial\nskeleton code\nllvm doc\nllvm programmers guide\ngoogle, github pilot and chatgpt are very useful."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#install-clang-and-cmake",
    "href": "lectures/revealjs_llvm.qmd.html#install-clang-and-cmake",
    "title": "using llvm",
    "section": "install clang and cmake",
    "text": "install clang and cmake\nhow to install clang and cmake\nsudo apt install clang cmake"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#llvm-ir",
    "href": "lectures/revealjs_llvm.qmd.html#llvm-ir",
    "title": "using llvm",
    "section": "llvm ir",
    "text": "llvm ir\nclang -emit-llvm -S -o - temp.c\n\n; ModuleID = 'temp.c'\nsource_filename = \"temp.c\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n; Function Attrs: noinline nounwind optnone uwtable\ndefine dso_local i32 @main(i32 noundef %0, i8** noundef %1) #0 {\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  ret i32 %6\n}\ncompared to bril\nmore complex types, variables are %n, assumes a stack, must be in ssa"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#using-copilot",
    "href": "lectures/revealjs_llvm.qmd.html#using-copilot",
    "title": "using llvm",
    "section": "using copilot",
    "text": "using copilot\nwhat does the llvm alloca do\nIn LLVM, the alloca instruction is used to allocate memory on the stack. It stands for “allocate” and is similar to the alloca function in C. The memory allocated by alloca is automatically freed when the function returns, making it suitable for allocating temporary storage within a function."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#alloca",
    "href": "lectures/revealjs_llvm.qmd.html#alloca",
    "title": "using llvm",
    "section": "alloca",
    "text": "alloca\nExplanation of alloca Instruction\nSyntax: alloca &lt;type&gt;, &lt;num_elements&gt;, &lt;alignment&gt;\n&lt;type&gt;: The type of the elements to be allocated.\n&lt;num_elements&gt;: The number of elements to allocate (optional, defaults to 1).\n&lt;alignment&gt;: The alignment of the allocated memory (optional)."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#example",
    "href": "lectures/revealjs_llvm.qmd.html#example",
    "title": "using llvm",
    "section": "example",
    "text": "example\nExplanation:\n%3 = alloca i32, align 4:\n\nAllocates memory for a single i32 (32-bit integer) on the stack.\nThe allocated memory is aligned to a 4-byte boundary.\nThe result is a pointer to the allocated memory, stored in the variable %3."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#example-2",
    "href": "lectures/revealjs_llvm.qmd.html#example-2",
    "title": "using llvm",
    "section": "example 2",
    "text": "example 2\n%4 = alloca i32, align 4:\n\nSimilar to the first instruction, this allocates memory for another i32 on the stack with 4-byte alignment.\nThe result is stored in the variable %4.\n\n\n%5 = alloca i8**, align 8:\n\nAllocates memory for a single i8** (pointer to a pointer to an 8-bit integer) on the stack.\nThe allocated memory is aligned to an 8-byte boundary.\nThe result is stored in the variable %5."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#allocate-usage",
    "href": "lectures/revealjs_llvm.qmd.html#allocate-usage",
    "title": "using llvm",
    "section": "allocate usage",
    "text": "allocate usage\nUsage:\nTemporary Storage: alloca is typically used for temporary storage within a function, such as local variables.\nAutomatic Deallocation: The memory allocated by alloca is automatically deallocated when the function returns, so there is no need to manually free it.\nExample in Context:\nIn the provided LLVM IR code, the alloca instructions are used to allocate space for local variables %3, %4, and %5 on the stack. These variables are then used to store values and perform operations within the main function."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#allocate",
    "href": "lectures/revealjs_llvm.qmd.html#allocate",
    "title": "using llvm",
    "section": "allocate",
    "text": "allocate\nIn this function:\n\n%3 is allocated for an i32 and initialized to 0.\n%4 is allocated for an i32 and initialized with the value of the first argument %0.\n%5 is allocated for an i8** and initialized with the value of the second argument %1.\nThe value stored in %4 is loaded into %6 and returned.\nThis demonstrates how alloca is used to manage local variables within an LLVM function."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#llvm-classes",
    "href": "lectures/revealjs_llvm.qmd.html#llvm-classes",
    "title": "using llvm",
    "section": "llvm classes",
    "text": "llvm classes\nwhat is an llvm value\nIn LLVM, a Value is a fundamental class that represents a typed constant or variable. It is the base class for all values computed by a program that may be used as operands to other values. This includes constants, instructions, function arguments, and more."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#value-continued",
    "href": "lectures/revealjs_llvm.qmd.html#value-continued",
    "title": "using llvm",
    "section": "value continued",
    "text": "value continued\nKey Points about llvm::Value:\nBase Class: llvm::Value is the base class for many other classes in LLVM, such as llvm::Instruction, llvm::Constant, llvm::Argument, and llvm::GlobalVariable.\nTyped: Every Value has a type, represented by the llvm::Type class. This type information is crucial for type checking and code generation.\nUse-Def Chain: Value objects maintain a list of uses, which are the places where the value is used. This is part of the use-def (use-definition) chain, which is important for optimizations and transformations."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#value-continued-1",
    "href": "lectures/revealjs_llvm.qmd.html#value-continued-1",
    "title": "using llvm",
    "section": "value continued",
    "text": "value continued\nCommon Subclasses of llvm::Value:\nllvm::Instruction: Represents an individual instruction in the LLVM IR.\nllvm::Constant: Represents a constant value, such as an integer or floating-point constant.\nllvm::Argument: Represents an argument to a function.\nllvm::GlobalVariable: Represents a global variable."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#llvm-classes-llvm-is-c-but-does-not-use-standard-library",
    "href": "lectures/revealjs_llvm.qmd.html#llvm-classes-llvm-is-c-but-does-not-use-standard-library",
    "title": "using llvm",
    "section": "llvm classes (llvm is c++ but does not use standard library)",
    "text": "llvm classes (llvm is c++ but does not use standard library)\n\nllvm does not use char* or std::string, it has something else called a StringRef.\nthere is no std::cout or std::cerr there are outs(), errs()\nlot of built in data structures\ncomplex class hierarchy"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#class-hierarchy",
    "href": "lectures/revealjs_llvm.qmd.html#class-hierarchy",
    "title": "using llvm",
    "section": "class hierarchy",
    "text": "class hierarchy\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\nflowchart TD;\nValue --&gt; Argument ;\nValue --&gt; other[\"...\"];\nValue --&gt; User;\nUser --&gt; Constant\nUser--&gt; Operator\nUser--&gt; Instruction\nConstant --&gt; ConstantExpr\nConstant--&gt; ConstantData\nOperator--&gt; ConcreteOperator\nInstruction--&gt; UnaryInst\nConstantData --&gt; ConstantInt\nConstantData --&gt; UndefValue\nInstruction --&gt; BinaryOperator\nInstruction--&gt; CallBase\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\nflowchart TD;\nValue --&gt; Argument ;\nValue --&gt; other[\"...\"];\nValue --&gt; User;\nUser --&gt; Constant\nUser--&gt; Operator\nUser--&gt; Instruction\nConstant --&gt; ConstantExpr\nConstant--&gt; ConstantData\nOperator--&gt; ConcreteOperator\nInstruction--&gt; UnaryInst\nConstantData --&gt; ConstantInt\nConstantData --&gt; UndefValue\nInstruction --&gt; BinaryOperator\nInstruction--&gt; CallBase\n\n\n\n\n\n\nInstructions are a kind of Value, since everything is in SSA form, operands are pointers to instructions"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#plugins",
    "href": "lectures/revealjs_llvm.qmd.html#plugins",
    "title": "using llvm",
    "section": "plugins",
    "text": "plugins\nAn LLVM plugin is a shared library that can add additional functionality to the LLVM infrastructure. Plugins can be used to add new passes, analyses, targets, and more.\nPlugins are dynamically loaded into LLVM. Once loaded, a plugin can register new command-line options, passes, etc., that are then available for use in that invocation of the tool.\nThe advantage for us is that using a plugin means you do not have to ever build llvm from source."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#pass-starter",
    "href": "lectures/revealjs_llvm.qmd.html#pass-starter",
    "title": "using llvm",
    "section": "pass starter",
    "text": "pass starter\nThere is a cs6120 package that makes setting up the build process for plugins simpler\npass starter\nThis has branches\nmaster - prints names of functions\ncontainers - prints everything\nmutate - changes the code\nrtlib - easier way to insert code with needing irbuilder"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#using-llvm-branches",
    "href": "lectures/revealjs_llvm.qmd.html#using-llvm-branches",
    "title": "using llvm",
    "section": "using llvm branches",
    "text": "using llvm branches\nTo clone a specific branch from a GitHub repository, you can use the git clone command with the -b option followed by the branch name and the repository URL. Here is the syntax:\ngit clone -b   \nto switch branches\ngit fetch –all\ngit checkout"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#using-the-master-branch",
    "href": "lectures/revealjs_llvm.qmd.html#using-the-master-branch",
    "title": "using llvm",
    "section": "using the master branch",
    "text": "using the master branch\ngit clone https://github.com/sampsyo/llvm-pass-skeleton\nls gives\nCMakeLists.txt LICENSE README.md skeleton\nls skeleton CMakeLists.txt Skeleton.cpp"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#skeleton.cpp",
    "href": "lectures/revealjs_llvm.qmd.html#skeleton.cpp",
    "title": "using llvm",
    "section": "Skeleton.cpp",
    "text": "Skeleton.cpp\n#include \"llvm/Pass.h\"\n#include \"llvm/Passes/PassBuilder.h\"\n#include \"llvm/Passes/PassPlugin.h\"\n#include \"llvm/Support/raw_ostream.h\"\n\nusing namespace llvm;\n\nnamespace {\n\nstruct SkeletonPass : public PassInfoMixin&lt;SkeletonPass&gt; {\n    PreservedAnalyses run(Module &M, ModuleAnalysisManager &AM) {\n        for (auto &F : M) {\n            errs() &lt;&lt; \"I saw a function called \" &lt;&lt; F.getName() &lt;&lt; \"!\\n\";\n        }\n        return PreservedAnalyses::all();\n    };\n};\n\n}\n\nextern \"C\" LLVM_ATTRIBUTE_WEAK ::llvm::PassPluginLibraryInfo\nllvmGetPassPluginInfo() {\n    return {\n        .APIVersion = LLVM_PLUGIN_API_VERSION,\n        .PluginName = \"Skeleton pass\",\n        .PluginVersion = \"v0.1\",\n        .RegisterPassBuilderCallbacks = [](PassBuilder &PB) {\n            PB.registerPipelineStartEPCallback(\n                [](ModulePassManager &MPM, OptimizationLevel Level) {\n                    MPM.addPass(SkeletonPass());\n                });\n        }\n    };\n}"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#how-to-build-this",
    "href": "lectures/revealjs_llvm.qmd.html#how-to-build-this",
    "title": "using llvm",
    "section": "how to build this",
    "text": "how to build this\n~/llvm/llvm-pass-skeleton$ mkdir build\ncd build \ncmake ..\nThis generates build/skeleton/SkeletonPass.so"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#how-to-run-a-plugin",
    "href": "lectures/revealjs_llvm.qmd.html#how-to-run-a-plugin",
    "title": "using llvm",
    "section": "how to run a plugin",
    "text": "how to run a plugin\nto run this\nclang -fpass-plugin=llvm-pass-skeleton/build/skeleton/SkeletonPass.so a.cpp\ncreates a.out and prints out info from the pass\nto make this easier to use you might create a bash script or a makefile"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#look-at-the-post-on-containers",
    "href": "lectures/revealjs_llvm.qmd.html#look-at-the-post-on-containers",
    "title": "using llvm",
    "section": "look at the post on containers",
    "text": "look at the post on containers"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#getting-more-info",
    "href": "lectures/revealjs_llvm.qmd.html#getting-more-info",
    "title": "using llvm",
    "section": "getting more info",
    "text": "getting more info\nSome helpful llvm operations\nerrs() &lt;&lt; \"function Body:\\n\";\nerrs() &lt;&lt; F &lt;&lt; '\\n\"; '\noutput\nI saw a function called main!\nfunction Body:\n; Function Attrs: noinline nounwind optnone uwtable\ndefine dso_local i32 @main(i32 noundef %0, i8** noundef %1) #0 {\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  ret i32 %6\n}"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#entering-containers",
    "href": "lectures/revealjs_llvm.qmd.html#entering-containers",
    "title": "using llvm",
    "section": "entering containers",
    "text": "entering containers\nPreservedAnalyses run(Module &M, ModuleAnalysisManager &AM)\n        {\n            for (auto &F : M)\n            {\n                errs() &lt;&lt; \"I saw a function called \" &lt;&lt; F.getName() &lt;&lt; \"!\\n\";\n                for (auto &B : F)\n                {\n                    errs() &lt;&lt; \"starting block\\n\";\n                    errs() &lt;&lt; B &lt;&lt; \"\\n\";\n                    for (auto &I : B)\n                    {\n                        errs() &lt;&lt; \"instr:\" &lt;&lt; I &lt;&lt; \"\\n\";\n                    }\n                };\n            };\n            return PreservedAnalyses::all();\n        };\n\nI saw a function called main!\nstarting block\n\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  ret i32 %6\n\ninstr:  %3 = alloca i32, align 4\ninstr:  %4 = alloca i32, align 4\ninstr:  %5 = alloca i8**, align 8\ninstr:  store i32 0, i32* %3, align 4\ninstr:  store i32 %0, i32* %4, align 4\ninstr:  store i8** %1, i8*** %5, align 8\ninstr:  %6 = load i32, i32* %4, align 4\ninstr:  ret i32 %6"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#modify-the-program",
    "href": "lectures/revealjs_llvm.qmd.html#modify-the-program",
    "title": "using llvm",
    "section": "modify the program",
    "text": "modify the program\nfind the binary operators and change them to multiplies\n#// temp.c\nint main(int argc, char **argv)\n{\n       return argc + 5;\n}"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#find-all-the-binary-operators",
    "href": "lectures/revealjs_llvm.qmd.html#find-all-the-binary-operators",
    "title": "using llvm",
    "section": "find all the binary operators",
    "text": "find all the binary operators\nhow do i find all the binary operators in a llvm plugin\nto find all the binary operators in an LLVM plugin, you can iterate over all instructions in each function and use the isa&lt;BinaryOperator&gt; function to check if an instruction is a binary operator. If it is, you can then cast it to a BinaryOperator using dyn_cast&lt;BinaryOperator&gt; and process it as needed.\n\nHere is an example of how you can modify your LLVM plugin to find and print all binary operators:"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#plugin",
    "href": "lectures/revealjs_llvm.qmd.html#plugin",
    "title": "using llvm",
    "section": "plugin",
    "text": "plugin\nPreservedAnalyses run(Module &M, ModuleAnalysisManager &AM)\n    {\n        for (auto &F : M)\n        {\n            errs() &lt;&lt; \"I saw a function called \" &lt;&lt; F.getName() &lt;&lt; \"!\\n\";\n            for (auto &B : F)\n            {\n                for (auto &I : B)\n                {\n                    if (isa&lt;BinaryOperator&gt;(&I))\n                    {\n                        errs() &lt;&lt; \"instr:\" &lt;&lt; I &lt;&lt; \"\\n\";\n                    }\n                    auto *op = dyn_cast&lt;BinaryOperator&gt;(&I);\n                    errs() &lt;&lt; \"from_op:\" &lt;&lt; op &lt;&lt; \"\\n\";\n                }\n            };\n        };\n        return PreservedAnalyses::all();\n    }\n\noutput\nI saw a function called main!\nI saw a function called main!\nfrom_op:0x0\nfrom_op:0x0\nfrom_op:0x0\nfrom_op:0x0\nfrom_op:0x0\nfrom_op:0x0\nfrom_op:0x0\ninstr:  %7 = add nsw i32 %6, 5\nfrom_op:0xb58610\nfrom_op:0x0"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#getting-analysis-info",
    "href": "lectures/revealjs_llvm.qmd.html#getting-analysis-info",
    "title": "using llvm",
    "section": "getting analysis info",
    "text": "getting analysis info\n   PreservedAnalyses run(Module &M, ModuleAnalysisManager &MAM) {\n        // Get the FunctionAnalysisManager.\n        FunctionAnalysisManager &FAM = MAM.getResult&lt;FunctionAnalysisManagerModuleProxy&gt;(M).getManager();\n\n        for (Function &F : M) {\n            // Skip external functions.\n            if (F.isDeclaration()) continue;\n\n            // Get the DominatorTree for the function.\n            DominatorTree &DT = FAM.getResult&lt;DominatorTreeAnalysis&gt;(F);\n\n            // Print the dominator tree.\n            errs() &lt;&lt; \"Dominator Tree for function: \" &lt;&lt; F.getName() &lt;&lt; \"\\n\";\n            DT.print(errs());\n        }"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#irbuilder",
    "href": "lectures/revealjs_llvm.qmd.html#irbuilder",
    "title": "using llvm",
    "section": "irbuilder",
    "text": "irbuilder\nerrs() &lt;&lt; \"I saw a function called \" &lt;&lt; F.getName() &lt;&lt; \"!\\n\";\nfor (auto &B : F){\n    errs() &lt;&lt; B &lt;&lt; \"\\n\";\n    for (auto &I : B){\n        if (auto *op = dyn_cast&lt;BinaryOperator&gt;(&I)){\n            errs() &lt;&lt; \"old inst \" &lt;&lt; *op &lt;&lt; \"\\n\";\n            IRBuilder&lt;&gt; builder(op);\n            Value *left = op-&gt;getOperand(0);  // first operand\n            Value *right = op-&gt;getOperand(1); // second operad\n            Value *mul = builder.CreateMul(left, right);\n\n            errs() &lt;&lt; \"new inst:\" &lt;&lt; *mul &lt;&lt; \"\\n\";\n\n            errs() &lt;&lt; B &lt;&lt; \"\\n\";\n\n            // replace uses of op with mul\n            for (auto &U : op-&gt;uses()){\n                int num = U.getOperandNo(); // which argument\n                User *user = U.getUser();   // the instruction with the use\n                errs() &lt;&lt; \" user:\" &lt;&lt; *user &lt;&lt; \"   \";\n                user-&gt;setOperand(num, mul);\n                errs() &lt;&lt; *user &lt;&lt; \"\\n\";\n                }\n            }\n        }\n    }\n};\nreturn PreservedAnalyses::none();"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#output",
    "href": "lectures/revealjs_llvm.qmd.html#output",
    "title": "using llvm",
    "section": "output",
    "text": "output\nI saw a function called main!\n\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  %7 = add nsw i32 %6, 5\n  ret i32 %7"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#output-2",
    "href": "lectures/revealjs_llvm.qmd.html#output-2",
    "title": "using llvm",
    "section": "output 2",
    "text": "output 2\n\nold inst   %7 = add nsw i32 %6, 5\nnew inst:  %7 = mul i32 %6, 5\n\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  %5 = alloca i8**, align 8\n  store i32 0, i32* %3, align 4\n  store i32 %0, i32* %4, align 4\n  store i8** %1, i8*** %5, align 8\n  %6 = load i32, i32* %4, align 4\n  %7 = mul i32 %6, 5\n  %8 = add nsw i32 %6, 5\n  ret i32 %8\n\n user:  ret i32 %8     ret i32 %7"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#more-complex-transforms---for-instrumentation",
    "href": "lectures/revealjs_llvm.qmd.html#more-complex-transforms---for-instrumentation",
    "title": "using llvm",
    "section": "more complex transforms - for instrumentation",
    "text": "more complex transforms - for instrumentation\ninstrumentation code in c not using builder\ninsert calls to functions and link them in\nusing IRBuilder is a mess, So I’m going to show a trick that makes it much simpler"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#chat-gpt",
    "href": "lectures/revealjs_llvm.qmd.html#chat-gpt",
    "title": "using llvm",
    "section": "chat gpt",
    "text": "chat gpt\nhow do i write an instrumentation function in c for llvm, use a plugin pass to insert a call to the instrumentation routine\n\nWrite the Instrumentation Function in C: Create a C function that you want to call from your LLVM pass.\nCreate an LLVM Pass: Write an LLVM pass that inserts a call to the instrumentation function.\nBuild and Load the Pass: Compile the pass and load it using the opt tool or integrate it into your build system."
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#instrumentation-routine",
    "href": "lectures/revealjs_llvm.qmd.html#instrumentation-routine",
    "title": "using llvm",
    "section": "instrumentation routine",
    "text": "instrumentation routine\nvoid instrument_function(const char* bb_name) {\n    printf(\"Instrumentation function called for basic block: %s\\n\", bb_name);\n}"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#pass",
    "href": "lectures/revealjs_llvm.qmd.html#pass",
    "title": "using llvm",
    "section": "pass",
    "text": "pass\n   PreservedAnalyses run(Module &M, ModuleAnalysisManager &MAM) {\n        LLVMContext &Ctx = M.getContext();\n        IRBuilder&lt;&gt; Builder(Ctx);\n\n        // Declare the instrumentation function as an external function\n        FunctionType *FuncType = FunctionType::get(Type::getVoidTy(Ctx), Type::getInt8PtrTy(Cctx), false);\n        FunctionCallee InstrumentFunc =\n                  M.getOrInsertFunction(\"instrument_function\", FuncType);\n\n        for (Function &F : M) {\n\n            // Insert the call at the beginning of each basic block\n            for (BasicBlock &BB : F) {\n                Builder.SetInsertPoint(&BB, BB.begin());\n\n                // Create a global string for the basic block name\n                Value *BBName = Builder.CreateGlobalStringPtr(BB.getName());\n\n                // Create the call to the instrumentation function\n                Builder.CreateCall(InstrumentFunc, BBName);\n            }\n        }\n\n        return PreservedAnalyses::none();"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#starter-code",
    "href": "lectures/revealjs_llvm.qmd.html#starter-code",
    "title": "using llvm",
    "section": "starter code",
    "text": "starter code\nrm -r llvm-pass-skeleton\ngit clone  -b rtlib  https://github.com/sampsyo/llvm-pass-skeleton.git\ncd llvm-pass-skeleton\nmkdir -p build \ncd build \ncmake ..\nmake"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#adrians-skeleton",
    "href": "lectures/revealjs_llvm.qmd.html#adrians-skeleton",
    "title": "using llvm",
    "section": "Adrians skeleton",
    "text": "Adrians skeleton\nstruct SkeletonPass : public PassInfoMixin&lt;SkeletonPass&gt; {\n    PreservedAnalyses run(Module &M, ModuleAnalysisManager &AM) {\n        for (auto &F : M.functions()) {\n\n            // Get the function to call from our runtime library.\n            LLVMContext &Ctx = F.getContext();\n            std::vector&lt;Type*&gt; paramTypes = {Type::getInt32Ty(Ctx)};\n            Type *retType = Type::getVoidTy(Ctx);\n            FunctionType *logFuncType = FunctionType::get(retType, paramTypes, false);\n            FunctionCallee logFunc =\n                F.getParent()-&gt;getOrInsertFunction(\"logop\", logFuncType);\n\n            for (auto &B : F) {\n                for (auto &I : B) {\n                    if (auto *op = dyn_cast&lt;BinaryOperator&gt;(&I)) {\n                        // Insert *after* `op`.\n                        IRBuilder&lt;&gt; builder(op);\n                        builder.SetInsertPoint(&B, ++builder.GetInsertPoint());\n\n                        // Insert a call to our function.\n                        Value* args[] = {op};\n                        builder.CreateCall(logFunc, args);\n\n                        return PreservedAnalyses::none();\n                    }\n                }\n            }\n\n        }\n        return PreservedAnalyses::all();\n    }\n};"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#adrians-instrumentation-code",
    "href": "lectures/revealjs_llvm.qmd.html#adrians-instrumentation-code",
    "title": "using llvm",
    "section": "Adrians instrumentation code",
    "text": "Adrians instrumentation code\n#include &lt;stdio.h&gt;\nvoid logop(int i) {\n    printf(\"computed: %i\\n\", i);\n}"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#link-together",
    "href": "lectures/revealjs_llvm.qmd.html#link-together",
    "title": "using llvm",
    "section": "link together",
    "text": "link together\ncd llvm-pass-skeleton\ncc -c rtlib.c --- generates rtlib.o \nclang  -fpass-plugin=build/skeleton/SkeletonPass.so -c test.cpp   -- generates test.o\ncc test.o rtlib.o  -- links it together \n./a.out     --   runs it"
  },
  {
    "objectID": "lectures/revealjs_llvm.qmd.html#homework",
    "href": "lectures/revealjs_llvm.qmd.html#homework",
    "title": "using llvm",
    "section": "Homework",
    "text": "Homework\nFollow the LLVM tutorial blog post far enough to implement a pass that changes program execution.\nThis is intentionally open-ended. You can be as ambitious or as unambitious as you want. An example of an unambitious but acceptable task would be to print out a message every time the program uses floating-point division.\nAn example of an ambitious task would be to implement an optimization on LLVM IR and make sure it speeds things up in actual wall-clock time execution.\nFind a real-ish C/C++ program somewhere and run your pass on it to observe the results."
  },
  {
    "objectID": "lectures/08_classic_loop_ops.html",
    "href": "lectures/08_classic_loop_ops.html",
    "title": "classic loop optimizations",
    "section": "",
    "text": "Loops optimizations are important because\nWhat are classic loop optimizations?\nLess classic loop optimizations\nFirst recall natural loops\ndef of loop invariant for an instruction d = op a,b\nin SSA form if we find a loop invariant instruction we can always move it into the pre-header, because the value it writes is never rewritten, and the values that it depends on come from outside the loop\nconditions when moving an instruction d = a op b is ok\ncan move d\nL0: d = 0 preheader L1: if (i&gt;=N) goto L2 i = i + 1 d = a ⊕ b = d goto L1 L2: x = d ```\nno good d used after the loop, would not be changed if the loop executes zero times\nno good d reassigned in the loop, do invar would be changed\nwhile (e) { j = loopinv // may never execute S }\nj = loopinv // always executes while (e) { S }\nif (e) { j = loopinv // may never execute while (e) { S }\n} ````",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/08_classic_loop_ops.html#induction-variable-elimination",
    "href": "lectures/08_classic_loop_ops.html#induction-variable-elimination",
    "title": "classic loop optimizations",
    "section": "induction variable elimination",
    "text": "induction variable elimination\nfor (int i = 0; i &lt; 100; ++1){\n    f(a[i])\n}\ncalculate a[i] as: &a[0] + 4 * i in every loop iteration, but the values at each step only differ by 4\n\na_i = &a[0] before the loop\na_i = a_i + 4 (add the stride) in every iteration\nthe only remaining use of i is the test i &lt; 100, which could become a_i &lt; &a[0] + 4*100 (which is loop invariant)\n\nsteps\n1find basic induction variables i = i + e, where e is loop invariant\nwhat does this look like in ssa\nloop header:\n i1 = phi(i0, i2)\nloop body:\ni2 = i1 + e\nloop header:\n i1 = phi(i0, i2)\nloop body:\na0 = i1 + e\ni2 = a0 + e1\nfor each instruction d = c +- loop invariant see if there is a strongly connected graph in the ssa edges that only has adds and subtracts of loop invariant expressions\nStep 2 find auxiliary induction variables\nj = basic_ind * loop inv + loop invar\nfor (int i = 0; i &lt; n; i++) {\n     j = 2*i + 1;     // Y \n     k = -i;          // Y \n     l = 2*i*i + 1;   // N \n     c = c + 5;       // Y* \n}\nstep 3 replace auxiliary induction variables (derived ) by new variables without the multiply\nstep4 if the only remaining use of the induction variable is the termination test, change the test to use the new variable\nsum = 0\nfor (i = 1, i &lt; 100; i++) {\n  sum = sum + a[i -1]\n}\nin SSA form:\n   sum0 = 0\n   i0 = 1\nL: sum1 = phi(sum0, sum2)\n   i1 = phi(i0, i2)\n   t10 = i1 -1 \n   t20 = t10 * 4\n   t30 = t20 + &a\n   t40 = load t30\n   sum2 = sum1 + t40\n   i2 = i1 + 1\n   if (i2 &lt;= 100)go to l\n\ni is a basic induction variable\nt10 is a aux induction variable\nt20 is an aux induction variable\nt30 is an aux induction variable\n\nt3 has a use in the load\nt3 = t20 + &a ==&gt; t10 * 4 + &a ==&gt; (i1-1)* 4+ &a\nt3 = 4* i1 + &a - 4\n   sum0 = 0\n   i0 = 1\n   t50 = &a -4  // initial value \nL: sum1 = phi(sum0, sum2)\n   i1 = phi(i0, i2)\n   t51 = phi(t50, t52)\n   //t10 = i1 -1 \n   //t20 = t10 * 4\n   //t30 = t20 + &a\n   t40 = load t50\n   sum2 = sum1 + t40\n   i2 = i1 + 1\n   t52 = t50 + 4\n   if (i2 &lt;= 100)go to l\n   sum0 = 0\n   i0 = 1\n   t50 = &a -4  // initial value \nL: sum1 = phi(sum0, sum2)\n   // i1 = phi(i0, i2)\n   t51 = phi(t50, t52)\n   //t10 = i1 -1 \n   //t20 = t10 * 4\n   //t30 = t20 + &a\n   t40 = load t50\n   sum2 = sum1 + t40\n   //i2 = i1 + 1\n   t52 = t50 + 4\n   if (t52 &lt;= 396 + &a )go to l",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/08_classic_loop_ops.html#loop-un-switching",
    "href": "lectures/08_classic_loop_ops.html#loop-un-switching",
    "title": "classic loop optimizations",
    "section": "loop un-switching",
    "text": "loop un-switching\nfor (int i = 0 ; i &lt; 100; ++1){\n    if (c) {  // c is loop invariant \n        f(i)\n    } else {\n        g(i)\n    }\n}\nlook for special patterns and replace\nif (c) {  // c is loop invariant \n   for (int i = 0 ; i &lt; 100; ++1){\n        f(i)\n    } \n}else {\n    for (int i = 0 ; i &lt; 100; ++1){\n        g(i)\n    }\n}\nThis is often done before vectorization\nloop fusion\nfor (i = 0; i &lt; 100 ; ++){\n s0:   b[i] = f(a[i])\n}\nfor (i = 0; i &lt; 100 ; ++){\n s1:   c[i] = f(b[i])\n}\n\nwhen is it legal to do this?\nWhen can we get rid of the b array?\n\nThere is also an optimization that goes the other way split a loop so that each statement becomes a separate loop incase we could run as vectors\nThese sort of loop optimizations would make good projects",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/revealjs_08_classic_loop_ops.qmd.html#induction-variable-elimination",
    "href": "lectures/revealjs_08_classic_loop_ops.qmd.html#induction-variable-elimination",
    "title": "classic loop optimizations",
    "section": "induction variable elimination",
    "text": "induction variable elimination\nfor (int i = 0; i &lt; 100; ++1){\n    f(a[i])\n}\ncalculate a[i] as: &a[0] + 4 * i in every loop iteration, but the values at each step only differ by 4\n\na_i = &a[0] before the loop\na_i = a_i + 4 (add the stride) in every iteration\nthe only remaining use of i is the test i &lt; 100, which could become a_i &lt; &a[0] + 4*100 (which is loop invariant)\n\nsteps\n1find basic induction variables i = i + e, where e is loop invariant\nwhat does this look like in ssa\nloop header:\n i1 = phi(i0, i2)\nloop body:\ni2 = i1 + e\nloop header:\n i1 = phi(i0, i2)\nloop body:\na0 = i1 + e\ni2 = a0 + e1\nfor each instruction d = c +- loop invariant see if there is a strongly connected graph in the ssa edges that only has adds and subtracts of loop invariant expressions\nStep 2 find auxiliary induction variables\nj = basic_ind * loop inv + loop invar\nfor (int i = 0; i &lt; n; i++) {\n     j = 2*i + 1;     // Y \n     k = -i;          // Y \n     l = 2*i*i + 1;   // N \n     c = c + 5;       // Y* \n}\nstep 3 replace auxiliary induction variables (derived ) by new variables without the multiply\nstep4 if the only remaining use of the induction variable is the termination test, change the test to use the new variable\nsum = 0\nfor (i = 1, i &lt; 100; i++) {\n  sum = sum + a[i -1]\n}\nin SSA form:\n   sum0 = 0\n   i0 = 1\nL: sum1 = phi(sum0, sum2)\n   i1 = phi(i0, i2)\n   t10 = i1 -1 \n   t20 = t10 * 4\n   t30 = t20 + &a\n   t40 = load t30\n   sum2 = sum1 + t40\n   i2 = i1 + 1\n   if (i2 &lt;= 100)go to l\n\ni is a basic induction variable\nt10 is a aux induction variable\nt20 is an aux induction variable\nt30 is an aux induction variable\n\nt3 has a use in the load\nt3 = t20 + &a ==&gt; t10 * 4 + &a ==&gt; (i1-1)* 4+ &a\nt3 = 4* i1 + &a - 4\n   sum0 = 0\n   i0 = 1\n   t50 = &a -4  // initial value \nL: sum1 = phi(sum0, sum2)\n   i1 = phi(i0, i2)\n   t51 = phi(t50, t52)\n   //t10 = i1 -1 \n   //t20 = t10 * 4\n   //t30 = t20 + &a\n   t40 = load t50\n   sum2 = sum1 + t40\n   i2 = i1 + 1\n   t52 = t50 + 4\n   if (i2 &lt;= 100)go to l\n   sum0 = 0\n   i0 = 1\n   t50 = &a -4  // initial value \nL: sum1 = phi(sum0, sum2)\n   // i1 = phi(i0, i2)\n   t51 = phi(t50, t52)\n   //t10 = i1 -1 \n   //t20 = t10 * 4\n   //t30 = t20 + &a\n   t40 = load t50\n   sum2 = sum1 + t40\n   //i2 = i1 + 1\n   t52 = t50 + 4\n   if (t52 &lt;= 396 + &a )go to l"
  },
  {
    "objectID": "lectures/revealjs_08_classic_loop_ops.qmd.html#loop-un-switching",
    "href": "lectures/revealjs_08_classic_loop_ops.qmd.html#loop-un-switching",
    "title": "classic loop optimizations",
    "section": "loop un-switching",
    "text": "loop un-switching\nfor (int i = 0 ; i &lt; 100; ++1){\n    if (c) {  // c is loop invariant \n        f(i)\n    } else {\n        g(i)\n    }\n}\nlook for special patterns and replace\nif (c) {  // c is loop invariant \n   for (int i = 0 ; i &lt; 100; ++1){\n        f(i)\n    } \n}else {\n    for (int i = 0 ; i &lt; 100; ++1){\n        g(i)\n    }\n}\nThis is often done before vectorization\nloop fusion\nfor (i = 0; i &lt; 100 ; ++){\n s0:   b[i] = f(a[i])\n}\nfor (i = 0; i &lt; 100 ; ++){\n s1:   c[i] = f(b[i])\n}\n\nwhen is it legal to do this?\nWhen can we get rid of the b array?\n\nThere is also an optimization that goes the other way split a loop so that each statement becomes a separate loop incase we could run as vectors\nThese sort of loop optimizations would make good projects"
  },
  {
    "objectID": "lectures/02b_bril.html",
    "href": "lectures/02b_bril.html",
    "title": "Overview of Bril",
    "section": "",
    "text": "Bril is very simple, very regular, ir.\nBril can be extended easily.\nBril has lots of tools and examples.\nBril tools are written in lots of languages so setup can be messy",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#how-to-use-bril-with-real-code",
    "href": "lectures/02b_bril.html#how-to-use-bril-with-real-code",
    "title": "Overview of Bril",
    "section": "",
    "text": "Bril is very simple, very regular, ir.\nBril can be extended easily.\nBril has lots of tools and examples.\nBril tools are written in lots of languages so setup can be messy",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#lets-look-at-a-bril-program.",
    "href": "lectures/02b_bril.html#lets-look-at-a-bril-program.",
    "title": "Overview of Bril",
    "section": "Lets look at a bril program.",
    "text": "Lets look at a bril program.\nBril is written in JSON format. Almost all programming languages have a way to read json.\n\nimport json\nimport subprocess\nimport os \nimport sys\n\n\n### temp \nout = subprocess.check_output('which python', shell=True)\nprint(out)\nprint('***********************')\n\n# read from a file \nwith open(\"images/add.json\",\"r\") as f:\n    bril_program = json.load(f)\n\n# read from a pipe\n# bril_program = json.load(sys.stdin)\n    \nprint(json.dumps(bril_program, \n    indent=2))\n\nb'/opt/hostedtoolcache/Python/3.10.15/x64/bin/python\\n'\n***********************\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"v0\",\n          \"value\": 1\n        },\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"v1\",\n          \"value\": 2\n        },\n        {\n          \"op\": \"add\",\n          \"type\": \"int\",\n          \"dest\": \"v2\",\n          \"args\": [\n            \"v0\",\n            \"v1\"\n          ]\n        },\n        {\n          \"op\": \"print\",\n          \"args\": [\n            \"v2\"\n          ]\n        }\n      ],\n      \"args\": []\n    }\n  ]\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#formatted",
    "href": "lectures/02b_bril.html#formatted",
    "title": "Overview of Bril",
    "section": "Formatted",
    "text": "Formatted\n{\n  \"functions\": [\n    {\n      \"instrs\": [\n        {\"dest\": \"v0\", \"op\": \"const\",\"type\": \"int\",\"value\": 1},\n        {\"dest\": \"v1\", \"op\": \"const\",\"type\": \"int\",\"value\": 2},\n        {\"dest\": \"v2\", \"op\": \"add\",  \"type\": \"int\",\"args\": [\"v0\",\"v1\"],},\n                       \"op\": \"print\",\"args\": [ \"v2\"],}],\n      \"name\": \"main\",\n    }\n  ]\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#getting-started",
    "href": "lectures/02b_bril.html#getting-started",
    "title": "Overview of Bril",
    "section": "getting started",
    "text": "getting started\nlinks:\n\nLanguage specification\ngithub site",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#step-1-clone-the-bril-repo-on-a-linux-or-wsl-machine",
    "href": "lectures/02b_bril.html#step-1-clone-the-bril-repo-on-a-linux-or-wsl-machine",
    "title": "Overview of Bril",
    "section": "step 1 clone the bril repo on a linux or wsl machine",
    "text": "step 1 clone the bril repo on a linux or wsl machine\ngit clone https://github.com/sampsyo/bril.git",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#step-2-support-packages",
    "href": "lectures/02b_bril.html#step-2-support-packages",
    "title": "Overview of Bril",
    "section": "step 2 support packages",
    "text": "step 2 support packages\n\ndeno is the runtime for typescript/javascript\n\ncurl -fsSL https://deno.land/install.sh | sh\non my ubuntu machine ‘sudo snap install deno’ also worked\nyou may need to add $HOME/.deno/bin to your $PATH.\n\nflit a python package manager\n\npython3 -m pip install flit",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#step-3-install-the-bril-interpreter-and-the-typescript-to-bril-compiler",
    "href": "lectures/02b_bril.html#step-3-install-the-bril-interpreter-and-the-typescript-to-bril-compiler",
    "title": "Overview of Bril",
    "section": "step 3 install the bril interpreter, and the typescript to bril compiler",
    "text": "step 3 install the bril interpreter, and the typescript to bril compiler\ncd bril\ndeno install brili.ts \ndeno install --allow-env --allow-read ts2bril.ts",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#running-the-interpreter",
    "href": "lectures/02b_bril.html#running-the-interpreter",
    "title": "Overview of Bril",
    "section": "running the interpreter",
    "text": "running the interpreter\nbrili &lt;images/add.json\nbrili -p &lt;images/add.json\nthe -p flag turns on profiling",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#text-to-json-and-back",
    "href": "lectures/02b_bril.html#text-to-json-and-back",
    "title": "Overview of Bril",
    "section": "text to json and back",
    "text": "text to json and back\nThere are programs bril2txt and bril2json that make it easy to convert. Keep in mind that the json format is Bril and thats where you will do all the work.\ninstall text tools\ncd bril-txt\nflit install --symlink --user\nrun json to text\nbril2txt &lt; images/add.json",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#connect-tools-via-pipes",
    "href": "lectures/02b_bril.html#connect-tools-via-pipes",
    "title": "Overview of Bril",
    "section": "connect tools via pipes",
    "text": "connect tools via pipes\ncat images/add.json'\nbril2txt &lt; images/add.json | bril2json",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#other-tools",
    "href": "lectures/02b_bril.html#other-tools",
    "title": "Overview of Bril",
    "section": "Other tools",
    "text": "Other tools\nThere is also a fast interpreter written in Rust see docs for installation",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#turnt-tiny-unified-runner-and-tester",
    "href": "lectures/02b_bril.html#turnt-tiny-unified-runner-and-tester",
    "title": "Overview of Bril",
    "section": "turnt Tiny unified runner and tester",
    "text": "turnt Tiny unified runner and tester\nBril uses turnt as a test tool\nTurnt is a simple snapshot testing tool inspired by LLVM’s lit. It’s good for testing things that translate text files to other text files, like compilers. The idea is that each test is one input file, and you want to run a command and check that it still matches the saved output file.\npip install –user turnt\nAs you think about your projects, you might consider adding a new tool. you can setup Bril on your local linux (can be wsl) machine",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#gen-cfg",
    "href": "lectures/02b_bril.html#gen-cfg",
    "title": "Overview of Bril",
    "section": "Gen CFG",
    "text": "Gen CFG\nLets write a sample program - that generates the cfg\nHow would you do that?\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v0\", \"value\": 1 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v1\", \"value\": 2 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"v2\",\n          \"args\": [\"v0\", \"v1\"] },\n        { \"op\": \"print\", \"args\": [\"v2\"] }\n      ],\n      \"args\": []\n    }\n  ]\n}\n\n. . .\nI’ll do this in two steps\n\nfind all the basic blocks\nadd all the cfg edges\n\nYou can also do this in a single step, adding cfg edges as soon as you reach the successor node.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#basic-blocks-from-a-list-of-instructions-",
    "href": "lectures/02b_bril.html#basic-blocks-from-a-list-of-instructions-",
    "title": "Overview of Bril",
    "section": "basic blocks from a list of instructions-",
    "text": "basic blocks from a list of instructions-\nkeep adding instructions till we get to a terminator or a label (do we add labels?)\n. . .\nin: list of instrs \nout: list of lists of instrs \n \nblocks = []\ncurr_block = []\nfor each instr in list \n    if the instruction is not a label put it on curr_block\n    if instr is a label or terminator \n       put curr_block on blocks\n       curr_block = []\n \n if curr_block is not empty add it to blocks\n return blocks \ntwo labels in a row do not need another block",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#step-2-add-edges",
    "href": "lectures/02b_bril.html#step-2-add-edges",
    "title": "Overview of Bril",
    "section": "step 2 add edges",
    "text": "step 2 add edges\nfind cfg: in: is bril program in json \nfor each function find the list of basic blocks\nfor each basic block\n   get last_instr \n   if it is a terminator  br/jmp/ret \n     add edge from current block to successor  \n    --- what do we want to do with call? \n   else it is a fall through\n      add edge to next block\n. . .\nwe need a map (block_map) label-&gt;block so we can add edges for blocks that end in br/jmp - can build this while getting the blocks or we can put the label as the first instruction\nhow do we handle fall through?\nwhat about a return\nif every block ends with a terminator, and every block has a label, then no fall through case\nwhat happens if try to delete the terminator (because the block never executes)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#code",
    "href": "lectures/02b_bril.html#code",
    "title": "Overview of Bril",
    "section": "code",
    "text": "code\nI’ll use a python data structure called OrderedDict, when you iterate over the items in a ordered dict, they come back in the order that they were installed.\n\nGitHub Copilot says:\nOrderedDict in Python is a dictionary subclass that maintains the order in which keys are inserted. When iterating over an OrderedDict, the items are returned in the order they were added. This behavior contrasts with a standard dictionary in Python 3.6 and earlier, where the iteration order was not guaranteed. However, starting from Python 3.7, the built-in dict type also maintains insertion order by default, making OrderedDict less necessary for most applications. OrderedDict still provides additional functionality, such as the move_to_end method, which allows moving an existing key to either end of the dictionary.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#ill-use-a-generator",
    "href": "lectures/02b_bril.html#ill-use-a-generator",
    "title": "Overview of Bril",
    "section": "I’ll use a generator",
    "text": "I’ll use a generator\nIn Python, a generator is an iterator that is defined with a function using the yield statement.\n\nProduce items only once\nDo not store all the items in memory\nWhen items from the generator are requested, the function executes until it reaches a yield statement, which produces the next value. Execution then pauses, preserving the function’s state, until the next item is requested.\n\n\nGiven a list of Bril instructions, generate a sequence of instruction lists representing the basic blocks in the program.\nEvery instruction in instr will show up in exactly one block. Jump and branch instructions may only appear at the end of a block, and control can transfer only to the top of a basic block—so labels can only appear at the start of a basic block. Basic blocks may not be empty.\n\n\n    #Instructions that terminate a basic block.\n    TERMINATORS = 'br', 'jmp', 'ret'\n\n    def form_blocks(instrs):\n        # Start with an empty block.\n        cur_block = []\n\n        for instr in instrs:\n            if 'op' in instr:  # It's an instruction.\n                # Add the instruction to the currently-being-formed block.\n                cur_block.append(instr)\n\n                # If this is a terminator (branching instruction), it's the\n                # last instruction in the block. Finish this block and\n                # start a new one.\n                if instr['op'] in TERMINATORS:\n                    yield cur_block\n                    cur_block = []\n            \n            else:  # It's a label.\n                # End the block here (if it contains anything).\n                if cur_block:\n                    yield cur_block\n\n                # Start a new block with the label.\n                cur_block = [instr]\n\n        # Produce the final block, if any.\n        if cur_block:\n            yield cur_block",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#as-a-test-lets-print-out-the-blocks",
    "href": "lectures/02b_bril.html#as-a-test-lets-print-out-the-blocks",
    "title": "Overview of Bril",
    "section": "as a test, lets print out the blocks",
    "text": "as a test, lets print out the blocks\n\ndef print_blocks(bril):\n    \"\"\"Given a Bril program, print out its basic blocks.\n    \"\"\"\n\n\n    func = bril['functions'][0]  # We only process one function.\n    for block in form_blocks(func['instrs']):\n        # Mark the block.\n        leader = block[0]\n        if 'label' in leader:\n            print( f\"block {leader['label']}\")\n            block = block[1:]  # Hide the label\n        else:\n            print('anonymous block:')\n\n        # Print the instructions.\n        for instr in block:\n            print(instr)\n\nprint_blocks(bril_program)\n\nanonymous block:\n{'op': 'const', 'type': 'int', 'dest': 'v0', 'value': 1}\n{'op': 'const', 'type': 'int', 'dest': 'v1', 'value': 2}\n{'op': 'add', 'type': 'int', 'dest': 'v2', 'args': ['v0', 'v1']}\n{'op': 'print', 'args': ['v2']}\n\n\nthis test program has one block so pretty easy",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#lets-try-a-second-example-with-a-jmp",
    "href": "lectures/02b_bril.html#lets-try-a-second-example-with-a-jmp",
    "title": "Overview of Bril",
    "section": "lets try a second example with a jmp",
    "text": "lets try a second example with a jmp\n@main {\n  v: int = const 4;\n  jmp .somewhere;\n  v: int = const 2;\n.somewhere:\n  print v;\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#running-commands-inside-python",
    "href": "lectures/02b_bril.html#running-commands-inside-python",
    "title": "Overview of Bril",
    "section": "running commands inside python",
    "text": "running commands inside python\nGitHub Copilot: subprocess.check_output is a function in Python’s subprocess module that runs a command with arguments and returns its output as a byte string. If the command exits with a non-zero exit status, it raises a CalledProcessError, which includes the exit status and output of the command. This function is useful for capturing the output of a command for further processing in Python.\n\n\nimport subprocess\n\n# Run a command and capture its output\noutput = subprocess.check_output(['ls', '-l'])\n\n# Convert the byte string to a regular string (assuming UTF-8 encoding)\noutput_str = output.decode('utf-8')\n\nprint(output_str)\n\ntotal 2288\n-rw-r--r-- 1 runner docker  40732 Dec 15 13:34 010_compiler_overview.html\n-rw-r--r-- 1 runner docker  11181 Dec 15 13:34 010_compiler_overview.qmd\n-rw-r--r-- 1 runner docker  32547 Dec 15 13:34 01a1_performance_measurement.html\n-rw-r--r-- 1 runner docker   3018 Dec 15 13:34 01a1_performance_measurement.qmd\ndrwxr-xr-x 4 runner docker   4096 Dec 15 13:34 01a1_performance_measurement_files\n-rw-r--r-- 1 runner docker  24184 Dec 15 13:34 01a2_performance_measurement.html\n-rw-r--r-- 1 runner docker   3018 Dec 15 13:34 01a2_performance_measurement.qmd\n-rw-r--r-- 1 runner docker  33835 Dec 15 13:34 01a_performance_measurement.html\n-rw-r--r-- 1 runner docker   4152 Dec 15 13:34 01a_performance_measurement.qmd\ndrwxr-xr-x 4 runner docker   4096 Dec 15 13:34 01a_performance_measurement_files\n-rw-r--r-- 1 runner docker  40303 Dec 15 13:34 02a_representation.html\n-rw-r--r-- 1 runner docker   7431 Dec 15 13:34 02a_representation.qmd\n-rw-r--r-- 1 runner docker  13795 Dec 15 13:34 02b_bril.qmd\n-rw-r--r-- 1 runner docker  23632 Dec 15 13:35 02b_bril.quarto_ipynb\n-rw-r--r-- 1 runner docker  74899 Dec 15 13:34 03_local.html\n-rw-r--r-- 1 runner docker  12760 Dec 15 13:34 03_local.qmd\n-rw-r--r-- 1 runner docker  13285 Dec 15 13:34 03b_local_value_numbering.qmd\n-rw-r--r-- 1 runner docker   9456 Dec 15 13:34 04_data_flow.qmd\n-rw-r--r-- 1 runner docker  56505 Dec 15 13:35 05_global.html\n-rw-r--r-- 1 runner docker   9992 Dec 15 13:34 05_global.qmd\n-rw-r--r-- 1 runner docker   8730 Dec 15 13:34 05b_licm.qmd\n-rw-r--r-- 1 runner docker  33395 Dec 15 13:35 05c_pre.html\n-rw-r--r-- 1 runner docker   3166 Dec 15 13:34 05c_pre.qmd\n-rw-r--r-- 1 runner docker  20202 Dec 15 13:34 06_ssa.qmd\n-rwxr-xr-x 1 runner docker  25643 Dec 15 13:32 07_llvm.notebook\n-rw-r--r-- 1 runner docker   5707 Dec 15 13:34 08_classic_loop_ops.qmd\n-rw-r--r-- 1 runner docker  99019 Dec 15 13:35 09_poly.html\n-rwxr-xr-x 1 runner docker  32475 Dec 15 13:34 09_poly.qmd\ndrwxr-xr-x 4 runner docker   4096 Dec 15 13:35 09_poly_files\n-rw-r--r-- 1 runner docker  20936 Dec 15 13:34 100_mlir.html\n-rw-r--r-- 1 runner docker    320 Dec 15 13:34 100_mlir.qmd\n-rw-r--r-- 1 runner docker  20970 Dec 15 13:34 110_whole_program.html\n-rw-r--r-- 1 runner docker    338 Dec 15 13:34 110_whole_program.qmd\n-rw-r--r-- 1 runner docker  21028 Dec 15 13:35 12_memory.html\n-rw-r--r-- 1 runner docker    334 Dec 15 13:34 12_memory.qmd\n-rw-r--r-- 1 runner docker 102297 Dec 15 13:34 13_dynamic_compilers.html\n-rw-r--r-- 1 runner docker  31886 Dec 15 13:34 13_dynamic_compilers.qmd\n-rw-r--r-- 1 runner docker  35314 Dec 15 13:35 14_gpu_compilers.html\n-rw-r--r-- 1 runner docker  10135 Dec 15 13:34 14_gpu_compilers.qmd\n-rw-r--r-- 1 runner docker   4682 Dec 15 13:32 a.json\n-rw-r--r-- 1 runner docker    248 Dec 15 13:32 a.ts\ndrwxr-xr-x 2 runner docker   4096 Dec 15 13:32 df\n-rw-r--r-- 1 runner docker  71550 Dec 15 13:35 diverg.html\n-rw-r--r-- 1 runner docker  27522 Dec 15 13:34 diverg.qmd\n-rw-r--r-- 1 runner docker  26721 Dec 15 13:34 diverg1.html\n-rw-r--r-- 1 runner docker   5001 Dec 15 13:34 diverg1.qmd\n-rw-r--r-- 1 runner docker   4595 Dec 15 13:32 errors\n-rw-r--r-- 1 runner docker   3623 Dec 15 13:32 foo.ll\n-rw-r--r-- 1 runner docker   2296 Dec 15 13:32 identity.bc\ndrwxr-xr-x 2 runner docker   4096 Dec 15 13:32 images\n-rw-r--r-- 1 runner docker  30615 Dec 15 13:35 junk.html\n-rw-r--r-- 1 runner docker   2152 Dec 15 13:34 junk.qmd\n-rw-r--r-- 1 runner docker    905 Dec 15 13:32 junk.txt\n-rw-r--r-- 1 runner docker  20731 Dec 15 13:34 llvm.qmd\n-rw-r--r-- 1 runner docker   7659 Dec 15 13:32 llvm.qmd.next\n-rw-r--r-- 1 runner docker  44411 Dec 15 13:35 mem_consistancy.html\n-rw-r--r-- 1 runner docker  15878 Dec 15 13:34 mem_consistancy.qmd\n-rw-r--r-- 1 runner docker  24125 Dec 15 13:34 mlir.html\n-rw-r--r-- 1 runner docker   2278 Dec 15 13:34 mlir.qmd\ndrwxr-xr-x 2 runner docker   4096 Dec 15 13:32 papers\n-rwxr-xr-x 1 runner docker  30724 Dec 15 13:34 poly_final.qmd\ndrwxr-xr-x 2 runner docker   4096 Dec 15 13:32 pytorch\ndrwxr-xr-x 2 runner docker   4096 Dec 15 13:32 ra\n-rw-r--r-- 1 runner docker  41702 Dec 15 13:34 ra-checking.html\n-rw-r--r-- 1 runner docker  15172 Dec 15 13:34 ra-checking.qmd\n-rw-r--r-- 1 runner docker   8032 Dec 15 13:34 register_allocation.qmd\n-rw-r--r-- 1 runner docker  40546 Dec 15 13:34 revealjs_010_compiler_overview.qmd.html\n-rw-r--r-- 1 runner docker  33435 Dec 15 13:34 revealjs_01a1_performance_measurement.qmd.html\n-rw-r--r-- 1 runner docker  24507 Dec 15 13:34 revealjs_01a2_performance_measurement.qmd.html\n-rw-r--r-- 1 runner docker  34770 Dec 15 13:34 revealjs_01a_performance_measurement.qmd.html\n-rw-r--r-- 1 runner docker  39260 Dec 15 13:34 revealjs_02a_representation.qmd.html\n-rw-r--r-- 1 runner docker  73582 Dec 15 13:34 revealjs_03_local.qmd.html\n-rw-r--r-- 1 runner docker  51948 Dec 15 13:35 revealjs_05_global.qmd.html\n-rw-r--r-- 1 runner docker  32916 Dec 15 13:35 revealjs_05c_pre.qmd.html\n-rw-r--r-- 1 runner docker  86492 Dec 15 13:35 revealjs_09_poly.qmd.html\n-rw-r--r-- 1 runner docker  21418 Dec 15 13:34 revealjs_100_mlir.qmd.html\n-rw-r--r-- 1 runner docker  21445 Dec 15 13:34 revealjs_110_whole_program.qmd.html\n-rw-r--r-- 1 runner docker  21468 Dec 15 13:35 revealjs_12_memory.qmd.html\n-rw-r--r-- 1 runner docker  87753 Dec 15 13:34 revealjs_13_dynamic_compilers.qmd.html\n-rw-r--r-- 1 runner docker  34449 Dec 15 13:35 revealjs_14_gpu_compilers.qmd.html\n-rw-r--r-- 1 runner docker  70744 Dec 15 13:35 revealjs_diverg.qmd.html\n-rw-r--r-- 1 runner docker  31673 Dec 15 13:34 revealjs_diverg1.qmd.html\n-rw-r--r-- 1 runner docker  30291 Dec 15 13:35 revealjs_junk.qmd.html\n-rw-r--r-- 1 runner docker  44717 Dec 15 13:35 revealjs_mem_consistancy.qmd.html\n-rw-r--r-- 1 runner docker  28411 Dec 15 13:34 revealjs_mlir.qmd.html\n-rw-r--r-- 1 runner docker  40196 Dec 15 13:34 revealjs_ra-checking.qmd.html\n-rw-r--r-- 1 runner docker  22316 Dec 15 13:32 xx",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#print-the-blocks",
    "href": "lectures/02b_bril.html#print-the-blocks",
    "title": "Overview of Bril",
    "section": "print the blocks",
    "text": "print the blocks\n\nimport json \nimport os\nimport subprocess\n\nresult =  subprocess.check_output('bril2json &lt; images/jmp.bril', shell=True)\ntest2json = json.loads(result)\nprint_blocks(test2json)\n\nanonymous block:\n{'dest': 'v', 'op': 'const', 'type': 'int', 'value': 4}\n{'labels': ['somewhere'], 'op': 'jmp'}\nanonymous block:\n{'dest': 'v', 'op': 'const', 'type': 'int', 'value': 2}\nblock somewhere\n{'args': ['v'], 'op': 'print'}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#the-map-label-names-to-blocks",
    "href": "lectures/02b_bril.html#the-map-label-names-to-blocks",
    "title": "Overview of Bril",
    "section": "the map (label names to blocks)",
    "text": "the map (label names to blocks)\n\nfrom collections import OrderedDict\n\n\ndef block_map(blocks):\n    \"\"\"Given a sequence of basic blocks, which are lists of instructions,\n    produce a `OrderedDict` mapping names to blocks.\n\n    The name of the block comes from the label it starts with, if any.\n    Anonymous blocks, which don't start with a label, get an\n    automatically generated name. Blocks in the mapping have their\n    labels removed.\n    \"\"\"\n    by_name = OrderedDict()\n\n    for block in blocks:\n        # Generate a name for the block.\n        if 'label' in block[0]:\n            # The block has a label. Remove the label but use it for the\n            # block's name.\n            name = block[0]['label']\n            block = block[1:]\n        else:\n            # Make up a new name for this anonymous block.\n            name = f'gen_bk_{len(by_name)}'\n\n        # Add the block to the mapping.\n        by_name[name] = block\n\n    return by_name\n\n\nblks = form_blocks(test2json['functions'][0]['instrs'])\nod = block_map(blks)\nfor (name, instrs) in od.items():\n    print (name, instrs)\n\ngen_bk_0 [{'dest': 'v', 'op': 'const', 'type': 'int', 'value': 4}, {'labels': ['somewhere'], 'op': 'jmp'}]\ngen_bk_1 [{'dest': 'v', 'op': 'const', 'type': 'int', 'value': 2}]\nsomewhere [{'args': ['v'], 'op': 'print'}]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#the-cfg-given-the-block-map-pseudo-code",
    "href": "lectures/02b_bril.html#the-cfg-given-the-block-map-pseudo-code",
    "title": "Overview of Bril",
    "section": "the cfg given the block map (pseudo code)",
    "text": "the cfg given the block map (pseudo code)\nout cfg = {} \n# map label -&gt; list of labels the successors of the block\n\nfor i,block in enumerate(blocks)  # blocks is a ordereddict \n    last = block[i]  # last instruction\n    if last is jmp:\n        cfg[block_name] = jmp.dest\n    elif last is br:\n        cfg[block.name] = [ last.if_label, last.else_label]\n    else\n        # fall through\n        cfg[block_name] = blocks[i+1].name  ## special case for last block",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#cfg",
    "href": "lectures/02b_bril.html#cfg",
    "title": "Overview of Bril",
    "section": "cfg",
    "text": "cfg\n\ndef get_cfg(ordered_blocks):\n    cfg = {}\n\n    labels = list(ordered_blocks.keys())\n\n    for i, (block_name, block) in enumerate(ordered_blocks.items()):\n        last = block[-1]\n        op = last['op']\n\n        if op == 'jmp':\n            cfg[block_name] = last['labels']\n        elif op == 'br':\n            cfg[block_name] = last['labels']\n        else:\n            if i+1 &lt; len(labels):  # last block does not fall through\n                cfg[block_name] = [labels[i+1]]\n            else:\n                cfg[block_name] = []\n    return cfg\n\n\nblks = form_blocks(test2json['functions'][0]['instrs'])\nod = block_map(blks)\ncfg = get_cfg(od)\n\nprint(cfg)\n\n{'gen_bk_0': ['somewhere'], 'gen_bk_1': ['somewhere'], 'somewhere': []}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#graph",
    "href": "lectures/02b_bril.html#graph",
    "title": "Overview of Bril",
    "section": "graph",
    "text": "graph\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\ngen_bk_0--&gt; somewhere\ngen_bk_1 --&gt; somewhere\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\ngen_bk_0--&gt; somewhere\ngen_bk_1 --&gt; somewhere",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/02b_bril.html#homework",
    "href": "lectures/02b_bril.html#homework",
    "title": "Overview of Bril",
    "section": "homework",
    "text": "homework\nDue in 1 week\nYour goal is to get familiar with Bril.\n\nPart 1\nWrite a new benchmark.\nYou can write it by hand, use the TypeScript compiler, or generate it some other way. Try running it with brili.\nUse turnt –save yours.bril to create the test outputs for your new benchmark. (See the Turnt README for details.)\nStart your blog post, talking about your benchmark.\n\npart 2\nWrite a program to analyze or transform Bril programs in some small way. Pick your favorite programming language—there is no “starter code,” so you can start from scratch.\nLoad up a JSON file. You can start with the tiny ones in lectures/images! Read the docs.\nDo something unambitious with it: count the number of add instructions, or add a print instruction before every jump, or whatever. Pick something small and contrived! Use Turnt to test your new tool.\nAlong the way, you will run into problems! Ask questions on github discussions, use open issues and pull requests to describe or fix problems. For example, even super simple benchmarks you might imagine probably can’t be written easily because Bril is too simple. Mention this in discussions, and consider pitching in to help add features.\nThink about how to write a good test, and add to your post describing your work, submit the post on github, and finally add a link to the post in canvas, homework 1",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Overview of Bril"
    ]
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#how-to-use-bril-with-real-code",
    "href": "lectures/revealjs_02b_bril.qmd.html#how-to-use-bril-with-real-code",
    "title": "Overview of Bril",
    "section": "How to use Bril with real code",
    "text": "How to use Bril with real code\n\nBril is very simple, very regular, ir.\nBril can be extended easily.\nBril has lots of tools and examples.\nBril tools are written in lots of languages so setup can be messy"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#lets-look-at-a-bril-program.",
    "href": "lectures/revealjs_02b_bril.qmd.html#lets-look-at-a-bril-program.",
    "title": "Overview of Bril",
    "section": "Lets look at a bril program.",
    "text": "Lets look at a bril program.\nBril is written in JSON format. Almost all programming languages have a way to read json.\n\n\nCode\nimport json\nimport subprocess\nimport os \nimport sys\n\n\n### temp \nout = subprocess.check_output('which python', shell=True)\nprint(out)\nprint('***********************')\n\n# read from a file \nwith open(\"images/add.json\",\"r\") as f:\n    bril_program = json.load(f)\n\n# read from a pipe\n# bril_program = json.load(sys.stdin)\n    \nprint(json.dumps(bril_program, \n    indent=2))"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#lets-look-at-a-bril-program.-output",
    "href": "lectures/revealjs_02b_bril.qmd.html#lets-look-at-a-bril-program.-output",
    "title": "Overview of Bril",
    "section": "Lets look at a bril program.",
    "text": "Lets look at a bril program.\n\nb'/opt/hostedtoolcache/Python/3.10.15/x64/bin/python\\n'\n***********************\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"v0\",\n          \"value\": 1\n        },\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"v1\",\n          \"value\": 2\n        },\n        {\n          \"op\": \"add\",\n          \"type\": \"int\",\n          \"dest\": \"v2\",\n          \"args\": [\n            \"v0\",\n            \"v1\"\n          ]\n        },\n        {\n          \"op\": \"print\",\n          \"args\": [\n            \"v2\"\n          ]\n        }\n      ],\n      \"args\": []\n    }\n  ]\n}"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#formatted",
    "href": "lectures/revealjs_02b_bril.qmd.html#formatted",
    "title": "Overview of Bril",
    "section": "Formatted",
    "text": "Formatted\n{\n  \"functions\": [\n    {\n      \"instrs\": [\n        {\"dest\": \"v0\", \"op\": \"const\",\"type\": \"int\",\"value\": 1},\n        {\"dest\": \"v1\", \"op\": \"const\",\"type\": \"int\",\"value\": 2},\n        {\"dest\": \"v2\", \"op\": \"add\",  \"type\": \"int\",\"args\": [\"v0\",\"v1\"],},\n                       \"op\": \"print\",\"args\": [ \"v2\"],}],\n      \"name\": \"main\",\n    }\n  ]\n}"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#getting-started",
    "href": "lectures/revealjs_02b_bril.qmd.html#getting-started",
    "title": "Overview of Bril",
    "section": "getting started",
    "text": "getting started\nlinks:\n\nLanguage specification\ngithub site"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#step-1-clone-the-bril-repo-on-a-linux-or-wsl-machine",
    "href": "lectures/revealjs_02b_bril.qmd.html#step-1-clone-the-bril-repo-on-a-linux-or-wsl-machine",
    "title": "Overview of Bril",
    "section": "step 1 clone the bril repo on a linux or wsl machine",
    "text": "step 1 clone the bril repo on a linux or wsl machine\ngit clone https://github.com/sampsyo/bril.git"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#step-2-support-packages",
    "href": "lectures/revealjs_02b_bril.qmd.html#step-2-support-packages",
    "title": "Overview of Bril",
    "section": "step 2 support packages",
    "text": "step 2 support packages\n\ndeno is the runtime for typescript/javascript\n\ncurl -fsSL https://deno.land/install.sh | sh\non my ubuntu machine ‘sudo snap install deno’ also worked\nyou may need to add $HOME/.deno/bin to your $PATH.\n\nflit a python package manager\n\npython3 -m pip install flit"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#step-3-install-the-bril-interpreter-and-the-typescript-to-bril-compiler",
    "href": "lectures/revealjs_02b_bril.qmd.html#step-3-install-the-bril-interpreter-and-the-typescript-to-bril-compiler",
    "title": "Overview of Bril",
    "section": "step 3 install the bril interpreter, and the typescript to bril compiler",
    "text": "step 3 install the bril interpreter, and the typescript to bril compiler\ncd bril\ndeno install brili.ts \ndeno install --allow-env --allow-read ts2bril.ts"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#running-the-interpreter",
    "href": "lectures/revealjs_02b_bril.qmd.html#running-the-interpreter",
    "title": "Overview of Bril",
    "section": "running the interpreter",
    "text": "running the interpreter\nbrili &lt;images/add.json\nbrili -p &lt;images/add.json\nthe -p flag turns on profiling"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#text-to-json-and-back",
    "href": "lectures/revealjs_02b_bril.qmd.html#text-to-json-and-back",
    "title": "Overview of Bril",
    "section": "text to json and back",
    "text": "text to json and back\nThere are programs bril2txt and bril2json that make it easy to convert. Keep in mind that the json format is Bril and thats where you will do all the work.\ninstall text tools\ncd bril-txt\nflit install --symlink --user\nrun json to text\nbril2txt &lt; images/add.json"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#connect-tools-via-pipes",
    "href": "lectures/revealjs_02b_bril.qmd.html#connect-tools-via-pipes",
    "title": "Overview of Bril",
    "section": "connect tools via pipes",
    "text": "connect tools via pipes\ncat images/add.json'\nbril2txt &lt; images/add.json | bril2json"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#other-tools",
    "href": "lectures/revealjs_02b_bril.qmd.html#other-tools",
    "title": "Overview of Bril",
    "section": "Other tools",
    "text": "Other tools\nThere is also a fast interpreter written in Rust see docs for installation"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#turnt-tiny-unified-runner-and-tester",
    "href": "lectures/revealjs_02b_bril.qmd.html#turnt-tiny-unified-runner-and-tester",
    "title": "Overview of Bril",
    "section": "turnt Tiny unified runner and tester",
    "text": "turnt Tiny unified runner and tester\nBril uses turnt as a test tool\nTurnt is a simple snapshot testing tool inspired by LLVM’s lit. It’s good for testing things that translate text files to other text files, like compilers. The idea is that each test is one input file, and you want to run a command and check that it still matches the saved output file.\npip install –user turnt\nAs you think about your projects, you might consider adding a new tool. you can setup Bril on your local linux (can be wsl) machine"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#gen-cfg",
    "href": "lectures/revealjs_02b_bril.qmd.html#gen-cfg",
    "title": "Overview of Bril",
    "section": "Gen CFG",
    "text": "Gen CFG\nLets write a sample program - that generates the cfg\nHow would you do that?\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v0\", \"value\": 1 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v1\", \"value\": 2 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"v2\",\n          \"args\": [\"v0\", \"v1\"] },\n        { \"op\": \"print\", \"args\": [\"v2\"] }\n      ],\n      \"args\": []\n    }\n  ]\n}\n\nI’ll do this in two steps\n\nfind all the basic blocks\nadd all the cfg edges\n\nYou can also do this in a single step, adding cfg edges as soon as you reach the successor node."
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#basic-blocks-from-a-list-of-instructions-",
    "href": "lectures/revealjs_02b_bril.qmd.html#basic-blocks-from-a-list-of-instructions-",
    "title": "Overview of Bril",
    "section": "basic blocks from a list of instructions-",
    "text": "basic blocks from a list of instructions-\nkeep adding instructions till we get to a terminator or a label (do we add labels?)\n\nin: list of instrs \nout: list of lists of instrs \n \nblocks = []\ncurr_block = []\nfor each instr in list \n    if the instruction is not a label put it on curr_block\n    if instr is a label or terminator \n       put curr_block on blocks\n       curr_block = []\n \n if curr_block is not empty add it to blocks\n return blocks \ntwo labels in a row do not need another block"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#step-2-add-edges",
    "href": "lectures/revealjs_02b_bril.qmd.html#step-2-add-edges",
    "title": "Overview of Bril",
    "section": "step 2 add edges",
    "text": "step 2 add edges\nfind cfg: in: is bril program in json \nfor each function find the list of basic blocks\nfor each basic block\n   get last_instr \n   if it is a terminator  br/jmp/ret \n     add edge from current block to successor  \n    --- what do we want to do with call? \n   else it is a fall through\n      add edge to next block\n\nwe need a map (block_map) label-&gt;block so we can add edges for blocks that end in br/jmp - can build this while getting the blocks or we can put the label as the first instruction\nhow do we handle fall through?\nwhat about a return\nif every block ends with a terminator, and every block has a label, then no fall through case\nwhat happens if try to delete the terminator (because the block never executes)"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#code",
    "href": "lectures/revealjs_02b_bril.qmd.html#code",
    "title": "Overview of Bril",
    "section": "code",
    "text": "code\nI’ll use a python data structure called OrderedDict, when you iterate over the items in a ordered dict, they come back in the order that they were installed."
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#ill-use-a-generator",
    "href": "lectures/revealjs_02b_bril.qmd.html#ill-use-a-generator",
    "title": "Overview of Bril",
    "section": "I’ll use a generator",
    "text": "I’ll use a generator\nIn Python, a generator is an iterator that is defined with a function using the yield statement.\n\nProduce items only once\nDo not store all the items in memory\nWhen items from the generator are requested, the function executes until it reaches a yield statement, which produces the next value. Execution then pauses, preserving the function’s state, until the next item is requested."
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#as-a-test-lets-print-out-the-blocks",
    "href": "lectures/revealjs_02b_bril.qmd.html#as-a-test-lets-print-out-the-blocks",
    "title": "Overview of Bril",
    "section": "as a test, lets print out the blocks",
    "text": "as a test, lets print out the blocks\n\n\nCode\ndef print_blocks(bril):\n    \"\"\"Given a Bril program, print out its basic blocks.\n    \"\"\"\n\n\n    func = bril['functions'][0]  # We only process one function.\n    for block in form_blocks(func['instrs']):\n        # Mark the block.\n        leader = block[0]\n        if 'label' in leader:\n            print( f\"block {leader['label']}\")\n            block = block[1:]  # Hide the label\n        else:\n            print('anonymous block:')\n\n        # Print the instructions.\n        for instr in block:\n            print(instr)\n\nprint_blocks(bril_program)\n\n\n\nthis test program has one block so pretty easy"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#as-a-test-lets-print-out-the-blocks-output",
    "href": "lectures/revealjs_02b_bril.qmd.html#as-a-test-lets-print-out-the-blocks-output",
    "title": "Overview of Bril",
    "section": "as a test, lets print out the blocks",
    "text": "as a test, lets print out the blocks\n\nanonymous block:\n{'op': 'const', 'type': 'int', 'dest': 'v0', 'value': 1}\n{'op': 'const', 'type': 'int', 'dest': 'v1', 'value': 2}\n{'op': 'add', 'type': 'int', 'dest': 'v2', 'args': ['v0', 'v1']}\n{'op': 'print', 'args': ['v2']}"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#lets-try-a-second-example-with-a-jmp",
    "href": "lectures/revealjs_02b_bril.qmd.html#lets-try-a-second-example-with-a-jmp",
    "title": "Overview of Bril",
    "section": "lets try a second example with a jmp",
    "text": "lets try a second example with a jmp\n@main {\n  v: int = const 4;\n  jmp .somewhere;\n  v: int = const 2;\n.somewhere:\n  print v;\n}"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#running-commands-inside-python",
    "href": "lectures/revealjs_02b_bril.qmd.html#running-commands-inside-python",
    "title": "Overview of Bril",
    "section": "running commands inside python",
    "text": "running commands inside python\nGitHub Copilot: subprocess.check_output is a function in Python’s subprocess module that runs a command with arguments and returns its output as a byte string. If the command exits with a non-zero exit status, it raises a CalledProcessError, which includes the exit status and output of the command. This function is useful for capturing the output of a command for further processing in Python."
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#print-the-blocks",
    "href": "lectures/revealjs_02b_bril.qmd.html#print-the-blocks",
    "title": "Overview of Bril",
    "section": "print the blocks",
    "text": "print the blocks\n\n\nCode\nimport json \nimport os\nimport subprocess\n\nresult =  subprocess.check_output('bril2json &lt; images/jmp.bril', shell=True)\ntest2json = json.loads(result)\nprint_blocks(test2json)\n\n\n\n\nanonymous block:\n{'dest': 'v', 'op': 'const', 'type': 'int', 'value': 4}\n{'labels': ['somewhere'], 'op': 'jmp'}\nanonymous block:\n{'dest': 'v', 'op': 'const', 'type': 'int', 'value': 2}\nblock somewhere\n{'args': ['v'], 'op': 'print'}"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#the-map-label-names-to-blocks",
    "href": "lectures/revealjs_02b_bril.qmd.html#the-map-label-names-to-blocks",
    "title": "Overview of Bril",
    "section": "the map (label names to blocks)",
    "text": "the map (label names to blocks)\n\n\nCode\nfrom collections import OrderedDict\n\n\ndef block_map(blocks):\n    \"\"\"Given a sequence of basic blocks, which are lists of instructions,\n    produce a `OrderedDict` mapping names to blocks.\n\n    The name of the block comes from the label it starts with, if any.\n    Anonymous blocks, which don't start with a label, get an\n    automatically generated name. Blocks in the mapping have their\n    labels removed.\n    \"\"\"\n    by_name = OrderedDict()\n\n    for block in blocks:\n        # Generate a name for the block.\n        if 'label' in block[0]:\n            # The block has a label. Remove the label but use it for the\n            # block's name.\n            name = block[0]['label']\n            block = block[1:]\n        else:\n            # Make up a new name for this anonymous block.\n            name = f'gen_bk_{len(by_name)}'\n\n        # Add the block to the mapping.\n        by_name[name] = block\n\n    return by_name\n\n\nblks = form_blocks(test2json['functions'][0]['instrs'])\nod = block_map(blks)\nfor (name, instrs) in od.items():\n    print (name, instrs)"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#the-map-label-names-to-blocks-output",
    "href": "lectures/revealjs_02b_bril.qmd.html#the-map-label-names-to-blocks-output",
    "title": "Overview of Bril",
    "section": "the map (label names to blocks)",
    "text": "the map (label names to blocks)\n\ngen_bk_0 [{'dest': 'v', 'op': 'const', 'type': 'int', 'value': 4}, {'labels': ['somewhere'], 'op': 'jmp'}]\ngen_bk_1 [{'dest': 'v', 'op': 'const', 'type': 'int', 'value': 2}]\nsomewhere [{'args': ['v'], 'op': 'print'}]"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#the-cfg-given-the-block-map-pseudo-code",
    "href": "lectures/revealjs_02b_bril.qmd.html#the-cfg-given-the-block-map-pseudo-code",
    "title": "Overview of Bril",
    "section": "the cfg given the block map (pseudo code)",
    "text": "the cfg given the block map (pseudo code)\nout cfg = {} \n# map label -&gt; list of labels the successors of the block\n\nfor i,block in enumerate(blocks)  # blocks is a ordereddict \n    last = block[i]  # last instruction\n    if last is jmp:\n        cfg[block_name] = jmp.dest\n    elif last is br:\n        cfg[block.name] = [ last.if_label, last.else_label]\n    else\n        # fall through\n        cfg[block_name] = blocks[i+1].name  ## special case for last block"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#cfg",
    "href": "lectures/revealjs_02b_bril.qmd.html#cfg",
    "title": "Overview of Bril",
    "section": "cfg",
    "text": "cfg\n\n\nCode\ndef get_cfg(ordered_blocks):\n    cfg = {}\n\n    labels = list(ordered_blocks.keys())\n\n    for i, (block_name, block) in enumerate(ordered_blocks.items()):\n        last = block[-1]\n        op = last['op']\n\n        if op == 'jmp':\n            cfg[block_name] = last['labels']\n        elif op == 'br':\n            cfg[block_name] = last['labels']\n        else:\n            if i+1 &lt; len(labels):  # last block does not fall through\n                cfg[block_name] = [labels[i+1]]\n            else:\n                cfg[block_name] = []\n    return cfg\n\n\nblks = form_blocks(test2json['functions'][0]['instrs'])\nod = block_map(blks)\ncfg = get_cfg(od)\n\nprint(cfg)"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#cfg-output",
    "href": "lectures/revealjs_02b_bril.qmd.html#cfg-output",
    "title": "Overview of Bril",
    "section": "cfg",
    "text": "cfg\n\n{'gen_bk_0': ['somewhere'], 'gen_bk_1': ['somewhere'], 'somewhere': []}"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#graph",
    "href": "lectures/revealjs_02b_bril.qmd.html#graph",
    "title": "Overview of Bril",
    "section": "graph",
    "text": "graph\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\ngen_bk_0--&gt; somewhere\ngen_bk_1 --&gt; somewhere"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#graph-output",
    "href": "lectures/revealjs_02b_bril.qmd.html#graph-output",
    "title": "Overview of Bril",
    "section": "graph",
    "text": "graph\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\ngen_bk_0--&gt; somewhere\ngen_bk_1 --&gt; somewhere"
  },
  {
    "objectID": "lectures/revealjs_02b_bril.qmd.html#homework",
    "href": "lectures/revealjs_02b_bril.qmd.html#homework",
    "title": "Overview of Bril",
    "section": "homework",
    "text": "homework\nDue in 1 week\nYour goal is to get familiar with Bril."
  },
  {
    "objectID": "lectures/12_memory.html",
    "href": "lectures/12_memory.html",
    "title": "Dynamic Memory Management",
    "section": "",
    "text": "Warning\n\n\n\nnot done\n\n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Memory Management"
    ]
  },
  {
    "objectID": "lectures/05_global.html",
    "href": "lectures/05_global.html",
    "title": "5 Global Analysis",
    "section": "",
    "text": "We are going to define assorted graph properties, that can be calculated on cfgs.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#graph-properties",
    "href": "lectures/05_global.html#graph-properties",
    "title": "5 Global Analysis",
    "section": "",
    "text": "We are going to define assorted graph properties, that can be calculated on cfgs.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#dominators",
    "href": "lectures/05_global.html#dominators",
    "title": "5 Global Analysis",
    "section": "dominators",
    "text": "dominators\nWe first define a binary relation on cfg nodes, called dominance. a node d dominates a node i (d dom i) if every possible execution path in the cfg that goes from the entry to i goes through d. \n\nDom is reflexive, so a dom a for all nodes a.\nDom is transitive, a dom b, b dom c ==&gt; a dom c\nDom is anti-symmetric if a dom b, and b dom a then b = a",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#dominator-trees",
    "href": "lectures/05_global.html#dominator-trees",
    "title": "5 Global Analysis",
    "section": "dominator trees",
    "text": "dominator trees\nWe next define immediate dominators a idom b, a != b and there is no c != a and c != b where a dom c and c dom b.\n\nidom is unique\nidom forms a tree called the dominator tree, root is the entry of the cfg\n\nA strict dominator a sdom b if a dom b and a != b",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#an-example",
    "href": "lectures/05_global.html#an-example",
    "title": "5 Global Analysis",
    "section": "an example",
    "text": "an example\n\n\nA control flow graph\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0 --&gt; n1;\nn1 --&gt; n2;\nn1 --&gt; n3;\nn2 --&gt; n4;\nn3 --&gt; n4;\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0 --&gt; n1;\nn1 --&gt; n2;\nn1 --&gt; n3;\nn2 --&gt; n4;\nn3 --&gt; n4;\n\n\n\n\n\n\n\nThe dominator tree\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0 --&gt; n1;\nn1 --&gt; n2;n1 --&gt; n3\nn1 --&gt; n4;\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0 --&gt; n1;\nn1 --&gt; n2;n1 --&gt; n3\nn1 --&gt; n4;",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#simple-implementation-dominators",
    "href": "lectures/05_global.html#simple-implementation-dominators",
    "title": "5 Global Analysis",
    "section": "simple implementation dominators",
    "text": "simple implementation dominators\n\\[\n\\begin{gathered}\n\\operatorname{Dom}\\left(n_o\\right)=\\left\\{n_o\\right\\} \\\\\n\\operatorname{Dom}(n)=\\{n\\} \\cup\\left(\\bigcap_{p \\in \\operatorname{preds}(n)} \\operatorname{Dom}(p)\\right)\n\\end{gathered}\n\\]\n\nTo find the dominators of a node, first put the node itself in the dominators set. Then, take all the common (i.e. intersection) dominators of its predecessors and put them in the set.\nWhat order do we want to process the nodes?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#pseudo-code",
    "href": "lectures/05_global.html#pseudo-code",
    "title": "5 Global Analysis",
    "section": "pseudo code",
    "text": "pseudo code\nassume nodes start at 0,\ncompute_dominators(CFG cfg) {\n  cfg[0].dominators = {0}\n  for (bb in cfg except 0) {\n    b.dominators = {all nodes in cfg}\n  }\n\n  do {\n    change = false;\n    for (bb in cfg except 0) {\n      temp = {all nodes in cfg}\n      for (pred in bb.predecessors) {\n        temp = intersect(temp, pred.dominators)\n      }\n      temp = union(temp, {bb})\n      if (temp != bb.dominators) {\n        change = true\n        bb.dominators = temp\n      }\n    }\n  } while (change);\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#how-do-we-implement-this",
    "href": "lectures/05_global.html#how-do-we-implement-this",
    "title": "5 Global Analysis",
    "section": "How do we implement this",
    "text": "How do we implement this\nnumber the vertices starting at 0, vertices are 0,1,2, number_of_vertices -1 so we could use a bit-vector for the set, and we should process vertices in reverse post order",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#a-faster-way",
    "href": "lectures/05_global.html#a-faster-way",
    "title": "5 Global Analysis",
    "section": "a faster way",
    "text": "a faster way\nCooper, Harvey, Kennedy Algorithm\nif we have the dominator tree, finding immediate dominators is easy, its the parent of the node Finding dominators is also easy, its all the parents on the path from the entry to the node\nsuppose we have a node in the cfg with two parents, like n4, if we takes paths backward in the dominator tree the first common ancestor is n1, (the dominator)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#a-more-complex-example",
    "href": "lectures/05_global.html#a-more-complex-example",
    "title": "5 Global Analysis",
    "section": "a more complex example",
    "text": "a more complex example\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0 --&gt; n5;\nn0 --&gt; n1;\nn5 --&gt; n7;\nn5 --&gt; n6;\nn1 --&gt; n2 ;\nn1 --&gt; n3;\nn7 --&gt; n8;\nn6 --&gt; n4;\nn2 --&gt; n4;\nn4 --&gt; n8 ;\nn3 --&gt; n8;\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0 --&gt; n5;\nn0 --&gt; n1;\nn5 --&gt; n7;\nn5 --&gt; n6;\nn1 --&gt; n2 ;\nn1 --&gt; n3;\nn7 --&gt; n8;\nn6 --&gt; n4;\nn2 --&gt; n4;\nn4 --&gt; n8 ;\nn3 --&gt; n8;\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0--&gt; n5\nn0 --&gt; n1\nn5--&gt; n7\nn5--&gt; n6\nn1--&gt; n2\nn1 --&gt; n3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0--&gt; n5\nn0 --&gt; n1\nn5--&gt; n7\nn5--&gt; n6\nn1--&gt; n2\nn1 --&gt; n3\n\n\n\n\n\n\nneed n4 and n8\n\nboth are dominated by n0",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#subproblem-find-lowest-common-ancestor-in-dt-of-two-nodes-a-and-b",
    "href": "lectures/05_global.html#subproblem-find-lowest-common-ancestor-in-dt-of-two-nodes-a-and-b",
    "title": "5 Global Analysis",
    "section": "subproblem: find lowest common ancestor in dt of two nodes a and b",
    "text": "subproblem: find lowest common ancestor in dt of two nodes a and b\nfor each node in the dom tree we have the depth, how far from the root, so if a and b have the same parent, that is the dominator, otherwise move the node with the higher depth up one\na fast way to determine which node is lower keep the nodes in post order, nodes at the top of the cfg have higher numbers",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#part1",
    "href": "lectures/05_global.html#part1",
    "title": "5 Global Analysis",
    "section": "part1",
    "text": "part1\nintersect(b1, b2, idoms,postorder_map) {\n  while (b1 != b2) {\n    if (postorder_map[b1] &lt; postorder_map[b2]) {\n      b1 = idoms[b1];\n    } else {\n      b2 = idoms[b2];\n    }\n  }\n  return b1;",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#pseudo-code-1",
    "href": "lectures/05_global.html#pseudo-code-1",
    "title": "5 Global Analysis",
    "section": "pseudo code",
    "text": "pseudo code\nvoid compute_dominators(CFG cfg) {\n  // Some initialization steps and e.g. get postorder.\n\n  // Map its basic block to its postorder traversal.\n  foreach (p ; postorder) {\n    postorder_map[p] = counter;\n    ++counter;\n  }\n\n  bool change;\n  do {\n    change = false;\n    foreach_reverse i in postorder) {\n      bb = cffg block i \n      new_idom = bb.preds[0];  // Arbitrarily choose the first predecessor\n\n      for pred in preds (bb)) {\n        if (cfg.idoms[pred] != CFG.UNDEFINED_IDOM) {\n          new_idom = intersect(new_idom, pred, cfg.idoms, postorder_map);\n        }\n      }\n      if (cfg.idoms[i] != new_idom) {\n        cfg.idoms[i] = new_idom;\n        change = true;\n      }\n    }\n  } while (change);\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#dominator-frontiers",
    "href": "lectures/05_global.html#dominator-frontiers",
    "title": "5 Global Analysis",
    "section": "dominator frontiers",
    "text": "dominator frontiers\nA node A has a dominance frontier which are set of nodes b where A does not dominate b but A dominates a pred of b. Lets see n5 dominance frontier\nFinally we have a post dominates b if all paths from b to the exit go through a. for instance n4 post dominates n6.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#natural-loops",
    "href": "lectures/05_global.html#natural-loops",
    "title": "5 Global Analysis",
    "section": "natural loops",
    "text": "natural loops\n\ngraph TD;\n  entry --&gt; loop\n  loop --&gt; if \n  if --&gt; then\n  if --&gt; else\n  then --&gt; endif\n  else --&gt; endif\n  endif --&gt; loop\n  loop --&gt; exit\n\n\n\n\n  graph TD;\n  entry --&gt; loop\n  loop --&gt; if \n  if --&gt; then\n  if --&gt; else\n  then --&gt; endif\n  else --&gt; endif\n  endif --&gt; loop\n  loop --&gt; exit\n\n\n\n\n\n\n\nhas to have a cycle in cfg (strongly connected)\nsingle entry point (called the header ) header",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#cycle-but-not-header",
    "href": "lectures/05_global.html#cycle-but-not-header",
    "title": "5 Global Analysis",
    "section": "cycle but not header",
    "text": "cycle but not header\nHow about an example that has a cycle and no header\n\ngraph TD;\n    entry --&gt; if;\n    if --&gt; loop1\n    if --&gt; loop2\n    loop2 --&gt; loop1\nloop1 --&gt; loop2\n\n\n\n\n    graph TD;\n    entry --&gt; if;\n    if --&gt; loop1\n    if --&gt; loop2\n    loop2 --&gt; loop1\nloop1 --&gt; loop2\n\n\n\n\n\n\nThis loop has two entry points.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#natural-loops-1",
    "href": "lectures/05_global.html#natural-loops-1",
    "title": "5 Global Analysis",
    "section": "natural loops",
    "text": "natural loops\nA back-edge is an edge A-&gt;B, where B dominates A\nother edges are forward edges\nNatural loops:\n\nfor a back-edge A-&gt;B, B is the header of the loop\nthe smallest set of vertices L including A and B, such that for all v in L either preds(v) are in L or v == B",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#example",
    "href": "lectures/05_global.html#example",
    "title": "5 Global Analysis",
    "section": "example",
    "text": "example\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    entry --&gt; H1\n    H1 --&gt; A\n    A --&gt; H2\n    H2 --&gt; B\n    B --&gt; H2\n    B --&gt; H1\n    H1 --&gt; exit\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    entry --&gt; H1\n    H1 --&gt; A\n    A --&gt; H2\n    H2 --&gt; B\n    B --&gt; H2\n    B --&gt; H1\n    H1 --&gt; exit\n\n\n\n\n\n\n\n\nBackedges B -&gt; H2,\nB-&gt; H1\n\nfor B-&gt; H2, loop is H2,\nfor B-&gt; H1, loop is H1, A, H2, B",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#reducible-control-flow",
    "href": "lectures/05_global.html#reducible-control-flow",
    "title": "5 Global Analysis",
    "section": "reducible control flow",
    "text": "reducible control flow\nin a reducible cfg every back edge has a natural loop.\nA reducible CFG is one with edges that can be partitioned into two disjoint sets: forward edges, and back edges, such that:\nForward edges form a directed acyclic graph with all nodes reachable from the entry node.\nFor all back edges (A, B), node B dominates node A.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#what-is-the-surface-version",
    "href": "lectures/05_global.html#what-is-the-surface-version",
    "title": "5 Global Analysis",
    "section": "what is the surface version",
    "text": "what is the surface version\nStructured programming languages are often designed such that all CFGs they produce are reducible, and common structured programming statements such as IF, FOR, WHILE, BREAK, and CONTINUE produce reducible graphs. To produce irreducible graphs, statements such as GOTO are needed. Irreducible graphs may also be produced by some compiler optimizations.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#t1-and-t2-transforms",
    "href": "lectures/05_global.html#t1-and-t2-transforms",
    "title": "5 Global Analysis",
    "section": "t1 and t2 transforms",
    "text": "t1 and t2 transforms\nLet G be a CFG. Suppose n is a node in G with a self-loop, that is, an edge from n to itself.\nTransformation T1: on node n is removal of this self-loop.\nLet n1 and n2 be nodes in G such that n2 has the unique direct ancestor n1, and n2 is not the initial node.\ntransformation T2: on node pair (n1,n2) is merging nodes n1 and n2 into one node,",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#t1-t2",
    "href": "lectures/05_global.html#t1-t2",
    "title": "5 Global Analysis",
    "section": "t1 / t2",
    "text": "t1 / t2\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n\na1[\" \"] --&gt; n\nn --&gt; n\n n --&gt; b[\" \"]\nn --&gt; b1[\" \"]\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n\na1[\" \"] --&gt; n\nn --&gt; n\n n --&gt; b[\" \"]\nn --&gt; b1[\" \"]\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n1\na1[\" \"] --&gt; n1\nn1 --&gt; n2\nn2--&gt; n1\n n2 --&gt; b[\" \"]\nn2 --&gt; b1[\" \"]\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n1\na1[\" \"] --&gt; n1\nn1 --&gt; n2\nn2--&gt; n1\n n2 --&gt; b[\" \"]\nn2 --&gt; b1[\" \"]\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n\na1[\" \"] --&gt; n\n n --&gt; b[\" \"]\nn --&gt; b1[\" \"]\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n\na1[\" \"] --&gt; n\n n --&gt; b[\" \"]\nn --&gt; b1[\" \"]\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n1[\"n1_n2\"]\na1[\" \"] --&gt; n1\n n1 --&gt; b[\" \"]\nn1 --&gt; b1[\" \"]\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n1[\"n1_n2\"]\na1[\" \"] --&gt; n1\n n1 --&gt; b[\" \"]\nn1 --&gt; b1[\" \"]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#example-1",
    "href": "lectures/05_global.html#example-1",
    "title": "5 Global Analysis",
    "section": "example",
    "text": "example\nint  n = (count + 7) / 8;\nswitch (count % 8) {\ncase 0: do { *to = *from++;\ncase 7:      *to = *from++;\ncase 6:      *to = *from++;\ncase 5:      *to = *from++;\ncase 4:      *to = *from++;\ncase 3:      *to = *from++;\ncase 2:      *to = *from++;\ncase 1:      *to = *from++;\n        } while (--n &gt; 0);\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#simplified-control-flow",
    "href": "lectures/05_global.html#simplified-control-flow",
    "title": "5 Global Analysis",
    "section": "simplified control flow",
    "text": "simplified control flow\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    entry --&gt; switch;\n    switch --&gt; case0-7\n    switch --&gt; case1\n    switch --&gt; case2\n    case0-7 --&gt; case2\n    case2--&gt; case1\n    case1 --&gt; dowhile\n    dowhile --&gt; case0-7\n    dowhile --&gt; exit\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    entry --&gt; switch;\n    switch --&gt; case0-7\n    switch --&gt; case1\n    switch --&gt; case2\n    case0-7 --&gt; case2\n    case2--&gt; case1\n    case1 --&gt; dowhile\n    dowhile --&gt; case0-7\n    dowhile --&gt; exit\n\n\n\n\n\n\nnot reducible",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#other-optimizations-interactions",
    "href": "lectures/05_global.html#other-optimizations-interactions",
    "title": "5 Global Analysis",
    "section": "other optimizations interactions",
    "text": "other optimizations interactions\nloop: if (cond) goto past_loop\n    s1\n    call bar()\n    goto loop\npastloop:\n\nfunction bar()\n    b1 \n    if () return\n    b2",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/05_global.html#inline-the-function-combine-jmps-to-jmps",
    "href": "lectures/05_global.html#inline-the-function-combine-jmps-to-jmps",
    "title": "5 Global Analysis",
    "section": "inline the function, combine jmps to jmps",
    "text": "inline the function, combine jmps to jmps\n\n\nloop: if (cond) goto past_loop\n    s1\n    b1\n    if () go to next\n    b2\n    next:\ngoto loop\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    loop--&gt; s1\n    loop---&gt; past_loop\n    s1--&gt; b1\n    b1 --&gt;inline_if\n    inline_if --&gt; b2\n    b2 --&gt; next_goto\n    inline_if --&gt; loop\n    next_goto --&gt; loop\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    loop--&gt; s1\n    loop---&gt; past_loop\n    s1--&gt; b1\n    b1 --&gt;inline_if\n    inline_if --&gt; b2\n    b2 --&gt; next_goto\n    inline_if --&gt; loop\n    next_goto --&gt; loop\n\n\n\n\n\n\n\n\nNow we have two back edges so two loops",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "5 Global Analysis"
    ]
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#graph-properties",
    "href": "lectures/revealjs_05_global.qmd.html#graph-properties",
    "title": "5 Global Analysis",
    "section": "Graph Properties",
    "text": "Graph Properties\nWe are going to define assorted graph properties, that can be calculated on cfgs."
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#dominators",
    "href": "lectures/revealjs_05_global.qmd.html#dominators",
    "title": "5 Global Analysis",
    "section": "dominators",
    "text": "dominators\nWe first define a binary relation on cfg nodes, called dominance. a node d dominates a node i (d dom i) if every possible execution path in the cfg that goes from the entry to i goes through d. \n\nDom is reflexive, so a dom a for all nodes a.\nDom is transitive, a dom b, b dom c ==&gt; a dom c\nDom is anti-symmetric if a dom b, and b dom a then b = a"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#dominator-trees",
    "href": "lectures/revealjs_05_global.qmd.html#dominator-trees",
    "title": "5 Global Analysis",
    "section": "dominator trees",
    "text": "dominator trees\nWe next define immediate dominators a idom b, a != b and there is no c != a and c != b where a dom c and c dom b.\n\nidom is unique\nidom forms a tree called the dominator tree, root is the entry of the cfg\n\nA strict dominator a sdom b if a dom b and a != b"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#an-example",
    "href": "lectures/revealjs_05_global.qmd.html#an-example",
    "title": "5 Global Analysis",
    "section": "an example",
    "text": "an example\n\n\nA control flow graph\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0 --&gt; n1;\nn1 --&gt; n2;\nn1 --&gt; n3;\nn2 --&gt; n4;\nn3 --&gt; n4;\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0 --&gt; n1;\nn1 --&gt; n2;\nn1 --&gt; n3;\nn2 --&gt; n4;\nn3 --&gt; n4;\n\n\n\n\n\n\n\nThe dominator tree\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0 --&gt; n1;\nn1 --&gt; n2;n1 --&gt; n3\nn1 --&gt; n4;\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0 --&gt; n1;\nn1 --&gt; n2;n1 --&gt; n3\nn1 --&gt; n4;"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#simple-implementation-dominators",
    "href": "lectures/revealjs_05_global.qmd.html#simple-implementation-dominators",
    "title": "5 Global Analysis",
    "section": "simple implementation dominators",
    "text": "simple implementation dominators\n\\[\n\\begin{gathered}\n\\operatorname{Dom}\\left(n_o\\right)=\\left\\{n_o\\right\\} \\\\\n\\operatorname{Dom}(n)=\\{n\\} \\cup\\left(\\bigcap_{p \\in \\operatorname{preds}(n)} \\operatorname{Dom}(p)\\right)\n\\end{gathered}\n\\]"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#pseudo-code",
    "href": "lectures/revealjs_05_global.qmd.html#pseudo-code",
    "title": "5 Global Analysis",
    "section": "pseudo code",
    "text": "pseudo code\nassume nodes start at 0,\ncompute_dominators(CFG cfg) {\n  cfg[0].dominators = {0}\n  for (bb in cfg except 0) {\n    b.dominators = {all nodes in cfg}\n  }\n\n  do {\n    change = false;\n    for (bb in cfg except 0) {\n      temp = {all nodes in cfg}\n      for (pred in bb.predecessors) {\n        temp = intersect(temp, pred.dominators)\n      }\n      temp = union(temp, {bb})\n      if (temp != bb.dominators) {\n        change = true\n        bb.dominators = temp\n      }\n    }\n  } while (change);\n}"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#how-do-we-implement-this",
    "href": "lectures/revealjs_05_global.qmd.html#how-do-we-implement-this",
    "title": "5 Global Analysis",
    "section": "How do we implement this",
    "text": "How do we implement this\nnumber the vertices starting at 0, vertices are 0,1,2, number_of_vertices -1 so we could use a bit-vector for the set, and we should process vertices in reverse post order"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#a-faster-way",
    "href": "lectures/revealjs_05_global.qmd.html#a-faster-way",
    "title": "5 Global Analysis",
    "section": "a faster way",
    "text": "a faster way\nCooper, Harvey, Kennedy Algorithm\nif we have the dominator tree, finding immediate dominators is easy, its the parent of the node Finding dominators is also easy, its all the parents on the path from the entry to the node\nsuppose we have a node in the cfg with two parents, like n4, if we takes paths backward in the dominator tree the first common ancestor is n1, (the dominator)"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#a-more-complex-example",
    "href": "lectures/revealjs_05_global.qmd.html#a-more-complex-example",
    "title": "5 Global Analysis",
    "section": "a more complex example",
    "text": "a more complex example\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0 --&gt; n5;\nn0 --&gt; n1;\nn5 --&gt; n7;\nn5 --&gt; n6;\nn1 --&gt; n2 ;\nn1 --&gt; n3;\nn7 --&gt; n8;\nn6 --&gt; n4;\nn2 --&gt; n4;\nn4 --&gt; n8 ;\nn3 --&gt; n8;\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0 --&gt; n5;\nn0 --&gt; n1;\nn5 --&gt; n7;\nn5 --&gt; n6;\nn1 --&gt; n2 ;\nn1 --&gt; n3;\nn7 --&gt; n8;\nn6 --&gt; n4;\nn2 --&gt; n4;\nn4 --&gt; n8 ;\nn3 --&gt; n8;\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0--&gt; n5\nn0 --&gt; n1\nn5--&gt; n7\nn5--&gt; n6\nn1--&gt; n2\nn1 --&gt; n3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn0--&gt; n5\nn0 --&gt; n1\nn5--&gt; n7\nn5--&gt; n6\nn1--&gt; n2\nn1 --&gt; n3\n\n\n\n\n\n\nneed n4 and n8\n\nboth are dominated by n0"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#subproblem-find-lowest-common-ancestor-in-dt-of-two-nodes-a-and-b",
    "href": "lectures/revealjs_05_global.qmd.html#subproblem-find-lowest-common-ancestor-in-dt-of-two-nodes-a-and-b",
    "title": "5 Global Analysis",
    "section": "subproblem: find lowest common ancestor in dt of two nodes a and b",
    "text": "subproblem: find lowest common ancestor in dt of two nodes a and b\nfor each node in the dom tree we have the depth, how far from the root, so if a and b have the same parent, that is the dominator, otherwise move the node with the higher depth up one\na fast way to determine which node is lower keep the nodes in post order, nodes at the top of the cfg have higher numbers"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#part1",
    "href": "lectures/revealjs_05_global.qmd.html#part1",
    "title": "5 Global Analysis",
    "section": "part1",
    "text": "part1\nintersect(b1, b2, idoms,postorder_map) {\n  while (b1 != b2) {\n    if (postorder_map[b1] &lt; postorder_map[b2]) {\n      b1 = idoms[b1];\n    } else {\n      b2 = idoms[b2];\n    }\n  }\n  return b1;"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#pseudo-code-1",
    "href": "lectures/revealjs_05_global.qmd.html#pseudo-code-1",
    "title": "5 Global Analysis",
    "section": "pseudo code",
    "text": "pseudo code\nvoid compute_dominators(CFG cfg) {\n  // Some initialization steps and e.g. get postorder.\n\n  // Map its basic block to its postorder traversal.\n  foreach (p ; postorder) {\n    postorder_map[p] = counter;\n    ++counter;\n  }\n\n  bool change;\n  do {\n    change = false;\n    foreach_reverse i in postorder) {\n      bb = cffg block i \n      new_idom = bb.preds[0];  // Arbitrarily choose the first predecessor\n\n      for pred in preds (bb)) {\n        if (cfg.idoms[pred] != CFG.UNDEFINED_IDOM) {\n          new_idom = intersect(new_idom, pred, cfg.idoms, postorder_map);\n        }\n      }\n      if (cfg.idoms[i] != new_idom) {\n        cfg.idoms[i] = new_idom;\n        change = true;\n      }\n    }\n  } while (change);\n}"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#dominator-frontiers",
    "href": "lectures/revealjs_05_global.qmd.html#dominator-frontiers",
    "title": "5 Global Analysis",
    "section": "dominator frontiers",
    "text": "dominator frontiers\nA node A has a dominance frontier which are set of nodes b where A does not dominate b but A dominates a pred of b. Lets see n5 dominance frontier\nFinally we have a post dominates b if all paths from b to the exit go through a. for instance n4 post dominates n6."
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#natural-loops",
    "href": "lectures/revealjs_05_global.qmd.html#natural-loops",
    "title": "5 Global Analysis",
    "section": "natural loops",
    "text": "natural loops\n\n\nCode\ngraph TD;\n  entry --&gt; loop\n  loop --&gt; if \n  if --&gt; then\n  if --&gt; else\n  then --&gt; endif\n  else --&gt; endif\n  endif --&gt; loop\n  loop --&gt; exit\n\n\n\n\n\n  graph TD;\n  entry --&gt; loop\n  loop --&gt; if \n  if --&gt; then\n  if --&gt; else\n  then --&gt; endif\n  else --&gt; endif\n  endif --&gt; loop\n  loop --&gt; exit\n\n\n\n\n\n\n\nhas to have a cycle in cfg (strongly connected)\nsingle entry point (called the header ) header"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#cycle-but-not-header",
    "href": "lectures/revealjs_05_global.qmd.html#cycle-but-not-header",
    "title": "5 Global Analysis",
    "section": "cycle but not header",
    "text": "cycle but not header\nHow about an example that has a cycle and no header\n\n\nCode\ngraph TD;\n    entry --&gt; if;\n    if --&gt; loop1\n    if --&gt; loop2\n    loop2 --&gt; loop1\nloop1 --&gt; loop2\n\n\n\n\n\n    graph TD;\n    entry --&gt; if;\n    if --&gt; loop1\n    if --&gt; loop2\n    loop2 --&gt; loop1\nloop1 --&gt; loop2\n\n\n\n\n\n\nThis loop has two entry points."
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#natural-loops-1",
    "href": "lectures/revealjs_05_global.qmd.html#natural-loops-1",
    "title": "5 Global Analysis",
    "section": "natural loops",
    "text": "natural loops\nA back-edge is an edge A-&gt;B, where B dominates A\nother edges are forward edges\nNatural loops:\n\nfor a back-edge A-&gt;B, B is the header of the loop\nthe smallest set of vertices L including A and B, such that for all v in L either preds(v) are in L or v == B"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#example",
    "href": "lectures/revealjs_05_global.qmd.html#example",
    "title": "5 Global Analysis",
    "section": "example",
    "text": "example\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    entry --&gt; H1\n    H1 --&gt; A\n    A --&gt; H2\n    H2 --&gt; B\n    B --&gt; H2\n    B --&gt; H1\n    H1 --&gt; exit\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    entry --&gt; H1\n    H1 --&gt; A\n    A --&gt; H2\n    H2 --&gt; B\n    B --&gt; H2\n    B --&gt; H1\n    H1 --&gt; exit\n\n\n\n\n\n\n\n\nBackedges B -&gt; H2,\nB-&gt; H1\n\nfor B-&gt; H2, loop is H2,\nfor B-&gt; H1, loop is H1, A, H2, B"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#reducible-control-flow",
    "href": "lectures/revealjs_05_global.qmd.html#reducible-control-flow",
    "title": "5 Global Analysis",
    "section": "reducible control flow",
    "text": "reducible control flow\nin a reducible cfg every back edge has a natural loop.\nA reducible CFG is one with edges that can be partitioned into two disjoint sets: forward edges, and back edges, such that:\nForward edges form a directed acyclic graph with all nodes reachable from the entry node.\nFor all back edges (A, B), node B dominates node A."
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#what-is-the-surface-version",
    "href": "lectures/revealjs_05_global.qmd.html#what-is-the-surface-version",
    "title": "5 Global Analysis",
    "section": "what is the surface version",
    "text": "what is the surface version\nStructured programming languages are often designed such that all CFGs they produce are reducible, and common structured programming statements such as IF, FOR, WHILE, BREAK, and CONTINUE produce reducible graphs. To produce irreducible graphs, statements such as GOTO are needed. Irreducible graphs may also be produced by some compiler optimizations."
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#t1-and-t2-transforms",
    "href": "lectures/revealjs_05_global.qmd.html#t1-and-t2-transforms",
    "title": "5 Global Analysis",
    "section": "t1 and t2 transforms",
    "text": "t1 and t2 transforms\nLet G be a CFG. Suppose n is a node in G with a self-loop, that is, an edge from n to itself.\nTransformation T1: on node n is removal of this self-loop.\nLet n1 and n2 be nodes in G such that n2 has the unique direct ancestor n1, and n2 is not the initial node.\ntransformation T2: on node pair (n1,n2) is merging nodes n1 and n2 into one node,"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#t1-t2",
    "href": "lectures/revealjs_05_global.qmd.html#t1-t2",
    "title": "5 Global Analysis",
    "section": "t1 / t2",
    "text": "t1 / t2\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n\na1[\" \"] --&gt; n\nn --&gt; n\n n --&gt; b[\" \"]\nn --&gt; b1[\" \"]\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n\na1[\" \"] --&gt; n\nn --&gt; n\n n --&gt; b[\" \"]\nn --&gt; b1[\" \"]\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n1\na1[\" \"] --&gt; n1\nn1 --&gt; n2\nn2--&gt; n1\n n2 --&gt; b[\" \"]\nn2 --&gt; b1[\" \"]\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n1\na1[\" \"] --&gt; n1\nn1 --&gt; n2\nn2--&gt; n1\n n2 --&gt; b[\" \"]\nn2 --&gt; b1[\" \"]\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n\na1[\" \"] --&gt; n\n n --&gt; b[\" \"]\nn --&gt; b1[\" \"]\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n\na1[\" \"] --&gt; n\n n --&gt; b[\" \"]\nn --&gt; b1[\" \"]\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n1[\"n1_n2\"]\na1[\" \"] --&gt; n1\n n1 --&gt; b[\" \"]\nn1 --&gt; b1[\" \"]\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    a0[\" \"] --&gt; n1[\"n1_n2\"]\na1[\" \"] --&gt; n1\n n1 --&gt; b[\" \"]\nn1 --&gt; b1[\" \"]"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#example-1",
    "href": "lectures/revealjs_05_global.qmd.html#example-1",
    "title": "5 Global Analysis",
    "section": "example",
    "text": "example\nint  n = (count + 7) / 8;\nswitch (count % 8) {\ncase 0: do { *to = *from++;\ncase 7:      *to = *from++;\ncase 6:      *to = *from++;\ncase 5:      *to = *from++;\ncase 4:      *to = *from++;\ncase 3:      *to = *from++;\ncase 2:      *to = *from++;\ncase 1:      *to = *from++;\n        } while (--n &gt; 0);\n}"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#simplified-control-flow",
    "href": "lectures/revealjs_05_global.qmd.html#simplified-control-flow",
    "title": "5 Global Analysis",
    "section": "simplified control flow",
    "text": "simplified control flow\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    entry --&gt; switch;\n    switch --&gt; case0-7\n    switch --&gt; case1\n    switch --&gt; case2\n    case0-7 --&gt; case2\n    case2--&gt; case1\n    case1 --&gt; dowhile\n    dowhile --&gt; case0-7\n    dowhile --&gt; exit\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    entry --&gt; switch;\n    switch --&gt; case0-7\n    switch --&gt; case1\n    switch --&gt; case2\n    case0-7 --&gt; case2\n    case2--&gt; case1\n    case1 --&gt; dowhile\n    dowhile --&gt; case0-7\n    dowhile --&gt; exit\n\n\n\n\n\n\nnot reducible"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#other-optimizations-interactions",
    "href": "lectures/revealjs_05_global.qmd.html#other-optimizations-interactions",
    "title": "5 Global Analysis",
    "section": "other optimizations interactions",
    "text": "other optimizations interactions\nloop: if (cond) goto past_loop\n    s1\n    call bar()\n    goto loop\npastloop:\n\nfunction bar()\n    b1 \n    if () return\n    b2"
  },
  {
    "objectID": "lectures/revealjs_05_global.qmd.html#inline-the-function-combine-jmps-to-jmps",
    "href": "lectures/revealjs_05_global.qmd.html#inline-the-function-combine-jmps-to-jmps",
    "title": "5 Global Analysis",
    "section": "inline the function, combine jmps to jmps",
    "text": "inline the function, combine jmps to jmps\n\n\nloop: if (cond) goto past_loop\n    s1\n    b1\n    if () go to next\n    b2\n    next:\ngoto loop\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    loop--&gt; s1\n    loop---&gt; past_loop\n    s1--&gt; b1\n    b1 --&gt;inline_if\n    inline_if --&gt; b2\n    b2 --&gt; next_goto\n    inline_if --&gt; loop\n    next_goto --&gt; loop\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n    graph TD;\n    loop--&gt; s1\n    loop---&gt; past_loop\n    s1--&gt; b1\n    b1 --&gt;inline_if\n    inline_if --&gt; b2\n    b2 --&gt; next_goto\n    inline_if --&gt; loop\n    next_goto --&gt; loop\n\n\n\n\n\n\n\nNow we have two back edges so two loops"
  },
  {
    "objectID": "lectures/junk.html",
    "href": "lectures/junk.html",
    "title": "test",
    "section": "",
    "text": "%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nentry[\"op1=\n       op2=\"]\nb1[\"  vc0.0: int = const 0;\n  v0.0: int = id op1;\n  v1.0: int = id op2;\n  v1.1 = v1.0 \nv0.1 = v0.0 \n  jmp .cmp.val;\"]\n\ncomp_val[\"comp_val:\n  v1.1 = v1.1\n  v0.1: = v0.1\n  v2.1: bool = lt v0.1 v1.1;\n  br v2.1 .if.1 .else.1;\"]\nif_1[\"if1:  v3.2: int = sub v1.1 v0.1;\n v3.3: = v3.2 \n  jmp .loop.bound;\"]\nelse_1[\" v3.1: int = sub v0.1 v1.1;\n v3.3 = v3.1 \n  jmp .loop.bound;\"]\nloop_bound[\"loop_bound   v3.3 = v3.3\n  v4.1: bool = eq v3.3 vc0.0;\n  br v4.1 .program.end .update.val;\"]\nupdate_val[\"br v2.1 .if.2 .else.2;\"]\nif_2[\"  v1.3: int = id v3.3;\nv1.2 = v1.3\n  v0.2 = v0.1\n  jmp .back;\"]\nelse_2[\"  v0.3: int = id v3.3;\nv1.2 = v1.1 v1.3 \n  v0.2 = v0.3 v0.1\n  jmp .back;\"]\nback[\"  v1.2= v1.2\n  v0.2: = v0.2\n   v3.0 =  v3.3 \n    v2.0 = v2.1\n    v1.1 = v1.2\n    v0.1 = v0.2\n  jmp .cmp.val;\"]\npend[\"= v1.1\"]\n\nentry -- op1 op2 --&gt; b1\nb1 -- Xv1.1 Xv0.1 --&gt; comp_val\ncomp_val -- v1_1 xv0_1 --&gt; if_1\ncomp_val -- Xv_1 Xv0_1  --&gt; else_1\nif_1 --   Xv1_1 Xv3_3 --&gt; loop_bound\nelse_1 -- Xv1_1 Xv3_3--&gt; loop_bound\nloop_bound -- Xv1_1 --&gt; pend\nloop_bound --&gt; update_val\nupdate_val -- v4_1 v3_3  v0_2 vc0_0  v0_1--&gt; if_2\nupdate_val -- v4_1 v3_3  v0_2 vc0_0  v0_1 --&gt; else_2\nif_2 -- v4_1 v3_3  vc0_0 v1_3 v0_1--&gt; back\nelse_2  -- v4_1 v3_3  v9_2 vc0_0 v1_1 v0_3--&gt; back\nback-- xv1.1 Xv0.0--&gt; comp_val\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nentry[\"op1=\n       op2=\"]\nb1[\"  vc0.0: int = const 0;\n  v0.0: int = id op1;\n  v1.0: int = id op2;\n  v1.1 = v1.0 \nv0.1 = v0.0 \n  jmp .cmp.val;\"]\n\ncomp_val[\"comp_val:\n  v1.1 = v1.1\n  v0.1: = v0.1\n  v2.1: bool = lt v0.1 v1.1;\n  br v2.1 .if.1 .else.1;\"]\nif_1[\"if1:  v3.2: int = sub v1.1 v0.1;\n v3.3: = v3.2 \n  jmp .loop.bound;\"]\nelse_1[\" v3.1: int = sub v0.1 v1.1;\n v3.3 = v3.1 \n  jmp .loop.bound;\"]\nloop_bound[\"loop_bound   v3.3 = v3.3\n  v4.1: bool = eq v3.3 vc0.0;\n  br v4.1 .program.end .update.val;\"]\nupdate_val[\"br v2.1 .if.2 .else.2;\"]\nif_2[\"  v1.3: int = id v3.3;\nv1.2 = v1.3\n  v0.2 = v0.1\n  jmp .back;\"]\nelse_2[\"  v0.3: int = id v3.3;\nv1.2 = v1.1 v1.3 \n  v0.2 = v0.3 v0.1\n  jmp .back;\"]\nback[\"  v1.2= v1.2\n  v0.2: = v0.2\n   v3.0 =  v3.3 \n    v2.0 = v2.1\n    v1.1 = v1.2\n    v0.1 = v0.2\n  jmp .cmp.val;\"]\npend[\"= v1.1\"]\n\nentry -- op1 op2 --&gt; b1\nb1 -- Xv1.1 Xv0.1 --&gt; comp_val\ncomp_val -- v1_1 xv0_1 --&gt; if_1\ncomp_val -- Xv_1 Xv0_1  --&gt; else_1\nif_1 --   Xv1_1 Xv3_3 --&gt; loop_bound\nelse_1 -- Xv1_1 Xv3_3--&gt; loop_bound\nloop_bound -- Xv1_1 --&gt; pend\nloop_bound --&gt; update_val\nupdate_val -- v4_1 v3_3  v0_2 vc0_0  v0_1--&gt; if_2\nupdate_val -- v4_1 v3_3  v0_2 vc0_0  v0_1 --&gt; else_2\nif_2 -- v4_1 v3_3  vc0_0 v1_3 v0_1--&gt; back\nelse_2  -- v4_1 v3_3  v9_2 vc0_0 v1_1 v0_3--&gt; back\nback-- xv1.1 Xv0.0--&gt; comp_val",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "test"
    ]
  },
  {
    "objectID": "lectures/junk.html#other",
    "href": "lectures/junk.html#other",
    "title": "test",
    "section": "other",
    "text": "other\nentry – a b –&gt; b1 b1 –v1_0 v0_0 v0_1 + c m–&gt; comp_val comp_val – v1_1 v0_1 + c m –&gt; if_1\ncomp_val – v0_1 v1_1 c m –&gt; else_1 if_1 – v3_2 v1_1 c m –&gt; loop_bound else_1 – v3_1 v1_1 c m –&gt; loop_bound loop_bound – m –&gt; pend loop_bound –c –&gt; update_val update_val – v3_3 v0_2 c v0_1 g –&gt; if_2 update_val – v3_3 v0_2 c v0_1 g –&gt; else_2 if_2 – v4_1 v3_3 c v1_3 v0_1–&gt; back else_2 – v3_3 v9_2 c v1_1 v0_3–&gt; back back– v3_3 v1_2 c p1 –&gt; comp_val\n```",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "test"
    ]
  },
  {
    "objectID": "lectures/revealjs_junk.qmd.html#other",
    "href": "lectures/revealjs_junk.qmd.html#other",
    "title": "test",
    "section": "other",
    "text": "other\nentry – a b –&gt; b1 b1 –v1_0 v0_0 v0_1 + c m–&gt; comp_val comp_val – v1_1 v0_1 + c m –&gt; if_1\ncomp_val – v0_1 v1_1 c m –&gt; else_1 if_1 – v3_2 v1_1 c m –&gt; loop_bound else_1 – v3_1 v1_1 c m –&gt; loop_bound loop_bound – m –&gt; pend loop_bound –c –&gt; update_val update_val – v3_3 v0_2 c v0_1 g –&gt; if_2 update_val – v3_3 v0_2 c v0_1 g –&gt; else_2 if_2 – v4_1 v3_3 c v1_3 v0_1–&gt; back else_2 – v3_3 v9_2 c v1_1 v0_3–&gt; back back– v3_3 v1_2 c p1 –&gt; comp_val\n```"
  },
  {
    "objectID": "lectures/diverg.html",
    "href": "lectures/diverg.html",
    "title": "divergent flow",
    "section": "",
    "text": "__global__ void dec2zero(int* v, int N) { \n    int xIndex = blockIdx.x*blockDim.x+threadIdx.x;   \n    if (xIndex &lt; N) {\n             while (v[xIndex] &gt; 0) { v[xIndex]--;     \n             }     \n        } \n} \nDepending on how we initialize the vector, we get different times and different subtracts\n\nSize of array 1048576\nThreads Per Block = 256\nBlocks In Grid = 4096\n\n256 threads means 8 warps\neach warp start running - calculates a unique index\neach thread checks if v[index]&gt; 0 giving a mask, each thread read v[xindex] decrements the value and if the mask is on, updates, if the mask bit is off, the thread does not write\nif half the masks are on, half the threads do work, the other half don’t so 50% active",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#what-is-the-cost-of-divergence",
    "href": "lectures/diverg.html#what-is-the-cost-of-divergence",
    "title": "divergent flow",
    "section": "",
    "text": "__global__ void dec2zero(int* v, int N) { \n    int xIndex = blockIdx.x*blockDim.x+threadIdx.x;   \n    if (xIndex &lt; N) {\n             while (v[xIndex] &gt; 0) { v[xIndex]--;     \n             }     \n        } \n} \nDepending on how we initialize the vector, we get different times and different subtracts\n\nSize of array 1048576\nThreads Per Block = 256\nBlocks In Grid = 4096\n\n256 threads means 8 warps\neach warp start running - calculates a unique index\neach thread checks if v[index]&gt; 0 giving a mask, each thread read v[xindex] decrements the value and if the mask is on, updates, if the mask bit is off, the thread does not write\nif half the masks are on, half the threads do work, the other half don’t so 50% active",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#some-initializers",
    "href": "lectures/diverg.html#some-initializers",
    "title": "divergent flow",
    "section": "some initializers",
    "text": "some initializers",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#constant",
    "href": "lectures/diverg.html#constant",
    "title": "divergent flow",
    "section": "constant",
    "text": "constant\n  // all 1\n  for (int i = 0; i &lt; n; i++) {\n    A[i] = 1; \n  }\n. . .\n\n\n\nkind\nsubtracts\ntime ms\n\n\n\n\nconstant one\n1048576\n0.1",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#decreasing",
    "href": "lectures/diverg.html#decreasing",
    "title": "divergent flow",
    "section": "decreasing",
    "text": "decreasing\n// decreasing values from n-1 to 0\n  for (int i = 0; i &lt; n; i++) {\n    A[i] = n - i - 1;  // count should be N*(n+1)/2 = 54975572...\n  }\n. . .\n\n\n\nkind\nsubtracts\ntime ms\n\n\n\n\nconstant one\n1048576\n0.1\n\n\ndecreasing\n549755289600\n45.7",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#middle-value",
    "href": "lectures/diverg.html#middle-value",
    "title": "divergent flow",
    "section": "middle value",
    "text": "middle value\n// Fill function to set all elements of the array to the middle value of n\n  for (int i = 0; i &lt; n; i++) {\n    A[i] = n / 2;  // count should be N*N/2 54975572...\n  }\n. . .\n\n\n\nkind\nsubtracts\ntime ms\n\n\n\n\nconstant one\n1048576\n0.1\n\n\ndecreasing\n549755289600\n45.7\n\n\nmiddle value\n549755813888\n45.6",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#alternate-values",
    "href": "lectures/diverg.html#alternate-values",
    "title": "divergent flow",
    "section": "alternate values",
    "text": "alternate values\n// Fill function to set alternate elements to 0 or n\n  for (int i = 0; i &lt; n; i++) {\n    A[i] = 0;\n    if (i%2){ A[i] = n;} \n  }\n. . .\n\n\n\nkind\nsubtracts\ntime ms\n\n\n\n\nconstant one\n1048576\n0.1\n\n\ndecreasing\n549755289600\n45.7\n\n\nmiddle value\n549755813888\n45.6\n\n\nalternate\n549755813888\n83.9",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#divergence-example",
    "href": "lectures/diverg.html#divergence-example",
    "title": "divergent flow",
    "section": "divergence example",
    "text": "divergence example\n\n\n__global__ void example(float* v){\n    if (v[tid]) &lt; 0.0){\n        v[tid] = /=2;\n    } else {\n        v[tid] = 0;\n    }\n}\n\nstart: \nr1 = addr v[tid]\nf1 = load r1\np1 = set.lt f0, 0.0\n\n@p1? less: f2 = div f1, 2\n@p1? less2: jmp Bstore\n\n!@p1? ge: f2 = 0.0\n\nBstore: store r1, f2",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#cfg",
    "href": "lectures/diverg.html#cfg",
    "title": "divergent flow",
    "section": "cfg",
    "text": "cfg\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nstart --&gt; less\nless--&gt; less2\nless2  --&gt; bstore\nless2 --&gt; ge\nge--&gt; bstore\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nstart --&gt; less\nless--&gt; less2\nless2  --&gt; bstore\nless2 --&gt; ge\nge--&gt; bstore\n\n\n\n\n\n\n\nstart: \nr1 = addr v[tid]\nf1 = load r1\np1 = set.lt f0, 0.0\n\n@p1? less: f2 = div f1, 2\n@p1? less2: jmp Bstore\n\n!@p1? ge: f2 = 0.0\n\nBstore: store r1, f2\n\n\n\nsimt\n##simple if -\nassume warp size is 4\nexecution mask per thread - if 1 perform instruction if 0 do nothing \n\noperation                active \nif      cond             r r r r  assume first two threads get true\nreset active to cond     r r - -\nthen statements          r r - - \ninvert active cond       --  r r \nelse statements          - - r r \njoin                     r r r r \nrestore active \ncontrol is uniform when all threads in warp - take the same path\ncontrol is divergent when different threads take different paths\nsuppose all threads take the same path",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#nested-if",
    "href": "lectures/diverg.html#nested-if",
    "title": "divergent flow",
    "section": "nested if",
    "text": "nested if\noperation                active \nif      cond             r r r r \nreset active to cond     r r - -\ninner if                 r r - -   assume first thread gets true \nreset active             r - - - \ninner then               r - - - \ninvert mask              - r - -\ninner else               - r - - \njoin                     r r - - \ninvert active cond       --  r r \nelse statements          - - r r \njoin                     r r r r \nrestore active \nWhen we start the then we need to know the new mask\nWhen we change from then to else - we need the new mask and we need to know the pc (for else)\nwhen we change from the else to the endif we need the new mask and we need to know the pc (for reconvergance)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#stack-verison",
    "href": "lectures/diverg.html#stack-verison",
    "title": "divergent flow",
    "section": "stack verison",
    "text": "stack verison\nhow do we do this in general\none way is a stack of masks\neach stack entry has 3 parts - reconvergence pc, next pc, mask when current pc == reconvergence pc, set pc to next pc, set mask, pop the stack\nwhen we have a branch\n\npush reconverge, next pc of then, mask\npush reconverge, next pc of else, mask\n\nwhen pc matches the reconverge point at tos, go to next pc and pop the stack",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#another-option",
    "href": "lectures/diverg.html#another-option",
    "title": "divergent flow",
    "section": "another option",
    "text": "another option\nuse a scalar processor with scalar registers that hold the mask",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#special-cases",
    "href": "lectures/diverg.html#special-cases",
    "title": "divergent flow",
    "section": "special cases",
    "text": "special cases\nspecial case if all threads go the same way, one of the masks has to be zero, ignore it",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#loop-case",
    "href": "lectures/diverg.html#loop-case",
    "title": "divergent flow",
    "section": "loop case",
    "text": "loop case\nloops - keep looping till all threads exit the loop\ni = 0\nwile (i &lt; tid){\n  i++\n}\nprint(i)\nfour threads\ni = 0  i&lt; tid      0 r r r \n       i           0 0 0 0\n\ni++    i &lt; tid     0 0  1 1\n       i           0 1 1 1 \n\ni++     i          0 1 2 2 \n        i&lt; tid     0 0 0  1\ni++     i          0 1 2 3 \n        i&lt; tid     0 0 0 0 \nno active threads restore mask and exit loop\nprint(i)   i   0 1 2 3 \nmask           1 1 1 1",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#kinds-of-flow",
    "href": "lectures/diverg.html#kinds-of-flow",
    "title": "divergent flow",
    "section": "kinds of flow",
    "text": "kinds of flow\nStructured control flow:\n\nsingle-entry, single exit\nproperly nested Conditionals: if-then-else\nSingle-entry, single-exit loops: while, do-while, for…\nFunction call-return\n\nUnstructured control flow:\n\nbreak,\ncontinue\n&& || short-circuit evaluation",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#short-circuit-if",
    "href": "lectures/diverg.html#short-circuit-if",
    "title": "divergent flow",
    "section": "short circuit if",
    "text": "short circuit if\nif (c || d) {\n   S1; \n   } else { \n   S2; \n   } \nS3;\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nentry-- t1 t2 t3 t4 --&gt; c[c?]  --t3 t4--&gt; d[d?]\nc -- t1 t2 --&gt; s1\nd -- t3 --&gt; s1\nd  --t4 --&gt; s2\ns1 -- t1 t2 t3 --&gt; s3\ns2 -- t4--&gt; s3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nentry-- t1 t2 t3 t4 --&gt; c[c?]  --t3 t4--&gt; d[d?]\nc -- t1 t2 --&gt; s1\nd -- t3 --&gt; s1\nd  --t4 --&gt; s2\ns1 -- t1 t2 t3 --&gt; s3\ns2 -- t4--&gt; s3\n\n\n\n\n\n\nc has a post dominator at s3\nforces s1 to run twice\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nentry-- t1 t2 t3 t4 --&gt; c[c?]  --t3 t4--&gt; d[d?]\nd  --t4 --&gt; s2 \nc -- t1 t2 --&gt; ptest\nd -- t3 --&gt; p1[p1=1]\n\ns2--&gt; p2[p1=0]\np1--&gt; null\np2--&gt; null\nnull --&gt; ptest\nptest -- t1 t2 t3 --&gt; s1\nptest --t4 --&gt; s3\ns1 -- t1 t2 t3 --&gt; s3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nentry-- t1 t2 t3 t4 --&gt; c[c?]  --t3 t4--&gt; d[d?]\nd  --t4 --&gt; s2 \nc -- t1 t2 --&gt; ptest\nd -- t3 --&gt; p1[p1=1]\n\ns2--&gt; p2[p1=0]\np1--&gt; null\np2--&gt; null\nnull --&gt; ptest\nptest -- t1 t2 t3 --&gt; s1\nptest --t4 --&gt; s3\ns1 -- t1 t2 t3 --&gt; s3\n\n\n\n\n\n\nexpansion by adding flags to get to reducible flow\nThe basic idea is to insert predicate assignments (p:=0and p :=1) and branches (p?) such that all splits and joins are properly nested, and the resultingCFG is structured. This\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nc1 --&gt; stmt1\nstmt1--&gt; past\nstmt2--&gt; past\nc1 --&gt; c2\nc2 --&gt; stmt2\nc2 --&gt; past\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nc1 --&gt; stmt1\nstmt1--&gt; past\nstmt2--&gt; past\nc1 --&gt; c2\nc2 --&gt; stmt2\nc2 --&gt; past\n\n\n\n\n\n\n\nstmt runs twice for different threads \n\n\n## simt deadlock problems\n\n\nforward progress cases \n\nproducer consummer cases \n\nhow can threads syncronize- \n\n1: *mutex = 0; 2: while(!atomicCAS(mutex,0,1)); 3: // Critical Section 4: atomicExch(mutex,0);\n\nNothing make sure threads make forward progress\n\n\n1. for a critical section- thread0 gets the lock\n2. other threads keep looping waiting for the lock to be released \n1, thread 0 never runs again- lock is never released \n\n\n\n## mask stacks vs per-thread pc\n\nstacks:\n1. O(n) memory \n1. structured control flow only \n\nper threadd pc:\n1. o(1) memory \n1. more expensive to implement \n\n\nnested control flow and skipped control flow \n\n\n\n\n\n# when does threading model break down?\n\nsome code deadlocks:\n\n\n\n## volta and newer \n\n[possible structure](https://arxiv.org/pdf/2407.02944)\n\nHandles unstructured code nicely \nalways makes forward progress \n\n\n## an example \n\nA: if (tid %4 &lt;2) { B C } else { D E } F\n\n\n## volta and later \n\nhardware keeps a pc for each thread\n\nat any time step, hardware picks an active pc and runs a step of all threads that have that pc\n\nstep    | mask  \n--|--|--\nA    | 1 1 1 \nB    |  1 1 00 \nD    |  00  11   \nc     |  1100 \nD   |    0011   \nF    | 1111   \n\n\n\n\n\n\n\n\n## what does this solve \n\nThe Volta architecture introduces Independent Thread Scheduling among threads in a warp. This feature enables intra-warp synchronization patterns previously unavailable and simplifies code changes when porting CPU code. However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity of previous hardware architectures.\n\n\n## changes \n\nWhen porting existing codes to Volta, the following three code patterns need careful attention. For more details see the CUDA C++ Programming Guide.\n\n## cross warp operations \nTo avoid data corruption, applications using warp intrinsics (__shfl*, __any, __all, and __ballot) should transition to the new, safe, synchronizing counterparts, with the *_sync suffix. The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic.\n\n\n## memory access \n\nApplications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new __syncwarp() warp-wide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory. Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid.\n\n## barriers \n\nApplications using __syncthreads() or the PTX bar.sync (and their derivatives) in such a way that a barrier will not be reached by some non-exited thread in the thread block must be modified to ensure that all non-exited threads reach the barrier.\n\n\n## AMD scalar processor compiler challenge \n\nprograming language does not talk about scalar processor. Compiler has to figure out where to use it.  \n\n## what does this do to control flow graph \n\ntwo kinds of edges- vector view and scalar view. \n\n```{mermaid}\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nif--&gt; then \nif--&gt; else\nthen --&gt; join\nelse --&gt; join\nif .-&gt; then\nthen.-&gt; else\nelse.-&gt; join",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#static-detection-of-divergences",
    "href": "lectures/diverg.html#static-detection-of-divergences",
    "title": "divergent flow",
    "section": "static detection of divergences",
    "text": "static detection of divergences\ncan we determine which branches may cause divergences and which branches are uniform?\nat a dirergent branch some threads go one way, some the other, we will need to insert instructions for reconvergence at a uniform branch all threads go the same way",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#divergent-and-uniform-variables",
    "href": "lectures/diverg.html#divergent-and-uniform-variables",
    "title": "divergent flow",
    "section": "divergent and uniform variables",
    "text": "divergent and uniform variables\nA program variable is divergent if different threads see different values.\nIf different threads always see that variable with the same value, then the variable is uniform\ndivergent variables\n\nv = tid\natomic()\nv is data dependent on a divergent variable\nv is control dependent on a divergent variable",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#thread-id-is-always-divergent",
    "href": "lectures/diverg.html#thread-id-is-always-divergent",
    "title": "divergent flow",
    "section": "thread id is always divergent",
    "text": "thread id is always divergent\n __global__ \n void saxpy (int n, float alpha, float *x, float *y) {\n   int i = blockIdx.x * blockDim.x + threadIdx.x;  \n  if (i &lt; n) y[i] = alpha * x[i] + y[i]; } \nEach thread sees a different value\nThreads in different blocks see the same threadid - is that a problem?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#variables-defined-by-atomic-operations",
    "href": "lectures/diverg.html#variables-defined-by-atomic-operations",
    "title": "divergent flow",
    "section": "variables defined by atomic operations",
    "text": "variables defined by atomic operations\n__global__ void ex_atomic (int index, float* v) {\n   int i = 0; \n   i = ATOMINC( v[index] ); }",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#dependences",
    "href": "lectures/diverg.html#dependences",
    "title": "divergent flow",
    "section": "dependences",
    "text": "dependences\nTwo types of dependences: data and control.\nIf the program contains an assignment such as v = f(v1, v2, …, vn), then v is data dependent on the arguments v1,v2 …\nIf the value assigned to variable v depends on a branch controlled by p, then we say that v is control dependent on p. \nDivergences propagate transitively on the graph determined by the dependence relation.\nA variable might be divergent at one program point and uniform at another",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#an-example",
    "href": "lectures/diverg.html#an-example",
    "title": "divergent flow",
    "section": "an example",
    "text": "an example\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph\nb0[\"bo: i0 = ld v[tid]\\n j0 = 0\"]\nb1[\"b1: i = phi(i0,i1)\\nj=phi(j0,j3\\np0 = i &lt; 100\\n branch p0 B2\"]\nb0 --&gt; b1\nb1--&gt; b2\nb2[\"b2: i1= i +1\\n j1 = j +1\\n t0 = j1 mod2 \\n p1 = t0 ==0\\n branch b1, b4\"]\nb5[\"b5: sync\\np2 = j &lt; 100\\n, branch p2, b7\"]\nb2--&gt; b3[\"b3:j2 = j1 -3\"]\nb2 --&gt; b4[\"b4: j3= phi(j2, j1)\\n jump b1\"]\nb4 --&gt; b1\nb3--&gt; b4\nb1--&gt; b5\nb5--&gt; b6[\"b6:x0 =1\\n jump b8\"]\nb5--&gt; b7[x1 =2]\nb7 --&gt; b8\nb6--&gt; b8[\"b8:x = phi(x0,x1)\\n sync\\st v[tid] = x0\"]\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph\nb0[\"bo: i0 = ld v[tid]\\n j0 = 0\"]\nb1[\"b1: i = phi(i0,i1)\\nj=phi(j0,j3\\np0 = i &lt; 100\\n branch p0 B2\"]\nb0 --&gt; b1\nb1--&gt; b2\nb2[\"b2: i1= i +1\\n j1 = j +1\\n t0 = j1 mod2 \\n p1 = t0 ==0\\n branch b1, b4\"]\nb5[\"b5: sync\\np2 = j &lt; 100\\n, branch p2, b7\"]\nb2--&gt; b3[\"b3:j2 = j1 -3\"]\nb2 --&gt; b4[\"b4: j3= phi(j2, j1)\\n jump b1\"]\nb4 --&gt; b1\nb3--&gt; b4\nb1--&gt; b5\nb5--&gt; b6[\"b6:x0 =1\\n jump b8\"]\nb5--&gt; b7[x1 =2]\nb7 --&gt; b8\nb6--&gt; b8[\"b8:x = phi(x0,x1)\\n sync\\st v[tid] = x0\"]\n\n\n\n\n\n\n\nWe can construct the data dependenc graph\n\na node for each variable\nan edge from u to v, if v is data depedent on u",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#dd-graph",
    "href": "lectures/diverg.html#dd-graph",
    "title": "divergent flow",
    "section": "dd graph",
    "text": "dd graph\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph \ntid --&gt; i0\ni0--&gt; i\ni1--&gt; i\nj0--&gt; j\nj3--&gt; j\ni--&gt; p0\ni--&gt; i1\nj--&gt; j1\nj1--&gt; t0\nj--&gt; p2\nj1--&gt; j2\nj2--&gt; j3\nj1--&gt; j3\nx0--&gt; x\nx1--&gt; x \nt0--&gt; t1\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph \ntid --&gt; i0\ni0--&gt; i\ni1--&gt; i\nj0--&gt; j\nj3--&gt; j\ni--&gt; p0\ni--&gt; i1\nj--&gt; j1\nj1--&gt; t0\nj--&gt; p2\nj1--&gt; j2\nj2--&gt; j3\nj1--&gt; j3\nx0--&gt; x\nx1--&gt; x \nt0--&gt; t1\n\n\n\n\n\n\n\nThe data divergences show that not all the nodes are data dependent on tid\nis j in b5 divergent?\ni is divergent, p0 is divergent so threads go though the loop diferent number of times so j varies\nwhat about x in block 8? efected by p2 which depends of j",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#memory-operations",
    "href": "lectures/diverg.html#memory-operations",
    "title": "divergent flow",
    "section": "memory operations",
    "text": "memory operations\nThe C semantics assume that (within a single thread) all loads and stores stay in order. That is is not allowed to re-order a store past a load of the same address.\nin ssa each argument of an instruction is a pointer to the source instruction. These edges force serialization of the code.\nWe want to apply this to loads and stores this will make ordering explicit\nIn Static Single Assignment (SSA) form, memory tokens, representing stores or loads to memory, are typically handled by introducing memory state variables\n\nload: dest = load addrs, memory_token\nstore memory_token = store value, address, memory_token\ncalls to functions that might modify memory also need to read and write memory tokens\n\ntreat a store as though it created a new copy of memory\nwe can use phi functions on memory tokens\nMaintaining Correct Memory Order: By tracking memory states explicitly in SSA form (through these memory tokens and versioning), SSA ensures that memory operations respect the correct order, even if the control flow of the program is complex. This helps compilers optimize code by making memory dependencies explicit.\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nstore1\nload1\nload2\nstore2\nload4\nload3\nstore1--&gt;load1\nstore1--&gt;load2\nstore1 --&gt; store2\nstore2 --&gt; load3\nstore2--&gt; load4\nstore2--&gt; exit\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nstore1\nload1\nload2\nstore2\nload4\nload3\nstore1--&gt;load1\nstore1--&gt;load2\nstore1 --&gt; store2\nstore2 --&gt; load3\nstore2--&gt; load4\nstore2--&gt; exit\n\n\n\n\n\n\nOptimize loads/stores\nwalk backwards - load from store\n\nif we can prove the load address is the same as the store address- remove the load\nif we can prove the load address is different move the load up a store\notherwise go on",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#multi-threaded-programs",
    "href": "lectures/diverg.html#multi-threaded-programs",
    "title": "divergent flow",
    "section": "multi-threaded programs",
    "text": "multi-threaded programs\nCompilers started out assuming targets are single threaded. What optimizations change for multi-threaded code? How do users tell compiler that the target is multi-threaded?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#shared-memory-multi-threading",
    "href": "lectures/diverg.html#shared-memory-multi-threading",
    "title": "divergent flow",
    "section": "shared memory multi-threading",
    "text": "shared memory multi-threading\nThe most common parallel system is\n\nA single big memory\nmultiple threads address that memory",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#what-is-sequential-consistency-sq",
    "href": "lectures/diverg.html#what-is-sequential-consistency-sq",
    "title": "divergent flow",
    "section": "what is sequential consistency SQ",
    "text": "what is sequential consistency SQ\nProgram Order is Maintained Within Threads:\nOperations (reads and writes) appear to occur in the order they are issued by each individual thread. If a thread performs a write followed by a read, the read cannot appear to happen before the write in the execution.\nGlobal Order of Operations Across Threads:\nAll threads see the effects of memory operations in the same sequential order. Every thread agrees on the order of reads and writes, though the specific order is not predefined—it just needs to be consistent across all threads. Interleaving of Operations:\nThe execution can be viewed as an interleaving of instructions from all threads. However, the interleaving must follow the program order within each thread.\nno real machine/compiler implements this",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#compiler-effects",
    "href": "lectures/diverg.html#compiler-effects",
    "title": "divergent flow",
    "section": "compiler effects",
    "text": "compiler effects\nCompiler transformations that break multi-thread sequential consistency (SC) often reorder or optimize instructions in ways that do not respect the original program order seen by other threads. These transformations can lead to subtle bugs in multithreaded programs where the expected interleaving of operations is violated.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#loadstore-reordering",
    "href": "lectures/diverg.html#loadstore-reordering",
    "title": "divergent flow",
    "section": "Load/Store Reordering",
    "text": "Load/Store Reordering\nTransformation: Compilers might reorder loads and stores to improve performance. Violation: In a multi-threaded environment, this can lead to a situation where one thread sees stale or unexpected data. Example:\nCopy code\n// Thread 1\nx = 1;     // Store\nr1 = y;    // Load\n\n// Thread 2\ny = 1;     // Store\nr2 = x;    // Load\nUnder sequential consistency, if thread 1’s x = 1 happens before thread 2’s r2 = x, then thread 2 should observe r2 == 1. But reordering could result in thread 2 reading x as 0.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#common-subexpression-elimination-cse",
    "href": "lectures/diverg.html#common-subexpression-elimination-cse",
    "title": "divergent flow",
    "section": "Common Subexpression Elimination (CSE)",
    "text": "Common Subexpression Elimination (CSE)\nTransformation: If a variable or expression is computed multiple times, the compiler may optimize by reusing the result of an earlier computation. Violation: This assumes that no other thread modifies shared variables between these uses. Example:\n// Original code\nr1 = x;\nr2 = x;\n\n// Transformed code (CSE applied)\ntemp = x;\nr1 = temp;\nr2 = temp;\nIf x is modified by another thread between the two reads, the transformed code will incorrectly assume the value of x hasn’t changed.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#dead-code-elimination-dce",
    "href": "lectures/diverg.html#dead-code-elimination-dce",
    "title": "divergent flow",
    "section": "Dead Code Elimination (DCE)",
    "text": "Dead Code Elimination (DCE)\nTransformation: The compiler may remove stores to variables that are not subsequently read in the same thread. Violation: If the variable is shared and accessed by other threads, removing the store could lead to unexpected behavior. Example:\n// Original code\nx = 1;\n\n// Transformed code (DCE applied)\n// x = 1 is removed because x is not used locally If another thread reads x, it expects the store to have happened, but DCE breaks this assumption.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#speculative-execution-out-of-order-execution",
    "href": "lectures/diverg.html#speculative-execution-out-of-order-execution",
    "title": "divergent flow",
    "section": "Speculative Execution (Out-of-Order Execution)",
    "text": "Speculative Execution (Out-of-Order Execution)\nTransformation: Compilers (or hardware) may execute instructions speculatively, assuming certain branches are likely to be taken. Violation: This can cause out-of-order writes or reads visible to other threads, breaking SC. Example:\nif (flag) {\n    r1 = x;\n}\nIf the compiler speculatively reads x before knowing the value of flag, another thread’s write to x might be missed or observed out-of-order.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#loop-invariant-code-motion",
    "href": "lectures/diverg.html#loop-invariant-code-motion",
    "title": "divergent flow",
    "section": "Loop Invariant Code Motion",
    "text": "Loop Invariant Code Motion\nTransformation: The compiler moves computations that are invariant inside a loop to outside the loop. Violation: If these computations involve shared variables modified by other threads within the loop, moving them outside could make the code see stale values. Example:\n// Original code\nwhile (condition) {\n    r = shared_variable;\n}\n\n// Transformed code (Loop Invariant Code Motion)\ntemp = shared_variable;\nwhile (condition) {\n    r = temp;\n}\nIf shared_variable is updated by another thread, the transformed code might keep using the old value.\n\nRegister Allocation (Caching Shared Variables in Registers)\nTransformation: Compilers can keep a shared variable in a register for efficiency rather than repeatedly loading it from memory. Violation: If another thread modifies that shared variable in memory, the compiler’s register optimization would cause the thread to read stale data. Example:\nwhile (flag == 0) {\n    // busy-wait\n}\nIf flag is cached in a register, updates to flag by another thread in memory won’t be reflected, breaking SC.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#instruction-fusion-combining-loadsstores",
    "href": "lectures/diverg.html#instruction-fusion-combining-loadsstores",
    "title": "divergent flow",
    "section": "Instruction Fusion (Combining Loads/Stores)",
    "text": "Instruction Fusion (Combining Loads/Stores)\nTransformation: The compiler may combine consecutive memory accesses into one, such as merging adjacent stores into a single store or combining two loads. Violation: If other threads expect these loads or stores to happen separately, they might see an inconsistent view of memory. Example:\n// Original code\nx = 1;\ny = 2;\n\n// Transformed code (store fusion)\n// x and y are stored together in a single transaction\nA thread expecting x and y to be updated separately might observe an inconsistent state if this transformation is applied.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#thread-libraries",
    "href": "lectures/diverg.html#thread-libraries",
    "title": "divergent flow",
    "section": "thread libraries",
    "text": "thread libraries\nstart out assuming single threaded, add a threads library like pthreads\nmultiple threads could access shared memory simultaneously, leading to race conditions, inconsistent data, and undefined behavior.\nModern CPUs and compilers perform optimizations like instruction reordering, which can break assumptions about the order of memory operations in multithreaded programs.\nMultithreaded code is harder to test because race conditions and bugs might only manifest under certain timing conditions.\nDebugging multithreaded programs is more difficult due to the unpredictable nature of thread execution and interactions.\nSome optimizations might reorder instructions in a way that is incompatible with multithreading, introducing subtle bugs or performance regressions.\nCaching, prefetching, or other memory optimizations need to account for the fact that multiple threads may be accessing the same memory, which a simple thread library does not handle.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#using-libraries",
    "href": "lectures/diverg.html#using-libraries",
    "title": "divergent flow",
    "section": "using libraries",
    "text": "using libraries\n\nFunctions such as pthread mutex lock() that are guaranteed by the standard to “synchronize memory” include hardware instructions (“memory barriers”) that prevent hardware reordering of memory operations around the call\nTo prevent the compiler from moving memory operations around calls to functions such as pthread mutex lock(), they are essentially treated as calls to opaque functions, about which the compiler has no information.\n\nThe compiler effectively assumes that pthread mutex lock() may read or write any global variable. Thus a memory reference cannot simply be moved across the call. This approach also ensures that transitive calls, e.g. a call to a function f() which then calls pthread mutex lock(), are handled in the same way more or less appropriately, i.e. memory operations are not moved across the call to f() either, whether or not the entire user program is being analyzed at once.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#adding-multi-threading-to-user-explaining-the-intent",
    "href": "lectures/diverg.html#adding-multi-threading-to-user-explaining-the-intent",
    "title": "divergent flow",
    "section": "adding multi-threading to user explaining the intent",
    "text": "adding multi-threading to user explaining the intent\nc++/c added atomics\nAtomic operations are operations that are completed as a single, uninterruptible action. No other thread can observe a partial update or interfere with the operation.\nThese operations ensure that read-modify-write sequences are safe without needing explicit locks.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#an-example-2",
    "href": "lectures/diverg.html#an-example-2",
    "title": "divergent flow",
    "section": "an example",
    "text": "an example\n#include &lt;atomic&gt;\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n\n// Global spinlock using atomic_flag\nstd::atomic_flag lock = ATOMIC_FLAG_INIT;\n\nvoid enter_critical_section() {\n    // Busy-wait (spin) until the lock is acquired\n    while (lock.test_and_set(std::memory_order_acquire)) {\n        // Spin and wait for the lock to become available\n    }\n}\n\nvoid leave_critical_section() {\n    // Release the lock\n    lock.clear(std::memory_order_release);\n}\n\n// Shared resource\nint shared_counter = 0;\n\nvoid critical_section_task(int num_increments) {\n    for (int i = 0; i &lt; num_increments; ++i) {\n        enter_critical_section();\n        // Begin critical section\n        ++shared_counter;\n        // End critical section\n        leave_critical_section();\n    }\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#load-acquire-needs-special-hardware",
    "href": "lectures/diverg.html#load-acquire-needs-special-hardware",
    "title": "divergent flow",
    "section": "load acquire (needs special hardware )",
    "text": "load acquire (needs special hardware )\nused by default with atomics not used for non-atomics\nall memory reads and writes after the load operation cannot be moved before the load. This ensures that after acquiring the value, any operations that depend on this value (like accessing shared data) will see consistent and up-to-date memory.\na one way fence - nothing can move up",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#write-release-needs-special-hardware",
    "href": "lectures/diverg.html#write-release-needs-special-hardware",
    "title": "divergent flow",
    "section": "write release (needs special hardware )",
    "text": "write release (needs special hardware )\nprevents the compiler or processor from reordering any memory operations (reads or writes) that appear before the release store. This guarantees that all operations that modify shared data before the release are visible to other threads that subsequently perform an acquire operation.\nalso a one way fence - nothing can move down\nload.acquire - \nloads and stores on non-atomics  - compiler picks the order for these operations \nstore.release",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#using-atomics",
    "href": "lectures/diverg.html#using-atomics",
    "title": "divergent flow",
    "section": "using atomics",
    "text": "using atomics\nAll operations appear to occur in a single total order that is consistent across all threads. This means that the results of operations are predictable and consistent as if all operations were executed in some sequential order.\nlimits the hardware and compiler because it prevents reordering",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#data-race-free",
    "href": "lectures/diverg.html#data-race-free",
    "title": "divergent flow",
    "section": "Data Race Free",
    "text": "Data Race Free\nData Race Free (DRF) means that a program is free from data races, which occur when:\n\nTwo or more threads access the same variable concurrently.\nAt least one of the accesses is a write.\n\nThere is no synchronization mechanism (like mutexes or atomic operations) to control the access. In a data race-free program, every shared variable is accessed in a way that ensures predictable results. C++ provides various synchronization primitives (such as mutexes and atomic types) to help developers write DRF code.\nAll shared variables must be accessed using synchronization to prevent concurrent threads from modifying shared data simultaneously without coordination.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#an-example-3",
    "href": "lectures/diverg.html#an-example-3",
    "title": "divergent flow",
    "section": "an example",
    "text": "an example\n#include &lt;iostream&gt;\n#include &lt;atomic&gt;\n#include &lt;thread&gt;\n\nint shared_counter1 = 0;                  // First non-atomic shared variable\nint shared_counter2 = 0;                  // Second non-atomic shared variable\nstd::atomic&lt;bool&gt; lock_flag(false);       // Atomic flag to control access\n\nvoid safe_increment() {\n    for (int i = 0; i &lt; 1000; ++i) {\n        // Spin until the lock is acquired\n        while (lock_flag.exchange(true)) {\n            // Busy-wait (spin) until the lock is free\n        }\n\n        // Critical section: update the non-atomic shared variables\n        ++shared_counter1;\n        ++shared_counter2;\n\n        // Release the lock\n        lock_flag.store(false);\n    }\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#language-rules",
    "href": "lectures/diverg.html#language-rules",
    "title": "divergent flow",
    "section": "language rules",
    "text": "language rules\nC and C++\ndo not define what happens in the presence of data races. If a program has data races (e.g., multiple threads concurrently reading and writing to the same variable without synchronization), the behavior is considered undefined. This means that the program may produce unexpected results, crash, or behave inconsistently across different executions or platforms.\nJava\ntries to define what happens but definition is very complex and maybe inconsistent\nRust\nCompile-Time Guarantees: Rust’s ownership and borrowing system prevents data races at compile time. If a program is not DRF, the Rust compiler will typically refuse to compile it, enforcing memory safety guarantees.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#can-the-compiler-add-a-race-to-a-drf-program",
    "href": "lectures/diverg.html#can-the-compiler-add-a-race-to-a-drf-program",
    "title": "divergent flow",
    "section": "can the compiler add a race to a drf program",
    "text": "can the compiler add a race to a drf program\nnew rule, compiler cannot add a write to a shared variable\nif (x ==1) y++\n\nto \ny++\nif (x!=1) y--",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/diverg.html#how-does-this-effect-hardware",
    "href": "lectures/diverg.html#how-does-this-effect-hardware",
    "title": "divergent flow",
    "section": "how does this effect hardware?",
    "text": "how does this effect hardware?\nstruct { char a; char b; char c; char d;} s;\ns.a = 1\ns.c = 3\n\ncan a compiler do \nchar temp[4] = s // load 32 bits \ntemp[0] = 1\ntemp[2] = 3\ns = temp\nnot allowed - reads/writes b and d, so compiler incorrectly added writes\noptions are either have byte addressable hardware, or pad so that each char gets 32 bits\nVendors forced to add 8 byte loads/stores",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow"
    ]
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#what-is-the-cost-of-divergence",
    "href": "lectures/revealjs_diverg.qmd.html#what-is-the-cost-of-divergence",
    "title": "divergent flow",
    "section": "What is the cost of divergence",
    "text": "What is the cost of divergence\n__global__ void dec2zero(int* v, int N) { \n    int xIndex = blockIdx.x*blockDim.x+threadIdx.x;   \n    if (xIndex &lt; N) {\n             while (v[xIndex] &gt; 0) { v[xIndex]--;     \n             }     \n        } \n} \nDepending on how we initialize the vector, we get different times and different subtracts\n\nSize of array 1048576\nThreads Per Block = 256\nBlocks In Grid = 4096\n\n256 threads means 8 warps\neach warp start running - calculates a unique index\neach thread checks if v[index]&gt; 0 giving a mask, each thread read v[xindex] decrements the value and if the mask is on, updates, if the mask bit is off, the thread does not write\nif half the masks are on, half the threads do work, the other half don’t so 50% active"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#some-initializers",
    "href": "lectures/revealjs_diverg.qmd.html#some-initializers",
    "title": "divergent flow",
    "section": "some initializers",
    "text": "some initializers"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#constant",
    "href": "lectures/revealjs_diverg.qmd.html#constant",
    "title": "divergent flow",
    "section": "constant",
    "text": "constant\n  // all 1\n  for (int i = 0; i &lt; n; i++) {\n    A[i] = 1; \n  }\n\n\n\n\nkind\nsubtracts\ntime ms\n\n\n\n\nconstant one\n1048576\n0.1"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#decreasing",
    "href": "lectures/revealjs_diverg.qmd.html#decreasing",
    "title": "divergent flow",
    "section": "decreasing",
    "text": "decreasing\n// decreasing values from n-1 to 0\n  for (int i = 0; i &lt; n; i++) {\n    A[i] = n - i - 1;  // count should be N*(n+1)/2 = 54975572...\n  }\n\n\n\n\nkind\nsubtracts\ntime ms\n\n\n\n\nconstant one\n1048576\n0.1\n\n\ndecreasing\n549755289600\n45.7"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#middle-value",
    "href": "lectures/revealjs_diverg.qmd.html#middle-value",
    "title": "divergent flow",
    "section": "middle value",
    "text": "middle value\n// Fill function to set all elements of the array to the middle value of n\n  for (int i = 0; i &lt; n; i++) {\n    A[i] = n / 2;  // count should be N*N/2 54975572...\n  }\n\n\n\n\nkind\nsubtracts\ntime ms\n\n\n\n\nconstant one\n1048576\n0.1\n\n\ndecreasing\n549755289600\n45.7\n\n\nmiddle value\n549755813888\n45.6"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#alternate-values",
    "href": "lectures/revealjs_diverg.qmd.html#alternate-values",
    "title": "divergent flow",
    "section": "alternate values",
    "text": "alternate values\n// Fill function to set alternate elements to 0 or n\n  for (int i = 0; i &lt; n; i++) {\n    A[i] = 0;\n    if (i%2){ A[i] = n;} \n  }\n\n\n\n\nkind\nsubtracts\ntime ms\n\n\n\n\nconstant one\n1048576\n0.1\n\n\ndecreasing\n549755289600\n45.7\n\n\nmiddle value\n549755813888\n45.6\n\n\nalternate\n549755813888\n83.9"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#divergence-example",
    "href": "lectures/revealjs_diverg.qmd.html#divergence-example",
    "title": "divergent flow",
    "section": "divergence example",
    "text": "divergence example\n\n\n__global__ void example(float* v){\n    if (v[tid]) &lt; 0.0){\n        v[tid] = /=2;\n    } else {\n        v[tid] = 0;\n    }\n}\n\nstart: \nr1 = addr v[tid]\nf1 = load r1\np1 = set.lt f0, 0.0\n\n@p1? less: f2 = div f1, 2\n@p1? less2: jmp Bstore\n\n!@p1? ge: f2 = 0.0\n\nBstore: store r1, f2"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#cfg",
    "href": "lectures/revealjs_diverg.qmd.html#cfg",
    "title": "divergent flow",
    "section": "cfg",
    "text": "cfg\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nstart --&gt; less\nless--&gt; less2\nless2  --&gt; bstore\nless2 --&gt; ge\nge--&gt; bstore\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nstart --&gt; less\nless--&gt; less2\nless2  --&gt; bstore\nless2 --&gt; ge\nge--&gt; bstore\n\n\n\n\n\n\n\nstart: \nr1 = addr v[tid]\nf1 = load r1\np1 = set.lt f0, 0.0\n\n@p1? less: f2 = div f1, 2\n@p1? less2: jmp Bstore\n\n!@p1? ge: f2 = 0.0\n\nBstore: store r1, f2"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#nested-if",
    "href": "lectures/revealjs_diverg.qmd.html#nested-if",
    "title": "divergent flow",
    "section": "nested if",
    "text": "nested if\noperation                active \nif      cond             r r r r \nreset active to cond     r r - -\ninner if                 r r - -   assume first thread gets true \nreset active             r - - - \ninner then               r - - - \ninvert mask              - r - -\ninner else               - r - - \njoin                     r r - - \ninvert active cond       --  r r \nelse statements          - - r r \njoin                     r r r r \nrestore active \nWhen we start the then we need to know the new mask\nWhen we change from then to else - we need the new mask and we need to know the pc (for else)\nwhen we change from the else to the endif we need the new mask and we need to know the pc (for reconvergance)"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#stack-verison",
    "href": "lectures/revealjs_diverg.qmd.html#stack-verison",
    "title": "divergent flow",
    "section": "stack verison",
    "text": "stack verison\nhow do we do this in general\none way is a stack of masks\neach stack entry has 3 parts - reconvergence pc, next pc, mask when current pc == reconvergence pc, set pc to next pc, set mask, pop the stack\nwhen we have a branch\n\npush reconverge, next pc of then, mask\npush reconverge, next pc of else, mask\n\nwhen pc matches the reconverge point at tos, go to next pc and pop the stack"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#another-option",
    "href": "lectures/revealjs_diverg.qmd.html#another-option",
    "title": "divergent flow",
    "section": "another option",
    "text": "another option\nuse a scalar processor with scalar registers that hold the mask"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#special-cases",
    "href": "lectures/revealjs_diverg.qmd.html#special-cases",
    "title": "divergent flow",
    "section": "special cases",
    "text": "special cases\nspecial case if all threads go the same way, one of the masks has to be zero, ignore it"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#loop-case",
    "href": "lectures/revealjs_diverg.qmd.html#loop-case",
    "title": "divergent flow",
    "section": "loop case",
    "text": "loop case\nloops - keep looping till all threads exit the loop\ni = 0\nwile (i &lt; tid){\n  i++\n}\nprint(i)\nfour threads\ni = 0  i&lt; tid      0 r r r \n       i           0 0 0 0\n\ni++    i &lt; tid     0 0  1 1\n       i           0 1 1 1 \n\ni++     i          0 1 2 2 \n        i&lt; tid     0 0 0  1\ni++     i          0 1 2 3 \n        i&lt; tid     0 0 0 0 \nno active threads restore mask and exit loop\nprint(i)   i   0 1 2 3 \nmask           1 1 1 1"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#kinds-of-flow",
    "href": "lectures/revealjs_diverg.qmd.html#kinds-of-flow",
    "title": "divergent flow",
    "section": "kinds of flow",
    "text": "kinds of flow\nStructured control flow:\n\nsingle-entry, single exit\nproperly nested Conditionals: if-then-else\nSingle-entry, single-exit loops: while, do-while, for…\nFunction call-return\n\nUnstructured control flow:\n\nbreak,\ncontinue\n&& || short-circuit evaluation"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#short-circuit-if",
    "href": "lectures/revealjs_diverg.qmd.html#short-circuit-if",
    "title": "divergent flow",
    "section": "short circuit if",
    "text": "short circuit if\nif (c || d) {\n   S1; \n   } else { \n   S2; \n   } \nS3;\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nentry-- t1 t2 t3 t4 --&gt; c[c?]  --t3 t4--&gt; d[d?]\nc -- t1 t2 --&gt; s1\nd -- t3 --&gt; s1\nd  --t4 --&gt; s2\ns1 -- t1 t2 t3 --&gt; s3\ns2 -- t4--&gt; s3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nentry-- t1 t2 t3 t4 --&gt; c[c?]  --t3 t4--&gt; d[d?]\nc -- t1 t2 --&gt; s1\nd -- t3 --&gt; s1\nd  --t4 --&gt; s2\ns1 -- t1 t2 t3 --&gt; s3\ns2 -- t4--&gt; s3\n\n\n\n\n\n\nc has a post dominator at s3\nforces s1 to run twice\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nentry-- t1 t2 t3 t4 --&gt; c[c?]  --t3 t4--&gt; d[d?]\nd  --t4 --&gt; s2 \nc -- t1 t2 --&gt; ptest\nd -- t3 --&gt; p1[p1=1]\n\ns2--&gt; p2[p1=0]\np1--&gt; null\np2--&gt; null\nnull --&gt; ptest\nptest -- t1 t2 t3 --&gt; s1\nptest --t4 --&gt; s3\ns1 -- t1 t2 t3 --&gt; s3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nentry-- t1 t2 t3 t4 --&gt; c[c?]  --t3 t4--&gt; d[d?]\nd  --t4 --&gt; s2 \nc -- t1 t2 --&gt; ptest\nd -- t3 --&gt; p1[p1=1]\n\ns2--&gt; p2[p1=0]\np1--&gt; null\np2--&gt; null\nnull --&gt; ptest\nptest -- t1 t2 t3 --&gt; s1\nptest --t4 --&gt; s3\ns1 -- t1 t2 t3 --&gt; s3\n\n\n\n\n\n\nexpansion by adding flags to get to reducible flow\nThe basic idea is to insert predicate assignments (p:=0and p :=1) and branches (p?) such that all splits and joins are properly nested, and the resultingCFG is structured. This\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nc1 --&gt; stmt1\nstmt1--&gt; past\nstmt2--&gt; past\nc1 --&gt; c2\nc2 --&gt; stmt2\nc2 --&gt; past\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nc1 --&gt; stmt1\nstmt1--&gt; past\nstmt2--&gt; past\nc1 --&gt; c2\nc2 --&gt; stmt2\nc2 --&gt; past\n\n\n\n\n\n\n\nstmt runs twice for different threads \n\n\n## simt deadlock problems\n\n\nforward progress cases \n\nproducer consummer cases \n\nhow can threads syncronize- \n\n1: *mutex = 0; 2: while(!atomicCAS(mutex,0,1)); 3: // Critical Section 4: atomicExch(mutex,0);\n\nNothing make sure threads make forward progress\n\n\n1. for a critical section- thread0 gets the lock\n2. other threads keep looping waiting for the lock to be released \n1, thread 0 never runs again- lock is never released \n\n\n\n## mask stacks vs per-thread pc\n\nstacks:\n1. O(n) memory \n1. structured control flow only \n\nper threadd pc:\n1. o(1) memory \n1. more expensive to implement \n\n\nnested control flow and skipped control flow \n\n\n\n\n\n# when does threading model break down?\n\nsome code deadlocks:\n\n\n\n## volta and newer \n\n[possible structure](https://arxiv.org/pdf/2407.02944)\n\nHandles unstructured code nicely \nalways makes forward progress \n\n\n## an example \n\nA: if (tid %4 &lt;2) { B C } else { D E } F\n\n\n## volta and later \n\nhardware keeps a pc for each thread\n\nat any time step, hardware picks an active pc and runs a step of all threads that have that pc\n\nstep    | mask  \n--|--|--\nA    | 1 1 1 \nB    |  1 1 00 \nD    |  00  11   \nc     |  1100 \nD   |    0011   \nF    | 1111   \n\n\n\n\n\n\n\n\n## what does this solve \n\nThe Volta architecture introduces Independent Thread Scheduling among threads in a warp. This feature enables intra-warp synchronization patterns previously unavailable and simplifies code changes when porting CPU code. However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity of previous hardware architectures.\n\n\n## changes \n\nWhen porting existing codes to Volta, the following three code patterns need careful attention. For more details see the CUDA C++ Programming Guide.\n\n## cross warp operations \nTo avoid data corruption, applications using warp intrinsics (__shfl*, __any, __all, and __ballot) should transition to the new, safe, synchronizing counterparts, with the *_sync suffix. The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic.\n\n\n## memory access \n\nApplications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new __syncwarp() warp-wide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory. Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid.\n\n## barriers \n\nApplications using __syncthreads() or the PTX bar.sync (and their derivatives) in such a way that a barrier will not be reached by some non-exited thread in the thread block must be modified to ensure that all non-exited threads reach the barrier.\n\n\n## AMD scalar processor compiler challenge \n\nprograming language does not talk about scalar processor. Compiler has to figure out where to use it.  \n\n## what does this do to control flow graph \n\ntwo kinds of edges- vector view and scalar view. \n\n```{mermaid}\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nif--&gt; then \nif--&gt; else\nthen --&gt; join\nelse --&gt; join\nif .-&gt; then\nthen.-&gt; else\nelse.-&gt; join"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#static-detection-of-divergences",
    "href": "lectures/revealjs_diverg.qmd.html#static-detection-of-divergences",
    "title": "divergent flow",
    "section": "static detection of divergences",
    "text": "static detection of divergences\ncan we determine which branches may cause divergences and which branches are uniform?\nat a dirergent branch some threads go one way, some the other, we will need to insert instructions for reconvergence at a uniform branch all threads go the same way"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#divergent-and-uniform-variables",
    "href": "lectures/revealjs_diverg.qmd.html#divergent-and-uniform-variables",
    "title": "divergent flow",
    "section": "divergent and uniform variables",
    "text": "divergent and uniform variables\nA program variable is divergent if different threads see different values.\nIf different threads always see that variable with the same value, then the variable is uniform\ndivergent variables\n\nv = tid\natomic()\nv is data dependent on a divergent variable\nv is control dependent on a divergent variable"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#thread-id-is-always-divergent",
    "href": "lectures/revealjs_diverg.qmd.html#thread-id-is-always-divergent",
    "title": "divergent flow",
    "section": "thread id is always divergent",
    "text": "thread id is always divergent\n __global__ \n void saxpy (int n, float alpha, float *x, float *y) {\n   int i = blockIdx.x * blockDim.x + threadIdx.x;  \n  if (i &lt; n) y[i] = alpha * x[i] + y[i]; } \nEach thread sees a different value\nThreads in different blocks see the same threadid - is that a problem?"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#variables-defined-by-atomic-operations",
    "href": "lectures/revealjs_diverg.qmd.html#variables-defined-by-atomic-operations",
    "title": "divergent flow",
    "section": "variables defined by atomic operations",
    "text": "variables defined by atomic operations\n__global__ void ex_atomic (int index, float* v) {\n   int i = 0; \n   i = ATOMINC( v[index] ); }"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#dependences",
    "href": "lectures/revealjs_diverg.qmd.html#dependences",
    "title": "divergent flow",
    "section": "dependences",
    "text": "dependences\nTwo types of dependences: data and control.\nIf the program contains an assignment such as v = f(v1, v2, …, vn), then v is data dependent on the arguments v1,v2 …\nIf the value assigned to variable v depends on a branch controlled by p, then we say that v is control dependent on p. \nDivergences propagate transitively on the graph determined by the dependence relation.\nA variable might be divergent at one program point and uniform at another"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#an-example",
    "href": "lectures/revealjs_diverg.qmd.html#an-example",
    "title": "divergent flow",
    "section": "an example",
    "text": "an example\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph\nb0[\"bo: i0 = ld v[tid]\\n j0 = 0\"]\nb1[\"b1: i = phi(i0,i1)\\nj=phi(j0,j3\\np0 = i &lt; 100\\n branch p0 B2\"]\nb0 --&gt; b1\nb1--&gt; b2\nb2[\"b2: i1= i +1\\n j1 = j +1\\n t0 = j1 mod2 \\n p1 = t0 ==0\\n branch b1, b4\"]\nb5[\"b5: sync\\np2 = j &lt; 100\\n, branch p2, b7\"]\nb2--&gt; b3[\"b3:j2 = j1 -3\"]\nb2 --&gt; b4[\"b4: j3= phi(j2, j1)\\n jump b1\"]\nb4 --&gt; b1\nb3--&gt; b4\nb1--&gt; b5\nb5--&gt; b6[\"b6:x0 =1\\n jump b8\"]\nb5--&gt; b7[x1 =2]\nb7 --&gt; b8\nb6--&gt; b8[\"b8:x = phi(x0,x1)\\n sync\\st v[tid] = x0\"]\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph\nb0[\"bo: i0 = ld v[tid]\\n j0 = 0\"]\nb1[\"b1: i = phi(i0,i1)\\nj=phi(j0,j3\\np0 = i &lt; 100\\n branch p0 B2\"]\nb0 --&gt; b1\nb1--&gt; b2\nb2[\"b2: i1= i +1\\n j1 = j +1\\n t0 = j1 mod2 \\n p1 = t0 ==0\\n branch b1, b4\"]\nb5[\"b5: sync\\np2 = j &lt; 100\\n, branch p2, b7\"]\nb2--&gt; b3[\"b3:j2 = j1 -3\"]\nb2 --&gt; b4[\"b4: j3= phi(j2, j1)\\n jump b1\"]\nb4 --&gt; b1\nb3--&gt; b4\nb1--&gt; b5\nb5--&gt; b6[\"b6:x0 =1\\n jump b8\"]\nb5--&gt; b7[x1 =2]\nb7 --&gt; b8\nb6--&gt; b8[\"b8:x = phi(x0,x1)\\n sync\\st v[tid] = x0\"]\n\n\n\n\n\n\n\nWe can construct the data dependenc graph\n\na node for each variable\nan edge from u to v, if v is data depedent on u"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#dd-graph",
    "href": "lectures/revealjs_diverg.qmd.html#dd-graph",
    "title": "divergent flow",
    "section": "dd graph",
    "text": "dd graph\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph \ntid --&gt; i0\ni0--&gt; i\ni1--&gt; i\nj0--&gt; j\nj3--&gt; j\ni--&gt; p0\ni--&gt; i1\nj--&gt; j1\nj1--&gt; t0\nj--&gt; p2\nj1--&gt; j2\nj2--&gt; j3\nj1--&gt; j3\nx0--&gt; x\nx1--&gt; x \nt0--&gt; t1\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph \ntid --&gt; i0\ni0--&gt; i\ni1--&gt; i\nj0--&gt; j\nj3--&gt; j\ni--&gt; p0\ni--&gt; i1\nj--&gt; j1\nj1--&gt; t0\nj--&gt; p2\nj1--&gt; j2\nj2--&gt; j3\nj1--&gt; j3\nx0--&gt; x\nx1--&gt; x \nt0--&gt; t1\n\n\n\n\n\n\n\nThe data divergences show that not all the nodes are data dependent on tid\nis j in b5 divergent?\ni is divergent, p0 is divergent so threads go though the loop diferent number of times so j varies\nwhat about x in block 8? efected by p2 which depends of j"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#memory-operations",
    "href": "lectures/revealjs_diverg.qmd.html#memory-operations",
    "title": "divergent flow",
    "section": "memory operations",
    "text": "memory operations\nThe C semantics assume that (within a single thread) all loads and stores stay in order. That is is not allowed to re-order a store past a load of the same address.\nin ssa each argument of an instruction is a pointer to the source instruction. These edges force serialization of the code.\nWe want to apply this to loads and stores this will make ordering explicit\nIn Static Single Assignment (SSA) form, memory tokens, representing stores or loads to memory, are typically handled by introducing memory state variables\n\nload: dest = load addrs, memory_token\nstore memory_token = store value, address, memory_token\ncalls to functions that might modify memory also need to read and write memory tokens\n\ntreat a store as though it created a new copy of memory\nwe can use phi functions on memory tokens\nMaintaining Correct Memory Order: By tracking memory states explicitly in SSA form (through these memory tokens and versioning), SSA ensures that memory operations respect the correct order, even if the control flow of the program is complex. This helps compilers optimize code by making memory dependencies explicit.\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nstore1\nload1\nload2\nstore2\nload4\nload3\nstore1--&gt;load1\nstore1--&gt;load2\nstore1 --&gt; store2\nstore2 --&gt; load3\nstore2--&gt; load4\nstore2--&gt; exit\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nstore1\nload1\nload2\nstore2\nload4\nload3\nstore1--&gt;load1\nstore1--&gt;load2\nstore1 --&gt; store2\nstore2 --&gt; load3\nstore2--&gt; load4\nstore2--&gt; exit\n\n\n\n\n\n\nOptimize loads/stores\nwalk backwards - load from store\n\nif we can prove the load address is the same as the store address- remove the load\nif we can prove the load address is different move the load up a store\notherwise go on"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#multi-threaded-programs",
    "href": "lectures/revealjs_diverg.qmd.html#multi-threaded-programs",
    "title": "divergent flow",
    "section": "multi-threaded programs",
    "text": "multi-threaded programs\nCompilers started out assuming targets are single threaded. What optimizations change for multi-threaded code? How do users tell compiler that the target is multi-threaded?"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#shared-memory-multi-threading",
    "href": "lectures/revealjs_diverg.qmd.html#shared-memory-multi-threading",
    "title": "divergent flow",
    "section": "shared memory multi-threading",
    "text": "shared memory multi-threading\nThe most common parallel system is\n\nA single big memory\nmultiple threads address that memory"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#what-is-sequential-consistency-sq",
    "href": "lectures/revealjs_diverg.qmd.html#what-is-sequential-consistency-sq",
    "title": "divergent flow",
    "section": "what is sequential consistency SQ",
    "text": "what is sequential consistency SQ\nProgram Order is Maintained Within Threads:\nOperations (reads and writes) appear to occur in the order they are issued by each individual thread. If a thread performs a write followed by a read, the read cannot appear to happen before the write in the execution.\nGlobal Order of Operations Across Threads:\nAll threads see the effects of memory operations in the same sequential order. Every thread agrees on the order of reads and writes, though the specific order is not predefined—it just needs to be consistent across all threads. Interleaving of Operations:\nThe execution can be viewed as an interleaving of instructions from all threads. However, the interleaving must follow the program order within each thread.\nno real machine/compiler implements this"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#compiler-effects",
    "href": "lectures/revealjs_diverg.qmd.html#compiler-effects",
    "title": "divergent flow",
    "section": "compiler effects",
    "text": "compiler effects\nCompiler transformations that break multi-thread sequential consistency (SC) often reorder or optimize instructions in ways that do not respect the original program order seen by other threads. These transformations can lead to subtle bugs in multithreaded programs where the expected interleaving of operations is violated."
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#loadstore-reordering",
    "href": "lectures/revealjs_diverg.qmd.html#loadstore-reordering",
    "title": "divergent flow",
    "section": "Load/Store Reordering",
    "text": "Load/Store Reordering\nTransformation: Compilers might reorder loads and stores to improve performance. Violation: In a multi-threaded environment, this can lead to a situation where one thread sees stale or unexpected data. Example:\nCopy code\n// Thread 1\nx = 1;     // Store\nr1 = y;    // Load\n\n// Thread 2\ny = 1;     // Store\nr2 = x;    // Load\nUnder sequential consistency, if thread 1’s x = 1 happens before thread 2’s r2 = x, then thread 2 should observe r2 == 1. But reordering could result in thread 2 reading x as 0."
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#common-subexpression-elimination-cse",
    "href": "lectures/revealjs_diverg.qmd.html#common-subexpression-elimination-cse",
    "title": "divergent flow",
    "section": "Common Subexpression Elimination (CSE)",
    "text": "Common Subexpression Elimination (CSE)\nTransformation: If a variable or expression is computed multiple times, the compiler may optimize by reusing the result of an earlier computation. Violation: This assumes that no other thread modifies shared variables between these uses. Example:\n// Original code\nr1 = x;\nr2 = x;\n\n// Transformed code (CSE applied)\ntemp = x;\nr1 = temp;\nr2 = temp;\nIf x is modified by another thread between the two reads, the transformed code will incorrectly assume the value of x hasn’t changed."
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#dead-code-elimination-dce",
    "href": "lectures/revealjs_diverg.qmd.html#dead-code-elimination-dce",
    "title": "divergent flow",
    "section": "Dead Code Elimination (DCE)",
    "text": "Dead Code Elimination (DCE)\nTransformation: The compiler may remove stores to variables that are not subsequently read in the same thread. Violation: If the variable is shared and accessed by other threads, removing the store could lead to unexpected behavior. Example:\n// Original code\nx = 1;\n\n// Transformed code (DCE applied)\n// x = 1 is removed because x is not used locally If another thread reads x, it expects the store to have happened, but DCE breaks this assumption."
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#speculative-execution-out-of-order-execution",
    "href": "lectures/revealjs_diverg.qmd.html#speculative-execution-out-of-order-execution",
    "title": "divergent flow",
    "section": "Speculative Execution (Out-of-Order Execution)",
    "text": "Speculative Execution (Out-of-Order Execution)\nTransformation: Compilers (or hardware) may execute instructions speculatively, assuming certain branches are likely to be taken. Violation: This can cause out-of-order writes or reads visible to other threads, breaking SC. Example:\nif (flag) {\n    r1 = x;\n}\nIf the compiler speculatively reads x before knowing the value of flag, another thread’s write to x might be missed or observed out-of-order."
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#loop-invariant-code-motion",
    "href": "lectures/revealjs_diverg.qmd.html#loop-invariant-code-motion",
    "title": "divergent flow",
    "section": "Loop Invariant Code Motion",
    "text": "Loop Invariant Code Motion\nTransformation: The compiler moves computations that are invariant inside a loop to outside the loop. Violation: If these computations involve shared variables modified by other threads within the loop, moving them outside could make the code see stale values. Example:\n// Original code\nwhile (condition) {\n    r = shared_variable;\n}\n\n// Transformed code (Loop Invariant Code Motion)\ntemp = shared_variable;\nwhile (condition) {\n    r = temp;\n}\nIf shared_variable is updated by another thread, the transformed code might keep using the old value.\nRegister Allocation (Caching Shared Variables in Registers)\nTransformation: Compilers can keep a shared variable in a register for efficiency rather than repeatedly loading it from memory. Violation: If another thread modifies that shared variable in memory, the compiler’s register optimization would cause the thread to read stale data. Example:\nwhile (flag == 0) {\n    // busy-wait\n}\nIf flag is cached in a register, updates to flag by another thread in memory won’t be reflected, breaking SC."
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#instruction-fusion-combining-loadsstores",
    "href": "lectures/revealjs_diverg.qmd.html#instruction-fusion-combining-loadsstores",
    "title": "divergent flow",
    "section": "Instruction Fusion (Combining Loads/Stores)",
    "text": "Instruction Fusion (Combining Loads/Stores)\nTransformation: The compiler may combine consecutive memory accesses into one, such as merging adjacent stores into a single store or combining two loads. Violation: If other threads expect these loads or stores to happen separately, they might see an inconsistent view of memory. Example:\n// Original code\nx = 1;\ny = 2;\n\n// Transformed code (store fusion)\n// x and y are stored together in a single transaction\nA thread expecting x and y to be updated separately might observe an inconsistent state if this transformation is applied."
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#thread-libraries",
    "href": "lectures/revealjs_diverg.qmd.html#thread-libraries",
    "title": "divergent flow",
    "section": "thread libraries",
    "text": "thread libraries\nstart out assuming single threaded, add a threads library like pthreads\nmultiple threads could access shared memory simultaneously, leading to race conditions, inconsistent data, and undefined behavior.\nModern CPUs and compilers perform optimizations like instruction reordering, which can break assumptions about the order of memory operations in multithreaded programs.\nMultithreaded code is harder to test because race conditions and bugs might only manifest under certain timing conditions.\nDebugging multithreaded programs is more difficult due to the unpredictable nature of thread execution and interactions.\nSome optimizations might reorder instructions in a way that is incompatible with multithreading, introducing subtle bugs or performance regressions.\nCaching, prefetching, or other memory optimizations need to account for the fact that multiple threads may be accessing the same memory, which a simple thread library does not handle."
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#using-libraries",
    "href": "lectures/revealjs_diverg.qmd.html#using-libraries",
    "title": "divergent flow",
    "section": "using libraries",
    "text": "using libraries\n\nFunctions such as pthread mutex lock() that are guaranteed by the standard to “synchronize memory” include hardware instructions (“memory barriers”) that prevent hardware reordering of memory operations around the call\nTo prevent the compiler from moving memory operations around calls to functions such as pthread mutex lock(), they are essentially treated as calls to opaque functions, about which the compiler has no information.\n\nThe compiler effectively assumes that pthread mutex lock() may read or write any global variable. Thus a memory reference cannot simply be moved across the call. This approach also ensures that transitive calls, e.g. a call to a function f() which then calls pthread mutex lock(), are handled in the same way more or less appropriately, i.e. memory operations are not moved across the call to f() either, whether or not the entire user program is being analyzed at once."
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#adding-multi-threading-to-user-explaining-the-intent",
    "href": "lectures/revealjs_diverg.qmd.html#adding-multi-threading-to-user-explaining-the-intent",
    "title": "divergent flow",
    "section": "adding multi-threading to user explaining the intent",
    "text": "adding multi-threading to user explaining the intent\nc++/c added atomics\nAtomic operations are operations that are completed as a single, uninterruptible action. No other thread can observe a partial update or interfere with the operation.\nThese operations ensure that read-modify-write sequences are safe without needing explicit locks."
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#an-example-2",
    "href": "lectures/revealjs_diverg.qmd.html#an-example-2",
    "title": "divergent flow",
    "section": "an example",
    "text": "an example\n#include &lt;atomic&gt;\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n\n// Global spinlock using atomic_flag\nstd::atomic_flag lock = ATOMIC_FLAG_INIT;\n\nvoid enter_critical_section() {\n    // Busy-wait (spin) until the lock is acquired\n    while (lock.test_and_set(std::memory_order_acquire)) {\n        // Spin and wait for the lock to become available\n    }\n}\n\nvoid leave_critical_section() {\n    // Release the lock\n    lock.clear(std::memory_order_release);\n}\n\n// Shared resource\nint shared_counter = 0;\n\nvoid critical_section_task(int num_increments) {\n    for (int i = 0; i &lt; num_increments; ++i) {\n        enter_critical_section();\n        // Begin critical section\n        ++shared_counter;\n        // End critical section\n        leave_critical_section();\n    }\n}"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#load-acquire-needs-special-hardware",
    "href": "lectures/revealjs_diverg.qmd.html#load-acquire-needs-special-hardware",
    "title": "divergent flow",
    "section": "load acquire (needs special hardware )",
    "text": "load acquire (needs special hardware )\nused by default with atomics not used for non-atomics\nall memory reads and writes after the load operation cannot be moved before the load. This ensures that after acquiring the value, any operations that depend on this value (like accessing shared data) will see consistent and up-to-date memory.\na one way fence - nothing can move up"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#write-release-needs-special-hardware",
    "href": "lectures/revealjs_diverg.qmd.html#write-release-needs-special-hardware",
    "title": "divergent flow",
    "section": "write release (needs special hardware )",
    "text": "write release (needs special hardware )\nprevents the compiler or processor from reordering any memory operations (reads or writes) that appear before the release store. This guarantees that all operations that modify shared data before the release are visible to other threads that subsequently perform an acquire operation.\nalso a one way fence - nothing can move down\nload.acquire - \nloads and stores on non-atomics  - compiler picks the order for these operations \nstore.release"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#using-atomics",
    "href": "lectures/revealjs_diverg.qmd.html#using-atomics",
    "title": "divergent flow",
    "section": "using atomics",
    "text": "using atomics\nAll operations appear to occur in a single total order that is consistent across all threads. This means that the results of operations are predictable and consistent as if all operations were executed in some sequential order.\nlimits the hardware and compiler because it prevents reordering"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#data-race-free",
    "href": "lectures/revealjs_diverg.qmd.html#data-race-free",
    "title": "divergent flow",
    "section": "Data Race Free",
    "text": "Data Race Free\nData Race Free (DRF) means that a program is free from data races, which occur when:\n\nTwo or more threads access the same variable concurrently.\nAt least one of the accesses is a write.\n\nThere is no synchronization mechanism (like mutexes or atomic operations) to control the access. In a data race-free program, every shared variable is accessed in a way that ensures predictable results. C++ provides various synchronization primitives (such as mutexes and atomic types) to help developers write DRF code.\nAll shared variables must be accessed using synchronization to prevent concurrent threads from modifying shared data simultaneously without coordination."
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#an-example-3",
    "href": "lectures/revealjs_diverg.qmd.html#an-example-3",
    "title": "divergent flow",
    "section": "an example",
    "text": "an example\n#include &lt;iostream&gt;\n#include &lt;atomic&gt;\n#include &lt;thread&gt;\n\nint shared_counter1 = 0;                  // First non-atomic shared variable\nint shared_counter2 = 0;                  // Second non-atomic shared variable\nstd::atomic&lt;bool&gt; lock_flag(false);       // Atomic flag to control access\n\nvoid safe_increment() {\n    for (int i = 0; i &lt; 1000; ++i) {\n        // Spin until the lock is acquired\n        while (lock_flag.exchange(true)) {\n            // Busy-wait (spin) until the lock is free\n        }\n\n        // Critical section: update the non-atomic shared variables\n        ++shared_counter1;\n        ++shared_counter2;\n\n        // Release the lock\n        lock_flag.store(false);\n    }\n}"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#language-rules",
    "href": "lectures/revealjs_diverg.qmd.html#language-rules",
    "title": "divergent flow",
    "section": "language rules",
    "text": "language rules\nC and C++\ndo not define what happens in the presence of data races. If a program has data races (e.g., multiple threads concurrently reading and writing to the same variable without synchronization), the behavior is considered undefined. This means that the program may produce unexpected results, crash, or behave inconsistently across different executions or platforms.\nJava\ntries to define what happens but definition is very complex and maybe inconsistent\nRust\nCompile-Time Guarantees: Rust’s ownership and borrowing system prevents data races at compile time. If a program is not DRF, the Rust compiler will typically refuse to compile it, enforcing memory safety guarantees."
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#can-the-compiler-add-a-race-to-a-drf-program",
    "href": "lectures/revealjs_diverg.qmd.html#can-the-compiler-add-a-race-to-a-drf-program",
    "title": "divergent flow",
    "section": "can the compiler add a race to a drf program",
    "text": "can the compiler add a race to a drf program\nnew rule, compiler cannot add a write to a shared variable\nif (x ==1) y++\n\nto \ny++\nif (x!=1) y--"
  },
  {
    "objectID": "lectures/revealjs_diverg.qmd.html#how-does-this-effect-hardware",
    "href": "lectures/revealjs_diverg.qmd.html#how-does-this-effect-hardware",
    "title": "divergent flow",
    "section": "how does this effect hardware?",
    "text": "how does this effect hardware?\nstruct { char a; char b; char c; char d;} s;\ns.a = 1\ns.c = 3\n\ncan a compiler do \nchar temp[4] = s // load 32 bits \ntemp[0] = 1\ntemp[2] = 3\ns = temp\nnot allowed - reads/writes b and d, so compiler incorrectly added writes\noptions are either have byte addressable hardware, or pad so that each char gets 32 bits\nVendors forced to add 8 byte loads/stores"
  },
  {
    "objectID": "lectures/02a_representation.html",
    "href": "lectures/02a_representation.html",
    "title": "Representation of programs",
    "section": "",
    "text": "The representation of a program\nWhat we read in and read out when transforming a program.\nWhat kind of properties make a good representation?\nThis lecture explores different representations and their implications.\n\nfrom graphviz import Digraph\nimport ast\nimport os \n\ndef cmd(x):\n  os.system(x)\n  \ndef ast_syntax(line):\n  return ast.dump(ast.parse(line).body[0], indent=4)\n\n  \n# Define a function to recursively add nodes to the Digraph\ndef add_node(dot, node, parent=None):\n  node_name = str(node.__class__.__name__)\n  dot.node(str(id(node)), node_name)\n  if parent:\n    dot.edge(str(id(parent)), str(id(node)))\n  for child in ast.iter_child_nodes(node):\n    add_node(dot, child, node)\n\n# Add nodes to the Digraph\n\ndef graph(line):\n  dot = Digraph()\n  add_node(dot, ast.parse(line).body[0])\n  return dot",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#how-do-we-represent-programs",
    "href": "lectures/02a_representation.html#how-do-we-represent-programs",
    "title": "Representation of programs",
    "section": "",
    "text": "The representation of a program\nWhat we read in and read out when transforming a program.\nWhat kind of properties make a good representation?\nThis lecture explores different representations and their implications.\n\nfrom graphviz import Digraph\nimport ast\nimport os \n\ndef cmd(x):\n  os.system(x)\n  \ndef ast_syntax(line):\n  return ast.dump(ast.parse(line).body[0], indent=4)\n\n  \n# Define a function to recursively add nodes to the Digraph\ndef add_node(dot, node, parent=None):\n  node_name = str(node.__class__.__name__)\n  dot.node(str(id(node)), node_name)\n  if parent:\n    dot.edge(str(id(parent)), str(id(node)))\n  for child in ast.iter_child_nodes(node):\n    add_node(dot, child, node)\n\n# Add nodes to the Digraph\n\ndef graph(line):\n  dot = Digraph()\n  add_node(dot, ast.parse(line).body[0])\n  return dot",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#concrete-syntax",
    "href": "lectures/02a_representation.html#concrete-syntax",
    "title": "Representation of programs",
    "section": "Concrete Syntax",
    "text": "Concrete Syntax\nConcrete syntax, or surface syntax, represents programs as they are written\nPrograms are text or surface syntax- just what you would type into an editor.\nvalue = 8\nresult = 1\nfor i in range(value):\n  result = result + i\nprint(result)\nWhat is good and what is bad about this representation?\nWhat is the level of abstraction?\nHow do you understand the semantics.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#abstract-syntax",
    "href": "lectures/02a_representation.html#abstract-syntax",
    "title": "Representation of programs",
    "section": "Abstract syntax",
    "text": "Abstract syntax\nAbstract syntax represents programs as tree structures, focusing on the nodes and their connections.\n\nNodes are parts of the program,\nEdges show how they are connected.\n\nWe can write this as a list or a graph\n\n\ndef pgm():\n    value = 8\n    result = 1\n    for i in range(value):\n        result = result * i\n    print(result)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#ast-tree-representation",
    "href": "lectures/02a_representation.html#ast-tree-representation",
    "title": "Representation of programs",
    "section": "AST tree representation",
    "text": "AST tree representation\nAn AST is a tree structure, nodes like ‘if’, ‘test’, ‘body’, assign’.\nEach node is one concept from the program\nRecursive function can walk over the tree, one chunk of code for each node.\n\nGood - each type of node is different, making special cases are easy\nBad - each type of node is different so analysis has to know about every type, making general cases hard\n\nThis is the classic way to write an interpreter.\nSimple (non optimizing) compilers often use this format.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#a-more-regular-representation",
    "href": "lectures/02a_representation.html#a-more-regular-representation",
    "title": "Representation of programs",
    "section": "A more regular representation",
    "text": "A more regular representation\nPrograms are lists of instructions. Like an assembly instructions. Same sort of representation as LLVM.\nts2bril images/toy.ts | bril2txt\n\n\n    //typescript program \n    let value = 8\n    let result = 1\n    for (let i = 0; i &lt; value;\n         i = i+1)\n    {\n        result = result * i\n    }\n    console.log(result)\n\n\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#bril",
    "href": "lectures/02a_representation.html#bril",
    "title": "Representation of programs",
    "section": "bril",
    "text": "bril\n\nLooks like assembly\nno limit on registers,\nno condition codes.\nfully typed,\nno complex addressing modes.\neasy to extend",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#bril-syntax",
    "href": "lectures/02a_representation.html#bril-syntax",
    "title": "Representation of programs",
    "section": "Bril syntax",
    "text": "Bril syntax\nDeclare functions, labels, instructions\ninstruction:\n\nvariable type = opcode arguments\nopcode list of arguments\n\nForm 1, variable is the destination, like a: int = add b, c\nForm 2, no destination, like print a\nwhat is good and what is about this representation?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#control-flow-graph-cfg-version-1",
    "href": "lectures/02a_representation.html#control-flow-graph-cfg-version-1",
    "title": "Representation of programs",
    "section": "control flow graph (CFG) (version 1)",
    "text": "control flow graph (CFG) (version 1)\nRepresentation is a directed graph.\n\nNodes are instructions,\nedges indicate possible flow of control,\none entry and one exit node.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#example-one",
    "href": "lectures/02a_representation.html#example-one",
    "title": "Representation of programs",
    "section": "Example one",
    "text": "Example one\n@main {\n    v: int = const 5;\n    print v;\n}\n. . .\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| fig-width: 10\n%%| fig-height: 9\n\nflowchart LR\nA[const] --&gt; B[print]\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| fig-width: 10\n%%| fig-height: 9\n\nflowchart LR\nA[const] --&gt; B[print]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#second-example",
    "href": "lectures/02a_representation.html#second-example",
    "title": "Representation of programs",
    "section": "second example",
    "text": "second example\n    @main {\n        v: int = const 4;\n        jmp  .somewhere;\n        v: int = const 2;\n        .somewhere;\n        print v;\n    }\n. . .\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| fig-width: 6.5\nflowchart LR\n  A[const 4] --&gt; B[jmp]\n  B --&gt; C[print]\n  D[const 2] --&gt; C\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| fig-width: 6.5\nflowchart LR\n  A[const 4] --&gt; B[jmp]\n  B --&gt; C[print]\n  D[const 2] --&gt; C\n\n\n\n\n\n\n. . .\nnotice label does not produce a node\nEasy to see a dead instruction.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#third-example",
    "href": "lectures/02a_representation.html#third-example",
    "title": "Representation of programs",
    "section": "Third example:",
    "text": "Third example:\n    @main {\n        v: int = const 4;\n        b: bool = const false;\n        br b .there .here;\n    .here:\n        v: int = const 2;\n    .there;\n        print v;\n    }\n. . .\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| fig-width: 6.5\nflowchart LR\n  A[v: int const 4] --&gt; B[b: bool const false]\n  B --&gt; C[br b .there, .false]\n  C --&gt; D[v: const 2]\n  C --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| fig-width: 6.5\nflowchart LR\n  A[v: int const 4] --&gt; B[b: bool const false]\n  B --&gt; C[br b .there, .false]\n  C --&gt; D[v: const 2]\n  C --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\n. . .\nwhich is the true edge and which is the false edge , could mark the edges or use a convention\nWhich is the entry, which is the exit?\nThere is a long chain of instructions entered at the top, exit at the bottom, no branches inside.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#cfg-cfg-form-2",
    "href": "lectures/02a_representation.html#cfg-cfg-form-2",
    "title": "Representation of programs",
    "section": "CFG (cfg form 2)",
    "text": "CFG (cfg form 2)\n\nnodes ares sequences of instructions.\njumps and branches can only be at the end of a sequence\nonly label has to be at the start\nevery instruction in the sequence executes the same number of times",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#construct-cfg",
    "href": "lectures/02a_representation.html#construct-cfg",
    "title": "Representation of programs",
    "section": "construct cfg",
    "text": "construct cfg\nwalk over the instructions:\nAs we construct basic blocks, we can add instructions up till something that ends the block (terminator)\nOption: do all blocks end in a terminator or not?\ngiven a block b, the predecessors of \\(b\\) are the blocks \\(b_{in}\\) where there is an edge \\(b_{in}-&gt;b\\). And the successors of \\(b\\) are the \\(b_{out}\\) where \\(b-&gt;b_{out}\\) is an edge.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#what-is-an-algorithm-that-forms-a-cfg",
    "href": "lectures/02a_representation.html#what-is-an-algorithm-that-forms-a-cfg",
    "title": "Representation of programs",
    "section": "What is an algorithm that forms a cfg",
    "text": "What is an algorithm that forms a cfg\n. . .\n\njust find all the basic blocks\nadd the control flow edges",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/02a_representation.html#pseudo-code-to-construct-cfg",
    "href": "lectures/02a_representation.html#pseudo-code-to-construct-cfg",
    "title": "Representation of programs",
    "section": "pseudo code to construct cfg",
    "text": "pseudo code to construct cfg\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\n\nstep 2 we need a map from labels to basic blocks\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\n    \n\nfor block in blocks:\n   last = block[-1]\n   if last is a jmp (one successor)\n      add edge from block to last.dest \n   else if last is a br (two successors)\n      add two edges from block to last.true, last.false \n   else  fall through \n      add edge to next block (if it exists)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Representation of programs"
    ]
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#how-do-we-represent-programs",
    "href": "lectures/revealjs_02a_representation.qmd.html#how-do-we-represent-programs",
    "title": "Representation of programs",
    "section": "How do we represent programs",
    "text": "How do we represent programs\nThe representation of a program\nWhat we read in and read out when transforming a program.\nWhat kind of properties make a good representation?\nThis lecture explores different representations and their implications.\n\n\nCode\nfrom graphviz import Digraph\nimport ast\nimport os \n\ndef cmd(x):\n  os.system(x)\n  \ndef ast_syntax(line):\n  return ast.dump(ast.parse(line).body[0], indent=4)\n\n  \n# Define a function to recursively add nodes to the Digraph\ndef add_node(dot, node, parent=None):\n  node_name = str(node.__class__.__name__)\n  dot.node(str(id(node)), node_name)\n  if parent:\n    dot.edge(str(id(parent)), str(id(node)))\n  for child in ast.iter_child_nodes(node):\n    add_node(dot, child, node)\n\n# Add nodes to the Digraph\n\ndef graph(line):\n  dot = Digraph()\n  add_node(dot, ast.parse(line).body[0])\n  return dot"
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#concrete-syntax",
    "href": "lectures/revealjs_02a_representation.qmd.html#concrete-syntax",
    "title": "Representation of programs",
    "section": "Concrete Syntax",
    "text": "Concrete Syntax\nConcrete syntax, or surface syntax, represents programs as they are written\nPrograms are text or surface syntax- just what you would type into an editor.\nvalue = 8\nresult = 1\nfor i in range(value):\n  result = result + i\nprint(result)\nWhat is good and what is bad about this representation?\nWhat is the level of abstraction?\nHow do you understand the semantics."
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#abstract-syntax",
    "href": "lectures/revealjs_02a_representation.qmd.html#abstract-syntax",
    "title": "Representation of programs",
    "section": "Abstract syntax",
    "text": "Abstract syntax\nAbstract syntax represents programs as tree structures, focusing on the nodes and their connections.\n\nNodes are parts of the program,\nEdges show how they are connected.\n\nWe can write this as a list or a graph\n\n\ndef pgm():\n    value = 8\n    result = 1\n    for i in range(value):\n        result = result * i\n    print(result)"
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#ast-tree-representation",
    "href": "lectures/revealjs_02a_representation.qmd.html#ast-tree-representation",
    "title": "Representation of programs",
    "section": "AST tree representation",
    "text": "AST tree representation\nAn AST is a tree structure, nodes like ‘if’, ‘test’, ‘body’, assign’.\nEach node is one concept from the program\nRecursive function can walk over the tree, one chunk of code for each node.\n\nGood - each type of node is different, making special cases are easy\nBad - each type of node is different so analysis has to know about every type, making general cases hard\n\nThis is the classic way to write an interpreter.\nSimple (non optimizing) compilers often use this format."
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#a-more-regular-representation",
    "href": "lectures/revealjs_02a_representation.qmd.html#a-more-regular-representation",
    "title": "Representation of programs",
    "section": "A more regular representation",
    "text": "A more regular representation\nPrograms are lists of instructions. Like an assembly instructions. Same sort of representation as LLVM.\nts2bril images/toy.ts | bril2txt\n\n\n    //typescript program \n    let value = 8\n    let result = 1\n    for (let i = 0; i &lt; value;\n         i = i+1)\n    {\n        result = result * i\n    }\n    console.log(result)\n\n\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}"
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#bril",
    "href": "lectures/revealjs_02a_representation.qmd.html#bril",
    "title": "Representation of programs",
    "section": "bril",
    "text": "bril\n\nLooks like assembly\nno limit on registers,\nno condition codes.\nfully typed,\nno complex addressing modes.\neasy to extend"
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#bril-syntax",
    "href": "lectures/revealjs_02a_representation.qmd.html#bril-syntax",
    "title": "Representation of programs",
    "section": "Bril syntax",
    "text": "Bril syntax\nDeclare functions, labels, instructions\ninstruction:\n\nvariable type = opcode arguments\nopcode list of arguments\n\nForm 1, variable is the destination, like a: int = add b, c\nForm 2, no destination, like print a\nwhat is good and what is about this representation?"
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#control-flow-graph-cfg-version-1",
    "href": "lectures/revealjs_02a_representation.qmd.html#control-flow-graph-cfg-version-1",
    "title": "Representation of programs",
    "section": "control flow graph (CFG) (version 1)",
    "text": "control flow graph (CFG) (version 1)\nRepresentation is a directed graph.\n\nNodes are instructions,\nedges indicate possible flow of control,\none entry and one exit node."
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#example-one",
    "href": "lectures/revealjs_02a_representation.qmd.html#example-one",
    "title": "Representation of programs",
    "section": "Example one",
    "text": "Example one\n@main {\n    v: int = const 5;\n    print v;\n}\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| fig-width: 10\n%%| fig-height: 9\n\nflowchart LR\nA[const] --&gt; B[print]\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| fig-width: 10\n%%| fig-height: 9\n\nflowchart LR\nA[const] --&gt; B[print]"
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#second-example",
    "href": "lectures/revealjs_02a_representation.qmd.html#second-example",
    "title": "Representation of programs",
    "section": "second example",
    "text": "second example\n    @main {\n        v: int = const 4;\n        jmp  .somewhere;\n        v: int = const 2;\n        .somewhere;\n        print v;\n    }\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| fig-width: 6.5\nflowchart LR\n  A[const 4] --&gt; B[jmp]\n  B --&gt; C[print]\n  D[const 2] --&gt; C\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| fig-width: 6.5\nflowchart LR\n  A[const 4] --&gt; B[jmp]\n  B --&gt; C[print]\n  D[const 2] --&gt; C\n\n\n\n\n\n\n\n\nnotice label does not produce a node\nEasy to see a dead instruction."
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#third-example",
    "href": "lectures/revealjs_02a_representation.qmd.html#third-example",
    "title": "Representation of programs",
    "section": "Third example:",
    "text": "Third example:\n    @main {\n        v: int = const 4;\n        b: bool = const false;\n        br b .there .here;\n    .here:\n        v: int = const 2;\n    .there;\n        print v;\n    }\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| fig-width: 6.5\nflowchart LR\n  A[v: int const 4] --&gt; B[b: bool const false]\n  B --&gt; C[br b .there, .false]\n  C --&gt; D[v: const 2]\n  C --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| fig-width: 6.5\nflowchart LR\n  A[v: int const 4] --&gt; B[b: bool const false]\n  B --&gt; C[br b .there, .false]\n  C --&gt; D[v: const 2]\n  C --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\n\n\nwhich is the true edge and which is the false edge , could mark the edges or use a convention\nWhich is the entry, which is the exit?\nThere is a long chain of instructions entered at the top, exit at the bottom, no branches inside."
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#cfg-cfg-form-2",
    "href": "lectures/revealjs_02a_representation.qmd.html#cfg-cfg-form-2",
    "title": "Representation of programs",
    "section": "CFG (cfg form 2)",
    "text": "CFG (cfg form 2)\n\nnodes ares sequences of instructions.\njumps and branches can only be at the end of a sequence\nonly label has to be at the start\nevery instruction in the sequence executes the same number of times"
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#construct-cfg",
    "href": "lectures/revealjs_02a_representation.qmd.html#construct-cfg",
    "title": "Representation of programs",
    "section": "construct cfg",
    "text": "construct cfg\nwalk over the instructions:\nAs we construct basic blocks, we can add instructions up till something that ends the block (terminator)\nOption: do all blocks end in a terminator or not?\ngiven a block b, the predecessors of \\(b\\) are the blocks \\(b_{in}\\) where there is an edge \\(b_{in}-&gt;b\\). And the successors of \\(b\\) are the \\(b_{out}\\) where \\(b-&gt;b_{out}\\) is an edge."
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#what-is-an-algorithm-that-forms-a-cfg",
    "href": "lectures/revealjs_02a_representation.qmd.html#what-is-an-algorithm-that-forms-a-cfg",
    "title": "Representation of programs",
    "section": "What is an algorithm that forms a cfg",
    "text": "What is an algorithm that forms a cfg\n\n\njust find all the basic blocks\nadd the control flow edges"
  },
  {
    "objectID": "lectures/revealjs_02a_representation.qmd.html#pseudo-code-to-construct-cfg",
    "href": "lectures/revealjs_02a_representation.qmd.html#pseudo-code-to-construct-cfg",
    "title": "Representation of programs",
    "section": "pseudo code to construct cfg",
    "text": "pseudo code to construct cfg\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []"
  },
  {
    "objectID": "lectures/diverg1.html",
    "href": "lectures/diverg1.html",
    "title": "divergent flow PART TWO",
    "section": "",
    "text": "At a branch- we turn off some threads, when do we go to the other threads, when do we reconverge?\nReconverge at the post-dominator of the branch. Closest point that must be reached by all paths from the branch.\nSome numbers from Fung micro 2007\n\nreconverge at post dominator vs never reconverge 93% speedup\nreconverge dynamically at best place 140% speedup\nestimate 5% chip area for simt-stack",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow PART TWO"
    ]
  },
  {
    "objectID": "lectures/diverg1.html#continuing-on-branch-divergence",
    "href": "lectures/diverg1.html#continuing-on-branch-divergence",
    "title": "divergent flow PART TWO",
    "section": "",
    "text": "At a branch- we turn off some threads, when do we go to the other threads, when do we reconverge?\nReconverge at the post-dominator of the branch. Closest point that must be reached by all paths from the branch.\nSome numbers from Fung micro 2007\n\nreconverge at post dominator vs never reconverge 93% speedup\nreconverge dynamically at best place 140% speedup\nestimate 5% chip area for simt-stack",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow PART TWO"
    ]
  },
  {
    "objectID": "lectures/diverg1.html#latency-hiding",
    "href": "lectures/diverg1.html#latency-hiding",
    "title": "divergent flow PART TWO",
    "section": "latency hiding",
    "text": "latency hiding\nExecute lots of warps, cache hit rates are low, do not stall the pipeline on a miss.\nWhen a load misses, remove warp from an active list, in the next cycle run another warp from the active list.\nTo make the switch fast, and hide the latency, start each warp with all the resources it needs, (registers/shared mem etc) switching a warp, is just changed the pointers to the active resources.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow PART TWO"
    ]
  },
  {
    "objectID": "lectures/diverg1.html#stack-reconvergence-an-example",
    "href": "lectures/diverg1.html#stack-reconvergence-an-example",
    "title": "divergent flow PART TWO",
    "section": "stack reconvergence an example",
    "text": "stack reconvergence an example\n\n\n__managed__ int *a, *b, *c;\n\n__global__ void kern() {\n  int t = threadIdx.x;\n\n\n\n  if (t &lt; 5) {\n\n\n    a[t] = 1;\n    b[t] = 2;\n    c[t] = 3;\n\n\n  } else {\n    a[t + 6] = 5;\n    b[t + 6] = 7;\n    c[t + 6] = 9;\n  }\n  a[6] = 99;\n}\n\n\n\n\n  SSY `(.L_x_0)\n\n\n    @!P0 BRA `(.L_x_1) ToTarget \n    // fallthrough\n\n    SYNC  &lt;  finish one side-\n  \n    .L_x_1:   &lt; branch target \n  \n    SYNC   &lt; finish one side \n\n\n  .L_x_0: &lt;  post dominator",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow PART TWO"
    ]
  },
  {
    "objectID": "lectures/diverg1.html#stack-control",
    "href": "lectures/diverg1.html#stack-control",
    "title": "divergent flow PART TWO",
    "section": "stack control",
    "text": "stack control\nWhen a kernel is launched, a stack is allocated for each corresponding warp.\nstack entry:\n\na 32-bit mask is stored\nthe address of the next instruction to execute (next program counter or npc) and\naddress of the instruction at which the threads must wait for reconvergence (reconvergence program counter or rpc).\n\nThe next pc to execute is the address on the stack, as each instruction executes this address advances\nEach stack is initialized with an entry composed of:\na mask in which all the threads in the warp are active, the start address of the program as npc and the last address in memory as rpc (note that it does not need to be a valid instruction address).",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow PART TWO"
    ]
  },
  {
    "objectID": "lectures/diverg1.html#ssy-instruction",
    "href": "lectures/diverg1.html#ssy-instruction",
    "title": "divergent flow PART TWO",
    "section": "ssy instruction",
    "text": "ssy instruction\nThe SSY instruction prepare the stack for a possible divergence\nadds two entries- what to do after the if-then-else, what to do on a branch\nWhen SSY @addr is executed, the top entry of the stack is popped. A new entry is pushed to handle the execution after reconvergence:\nThe npc is the reconvergence address (@addr), and the rpc and the mask are copied from the popped entry.\nA second new entry is then pushed to handle the potentially diverging portion of code: the npc is the actual next instruction (the npc of the popped entry + 8, since instructions are encoded in 64 bits), the rpc is the reconvergence address (@addr) and the mask is the same as in the popped entry.\nThe top entry is used to let all active threads execute the next instruction. Note that these instructions do not create divergence but instead prepare the stack for a possible upcoming divergence, which is why the mask remains unchanged at this point.\nreference",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow PART TWO"
    ]
  },
  {
    "objectID": "lectures/diverg1.html#actual-divergence",
    "href": "lectures/diverg1.html#actual-divergence",
    "title": "divergent flow PART TWO",
    "section": "Actual Divergence",
    "text": "Actual Divergence\nActual divergence happens when a branch (BRA @addr) instruction is executed conditionally by only a subset of the threads of a warp. When this happens, the top entry of the corresponding stack is popped. Two new entries are then pushed:\n\nThe first pushed entry concerns the threads which do not take the branch:\n\nThe rpc is the same as in the popped entry.\nThe npc corresponds to the next instruction in the code (i.e., the current npc + 8).\nThe mask activates only the threads that do not take the branch (the ones for which the condition is false).\n\nThe second entry has the target address of the branch as npc and the same rpc as the popped entry:\n\nIts mask activates only the threads that take the branch (the ones for which the condition is true).\n\n\nAs a consequence, the GPU first executes the threads that take the branch until they reach a reconvergence instruction added by the compiler: SYNC or BRK (depending on whether an SSY or a PBK was executed before).",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow PART TWO"
    ]
  },
  {
    "objectID": "lectures/diverg1.html#sync",
    "href": "lectures/diverg1.html#sync",
    "title": "divergent flow PART TWO",
    "section": "sync",
    "text": "sync\nThe reconvergence instruction sync pops the top entry from the stack.\nThe GPU then resumes execution with the group of threads active in the mask of the new entry at the top of the stack: the threads that do not take the branch. When they reach a SYNC instruction, their corresponding entry is popped from the stack: the reconvergence is done and the execution flow resumes at the reconvergence address (which is the npc of the entry at the top of the stack at this point).",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "divergent flow PART TWO"
    ]
  },
  {
    "objectID": "lectures/revealjs_diverg1.qmd.html#continuing-on-branch-divergence",
    "href": "lectures/revealjs_diverg1.qmd.html#continuing-on-branch-divergence",
    "title": "divergent flow PART TWO",
    "section": "continuing on branch divergence",
    "text": "continuing on branch divergence\nAt a branch- we turn off some threads, when do we go to the other threads, when do we reconverge?\nReconverge at the post-dominator of the branch. Closest point that must be reached by all paths from the branch.\nSome numbers from Fung micro 2007\n\nreconverge at post dominator vs never reconverge 93% speedup\nreconverge dynamically at best place 140% speedup\nestimate 5% chip area for simt-stack"
  },
  {
    "objectID": "lectures/revealjs_diverg1.qmd.html#latency-hiding",
    "href": "lectures/revealjs_diverg1.qmd.html#latency-hiding",
    "title": "divergent flow PART TWO",
    "section": "latency hiding",
    "text": "latency hiding\nExecute lots of warps, cache hit rates are low, do not stall the pipeline on a miss.\nWhen a load misses, remove warp from an active list, in the next cycle run another warp from the active list.\nTo make the switch fast, and hide the latency, start each warp with all the resources it needs, (registers/shared mem etc) switching a warp, is just changed the pointers to the active resources."
  },
  {
    "objectID": "lectures/revealjs_diverg1.qmd.html#stack-reconvergence-an-example",
    "href": "lectures/revealjs_diverg1.qmd.html#stack-reconvergence-an-example",
    "title": "divergent flow PART TWO",
    "section": "stack reconvergence an example",
    "text": "stack reconvergence an example\n\n\n__managed__ int *a, *b, *c;\n\n__global__ void kern() {\n  int t = threadIdx.x;\n\n\n\n  if (t &lt; 5) {\n\n\n    a[t] = 1;\n    b[t] = 2;\n    c[t] = 3;\n\n\n  } else {\n    a[t + 6] = 5;\n    b[t + 6] = 7;\n    c[t + 6] = 9;\n  }\n  a[6] = 99;\n}\n\n\n\n\n  SSY `(.L_x_0)\n\n\n    @!P0 BRA `(.L_x_1) ToTarget \n    // fallthrough\n\n    SYNC  &lt;  finish one side-\n  \n    .L_x_1:   &lt; branch target \n  \n    SYNC   &lt; finish one side \n\n\n  .L_x_0: &lt;  post dominator"
  },
  {
    "objectID": "lectures/revealjs_diverg1.qmd.html#stack-control",
    "href": "lectures/revealjs_diverg1.qmd.html#stack-control",
    "title": "divergent flow PART TWO",
    "section": "stack control",
    "text": "stack control\nWhen a kernel is launched, a stack is allocated for each corresponding warp.\nstack entry:\n\na 32-bit mask is stored\nthe address of the next instruction to execute (next program counter or npc) and\naddress of the instruction at which the threads must wait for reconvergence (reconvergence program counter or rpc).\n\nThe next pc to execute is the address on the stack, as each instruction executes this address advances\nEach stack is initialized with an entry composed of:\na mask in which all the threads in the warp are active, the start address of the program as npc and the last address in memory as rpc (note that it does not need to be a valid instruction address)."
  },
  {
    "objectID": "lectures/revealjs_diverg1.qmd.html#ssy-instruction",
    "href": "lectures/revealjs_diverg1.qmd.html#ssy-instruction",
    "title": "divergent flow PART TWO",
    "section": "ssy instruction",
    "text": "ssy instruction\nThe SSY instruction prepare the stack for a possible divergence\nadds two entries- what to do after the if-then-else, what to do on a branch\nWhen SSY @addr is executed, the top entry of the stack is popped. A new entry is pushed to handle the execution after reconvergence:\nThe npc is the reconvergence address (@addr), and the rpc and the mask are copied from the popped entry.\nA second new entry is then pushed to handle the potentially diverging portion of code: the npc is the actual next instruction (the npc of the popped entry + 8, since instructions are encoded in 64 bits), the rpc is the reconvergence address (@addr) and the mask is the same as in the popped entry.\nThe top entry is used to let all active threads execute the next instruction. Note that these instructions do not create divergence but instead prepare the stack for a possible upcoming divergence, which is why the mask remains unchanged at this point.\nreference"
  },
  {
    "objectID": "lectures/revealjs_diverg1.qmd.html#actual-divergence",
    "href": "lectures/revealjs_diverg1.qmd.html#actual-divergence",
    "title": "divergent flow PART TWO",
    "section": "Actual Divergence",
    "text": "Actual Divergence\nActual divergence happens when a branch (BRA @addr) instruction is executed conditionally by only a subset of the threads of a warp. When this happens, the top entry of the corresponding stack is popped. Two new entries are then pushed:\n\nThe first pushed entry concerns the threads which do not take the branch:\n\nThe rpc is the same as in the popped entry.\nThe npc corresponds to the next instruction in the code (i.e., the current npc + 8).\nThe mask activates only the threads that do not take the branch (the ones for which the condition is false).\n\nThe second entry has the target address of the branch as npc and the same rpc as the popped entry:\n\nIts mask activates only the threads that take the branch (the ones for which the condition is true).\n\n\nAs a consequence, the GPU first executes the threads that take the branch until they reach a reconvergence instruction added by the compiler: SYNC or BRK (depending on whether an SSY or a PBK was executed before)."
  },
  {
    "objectID": "lectures/revealjs_diverg1.qmd.html#sync",
    "href": "lectures/revealjs_diverg1.qmd.html#sync",
    "title": "divergent flow PART TWO",
    "section": "sync",
    "text": "sync\nThe reconvergence instruction sync pops the top entry from the stack.\nThe GPU then resumes execution with the group of threads active in the mask of the new entry at the top of the stack: the threads that do not take the branch. When they reach a SYNC instruction, their corresponding entry is popped from the stack: the reconvergence is done and the execution flow resumes at the reconvergence address (which is the npc of the entry at the top of the stack at this point)."
  },
  {
    "objectID": "lectures/ra-checking.html",
    "href": "lectures/ra-checking.html",
    "title": "Testing Register allocators",
    "section": "",
    "text": "cranelift register allocator",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#this-is-based-on-the-cranelift-compiler-used-for-web-assembly-and-rust",
    "href": "lectures/ra-checking.html#this-is-based-on-the-cranelift-compiler-used-for-web-assembly-and-rust",
    "title": "Testing Register allocators",
    "section": "",
    "text": "cranelift register allocator",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#what-is-register-allocation",
    "href": "lectures/ra-checking.html#what-is-register-allocation",
    "title": "Testing Register allocators",
    "section": "what is register allocation",
    "text": "what is register allocation\nIn Bril and LLVM a program can use an arbitrary number of registers,\nvoid f() {\n    int x0 = compute(0);\n    int x1 = compute(1);\n    // ...\n    int x99 = compute(99);\n    \n    // --- 100 possibly different values were computed \n    // --- where are those values stored?\n    \n    consume(x0);\n    consume(x1);\n    // ...\n    consume(x99);\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#storing-variables",
    "href": "lectures/ra-checking.html#storing-variables",
    "title": "Testing Register allocators",
    "section": "storing variables",
    "text": "storing variables\none option\nAllocate a memory location for each local variable. All of the \\(x_N\\) variables live in memory.\nWhen the function is called, it allocates a new area on the stack called the stack frame and uses it to store local variables.\nThis means that adding two variables, takes two loads, one add, and one store so it is very slow .\nCompiling code in this way is very fast because we need to make almost no decisions: a variable reference always becomes a memory load,\nfaster would be to keep variables in registers but, we have a limited set of registers",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#definitions",
    "href": "lectures/ra-checking.html#definitions",
    "title": "Testing Register allocators",
    "section": "Definitions",
    "text": "Definitions\nRegister allocation: is assigning a value in the program to a register for storage. The register allocator decides how to shuffle values between memory and registers, and between register.\nIn Bril and LLVM we have virtual registers - as many as you want. The register allocator has to rewrite the instructions to use physical registers. Since the number of physical registers is limited, The allocator might insert additional instructions:",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#spillsreloads",
    "href": "lectures/ra-checking.html#spillsreloads",
    "title": "Testing Register allocators",
    "section": "spills/reloads",
    "text": "spills/reloads\n\nstores (called spills) to move a register to memory\nloads (called reloads) to move memory to a register\nmoves to copy from one register to another\n\nThe locations in memory are usually on the stack and are called spill-slots",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#example-of-register-allocation-on-a-machine-with-two-physical-registers",
    "href": "lectures/ra-checking.html#example-of-register-allocation-on-a-machine-with-two-physical-registers",
    "title": "Testing Register allocators",
    "section": "example of register allocation on a machine with two physical registers",
    "text": "example of register allocation on a machine with two physical registers\n\n\n\n\n\n\n\n\nvirtual register code\nphysical register code\nassigments\n\n\n\n\n\n\n{v0 -&gt; r0, v1 -&gt; r1}\n\n\n\nstore r1, [sp+0]\n\n\n\nadd v2, v0, v1\nadd r1, r0, r1\n{v0 -&gt; r0, v1-&gt;[sp+0], v2-&gt;r1}\n\n\nsub v3, v2, v0\nsub r1, r1, r0\n\n\n\n\nload r0, [sp+0]\n\n\n\nmul v4, v3, v1\nmul r0, r1, r0\n{v4-&gt;r0}\n\n\nstore v4, [sp+48]\nstore r0, [sp+48]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#complexity",
    "href": "lectures/ra-checking.html#complexity",
    "title": "Testing Register allocators",
    "section": "complexity",
    "text": "complexity\nif you do register allocation for code that is not in SSA, this is NP-complete But if you do it on code that is in SSA, the time complexity is polynomial.\nThere are lots of approximate algorithms- all complicated, lots of machines have extra constraints for instance there is a GPU load instruction that read 128 bits from memory and puts the value into 4 consecutive registers\nhowever, there is no known fast algorithm for deciding which registers to spill\nI used this code\n    def select_spill_register(self, live: Set[str], interference_graph) -&gt; str:\n        # for each register in live set, that can be spilled, find the register with the most neighbors in the interference graph\n        max_neighbors = -1\n        spill_register = None\n        for reg in live:\n            if reg in self.spilled_registers:\n                continue\n            if can_spill(reg)\n                neighbors = len(interference_graph.graph[reg])\n                if neighbors &gt; max_neighbors:\n                    max_neighbors = neighbors\n                    spill_register = reg\n        self.spilled_registers.append(spill_register)\n        return spill_register\nnever spill the stack pointer",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#how-to-verify-correctness-of-an-allocator",
    "href": "lectures/ra-checking.html#how-to-verify-correctness-of-an-allocator",
    "title": "Testing Register allocators",
    "section": "How to Verify Correctness of an allocator?",
    "text": "How to Verify Correctness of an allocator?\nBefore and after the allocator, we have the same instructions (except for those added by the allocator)\nAssume we have a machine with an infinite register set and a second machine with a finite register set.\nCorrect means both programs executed on these two machines get the same answer for all possible inputs",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#how-do-we-test-this",
    "href": "lectures/ra-checking.html#how-do-we-test-this",
    "title": "Testing Register allocators",
    "section": "how do we test this?",
    "text": "how do we test this?\nHow do we test this equivalence?\npick a random program and a random input. interpret and see if the result is the same.\nCould try more random inputs, could generate more random programs (fuzzer tools)\nCould reasonably confident but not 100% and very expensive",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#use-value-numbering-check-one-program-all-possible-inputs",
    "href": "lectures/ra-checking.html#use-value-numbering-check-one-program-all-possible-inputs",
    "title": "Testing Register allocators",
    "section": "use value numbering check one program, all possible inputs",
    "text": "use value numbering check one program, all possible inputs\n// original               // allocated \nld v0, [A]                ld r0, [A]\nld v1, [B]                ld r1, [B]\nld v2, [C]                ld r2, [C]\nadd v3, v0, v1            add r0, r0, r1\nadd v4, v2, v3            add r0, r2, r0\nreturn v4                 return r0\n\nv0 -&gt; vn 1 = ld [A]         r0 -&gt; vn 1 = ld [A]\nv1 -&gt; vn 2 = ld [B]         r1 -&gt; vn 2 = ld [B]\nv2 -&gt; vn 3 = ld [C]         r2 -&gt; vn 3 = ld [C]\nv3 -&gt; vn 4 = add 1,2        r0 -&gt; vn 4 = add 1,2 \nv4 -&gt; vn 5 = add 3,4        r0 -&gt; vn 5 = add 3,4 \nreturn vn 5               return vn 5",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#weak-value-numbering",
    "href": "lectures/ra-checking.html#weak-value-numbering",
    "title": "Testing Register allocators",
    "section": "weak value numbering",
    "text": "weak value numbering\n// original               // allocated \nld v0, [A]                ld r0, [A]\n                          write r0 holds v0\nld v1, [B]                ld r1, [B]\n                          write r1 holds v1 \nld v2, [C]                ld r2, [C]\n                          write r2 holds v2\nadd v3, v0, v1            add r0, r0, r1\n                          read r0 - holds  v0 - match\n                          read r1 - holds  v1 - match\n                          write r0 holds v3\nadd v4, v2, v3            add r0, r2, r0\n                          read r2 holds v2 - match\n                          read r0 holds v3 - match\n                          write r0 holds v4 \nreturn v4                 return r0\n                          read r0 holds v4 match \n\n\n\n##  check more then one program  (all programs at once)\n\nThis requires a proof that the two programs get the same result - this is an active research question -  \n\nsome success but not easy \n\nnot used in production \n\n## best we can do is generate lots of programs check each one \n\n```{mermaid}\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph LR\nA[Virtual code]\nB[Register allocator]\nC[Machine code]\nD[Checker]\nE[Fuzzing engine]\nA--&gt; B\nB--&gt; C\nC --&gt; D\nD --&gt; E\nE --&gt; A\nA --&gt; D\nWe could use a fuzzer to generate random programs or we could use a test set",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#algorithm-linear-in-number-of-instructions",
    "href": "lectures/ra-checking.html#algorithm-linear-in-number-of-instructions",
    "title": "Testing Register allocators",
    "section": "algorithm (linear in number of instructions)",
    "text": "algorithm (linear in number of instructions)\nfor each instruction we need to form pairs - virtual and physical register that holds the same value\nfor instruction v and p, check that the arguments are equal, if not fail add the pair dest of v == dest of p\ndoes not matter what the original op code was, just need register names",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#state-for-each-physcial-register",
    "href": "lectures/ra-checking.html#state-for-each-physcial-register",
    "title": "Testing Register allocators",
    "section": "state (for each physcial register)",
    "text": "state (for each physcial register)\nfor each physical-register | vertual register or Unknown for each spill slot | verturaL register or unknown\nTreat the allocated program as containing:\nSpill , : copy data (symbol representing virtual register) from a register to a spill slot.\nupdate spill slot state to virtual register if we know the target\nReload , : copy data from a spill slot to a register.\ncopy state from spill slot to physical register\n\ncopy , : move data from one CPU register to another (N.B.: only regalloc-inserted moves are recognized as a Move, not moves in the original input program.)\n\ncopy state from physical register to destination\n\nOp read:, read_orig: write: write_orig:: some arbitrary operation that reads some registers and writes some other registers.\n\nfor each read:\ncheck if the state of the physcial register is the corresponding virtual regster- if not we found a bug\nfor each dest, update the state of the physical dest to the virtural register",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#an-example-in-bril",
    "href": "lectures/ra-checking.html#an-example-in-bril",
    "title": "Testing Register allocators",
    "section": "an example in Bril",
    "text": "an example in Bril\n@main {\n  a: int = const 1;\n  b: int = const 2;\n  c: int = const 3;\n  d: int = const 4;\n  e: int = const 5;\n  f: int = const 6;\n  g: int = const 7;\n  h: int = add a b;\n  i: int = mul c d;\n  j: int = sub e f;\n  k: int = div g a;\n  l: int = add h i;\n  m: int = mul j k;\n  print l m;\n}\nAssume we have 5 registers (will need to spill)\nIn Bril we allocate a stack by:\n  pr1: int = const 7;              ## register that sets the size of the stack\n  pr3: ptr&lt;int&gt; = alloc pr1;       ## allocate a block of memory for the stack\nStore into the stack, at fixed offset 2, assume stack ptr is in pr3\n  pr2: int = const 2;\n  pr2 = ptradd pr3 pr2;\n  store pr2 pr1;\nload from the stack,\n  pr2: int = const 1;\n  pr2 = ptradd pr3 pr2;\n  pr2 = load pr2;",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#register-allocated-code",
    "href": "lectures/ra-checking.html#register-allocated-code",
    "title": "Testing Register allocators",
    "section": "register allocated code",
    "text": "register allocated code\n@main {\n               pr1: int = const 7;\n               pr3: ptr&lt;int&gt; = alloc pr1;  allocate stack with 7 ints \n  pr1: int = const 1;\n               pr2: int = const 2; \n               pr2 = ptradd pr3 pr2;   stack[2] = pr1\n              store pr2 pr1;\n  pr2: int = const 2;\n  pr4: int = const 3;\n  pr1: int = const 4;\n\n                pr5: int = const 6;\n                pr5 = ptradd pr3 pr5;  stack[6] = pr1\n                store pr5 pr1;\n  pr1: int = const 5;\n                pr5: int = const 4;\n                pr5 = ptradd pr3 pr5; stack[3] = pr1\n                store pr5 pr1;\n  pr1: int = const 6;\n                 pr5: int = const 5;\n                 pr5 = ptradd pr3 pr5; stack[5] = pr1 \n                 store pr5 pr1;\n  pr1: int = const 7;\n                 pr5: int = const 3;\n                 pr5 = ptradd pr3 pr5;  stack[3] = pr1\n                 store pr5 pr1;\n  \n                 pr5: int = const 2;\n                 pr5 = ptradd pr3 pr5;  pr5 = stack[2]\n                 pr5 = load pr5;\n  pr2: int = add pr5 pr2;\n                 pr5: int = const 0;\n                 pr5 = ptradd pr3 pr5; stack[0] = pr2\n                 store pr5 pr2;\n  \n                 pr1: int = const 6;\n                 pr1 = ptradd pr3 pr1; pr1 = stack[6]\n                 pr1 = load pr1;\n  pr2: int = mul pr4 pr1;\n                 pr4: int = const 1;\n                 pr4 = ptradd pr3 pr4; pr2 = stack[1] \n                 store pr4 pr2;\n  \n                 pr2: int = const 4;\n                 pr2 = ptradd pr3 pr2; pr2 = stack[4]\n                 pr2 = load pr2;\n  \n                 pr1: int = const 5;\n                 pr1 = ptradd pr3 pr1; pr1 = stack[5]\n                 pr1 = load pr1;\n\n  pr4: int = sub pr2 pr1;\n                 pr2: int = const 2;\n                 pr2 = ptradd pr3 pr2; pr3 = stack[2] \n                 pr2 = load pr2;\n  \n                 pr1: int = const 3;\n                 pr1 = ptradd pr3 pr1; pr1 = stack[3]\n                 pr1 = load pr1;\n  pr1: int = div pr1 pr2;\n                 pr5: int = const 0;\n                 pr5 = ptradd pr3 pr5; pr5 = stack[-]\n                 pr5 = load pr5;\n  \n                 pr2: int = const 1;\n                 pr2 = ptradd pr3 pr2; pr2 = stack[1]\n                 pr2 = load pr2;\n  pr2: int = add pr5 pr2;\n  pr1: int = mul pr4 pr1;\n  print pr2 pr1;\n  free pr3;      \n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#check",
    "href": "lectures/ra-checking.html#check",
    "title": "Testing Register allocators",
    "section": "check",
    "text": "check\npr3 is the stack pointer\ncompiler has to mark all the spills and reloads\n\n\n\n\n\n\n\n\n\nvirtual\nphysical\nregs\nstack\n\n\n\n\na=const\npr1 = const\npr1=a\n\n\n\n\nstack[2]=pr1\npr1=a\nstack[2] = a\n\n\nb=const\npr2=const\npr1=a, pr2=b\nstack[2] = a\n\n\nc=const\npr1=const\npr1=c, pr2=b\nstack[2] = a\n\n\n\nstack[6]=pr1\npr1=c, pr2=b\nstack[2] = a, stack[6]=c\n\n\nd=const\npr4=const\npr1=c, pr2=b, pr4=d\nstack[2] = a, stack[6]=c\n\n\ne=const\npr1= const\npr1=e, pr2=b, pr4=d\nstack[2] = a, stack[6]=c\n\n\n\nstack[5]=pr1\npr1=e, pr2=b, pr4=d\nstack[2] = a, stack[5] = e, stack[6]=c\n\n\nf=const\npr1=const\npr1=f, pr2=b, pr4=d\nstack[2] = a, stack[5] = e, stack[6]=c\n\n\n\nstack[4]=pr1\npr1=f, pr2=b, pr4=d\nstack[2] = a, stack[4] = f, stack[5] = e, stack[6]=c\n\n\ng=const\npr1=const\npr1=g, pr2=b, pr4=d\nstack[2] = a, stack[4] = f, stack[5] = e, stack[6]=c\n\n\n\nstack[3] = pr1\npr1=g, pr2=b, pr4=d\nstack[2] = a, stack[3]=g, stack[4] = f, stack[5] = e, stack[6]=c\n\n\n\npr5= stack[2]\npr1=g, pr2=b, pr4=d, pr5=a\nstack[2] = a, stack[3]=g, stack[4] = f, stack[5] = e, stack[6]=c\n\n\nh=a+b\npr2=pr5+pr2\npr1=g, pr2=h, pr4=d, pr5=a\nstack[2] = a, stack[3]=g, stack[4] = f, stack[5] = e, stack[6]=c\n\n\n\nstack[0]=pr2\npr1=g, pr2=h, pr4=d, pr5=a\nstack[0]=h, stack[2] = a, stack[3]=g, stack[4] = f, stack[5] = e, stack[6]=c\n\n\n\npr1=stack[6]\npr1=c, pr2=h, pr4=d, pr5=a\nstack[0]=h, stack[2] = a, stack[3]=g, stack[4] = f, stack[5] = e, stack[6]=c\n\n\ni=c*d\npr2=pr1*pr4\npr1=c, pr2=i, pr4=d, pr5=a\nstack[0]=h, stack[2] = a, stack[3]=g, stack[4] = f, stack[5] = e, stack[6]=c\n\n\n\nstack[1]=pr2\npr1=c, pr2=i, pr4=d, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\n\npr2=stack[4]\npr1=c, pr2=f, pr4=d, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\n\npr1=stack[5]\npr1=e, pr2=f, pr4=d, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\nj=e-f\npr4=pr1-pr2\npr1=e, pr2=f, pr4=j, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\n\npr2=stack[2]\npr1=e, pr2=a, pr4=j, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\n\npr1=stack[3]\npr1=g, pr2=a, pr4=j, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\nk=g/a\npr1= pr1/pr2\npr1=k, pr2=a, pr4=j, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\n\npr5=stack[0]\npr1=k, pr2=a, pr4=j, pr5=h\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\n\npr2= stack[1]\npr1=k, pr2=i, pr4=j, pr5=h\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\nl=h+i\npr2=pr5+pr2\npr1=k, pr2=l, pr4=j, pr5=h\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\nm= j*k\npr1=pr4*pr1\npr1=m, pr2=l, pr4=j, pr5=h\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\nprint l,m\nprint pr2,pr1",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/ra-checking.html#control-flow",
    "href": "lectures/ra-checking.html#control-flow",
    "title": "Testing Register allocators",
    "section": "control flow",
    "text": "control flow",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Testing Register allocators"
    ]
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#this-is-based-on-the-cranelift-compiler-used-for-web-assembly-and-rust",
    "href": "lectures/revealjs_ra-checking.qmd.html#this-is-based-on-the-cranelift-compiler-used-for-web-assembly-and-rust",
    "title": "Testing Register allocators",
    "section": "This is based on the cranelift compiler used for web-assembly and rust",
    "text": "This is based on the cranelift compiler used for web-assembly and rust\ncranelift register allocator"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#what-is-register-allocation",
    "href": "lectures/revealjs_ra-checking.qmd.html#what-is-register-allocation",
    "title": "Testing Register allocators",
    "section": "what is register allocation",
    "text": "what is register allocation\nIn Bril and LLVM a program can use an arbitrary number of registers,\nvoid f() {\n    int x0 = compute(0);\n    int x1 = compute(1);\n    // ...\n    int x99 = compute(99);\n    \n    // --- 100 possibly different values were computed \n    // --- where are those values stored?\n    \n    consume(x0);\n    consume(x1);\n    // ...\n    consume(x99);\n}"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#storing-variables",
    "href": "lectures/revealjs_ra-checking.qmd.html#storing-variables",
    "title": "Testing Register allocators",
    "section": "storing variables",
    "text": "storing variables\none option\nAllocate a memory location for each local variable. All of the \\(x_N\\) variables live in memory.\nWhen the function is called, it allocates a new area on the stack called the stack frame and uses it to store local variables.\nThis means that adding two variables, takes two loads, one add, and one store so it is very slow .\nCompiling code in this way is very fast because we need to make almost no decisions: a variable reference always becomes a memory load,\nfaster would be to keep variables in registers but, we have a limited set of registers"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#definitions",
    "href": "lectures/revealjs_ra-checking.qmd.html#definitions",
    "title": "Testing Register allocators",
    "section": "Definitions",
    "text": "Definitions\nRegister allocation: is assigning a value in the program to a register for storage. The register allocator decides how to shuffle values between memory and registers, and between register.\nIn Bril and LLVM we have virtual registers - as many as you want. The register allocator has to rewrite the instructions to use physical registers. Since the number of physical registers is limited, The allocator might insert additional instructions:"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#spillsreloads",
    "href": "lectures/revealjs_ra-checking.qmd.html#spillsreloads",
    "title": "Testing Register allocators",
    "section": "spills/reloads",
    "text": "spills/reloads\n\nstores (called spills) to move a register to memory\nloads (called reloads) to move memory to a register\nmoves to copy from one register to another\n\nThe locations in memory are usually on the stack and are called spill-slots"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#example-of-register-allocation-on-a-machine-with-two-physical-registers",
    "href": "lectures/revealjs_ra-checking.qmd.html#example-of-register-allocation-on-a-machine-with-two-physical-registers",
    "title": "Testing Register allocators",
    "section": "example of register allocation on a machine with two physical registers",
    "text": "example of register allocation on a machine with two physical registers\n\n\n\n\n\n\n\n\nvirtual register code\nphysical register code\nassigments\n\n\n\n\n\n\n{v0 -&gt; r0, v1 -&gt; r1}\n\n\n\nstore r1, [sp+0]\n\n\n\nadd v2, v0, v1\nadd r1, r0, r1\n{v0 -&gt; r0, v1-&gt;[sp+0], v2-&gt;r1}\n\n\nsub v3, v2, v0\nsub r1, r1, r0\n\n\n\n\nload r0, [sp+0]\n\n\n\nmul v4, v3, v1\nmul r0, r1, r0\n{v4-&gt;r0}\n\n\nstore v4, [sp+48]\nstore r0, [sp+48]"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#complexity",
    "href": "lectures/revealjs_ra-checking.qmd.html#complexity",
    "title": "Testing Register allocators",
    "section": "complexity",
    "text": "complexity\nif you do register allocation for code that is not in SSA, this is NP-complete But if you do it on code that is in SSA, the time complexity is polynomial.\nThere are lots of approximate algorithms- all complicated, lots of machines have extra constraints for instance there is a GPU load instruction that read 128 bits from memory and puts the value into 4 consecutive registers\nhowever, there is no known fast algorithm for deciding which registers to spill\nI used this code\n    def select_spill_register(self, live: Set[str], interference_graph) -&gt; str:\n        # for each register in live set, that can be spilled, find the register with the most neighbors in the interference graph\n        max_neighbors = -1\n        spill_register = None\n        for reg in live:\n            if reg in self.spilled_registers:\n                continue\n            if can_spill(reg)\n                neighbors = len(interference_graph.graph[reg])\n                if neighbors &gt; max_neighbors:\n                    max_neighbors = neighbors\n                    spill_register = reg\n        self.spilled_registers.append(spill_register)\n        return spill_register\nnever spill the stack pointer"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#how-to-verify-correctness-of-an-allocator",
    "href": "lectures/revealjs_ra-checking.qmd.html#how-to-verify-correctness-of-an-allocator",
    "title": "Testing Register allocators",
    "section": "How to Verify Correctness of an allocator?",
    "text": "How to Verify Correctness of an allocator?\nBefore and after the allocator, we have the same instructions (except for those added by the allocator)\nAssume we have a machine with an infinite register set and a second machine with a finite register set.\nCorrect means both programs executed on these two machines get the same answer for all possible inputs"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#how-do-we-test-this",
    "href": "lectures/revealjs_ra-checking.qmd.html#how-do-we-test-this",
    "title": "Testing Register allocators",
    "section": "how do we test this?",
    "text": "how do we test this?\nHow do we test this equivalence?\npick a random program and a random input. interpret and see if the result is the same.\nCould try more random inputs, could generate more random programs (fuzzer tools)\nCould reasonably confident but not 100% and very expensive"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#use-value-numbering-check-one-program-all-possible-inputs",
    "href": "lectures/revealjs_ra-checking.qmd.html#use-value-numbering-check-one-program-all-possible-inputs",
    "title": "Testing Register allocators",
    "section": "use value numbering check one program, all possible inputs",
    "text": "use value numbering check one program, all possible inputs\n// original               // allocated \nld v0, [A]                ld r0, [A]\nld v1, [B]                ld r1, [B]\nld v2, [C]                ld r2, [C]\nadd v3, v0, v1            add r0, r0, r1\nadd v4, v2, v3            add r0, r2, r0\nreturn v4                 return r0\n\nv0 -&gt; vn 1 = ld [A]         r0 -&gt; vn 1 = ld [A]\nv1 -&gt; vn 2 = ld [B]         r1 -&gt; vn 2 = ld [B]\nv2 -&gt; vn 3 = ld [C]         r2 -&gt; vn 3 = ld [C]\nv3 -&gt; vn 4 = add 1,2        r0 -&gt; vn 4 = add 1,2 \nv4 -&gt; vn 5 = add 3,4        r0 -&gt; vn 5 = add 3,4 \nreturn vn 5               return vn 5"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#weak-value-numbering",
    "href": "lectures/revealjs_ra-checking.qmd.html#weak-value-numbering",
    "title": "Testing Register allocators",
    "section": "weak value numbering",
    "text": "weak value numbering\n// original               // allocated \nld v0, [A]                ld r0, [A]\n                          write r0 holds v0\nld v1, [B]                ld r1, [B]\n                          write r1 holds v1 \nld v2, [C]                ld r2, [C]\n                          write r2 holds v2\nadd v3, v0, v1            add r0, r0, r1\n                          read r0 - holds  v0 - match\n                          read r1 - holds  v1 - match\n                          write r0 holds v3\nadd v4, v2, v3            add r0, r2, r0\n                          read r2 holds v2 - match\n                          read r0 holds v3 - match\n                          write r0 holds v4 \nreturn v4                 return r0\n                          read r0 holds v4 match \n\n\n\n##  check more then one program  (all programs at once)\n\nThis requires a proof that the two programs get the same result - this is an active research question -  \n\nsome success but not easy \n\nnot used in production \n\n## best we can do is generate lots of programs check each one \n\n```{mermaid}\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph LR\nA[Virtual code]\nB[Register allocator]\nC[Machine code]\nD[Checker]\nE[Fuzzing engine]\nA--&gt; B\nB--&gt; C\nC --&gt; D\nD --&gt; E\nE --&gt; A\nA --&gt; D\nWe could use a fuzzer to generate random programs or we could use a test set"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#algorithm-linear-in-number-of-instructions",
    "href": "lectures/revealjs_ra-checking.qmd.html#algorithm-linear-in-number-of-instructions",
    "title": "Testing Register allocators",
    "section": "algorithm (linear in number of instructions)",
    "text": "algorithm (linear in number of instructions)\nfor each instruction we need to form pairs - virtual and physical register that holds the same value\nfor instruction v and p, check that the arguments are equal, if not fail add the pair dest of v == dest of p\ndoes not matter what the original op code was, just need register names"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#state-for-each-physcial-register",
    "href": "lectures/revealjs_ra-checking.qmd.html#state-for-each-physcial-register",
    "title": "Testing Register allocators",
    "section": "state (for each physcial register)",
    "text": "state (for each physcial register)\nfor each physical-register | vertual register or Unknown for each spill slot | verturaL register or unknown\nTreat the allocated program as containing:\nSpill , : copy data (symbol representing virtual register) from a register to a spill slot.\nupdate spill slot state to virtual register if we know the target\nReload , : copy data from a spill slot to a register.\ncopy state from spill slot to physical register\n\ncopy , : move data from one CPU register to another (N.B.: only regalloc-inserted moves are recognized as a Move, not moves in the original input program.)\n\ncopy state from physical register to destination\n\nOp read:, read_orig: write: write_orig:: some arbitrary operation that reads some registers and writes some other registers.\n\nfor each read:\ncheck if the state of the physcial register is the corresponding virtual regster- if not we found a bug\nfor each dest, update the state of the physical dest to the virtural register"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#an-example-in-bril",
    "href": "lectures/revealjs_ra-checking.qmd.html#an-example-in-bril",
    "title": "Testing Register allocators",
    "section": "an example in Bril",
    "text": "an example in Bril\n@main {\n  a: int = const 1;\n  b: int = const 2;\n  c: int = const 3;\n  d: int = const 4;\n  e: int = const 5;\n  f: int = const 6;\n  g: int = const 7;\n  h: int = add a b;\n  i: int = mul c d;\n  j: int = sub e f;\n  k: int = div g a;\n  l: int = add h i;\n  m: int = mul j k;\n  print l m;\n}\nAssume we have 5 registers (will need to spill)\nIn Bril we allocate a stack by:\n  pr1: int = const 7;              ## register that sets the size of the stack\n  pr3: ptr&lt;int&gt; = alloc pr1;       ## allocate a block of memory for the stack\nStore into the stack, at fixed offset 2, assume stack ptr is in pr3\n  pr2: int = const 2;\n  pr2 = ptradd pr3 pr2;\n  store pr2 pr1;\nload from the stack,\n  pr2: int = const 1;\n  pr2 = ptradd pr3 pr2;\n  pr2 = load pr2;"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#register-allocated-code",
    "href": "lectures/revealjs_ra-checking.qmd.html#register-allocated-code",
    "title": "Testing Register allocators",
    "section": "register allocated code",
    "text": "register allocated code\n@main {\n               pr1: int = const 7;\n               pr3: ptr&lt;int&gt; = alloc pr1;  allocate stack with 7 ints \n  pr1: int = const 1;\n               pr2: int = const 2; \n               pr2 = ptradd pr3 pr2;   stack[2] = pr1\n              store pr2 pr1;\n  pr2: int = const 2;\n  pr4: int = const 3;\n  pr1: int = const 4;\n\n                pr5: int = const 6;\n                pr5 = ptradd pr3 pr5;  stack[6] = pr1\n                store pr5 pr1;\n  pr1: int = const 5;\n                pr5: int = const 4;\n                pr5 = ptradd pr3 pr5; stack[3] = pr1\n                store pr5 pr1;\n  pr1: int = const 6;\n                 pr5: int = const 5;\n                 pr5 = ptradd pr3 pr5; stack[5] = pr1 \n                 store pr5 pr1;\n  pr1: int = const 7;\n                 pr5: int = const 3;\n                 pr5 = ptradd pr3 pr5;  stack[3] = pr1\n                 store pr5 pr1;\n  \n                 pr5: int = const 2;\n                 pr5 = ptradd pr3 pr5;  pr5 = stack[2]\n                 pr5 = load pr5;\n  pr2: int = add pr5 pr2;\n                 pr5: int = const 0;\n                 pr5 = ptradd pr3 pr5; stack[0] = pr2\n                 store pr5 pr2;\n  \n                 pr1: int = const 6;\n                 pr1 = ptradd pr3 pr1; pr1 = stack[6]\n                 pr1 = load pr1;\n  pr2: int = mul pr4 pr1;\n                 pr4: int = const 1;\n                 pr4 = ptradd pr3 pr4; pr2 = stack[1] \n                 store pr4 pr2;\n  \n                 pr2: int = const 4;\n                 pr2 = ptradd pr3 pr2; pr2 = stack[4]\n                 pr2 = load pr2;\n  \n                 pr1: int = const 5;\n                 pr1 = ptradd pr3 pr1; pr1 = stack[5]\n                 pr1 = load pr1;\n\n  pr4: int = sub pr2 pr1;\n                 pr2: int = const 2;\n                 pr2 = ptradd pr3 pr2; pr3 = stack[2] \n                 pr2 = load pr2;\n  \n                 pr1: int = const 3;\n                 pr1 = ptradd pr3 pr1; pr1 = stack[3]\n                 pr1 = load pr1;\n  pr1: int = div pr1 pr2;\n                 pr5: int = const 0;\n                 pr5 = ptradd pr3 pr5; pr5 = stack[-]\n                 pr5 = load pr5;\n  \n                 pr2: int = const 1;\n                 pr2 = ptradd pr3 pr2; pr2 = stack[1]\n                 pr2 = load pr2;\n  pr2: int = add pr5 pr2;\n  pr1: int = mul pr4 pr1;\n  print pr2 pr1;\n  free pr3;      \n}"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#check",
    "href": "lectures/revealjs_ra-checking.qmd.html#check",
    "title": "Testing Register allocators",
    "section": "check",
    "text": "check\npr3 is the stack pointer\ncompiler has to mark all the spills and reloads\n\n\n\n\n\n\n\n\n\nvirtual\nphysical\nregs\nstack\n\n\n\n\na=const\npr1 = const\npr1=a\n\n\n\n\nstack[2]=pr1\npr1=a\nstack[2] = a\n\n\nb=const\npr2=const\npr1=a, pr2=b\nstack[2] = a\n\n\nc=const\npr1=const\npr1=c, pr2=b\nstack[2] = a\n\n\n\nstack[6]=pr1\npr1=c, pr2=b\nstack[2] = a, stack[6]=c\n\n\nd=const\npr4=const\npr1=c, pr2=b, pr4=d\nstack[2] = a, stack[6]=c\n\n\ne=const\npr1= const\npr1=e, pr2=b, pr4=d\nstack[2] = a, stack[6]=c\n\n\n\nstack[5]=pr1\npr1=e, pr2=b, pr4=d\nstack[2] = a, stack[5] = e, stack[6]=c\n\n\nf=const\npr1=const\npr1=f, pr2=b, pr4=d\nstack[2] = a, stack[5] = e, stack[6]=c\n\n\n\nstack[4]=pr1\npr1=f, pr2=b, pr4=d\nstack[2] = a, stack[4] = f, stack[5] = e, stack[6]=c\n\n\ng=const\npr1=const\npr1=g, pr2=b, pr4=d\nstack[2] = a, stack[4] = f, stack[5] = e, stack[6]=c\n\n\n\nstack[3] = pr1\npr1=g, pr2=b, pr4=d\nstack[2] = a, stack[3]=g, stack[4] = f, stack[5] = e, stack[6]=c\n\n\n\npr5= stack[2]\npr1=g, pr2=b, pr4=d, pr5=a\nstack[2] = a, stack[3]=g, stack[4] = f, stack[5] = e, stack[6]=c\n\n\nh=a+b\npr2=pr5+pr2\npr1=g, pr2=h, pr4=d, pr5=a\nstack[2] = a, stack[3]=g, stack[4] = f, stack[5] = e, stack[6]=c\n\n\n\nstack[0]=pr2\npr1=g, pr2=h, pr4=d, pr5=a\nstack[0]=h, stack[2] = a, stack[3]=g, stack[4] = f, stack[5] = e, stack[6]=c\n\n\n\npr1=stack[6]\npr1=c, pr2=h, pr4=d, pr5=a\nstack[0]=h, stack[2] = a, stack[3]=g, stack[4] = f, stack[5] = e, stack[6]=c\n\n\ni=c*d\npr2=pr1*pr4\npr1=c, pr2=i, pr4=d, pr5=a\nstack[0]=h, stack[2] = a, stack[3]=g, stack[4] = f, stack[5] = e, stack[6]=c\n\n\n\nstack[1]=pr2\npr1=c, pr2=i, pr4=d, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\n\npr2=stack[4]\npr1=c, pr2=f, pr4=d, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\n\npr1=stack[5]\npr1=e, pr2=f, pr4=d, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\nj=e-f\npr4=pr1-pr2\npr1=e, pr2=f, pr4=j, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\n\npr2=stack[2]\npr1=e, pr2=a, pr4=j, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\n\npr1=stack[3]\npr1=g, pr2=a, pr4=j, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\nk=g/a\npr1= pr1/pr2\npr1=k, pr2=a, pr4=j, pr5=a\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\n\npr5=stack[0]\npr1=k, pr2=a, pr4=j, pr5=h\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\n\npr2= stack[1]\npr1=k, pr2=i, pr4=j, pr5=h\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\nl=h+i\npr2=pr5+pr2\npr1=k, pr2=l, pr4=j, pr5=h\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\nm= j*k\npr1=pr4*pr1\npr1=m, pr2=l, pr4=j, pr5=h\nstack[0]=h, stack[1]=i, stack[2]=a, stack[3]=g, stack[4]=f, stack[5]=e, stack[6]=c\n\n\nprint l,m\nprint pr2,pr1"
  },
  {
    "objectID": "lectures/revealjs_ra-checking.qmd.html#control-flow",
    "href": "lectures/revealjs_ra-checking.qmd.html#control-flow",
    "title": "Testing Register allocators",
    "section": "control flow",
    "text": "control flow"
  },
  {
    "objectID": "lectures/01a1_performance_measurement.html",
    "href": "lectures/01a1_performance_measurement.html",
    "title": "Performance and Measurement part 1",
    "section": "",
    "text": "Producing Wrong Data Without Doing Anything Obviously Wrong! Todd Mytkowicz, Amer Diwan, Matthias Hauswirth, and Peter F. Sweeney. ASPLOS 2009.\n445 references\n\nA sample blog post about this paper blog\n\n\n\ndata\nA dataset with 2,185 CPUs and 2,668 GPUs to help researchers understand the development trend of CPUs and GPUs. Setup by Kaggle\n\n# library & dataset\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv('images/chip_dataset.csv')\nprint(df.head())\n\nsns.set_palette(\"pastel\")\n\nsns.violinplot(x=df[\"Vendor\"], y=np.log(df[\"Freq (MHz)\"]), hue=df['Type'])\n\n   Unnamed: 0                  Product Type Release Date  Process Size (nm)  \\\n0           0      AMD Athlon 64 3500+  CPU   2007-02-20               65.0   \n1           1         AMD Athlon 200GE  CPU   2018-09-06               14.0   \n2           2     Intel Core i5-1145G7  CPU   2020-09-02               10.0   \n3           3    Intel Xeon E5-2603 v2  CPU   2013-09-01               22.0   \n4           4  AMD Phenom II X4 980 BE  CPU   2011-05-03               45.0   \n\n   TDP (W)  Die Size (mm^2)  Transistors (million)  Freq (MHz)  Foundry  \\\n0     45.0             77.0                  122.0      2200.0  Unknown   \n1     35.0            192.0                 4800.0      3200.0  Unknown   \n2     28.0              NaN                    NaN      2600.0    Intel   \n3     80.0            160.0                 1400.0      1800.0    Intel   \n4    125.0            258.0                  758.0      3700.0  Unknown   \n\n  Vendor  FP16 GFLOPS  FP32 GFLOPS  FP64 GFLOPS  \n0    AMD          NaN          NaN          NaN  \n1    AMD          NaN          NaN          NaN  \n2  Intel          NaN          NaN          NaN  \n3  Intel          NaN          NaN          NaN  \n4    AMD          NaN          NaN          NaN  \n\n\n\n\n\n\n\n\n\napply np.log to the Freq (MHz) column. Because of wide range of values\nSet a pastel palette with sns.set_palette(“pastel”). These colors make it easier to see the parts of the violin plot\nHue Parameter: I’m using the hue parameter to differentiate between types of chips. Hue sets a color within the palette\nData Source and Preparation: Include a brief note on where the data comes from (you’ve provided a link, but a sentence or two summarizing the dataset would be helpful) and any preprocessing steps taken before visualization.\nI might want to take date into account in these plots\n\nA violin plot shows density curves. The width is the approximate frequency of data points at that value\nBest for comparing distributions\nconsider ordering the groups\nThe details\n\nthe white dot represents the median\nthe thick gray bar in the center represents the inter-quartile range\nthe thin gray line represents the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the inter-quartile range.\nOn each side of the gray line is a kernel density estimation to show the distribution shape of the data. Wider sections of the violin plot represent a higher probability that members of the population will take on the given value; the skinnier sections represent a lower probability.\n\n\n\n\n\nprint(df.describe())\n\nimport pandas as pd\n\n# Assuming df is your DataFrame and 'Release Date' is the column\ndf['Release Date'] = pd.to_datetime(df['Release Date'], errors='coerce')\ndf['Release Year'] = ((df['Release Date'].dt.year) // 5) * 5\n\n# Now df['Release Year'] contains the year extracted from 'Release Date'\n\n\n# plot a bar chart\nax = sns.barplot(x=df['Release Year'], y=df[\"TDP (W)\"], hue =df['Type'], estimator=np.mean, errorbar=(\"sd\"))\n\n        Unnamed: 0  Process Size (nm)      TDP (W)  Die Size (mm^2)  \\\ncount  4854.000000        4845.000000  4228.000000      4139.000000   \nmean   2426.500000          55.109598    81.359981       188.440445   \nstd    1401.373433          44.998676    76.807808       126.189383   \nmin       0.000000           0.000000     1.000000         1.000000   \n25%    1213.250000          22.000000    33.000000       104.000000   \n50%    2426.500000          40.000000    65.000000       148.000000   \n75%    3639.750000          90.000000   100.000000       239.000000   \nmax    4853.000000         250.000000   900.000000       826.000000   \n\n       Transistors (million)   Freq (MHz)    FP16 GFLOPS   FP32 GFLOPS  \\\ncount            4143.000000  4854.000000     536.000000   1948.000000   \nmean             1929.922279  1484.406057    8397.459851   2134.756653   \nstd              4044.891098  1066.701523   13799.551131   3898.431487   \nmin                 8.000000   100.000000      10.020000     12.800000   \n25%               154.000000   590.000000     768.800000    257.300000   \n50%               624.000000  1073.500000    2965.500000    696.000000   \n75%              1550.000000  2400.000000   10600.000000   2116.750000   \nmax             54200.000000  4700.000000  184600.000000  40000.000000   \n\n        FP64 GFLOPS  \ncount   1306.000000  \nmean     363.670511  \nstd     1145.931856  \nmin        3.600000  \n25%       38.295000  \n50%       89.280000  \n75%      220.000000  \nmax    11540.000000",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Performance and Measurement part 1"
    ]
  },
  {
    "objectID": "lectures/01a1_performance_measurement.html#example-1",
    "href": "lectures/01a1_performance_measurement.html#example-1",
    "title": "Performance and Measurement part 1",
    "section": "",
    "text": "Producing Wrong Data Without Doing Anything Obviously Wrong! Todd Mytkowicz, Amer Diwan, Matthias Hauswirth, and Peter F. Sweeney. ASPLOS 2009.\n445 references\n\nA sample blog post about this paper blog",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Performance and Measurement part 1"
    ]
  },
  {
    "objectID": "lectures/01a1_performance_measurement.html#violin-plots",
    "href": "lectures/01a1_performance_measurement.html#violin-plots",
    "title": "Performance and Measurement part 1",
    "section": "",
    "text": "data\nA dataset with 2,185 CPUs and 2,668 GPUs to help researchers understand the development trend of CPUs and GPUs. Setup by Kaggle\n\n# library & dataset\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv('images/chip_dataset.csv')\nprint(df.head())\n\nsns.set_palette(\"pastel\")\n\nsns.violinplot(x=df[\"Vendor\"], y=np.log(df[\"Freq (MHz)\"]), hue=df['Type'])\n\n   Unnamed: 0                  Product Type Release Date  Process Size (nm)  \\\n0           0      AMD Athlon 64 3500+  CPU   2007-02-20               65.0   \n1           1         AMD Athlon 200GE  CPU   2018-09-06               14.0   \n2           2     Intel Core i5-1145G7  CPU   2020-09-02               10.0   \n3           3    Intel Xeon E5-2603 v2  CPU   2013-09-01               22.0   \n4           4  AMD Phenom II X4 980 BE  CPU   2011-05-03               45.0   \n\n   TDP (W)  Die Size (mm^2)  Transistors (million)  Freq (MHz)  Foundry  \\\n0     45.0             77.0                  122.0      2200.0  Unknown   \n1     35.0            192.0                 4800.0      3200.0  Unknown   \n2     28.0              NaN                    NaN      2600.0    Intel   \n3     80.0            160.0                 1400.0      1800.0    Intel   \n4    125.0            258.0                  758.0      3700.0  Unknown   \n\n  Vendor  FP16 GFLOPS  FP32 GFLOPS  FP64 GFLOPS  \n0    AMD          NaN          NaN          NaN  \n1    AMD          NaN          NaN          NaN  \n2  Intel          NaN          NaN          NaN  \n3  Intel          NaN          NaN          NaN  \n4    AMD          NaN          NaN          NaN  \n\n\n\n\n\n\n\n\n\napply np.log to the Freq (MHz) column. Because of wide range of values\nSet a pastel palette with sns.set_palette(“pastel”). These colors make it easier to see the parts of the violin plot\nHue Parameter: I’m using the hue parameter to differentiate between types of chips. Hue sets a color within the palette\nData Source and Preparation: Include a brief note on where the data comes from (you’ve provided a link, but a sentence or two summarizing the dataset would be helpful) and any preprocessing steps taken before visualization.\nI might want to take date into account in these plots\n\nA violin plot shows density curves. The width is the approximate frequency of data points at that value\nBest for comparing distributions\nconsider ordering the groups\nThe details\n\nthe white dot represents the median\nthe thick gray bar in the center represents the inter-quartile range\nthe thin gray line represents the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the inter-quartile range.\nOn each side of the gray line is a kernel density estimation to show the distribution shape of the data. Wider sections of the violin plot represent a higher probability that members of the population will take on the given value; the skinnier sections represent a lower probability.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Performance and Measurement part 1"
    ]
  },
  {
    "objectID": "lectures/01a1_performance_measurement.html#error-bars",
    "href": "lectures/01a1_performance_measurement.html#error-bars",
    "title": "Performance and Measurement part 1",
    "section": "",
    "text": "print(df.describe())\n\nimport pandas as pd\n\n# Assuming df is your DataFrame and 'Release Date' is the column\ndf['Release Date'] = pd.to_datetime(df['Release Date'], errors='coerce')\ndf['Release Year'] = ((df['Release Date'].dt.year) // 5) * 5\n\n# Now df['Release Year'] contains the year extracted from 'Release Date'\n\n\n# plot a bar chart\nax = sns.barplot(x=df['Release Year'], y=df[\"TDP (W)\"], hue =df['Type'], estimator=np.mean, errorbar=(\"sd\"))\n\n        Unnamed: 0  Process Size (nm)      TDP (W)  Die Size (mm^2)  \\\ncount  4854.000000        4845.000000  4228.000000      4139.000000   \nmean   2426.500000          55.109598    81.359981       188.440445   \nstd    1401.373433          44.998676    76.807808       126.189383   \nmin       0.000000           0.000000     1.000000         1.000000   \n25%    1213.250000          22.000000    33.000000       104.000000   \n50%    2426.500000          40.000000    65.000000       148.000000   \n75%    3639.750000          90.000000   100.000000       239.000000   \nmax    4853.000000         250.000000   900.000000       826.000000   \n\n       Transistors (million)   Freq (MHz)    FP16 GFLOPS   FP32 GFLOPS  \\\ncount            4143.000000  4854.000000     536.000000   1948.000000   \nmean             1929.922279  1484.406057    8397.459851   2134.756653   \nstd              4044.891098  1066.701523   13799.551131   3898.431487   \nmin                 8.000000   100.000000      10.020000     12.800000   \n25%               154.000000   590.000000     768.800000    257.300000   \n50%               624.000000  1073.500000    2965.500000    696.000000   \n75%              1550.000000  2400.000000   10600.000000   2116.750000   \nmax             54200.000000  4700.000000  184600.000000  40000.000000   \n\n        FP64 GFLOPS  \ncount   1306.000000  \nmean     363.670511  \nstd     1145.931856  \nmin        3.600000  \n25%       38.295000  \n50%       89.280000  \n75%      220.000000  \nmax    11540.000000",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Performance and Measurement part 1"
    ]
  },
  {
    "objectID": "lectures/revealjs_01a1_performance_measurement.qmd.html#example-1",
    "href": "lectures/revealjs_01a1_performance_measurement.qmd.html#example-1",
    "title": "Performance and Measurement part 1",
    "section": "example 1",
    "text": "example 1\nProducing Wrong Data Without Doing Anything Obviously Wrong! Todd Mytkowicz, Amer Diwan, Matthias Hauswirth, and Peter F. Sweeney. ASPLOS 2009.\n445 references"
  },
  {
    "objectID": "lectures/revealjs_01a1_performance_measurement.qmd.html#violin-plots",
    "href": "lectures/revealjs_01a1_performance_measurement.qmd.html#violin-plots",
    "title": "Performance and Measurement part 1",
    "section": "violin plots",
    "text": "violin plots\ndata\nA dataset with 2,185 CPUs and 2,668 GPUs to help researchers understand the development trend of CPUs and GPUs. Setup by Kaggle\n\n\nCode\n# library & dataset\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv('images/chip_dataset.csv')\nprint(df.head())\n\nsns.set_palette(\"pastel\")\n\nsns.violinplot(x=df[\"Vendor\"], y=np.log(df[\"Freq (MHz)\"]), hue=df['Type'])\n\n\n   Unnamed: 0                  Product Type Release Date  Process Size (nm)  \\\n0           0      AMD Athlon 64 3500+  CPU   2007-02-20               65.0   \n1           1         AMD Athlon 200GE  CPU   2018-09-06               14.0   \n2           2     Intel Core i5-1145G7  CPU   2020-09-02               10.0   \n3           3    Intel Xeon E5-2603 v2  CPU   2013-09-01               22.0   \n4           4  AMD Phenom II X4 980 BE  CPU   2011-05-03               45.0   \n\n   TDP (W)  Die Size (mm^2)  Transistors (million)  Freq (MHz)  Foundry  \\\n0     45.0             77.0                  122.0      2200.0  Unknown   \n1     35.0            192.0                 4800.0      3200.0  Unknown   \n2     28.0              NaN                    NaN      2600.0    Intel   \n3     80.0            160.0                 1400.0      1800.0    Intel   \n4    125.0            258.0                  758.0      3700.0  Unknown   \n\n  Vendor  FP16 GFLOPS  FP32 GFLOPS  FP64 GFLOPS  \n0    AMD          NaN          NaN          NaN  \n1    AMD          NaN          NaN          NaN  \n2  Intel          NaN          NaN          NaN  \n3  Intel          NaN          NaN          NaN  \n4    AMD          NaN          NaN          NaN  \n\n\n\n\n\n\n\n\n\napply np.log to the Freq (MHz) column. Because of wide range of values\nSet a pastel palette with sns.set_palette(“pastel”). These colors make it easier to see the parts of the violin plot\nHue Parameter: I’m using the hue parameter to differentiate between types of chips. Hue sets a color within the palette\nData Source and Preparation: Include a brief note on where the data comes from (you’ve provided a link, but a sentence or two summarizing the dataset would be helpful) and any preprocessing steps taken before visualization.\nI might want to take date into account in these plots"
  },
  {
    "objectID": "lectures/revealjs_01a1_performance_measurement.qmd.html#error-bars",
    "href": "lectures/revealjs_01a1_performance_measurement.qmd.html#error-bars",
    "title": "Performance and Measurement part 1",
    "section": "error bars",
    "text": "error bars\n\n\nCode\nprint(df.describe())\n\nimport pandas as pd\n\n# Assuming df is your DataFrame and 'Release Date' is the column\ndf['Release Date'] = pd.to_datetime(df['Release Date'], errors='coerce')\ndf['Release Year'] = ((df['Release Date'].dt.year) // 5) * 5\n\n# Now df['Release Year'] contains the year extracted from 'Release Date'\n\n\n# plot a bar chart\nax = sns.barplot(x=df['Release Year'], y=df[\"TDP (W)\"], hue =df['Type'], estimator=np.mean, errorbar=(\"sd\"))\n\n\n        Unnamed: 0  Process Size (nm)      TDP (W)  Die Size (mm^2)  \\\ncount  4854.000000        4845.000000  4228.000000      4139.000000   \nmean   2426.500000          55.109598    81.359981       188.440445   \nstd    1401.373433          44.998676    76.807808       126.189383   \nmin       0.000000           0.000000     1.000000         1.000000   \n25%    1213.250000          22.000000    33.000000       104.000000   \n50%    2426.500000          40.000000    65.000000       148.000000   \n75%    3639.750000          90.000000   100.000000       239.000000   \nmax    4853.000000         250.000000   900.000000       826.000000   \n\n       Transistors (million)   Freq (MHz)    FP16 GFLOPS   FP32 GFLOPS  \\\ncount            4143.000000  4854.000000     536.000000   1948.000000   \nmean             1929.922279  1484.406057    8397.459851   2134.756653   \nstd              4044.891098  1066.701523   13799.551131   3898.431487   \nmin                 8.000000   100.000000      10.020000     12.800000   \n25%               154.000000   590.000000     768.800000    257.300000   \n50%               624.000000  1073.500000    2965.500000    696.000000   \n75%              1550.000000  2400.000000   10600.000000   2116.750000   \nmax             54200.000000  4700.000000  184600.000000  40000.000000   \n\n        FP64 GFLOPS  \ncount   1306.000000  \nmean     363.670511  \nstd     1145.931856  \nmin        3.600000  \n25%       38.295000  \n50%       89.280000  \n75%      220.000000  \nmax    11540.000000"
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html",
    "href": "lectures/13_dynamic_compilers.html",
    "title": "Dynamic Compilers",
    "section": "",
    "text": "a jit compiler translates code into isa while the program executes\npro:\nmore information\ncon:\ncompile time slows down execution",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#jit-just-in-time-compilers-vs-aotahead-of-time-compilers",
    "href": "lectures/13_dynamic_compilers.html#jit-just-in-time-compilers-vs-aotahead-of-time-compilers",
    "title": "Dynamic Compilers",
    "section": "",
    "text": "a jit compiler translates code into isa while the program executes\npro:\nmore information\ncon:\ncompile time slows down execution",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#some-options",
    "href": "lectures/13_dynamic_compilers.html#some-options",
    "title": "Dynamic Compilers",
    "section": "some options",
    "text": "some options\n\ncompile a function the first time it is called\ncompile a function after it has been called a lot (needs an interpreter) We call these hot functions\nbuild a trace of instructions executed and compile the hot traces (a trace has no branches)\nA variation I used: ran the program to completion using a tracing interpreter, recompile off line, future execution is a mix of interpreter and compiled code",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#can-jit-compiled-code-run-faster-then-aot-code",
    "href": "lectures/13_dynamic_compilers.html#can-jit-compiled-code-run-faster-then-aot-code",
    "title": "Dynamic Compilers",
    "section": "Can jit compiled code run faster then aot code?",
    "text": "Can jit compiled code run faster then aot code?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#comparison",
    "href": "lectures/13_dynamic_compilers.html#comparison",
    "title": "Dynamic Compilers",
    "section": "Comparison",
    "text": "Comparison\n\n\n\naot\njit\n\n\n\n\ncannot inline libraries\ncan inline (even class methods)\n\n\nno runtime code gen\ncan use run time code gen\n\n\nno speculative opts\ncan use spec opts\n\n\nless information\nmore information\n\n\noverall performance lower\noverall performance often higher\n\n\nfull speed from the start\nrequires warmup\n\n\nno compile cost at run time\noverhead to run compiler",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#tradeoffs",
    "href": "lectures/13_dynamic_compilers.html#tradeoffs",
    "title": "Dynamic Compilers",
    "section": "Tradeoffs",
    "text": "Tradeoffs\n\nThe time to compile is part of the total execution time\nmight run less optimizations to speed up execution time\nmight look at run time info\nsame code might be compiled many times\n\nWhy would the same code be compiled more than once?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#tiered-compilers",
    "href": "lectures/13_dynamic_compilers.html#tiered-compilers",
    "title": "Dynamic Compilers",
    "section": "tiered compilers",
    "text": "tiered compilers\nSince compilation is costly, do not compile functions that are only called once and do not contain a long running loop\nwe have a series of compilers, each with more aggressive optimization and each allowed to take longer\n\nthe lowest tier is the interpreter\nthe next is the base line compiler\n\n\n\nstart interpreting the code\nif some part of the code takes a long time, compile it with the next higher tier\nis some runtime info changes, compile it again",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#magic-numbers",
    "href": "lectures/13_dynamic_compilers.html#magic-numbers",
    "title": "Dynamic Compilers",
    "section": "magic numbers",
    "text": "magic numbers\nassociate a counter with branches and functions if the counter reaches some magic number use one of the compilers\nif the counter for a backward branch, you recompile, but the code is executing in the middle of a loop, how do you insert the newly compiled code?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#questions-when-building-a-jit",
    "href": "lectures/13_dynamic_compilers.html#questions-when-building-a-jit",
    "title": "Dynamic Compilers",
    "section": "questions when building a JIT",
    "text": "questions when building a JIT\n\nwhat strategy do you use to invoke the jit\ndo you have to execute for a while before calling the jit\nhow much info do you need\nwhat is the price of wrong info\nare there easy and hard programs\ndo the easy programs match up with users common programs",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#speculation",
    "href": "lectures/13_dynamic_compilers.html#speculation",
    "title": "Dynamic Compilers",
    "section": "Speculation",
    "text": "Speculation\n\nassume some property is true, compile using that info this is always a gamble, so you need to recover if the assumption was wrong\nassume a variable is an int, and does not overflow\nassume properties of an object is fixed\nassume the target of call is always the same\nassume past behavior predicts future behavior",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#flow",
    "href": "lectures/13_dynamic_compilers.html#flow",
    "title": "Dynamic Compilers",
    "section": "flow",
    "text": "flow\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph LR\ninterpreter -- hot? --&gt; profiling \nprofiling -- stats --&gt; optimizing_compiler\noptimizing_compiler --&gt; compiled_code\ncompiled_code -- deoptimze --&gt; interpreter\ninterpreter -- already_compiled --&gt; compiled_code\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph LR\ninterpreter -- hot? --&gt; profiling \nprofiling -- stats --&gt; optimizing_compiler\noptimizing_compiler --&gt; compiled_code\ncompiled_code -- deoptimze --&gt; interpreter\ninterpreter -- already_compiled --&gt; compiled_code",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#boxed-values",
    "href": "lectures/13_dynamic_compilers.html#boxed-values",
    "title": "Dynamic Compilers",
    "section": "boxed values",
    "text": "boxed values\nMany languages do not use strong static typing\nfor example in python\nx = x + 1\nx could be an int/float/object/string etc\nthe value of x needs to carry a type. Represent x as a pair (type, pointer or bits) The pair is called a boxed value\nthen to generate code for the plus we have to figure out what kind of add, based on the type",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#inline-caches",
    "href": "lectures/13_dynamic_compilers.html#inline-caches",
    "title": "Dynamic Compilers",
    "section": "inline caches",
    "text": "inline caches\nin languages like python, calls to a method are more expensive then calls to a method in c++ why?\n. . .\nPython objects are implemented as hash tables. While C++ uses virtual tables\nhow does that effect the cost?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#first-c-virtual-tables",
    "href": "lectures/13_dynamic_compilers.html#first-c-virtual-tables",
    "title": "Dynamic Compilers",
    "section": "first C++ virtual tables",
    "text": "first C++ virtual tables\nin C++ a method call takes two dereferences\n\nfirst find the v-table\nsecond used a fixed offset from the table start to find the address",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#what-do-we-need-to-keep-the-offset-fixed",
    "href": "lectures/13_dynamic_compilers.html#what-do-we-need-to-keep-the-offset-fixed",
    "title": "Dynamic Compilers",
    "section": "What do we need to keep the offset fixed?",
    "text": "What do we need to keep the offset fixed?\nif derived inherits from base, and both have a function f. the offset to f has to be the same.\nin languages where objects are hash tables, the c++ dereference becomes a hash table lookup, which is slower",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#tradeoffs-1",
    "href": "lectures/13_dynamic_compilers.html#tradeoffs-1",
    "title": "Dynamic Compilers",
    "section": "tradeoffs",
    "text": "tradeoffs\nIn a dynamically typed language like python we can add or remove methods easily\nbut method calls are expensive\nwe want to make these calls cheaper",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#inline-caches-at-each-call-site",
    "href": "lectures/13_dynamic_compilers.html#inline-caches-at-each-call-site",
    "title": "Dynamic Compilers",
    "section": "inline caches at each call site",
    "text": "inline caches at each call site\nthe first time we call a method, we know the type (because we are generating code at runtime)\n\n\ndef func(a,b,c):\n  for i in range(10):\n     foo(a,b,c)\n\ndef func(a,b,c):\n  for i in range(10):\n    if isinstance(a, type1)\n      body of foo  \n    else:\n      other = lookup 'foo' in the hash\n      call other(a,b,c\n      )",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#inline-caches-at-the-function-site",
    "href": "lectures/13_dynamic_compilers.html#inline-caches-at-the-function-site",
    "title": "Dynamic Compilers",
    "section": "inline caches at the function site",
    "text": "inline caches at the function site\n\n\ndef func(a,b,c):\n  for i in range(10):\n     _foo(a,b,c\n\ndef _foo(a,b,c)\n  if isinstance(a, type1)\n      body of foo  \n    else:\n      other = lookup 'foo' in a\n      call other(a,b,c)\n\n\nis it better to do this at the call site or at the function site?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#polymorphic-calls",
    "href": "lectures/13_dynamic_compilers.html#polymorphic-calls",
    "title": "Dynamic Compilers",
    "section": "polymorphic calls",
    "text": "polymorphic calls\nif the type changes at runtime (the call to other is taken) does the optimization help?\ncould invalidate the table and rebuild it with another case",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#what-are-the-costs",
    "href": "lectures/13_dynamic_compilers.html#what-are-the-costs",
    "title": "Dynamic Compilers",
    "section": "what are the costs",
    "text": "what are the costs\nfor example v8 compiler\nmonomorphic inline hit - 10 instructions\npolymorphic hit - 35 instructions for 10 types, 60 instructions for 20 types\ncache miss 1000-4000 instructions",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#value-specialization",
    "href": "lectures/13_dynamic_compilers.html#value-specialization",
    "title": "Dynamic Compilers",
    "section": "value specialization",
    "text": "value specialization\nOddly many functions are called with the same arguments",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#an-example",
    "href": "lectures/13_dynamic_compilers.html#an-example",
    "title": "Dynamic Compilers",
    "section": "an example",
    "text": "an example\ngiven a vector v of size n, and a parameter q find the element of v that is closest to q\n function closest(v, q, n) {\n    if (n == 0) {\n          throw \"Error\";\n    } else {\n        var i = 0;\n        var d = 0ffffffff;\n        while (i &lt; n) {\n           var nd = abs(v[i] - q);\n           if (nd &lt;= d) d = nd; \n           i++;\n        }    \n        return d;  \n      } \n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#the-cfg",
    "href": "lectures/13_dynamic_compilers.html#the-cfg",
    "title": "Dynamic Compilers",
    "section": "the cfg",
    "text": "the cfg\nwe want to recompile this for specific v,q, and n, where we restart at the while test\n\n\n\n function closest(v, q, n) {\n    if (n == 0) {\n          throw \"Error\";\n    } else {\n      var i = 0;\n      var d = 0ffffffff;\n      while (i &lt; n) {\n         var nd = abs(v[i] - q);\n         if (nd &lt;= d) d = nd; \n         i++;\n        }    \n        return d;  \n      } \n}\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nnormal_entry[\"function entry\n              v = param[0]\n              q = param[1]\n              n = param[2]\n              if (n ==0) goto l1\"]\n\nl1[\"l1: throw error\"]\nl2[\" l2: i0 = 0\n     d = 0fffffff\"]\nnormal_entry --&gt; l1\nnormal_entry--&gt; l2\nl3[\"l3: i1 = phi(i0, i2, i3)\n    d1 = phi(d0, d3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nl2--&gt; l3\nentry_on_stack_rep[\"start replace\n                   v = param[0]\n                  q = param[1]\n                  n = param[2]\n                  i3 = stack[0]\n                  d4 = stack[1]\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nnormal_entry[\"function entry\n              v = param[0]\n              q = param[1]\n              n = param[2]\n              if (n ==0) goto l1\"]\n\nl1[\"l1: throw error\"]\nl2[\" l2: i0 = 0\n     d = 0fffffff\"]\nnormal_entry --&gt; l1\nnormal_entry--&gt; l2\nl3[\"l3: i1 = phi(i0, i2, i3)\n    d1 = phi(d0, d3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nl2--&gt; l3\nentry_on_stack_rep[\"start replace\n                   v = param[0]\n                  q = param[1]\n                  n = param[2]\n                  i3 = stack[0]\n                  d4 = stack[1]\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#two-entries",
    "href": "lectures/13_dynamic_compilers.html#two-entries",
    "title": "Dynamic Compilers",
    "section": "two entries",
    "text": "two entries\nFirst entry is the regular starting point, second is the entry if we are currently running the loop in the interpreter\nSince we are compiling the function while in the loop we can ask the interpreter for values\n\nv == load[0]\nq = 42\nn = 100\ni = 40\nd = 0fffffff\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nnormal_entry[\"function entry\n              v = param[0]\n              q = param[1]\n              n = param[2]\n              if (n ==0) goto l1\"]\n\nl1[\"l1: throw error\"]\nl2[\" l2: i0 = 0\n     d = 0fffffff\"]\nnormal_entry --&gt; l1\nnormal_entry--&gt; l2\nl3[\"l3: i1 = phi(i0, i2, i3)\n    d1 = phi(d0, d3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nl2--&gt; l3\nentry_on_stack_rep[\"start replace\n                   v = param[0]\n                  q = param[1]\n                  n = param[2]\n                  i3 = stack[0]\n                  d4 = stack[1]\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nnormal_entry[\"function entry\n              v = param[0]\n              q = param[1]\n              n = param[2]\n              if (n ==0) goto l1\"]\n\nl1[\"l1: throw error\"]\nl2[\" l2: i0 = 0\n     d = 0fffffff\"]\nnormal_entry --&gt; l1\nnormal_entry--&gt; l2\nl3[\"l3: i1 = phi(i0, i2, i3)\n    d1 = phi(d0, d3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nl2--&gt; l3\nentry_on_stack_rep[\"start replace\n                   v = param[0]\n                  q = param[1]\n                  n = param[2]\n                  i3 = stack[0]\n                  d4 = stack[1]\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nnormal_entry[\"function entry\n              v = load[0]\n              q = q = 42 \n              n = 100\n              if (n ==0) goto l1\"]\n\nl1[\"l1: throw error\"]\nl2[\" l2: i0 = 0\n     d = 0fffffff\"]\nnormal_entry --&gt; l1\nnormal_entry--&gt; l2\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(d3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nl2--&gt; l3\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nnormal_entry[\"function entry\n              v = load[0]\n              q = q = 42 \n              n = 100\n              if (n ==0) goto l1\"]\n\nl1[\"l1: throw error\"]\nl2[\" l2: i0 = 0\n     d = 0fffffff\"]\nnormal_entry --&gt; l1\nnormal_entry--&gt; l2\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(d3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nl2--&gt; l3\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#dead-code-elimination",
    "href": "lectures/13_dynamic_compilers.html#dead-code-elimination",
    "title": "Dynamic Compilers",
    "section": "dead code elimination",
    "text": "dead code elimination\nAfter this all calls to the function assume these arguments so no need to keep the regular entry\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nnormal_entry[\"function entry\n              v = load[0]\n              q = q = 42 \n              n = 100\n              if (n ==0) goto l1\"]\n\nl1[\"l1: throw error\"]\nl2[\" l2: i0 = 0\n     d = 0fffffff\"]\nnormal_entry --&gt; l1\nnormal_entry--&gt; l2\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(d3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nl2--&gt; l3\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nnormal_entry[\"function entry\n              v = load[0]\n              q = q = 42 \n              n = 100\n              if (n ==0) goto l1\"]\n\nl1[\"l1: throw error\"]\nl2[\" l2: i0 = 0\n     d = 0fffffff\"]\nnormal_entry --&gt; l1\nnormal_entry--&gt; l2\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(d3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nl2--&gt; l3\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(dd3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(dd3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#array-in-bounds-check",
    "href": "lectures/13_dynamic_compilers.html#array-in-bounds-check",
    "title": "Dynamic Compilers",
    "section": "array in bounds check",
    "text": "array in bounds check\nwe can pattern match loops with bounds checks if we know the limit\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(dd3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(dd3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n\nl3[\"l3: i1 = phi(i0, i2, i3)\n    d1 = phi(d0, d3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     \"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl9--&gt; l3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n\nl3[\"l3: i1 = phi(i0, i2, i3)\n    d1 = phi(d0, d3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     \"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl9--&gt; l3",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#loop-inversion",
    "href": "lectures/13_dynamic_compilers.html#loop-inversion",
    "title": "Dynamic Compilers",
    "section": "loop inversion",
    "text": "loop inversion\na general while loop, loop might never run so cannot move code out of the loop\nwhile(cond){\n  ...\n}\ncan be changed into\nif (cond){\n  do {\n    ...\n  } while(cond)\n}\nfor this loop the first time around i = 40, n = 100 so the first condition is true",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#after-loop-inversion",
    "href": "lectures/13_dynamic_compilers.html#after-loop-inversion",
    "title": "Dynamic Compilers",
    "section": "after loop inversion",
    "text": "after loop inversion\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(d3, d4)\" ]\nentry_on_stack_rep[\"v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl3 --&gt; l7\n\n\nl4[\"l4: return d1\"]\nl7[\" l7: l5: t0 = 4* i\nt1 = v[t0]\nnd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   if (i2 &gt; n) goto l4\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl9--&gt; l3\nl9--&gt; l4\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(d3, d4)\" ]\nentry_on_stack_rep[\"v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl3 --&gt; l7\n\n\nl4[\"l4: return d1\"]\nl7[\" l7: l5: t0 = 4* i\nt1 = v[t0]\nnd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   if (i2 &gt; n) goto l4\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl9--&gt; l3\nl9--&gt; l4",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#results",
    "href": "lectures/13_dynamic_compilers.html#results",
    "title": "Dynamic Compilers",
    "section": "results",
    "text": "results\nspecialized code is shorter and compiles faster\nsince we know that the loop goes from 42 to 100, we could unroll the loop",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#dynamic-compilation-and-pgo-profile-guided-optimization",
    "href": "lectures/13_dynamic_compilers.html#dynamic-compilation-and-pgo-profile-guided-optimization",
    "title": "Dynamic Compilers",
    "section": "dynamic compilation and pgo (profile guided optimization)",
    "text": "dynamic compilation and pgo (profile guided optimization)\nworkload dependent – dynamically you only measure what you actually see – that can lead to sub-optimal choices if you make them “forever”\nIf your program has phases, where it does some kind of behavior for a while, and then shifts to another behavior afterwards, then that workload that you originally measured isn’t representative anymore!\nWhen you do PGO you’re supposed to do it on the entire program and it’s supposed to be representative, whereas when you’re tracing you’re doing that live!",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#confidence-vs-time",
    "href": "lectures/13_dynamic_compilers.html#confidence-vs-time",
    "title": "Dynamic Compilers",
    "section": "confidence vs time",
    "text": "confidence vs time\nset the magic numbers high- run a long time slowly, but gather more info\nset the magic number low, quickly recompile based on less info",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#c-method-call",
    "href": "lectures/13_dynamic_compilers.html#c-method-call",
    "title": "Dynamic Compilers",
    "section": "C++ method call",
    "text": "C++ method call\neach object has a pointer to a vtable, which is a table of function pointers to all the virtual functions\neach derived object has its own vtable, which has the same offsets for all the common virtual functions\nto find the code takes two dereferences\n\nfind the vtable (one per class) (one load)\nat a fixed offset (determined by the virtual function name) find the code (second load)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#a-python-example",
    "href": "lectures/13_dynamic_compilers.html#a-python-example",
    "title": "Dynamic Compilers",
    "section": "a python example",
    "text": "a python example\nmore flexible\n\nfind the hash table (one per instance)\nlookup the virtual function in the hash table\n\n\nclass Thing:\n    def __init__(self, kind):\n        self.kind = kind \n\nthing = Thing('car')\n\ndef honk(self):\n    print(f\"{self.kind} says Honk\")\n\nthing.honk = honk.__get__(thing)  ## add a method dynamically to one instance \n\nthing.honk()  ## call it \n\nhonk.__get__(thing) returns a bound method, when this method is called thing is passed as first argument",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#dynamic-chunks",
    "href": "lectures/13_dynamic_compilers.html#dynamic-chunks",
    "title": "Dynamic Compilers",
    "section": "dynamic chunks",
    "text": "dynamic chunks\nSo far:\n\nrun interpreter or tier 0 compiler\ncollect statistics on call counts or branch counts\nwhen count is high enough recompile the hot functions\nspecialize the hot functions based on common values",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#the-unit-of-compilation-is-the-static-function",
    "href": "lectures/13_dynamic_compilers.html#the-unit-of-compilation-is-the-static-function",
    "title": "Dynamic Compilers",
    "section": "The unit of compilation is the static function",
    "text": "The unit of compilation is the static function\nan alterative is called trace compilation\n\nrun interpreter or tier 0 compiler\ncollect the statements executed (no not collect control flow)\nthis produces a linear trace of instructions\nrecompile the trace using optimizations like value numbering\nif the next time the code executes, it takes a different path, fix things up",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#trace-compilation-0",
    "href": "lectures/13_dynamic_compilers.html#trace-compilation-0",
    "title": "Dynamic Compilers",
    "section": "trace compilation 0",
    "text": "trace compilation 0\nIn a linear trace the number of assumptions you’re making accumulates as you execute [towards the end of the trace you have the most assumptions built up]\nIf you have an always-taken control flow, e.g. some virtual function call that’s always calling the same actual function, a tracing compiler will treat all back-to-back branches as one set of straight line code\nexecute this all back to back, and, whenever convenient, check whether any of those assumptions were wrong”",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#cold-path",
    "href": "lectures/13_dynamic_compilers.html#cold-path",
    "title": "Dynamic Compilers",
    "section": "cold path",
    "text": "cold path\nOn the “cold path” – again, when it’s convenient undo all the inapplicable things if it turns out the branches weren’t true\nCalled a “bailout” [“bailing out” of the trace]\nAt a bailout there is new information. something you didn’t observe when you were tracing\nYou trust that everything you’ve trace is going to happen, it’s all going to well, and you’re going to be able to optimize for it\nBut then at runtime, when convenient, you’re going to check, and then bail if you were wrong, and have the rollback on the cold path\nSo the hot path, the one you’re pretty sure is going to execute, is quite optimal",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#trace-compilation-2",
    "href": "lectures/13_dynamic_compilers.html#trace-compilation-2",
    "title": "Dynamic Compilers",
    "section": "trace compilation 2",
    "text": "trace compilation 2\ntracing jit: extract a hot path (not a function)\nHot paths are compiled as a single basic block, but the path might go through a call\ngamble: next execution starting at this point, go the same way, no branches leave the path\ngenerate machine code for hot paths interpret the rest of the program\nunlike specialization, tracing assumes the same path but not the same values",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#an-example-x-42",
    "href": "lectures/13_dynamic_compilers.html#an-example-x-42",
    "title": "Dynamic Compilers",
    "section": "an example (x = 42)",
    "text": "an example (x = 42)\n\n\nfunction main(x){\n   y = x +1 \n   if x &lt;100 {\n      z = f(y)\n   } else {\n      z = g(y)\n   }\n   return z\n}\n\nfunction f(a){\n   return a -1 \n}\n\n\n\ny = x +1\nguard(x &lt; 100)\na = y\nz = a - 1\nreturn z\n\n\n\n\nguards at divergence, guards never return\noptimize assuming guards are true, ok to be slow if guard is false",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#move-guards-up",
    "href": "lectures/13_dynamic_compilers.html#move-guards-up",
    "title": "Dynamic Compilers",
    "section": "move guards up",
    "text": "move guards up\nwhy is this a good idea?\n. . .\n\nfail fast\nlonger region to optimize",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#use-local-value-numbering",
    "href": "lectures/13_dynamic_compilers.html#use-local-value-numbering",
    "title": "Dynamic Compilers",
    "section": "use local value numbering",
    "text": "use local value numbering\n\n\n\nguard(x &lt; 100)\ny = x + 1\na = y\nz = a - 1\nreturn z\n\n\n\nguard(x &lt; 100)\nreturn x",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#how-do-this-in-bril",
    "href": "lectures/13_dynamic_compilers.html#how-do-this-in-bril",
    "title": "Dynamic Compilers",
    "section": "how do this in Bril?",
    "text": "how do this in Bril?\n3 new operations (sort of like out-of-order instructions)\n\nspeculate - Enter a speculative execution context. No arguments.\ncommit - End the current speculative context, committing the current speculative state as the “real” state. No arguments.\nguard - Check a condition and possibly abort the current speculative context. One argument, the Boolean condition, and one label, to which control is transferred on abort.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#speculate-extension",
    "href": "lectures/13_dynamic_compilers.html#speculate-extension",
    "title": "Dynamic Compilers",
    "section": "speculate extension",
    "text": "speculate extension\nspeculative execution extension",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#example",
    "href": "lectures/13_dynamic_compilers.html#example",
    "title": "Dynamic Compilers",
    "section": "example",
    "text": "example\nb: bool = const false;\nv: int = const 4; v == 4\nspeculate; v: int = const 2; v == 2 (speculate state) guard b .failed; v == 2 (speculate state) commit;\n.failed: print v; v == 4",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#implementation",
    "href": "lectures/13_dynamic_compilers.html#implementation",
    "title": "Dynamic Compilers",
    "section": "implementation",
    "text": "implementation\nyou can add a tracer to an interpreter\nIn a lot of language environments you’ll have an interpreter that’s executing “op at a time”\nhook in a tracer which observes what the interpreter is doing and “make some machine code on the side” based on how the interpreter ran\nyou can implement just a subset of the operations [ed: you might call this property “compiler completeness” for your op set",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#common-bytecode-operations",
    "href": "lectures/13_dynamic_compilers.html#common-bytecode-operations",
    "title": "Dynamic Compilers",
    "section": "common bytecode operations",
    "text": "common bytecode operations\nimplement only the common ones and simply end the trace when you hit one that was not implemented, because it was uncommon\nYou can build up this trace JIT-ing capability over time, because the system is built with this assumption you can bail out of the trace for whatever reason and go back to thr interpreter",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#an-example-1",
    "href": "lectures/13_dynamic_compilers.html#an-example-1",
    "title": "Dynamic Compilers",
    "section": "an example",
    "text": "an example\nCould imagine making a JIT that just: Covered MULs and ADDs and could make fused/composite MUL/ADD bytecode combinations\nSpecialize that for one common type; e.g. if you have many types in your language, could support that just for integer types, or just for FP ops e.g. if it were numerical code, and then just bail if any other types showed up at runtime;\ntrace invariants: suppose traces call to other traces;\ntrace1 set of ops A, trace2 with set of ops B and we see a transfer from A to B\nmake sure that the assumptions between those two things are lining up – called trace fusion\nknow the invariants (i.e. “what must be true”) on the exit from A and the entry to B are lining up / compatible with each other",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#method-inlining",
    "href": "lectures/13_dynamic_compilers.html#method-inlining",
    "title": "Dynamic Compilers",
    "section": "method inlining",
    "text": "method inlining\nIn trace compiler you just execute through methods\nInlining kind of the natural path of compilation when doing trace compilation – just linear execution where the jumps/calls/returns simply disappear",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#tail-duplication",
    "href": "lectures/13_dynamic_compilers.html#tail-duplication",
    "title": "Dynamic Compilers",
    "section": "tail duplication",
    "text": "tail duplication\nit is common that multiple traces have a common tail\nfor() {\n  if op_Eq{\n     op1 \n   } else {\n      op2\n   }\n}\nop_t\nop_a\nop_i\nop_l\ntrace0:  op_eq quard\\true  op1 op_t op_a op_i op_l \n\ntrace1:              \\false op2 op_t op_a op_i op_l \ntwo traces with the same ending, could generate\none copy of the tail- with arguments showing the header\ntwo copies of the tail- frozen header",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#adding-traces-to-bril",
    "href": "lectures/13_dynamic_compilers.html#adding-traces-to-bril",
    "title": "Dynamic Compilers",
    "section": "adding traces to bril",
    "text": "adding traces to bril\nHow to modify the reference interpreter (warning typescript!)\nbrili\nthere are two functions to consider\n\nevalFunc interprets a function by calling evalInstr on each instruction\nevalInstr interprets one instruction, large case statement for each instruction",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#print-instructions-as-they-execute",
    "href": "lectures/13_dynamic_compilers.html#print-instructions-as-they-execute",
    "title": "Dynamic Compilers",
    "section": "print instructions as they execute",
    "text": "print instructions as they execute\n\nfigure out when to start and when to stop\nhow to print instructions (modify evalInstr by printing instructions) console.log(instr)\n\nyou have to optimize the trace and put it back",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#traces-and-users",
    "href": "lectures/13_dynamic_compilers.html#traces-and-users",
    "title": "Dynamic Compilers",
    "section": "traces and users",
    "text": "traces and users\nwhen a trace works well- it looks amazing - it finds the inner loop and optimizes even through libraries\nbut users find in hard to understand what the compiler did,\na tiny source change can make a big trace change\nhard to fit in a debugger\nsecurity is a problem",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#pytorch-2.0",
    "href": "lectures/13_dynamic_compilers.html#pytorch-2.0",
    "title": "Dynamic Compilers",
    "section": "pytorch 2.0",
    "text": "pytorch 2.0\nml frameworks have two modes",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#eager-mode",
    "href": "lectures/13_dynamic_compilers.html#eager-mode",
    "title": "Dynamic Compilers",
    "section": "Eager Mode",
    "text": "Eager Mode\n\nPreferred by users\nEasier to use programming model\nEasy to debug\n\na + b + c executes two calls to torch.add (if they are tensors)\nno place to optimize, allows any kind of python, and any control flow\n\nPyTorch is a primarily an eager mode framework",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#graph-mode",
    "href": "lectures/13_dynamic_compilers.html#graph-mode",
    "title": "Dynamic Compilers",
    "section": "Graph Mode",
    "text": "Graph Mode\n\nPreferred by backends and framework builders\nEasier to optimize with a compiler\nEasier to do automated transformations\n\nconstruct a graph with two add nodes and 3 input nodes, then execute the graph\neasy to optimize, only graph nodes allowed, no control flow\nMain optimization is fusing operations to avoid memory copies",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#how-does-the-compiler-fit",
    "href": "lectures/13_dynamic_compilers.html#how-does-the-compiler-fit",
    "title": "Dynamic Compilers",
    "section": "how does the compiler fit",
    "text": "how does the compiler fit\nin Eager mode there is only a library - no compiler\nif you have a matmul followed by an activation function, it is up to the developer to notice that the memory traffic is more expensive then the activation and its up to the developer to know there is another pytorch call (2000 different calls) which does the combined operation and it is up to the developer to change the code\nif graph mode (compiler writers call this defered or late) the operations get recorded (not executed) and only get executed when we need the result",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#pytorchs-many-attempts-at-graph-modes",
    "href": "lectures/13_dynamic_compilers.html#pytorchs-many-attempts-at-graph-modes",
    "title": "Dynamic Compilers",
    "section": "PyTorch’s Many Attempts at Graph Modes",
    "text": "PyTorch’s Many Attempts at Graph Modes\ntorch.jit.trace\n\nRecord + replay\nUnsound\nCan give incorrect results because it ignores Python part of program\n\ntorch. jit.script\n\nAOT parses Python into graph format\nOnly works on ~45% of real world models\nHigh effort to “TorchScript” models\nPyTorch Models Are Not Static Graphs\n\nPyTorch users write models where program graphs are impossible\nConvert tensors to native Python types (x.item(), x.tolist(), int(x), etc)\nUse other frameworks (numpy/xarray/etc) for part of their model\nData dependent Python control flow or other dynamism Exceptions, closures, generators, classes, etc\ntorch xla\ndefered execution. rather then do the graph operation, just save it and execute as late as possible\nvery slow, big performance cliffs",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#torch.compilemodel---converts-a-pytorch-eager-program-to-a-graph",
    "href": "lectures/13_dynamic_compilers.html#torch.compilemodel---converts-a-pytorch-eager-program-to-a-graph",
    "title": "Dynamic Compilers",
    "section": "torch.compile(model) - converts a pytorch eager program to a graph",
    "text": "torch.compile(model) - converts a pytorch eager program to a graph\ntorch.dynamo - which dynamically captures Python code execution and creates a static computational graph.\ntorch.Inductor- compiler that optimimzes static computation graphs",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#dynamo",
    "href": "lectures/13_dynamic_compilers.html#dynamo",
    "title": "Dynamic Compilers",
    "section": "dynamo",
    "text": "dynamo\nimport torch\nfrom typing import List\n\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\n\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my compiler() called with fx graph\")\n    gm.graph.print_tabular()\n    return gm\n\n@torch.compile(backend=my_compiler)\ndef toy_example(a,b):\n    x = a / (torch.abs(a)+1)\n    if b.sum() &lt; 0:\n        b  = b * -1\n    return x *b\n\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#output",
    "href": "lectures/13_dynamic_compilers.html#output",
    "title": "Dynamic Compilers",
    "section": "output",
    "text": "output\n\n\ndef toy_example(a,b):\n    x = a / (torch.abs(a)+1)\n    if b.sum() &lt; 0:\n        b  = b * -1\n    return x *b\n\nopcode         name    target        args      \n------       ------  ---------    -----------\nplaceholder    l_a_    L_a_         ()         \nplaceholder    l_b_    L_b_         ()         \ncall_function  abs_1   &lt;abs&gt;        (l_a_,)   \ncall_function  add     &lt;add&gt;        (abs_1, 1) \ncall_function  x       &lt;truediv&gt;    (l_a_, add)\ncall_method    sum_1   sum          (l_b_,)   \ncall_function  lt      &lt;lt&gt;         (sum_1, 0) \noutput         output  output       ((x, lt),)\n------       ------  ---------    -----------\nplaceholder    l_b_    L_b_         ()           \nplaceholder    l_x_    L_x_         ()           \ncall_function  b       &lt;mul&gt;        (l_b_, -1)   \ncall_function  mul_1   &lt;mul&gt;        (l_x_, b)    \noutput         output  output       ((mul_1,),) \n------       ------  ---------    -----------        \nplaceholder    l_x_    L_x_         ()            \nplaceholder    l_b_    L_b_         ()            \ncall_function  mul     &lt;mul&gt;        (l_x_, l_b_)  \noutput         output  output       ((mul,),)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#implementation-1",
    "href": "lectures/13_dynamic_compilers.html#implementation-1",
    "title": "Dynamic Compilers",
    "section": "implementation",
    "text": "implementation\npython builds a frameObject (pointer to codeObject + arguments)\npasses this to eval\ncodeObject allows for extra user data and for a user function to be called between the frameObject and eval\nThis makes it easy to add a custom JIT\nsplit the function into two parts - the python part and the torch part\nThis is reused if the guards pass",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#pytorch-example-1",
    "href": "lectures/13_dynamic_compilers.html#pytorch-example-1",
    "title": "Dynamic Compilers",
    "section": "pytorch example 1",
    "text": "pytorch example 1\ndef toy_example(a,b):\n    x = a / (torch.abs(a)+1)\n    if b.sum() &lt; 0:\n        b  = b * -1\n    return x *b\n\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\nsometimes the sum is negative but not always",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#implementation-2",
    "href": "lectures/13_dynamic_compilers.html#implementation-2",
    "title": "Dynamic Compilers",
    "section": "implementation",
    "text": "implementation",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#graph",
    "href": "lectures/13_dynamic_compilers.html#graph",
    "title": "Dynamic Compilers",
    "section": "graph",
    "text": "graph\n\n\n\n\n\n\n\n\n\nopcode\nname\ntarget\nargs\n\n\n\n\nplaceholder\nl_a_\nL_a_\n()\n\n\nplaceholder\nl_b_\nL_b_\n()\n\n\ncall_function\nabs_1\n&lt;built-in method abs of type object at 0x728736add8a0&gt;\n(l_a_,)\n\n\ncall_function\nadd\n\n(abs_1, 1)\n\n\ncall_function\nx\n\n(l_a_, add)\n\n\ncall_method\nsum_1\nsum\n(l_b_,)\n\n\ncall_function\nlt\n\n(sum_1, 0)\n\n\noutput\noutput\noutput\n((x, lt),)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#code",
    "href": "lectures/13_dynamic_compilers.html#code",
    "title": "Dynamic Compilers",
    "section": "code",
    "text": "code\ndef toy_example(a,b):\n   (x,lt) = call1(a,b)\n   if lt:\n      f1(b,x)\n   else:\n      f2(b,x)\n\ndef f1(b, x):\n   b  = b * -1\n   return x *b\n\ndef f2(b,x):\n   return x *b",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#guards",
    "href": "lectures/13_dynamic_compilers.html#guards",
    "title": "Dynamic Compilers",
    "section": "guards:",
    "text": "guards:\ncheck_tensor(L[‘a’], Tensor, torch.float32, size=[10], stride=[1]) check_tensor(L[‘b’], Tensor, torch.float32, size=[10], stride=[1])\nwalk the byte code again for f1\nb  = b * -1\nreturn x *b",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#traced-graph",
    "href": "lectures/13_dynamic_compilers.html#traced-graph",
    "title": "Dynamic Compilers",
    "section": "TRACED GRAPH",
    "text": "TRACED GRAPH\n\n\n\npcode n\name t\narget a\nrgs k\nwargs\n\n\n\n\nplaceholder\nl_b_\nL_b_\n()\n{}\n\n\nplaceholder\nl_x_\nL_x_\n()\n{}\n\n\ncall_function\nb\n\n(l_b_, -1)\n{}\n\n\ncall_function\nmul_1\n\n(l_x_, b)\n{}\n\n\nutput o\nutput o\nutput (\n(mul_1,),) {\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#other-branch",
    "href": "lectures/13_dynamic_compilers.html#other-branch",
    "title": "Dynamic Compilers",
    "section": "other branch",
    "text": "other branch\nguards: check_tensor(L['b'], torch.float32, size=[10], stride=[1]) \n        check_tensor(L['x'], torch.float32, size=[10], stride=[1]) \nTRACED GRAPH\n\n\n\npcode n\name t\narget a\nrgs k\nwarg\n\n\n\n\naceholder l_\nx_ L_\nx_ ()\n{}\n\n\n\naceholder l_\nb_ L_\nb_ ()\n{}\n\n\n\nll_function mu\nl &lt;b\nuilt-in function mul&gt; (l\nx, l_b_) {}\n\n\n\ntput ou\ntput ou\ntput ((\nmul,),) {}\n\n\n\n\ncheck_tensor(L[‘b’], torch.float32, size=[10], stride=[1]) check_tensor(L[‘x’], torch.float32, size=[10], stride=[1]) # return x *b # mp/ipykernel_1179164/26",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#linear-traces",
    "href": "lectures/13_dynamic_compilers.html#linear-traces",
    "title": "Dynamic Compilers",
    "section": "linear traces",
    "text": "linear traces\nDynamo removes all control flow, if/else, loops, exceptions\nspecializes (bakes in) all non-tensor objects (numbers, strings, classes )\n\n\n\n@torch.compile\ndef fn(f,n):\n  y = x ** 2\n  if n &gt;= 0:\n    return (n +1)* y\n  else:\n    return x /y \n\nx = torch.randn(200)\nfn(x,2)\n\ndef forward(l_x_: torch.Tensor):\n   y = l_x_ ** 2\n   mul 3*x\n   return (mul,)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#special-cases",
    "href": "lectures/13_dynamic_compilers.html#special-cases",
    "title": "Dynamic Compilers",
    "section": "special cases",
    "text": "special cases\nTrace integers symbolically\nby default it specilizes on every integer in the graph but if a subsequent vall the value changes it traces symbolically but 0 or 1 are always speciaized",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#multiple-traces",
    "href": "lectures/13_dynamic_compilers.html#multiple-traces",
    "title": "Dynamic Compilers",
    "section": "multiple traces",
    "text": "multiple traces\nimport torch\nfrom typing import List\n\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\n\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my compiler() called with fx graph\")\n    gm.graph.print_tabular()\n    return gm\n\n@torch.compile(backend=my_compiler)\ndef toy_example(a,b):\n    x = a / (torch.abs(a)+1)\n    if b.sum() &lt; 0:\n        b  = b * -1\n    return x *b\n\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\n\n\nimport torch\nfrom typing import List\n\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\n\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my compiler() called with fx graph\")\n    gm.graph.print_tabular()\n    return gm",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/13_dynamic_compilers.html#implementation-3",
    "href": "lectures/13_dynamic_compilers.html#implementation-3",
    "title": "Dynamic Compilers",
    "section": "implementation",
    "text": "implementation\npep 523, allows python function to see unevaluated frames, function + arguments\nnormally just calls the function",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Dynamic Compilers"
    ]
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#jit-just-in-time-compilers-vs-aotahead-of-time-compilers",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#jit-just-in-time-compilers-vs-aotahead-of-time-compilers",
    "title": "Dynamic Compilers",
    "section": "jit (just in time) compilers vs aot(ahead of time) compilers",
    "text": "jit (just in time) compilers vs aot(ahead of time) compilers\na jit compiler translates code into isa while the program executes\npro:\nmore information\ncon:\ncompile time slows down execution"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#some-options",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#some-options",
    "title": "Dynamic Compilers",
    "section": "some options",
    "text": "some options\n\ncompile a function the first time it is called\ncompile a function after it has been called a lot (needs an interpreter) We call these hot functions\nbuild a trace of instructions executed and compile the hot traces (a trace has no branches)\nA variation I used: ran the program to completion using a tracing interpreter, recompile off line, future execution is a mix of interpreter and compiled code"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#can-jit-compiled-code-run-faster-then-aot-code",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#can-jit-compiled-code-run-faster-then-aot-code",
    "title": "Dynamic Compilers",
    "section": "Can jit compiled code run faster then aot code?",
    "text": "Can jit compiled code run faster then aot code?"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#comparison",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#comparison",
    "title": "Dynamic Compilers",
    "section": "Comparison",
    "text": "Comparison\n\n\n\naot\njit\n\n\n\n\ncannot inline libraries\ncan inline (even class methods)\n\n\nno runtime code gen\ncan use run time code gen\n\n\nno speculative opts\ncan use spec opts\n\n\nless information\nmore information\n\n\noverall performance lower\noverall performance often higher\n\n\nfull speed from the start\nrequires warmup\n\n\nno compile cost at run time\noverhead to run compiler"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#tradeoffs",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#tradeoffs",
    "title": "Dynamic Compilers",
    "section": "Tradeoffs",
    "text": "Tradeoffs\n\nThe time to compile is part of the total execution time\nmight run less optimizations to speed up execution time\nmight look at run time info\nsame code might be compiled many times\n\nWhy would the same code be compiled more than once?"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#tiered-compilers",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#tiered-compilers",
    "title": "Dynamic Compilers",
    "section": "tiered compilers",
    "text": "tiered compilers\nSince compilation is costly, do not compile functions that are only called once and do not contain a long running loop\nwe have a series of compilers, each with more aggressive optimization and each allowed to take longer\n\nthe lowest tier is the interpreter\nthe next is the base line compiler"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#magic-numbers",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#magic-numbers",
    "title": "Dynamic Compilers",
    "section": "magic numbers",
    "text": "magic numbers\nassociate a counter with branches and functions if the counter reaches some magic number use one of the compilers\nif the counter for a backward branch, you recompile, but the code is executing in the middle of a loop, how do you insert the newly compiled code?"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#questions-when-building-a-jit",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#questions-when-building-a-jit",
    "title": "Dynamic Compilers",
    "section": "questions when building a JIT",
    "text": "questions when building a JIT\n\nwhat strategy do you use to invoke the jit\ndo you have to execute for a while before calling the jit\nhow much info do you need\nwhat is the price of wrong info\nare there easy and hard programs\ndo the easy programs match up with users common programs"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#speculation",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#speculation",
    "title": "Dynamic Compilers",
    "section": "Speculation",
    "text": "Speculation\n\nassume some property is true, compile using that info this is always a gamble, so you need to recover if the assumption was wrong\nassume a variable is an int, and does not overflow\nassume properties of an object is fixed\nassume the target of call is always the same\nassume past behavior predicts future behavior"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#flow",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#flow",
    "title": "Dynamic Compilers",
    "section": "flow",
    "text": "flow\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph LR\ninterpreter -- hot? --&gt; profiling \nprofiling -- stats --&gt; optimizing_compiler\noptimizing_compiler --&gt; compiled_code\ncompiled_code -- deoptimze --&gt; interpreter\ninterpreter -- already_compiled --&gt; compiled_code\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph LR\ninterpreter -- hot? --&gt; profiling \nprofiling -- stats --&gt; optimizing_compiler\noptimizing_compiler --&gt; compiled_code\ncompiled_code -- deoptimze --&gt; interpreter\ninterpreter -- already_compiled --&gt; compiled_code"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#boxed-values",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#boxed-values",
    "title": "Dynamic Compilers",
    "section": "boxed values",
    "text": "boxed values\nMany languages do not use strong static typing\nfor example in python\nx = x + 1\nx could be an int/float/object/string etc\nthe value of x needs to carry a type. Represent x as a pair (type, pointer or bits) The pair is called a boxed value\nthen to generate code for the plus we have to figure out what kind of add, based on the type"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#inline-caches",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#inline-caches",
    "title": "Dynamic Compilers",
    "section": "inline caches",
    "text": "inline caches\nin languages like python, calls to a method are more expensive then calls to a method in c++ why?\n\nPython objects are implemented as hash tables. While C++ uses virtual tables\nhow does that effect the cost?"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#first-c-virtual-tables",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#first-c-virtual-tables",
    "title": "Dynamic Compilers",
    "section": "first C++ virtual tables",
    "text": "first C++ virtual tables\nin C++ a method call takes two dereferences\n\nfirst find the v-table\nsecond used a fixed offset from the table start to find the address"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#what-do-we-need-to-keep-the-offset-fixed",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#what-do-we-need-to-keep-the-offset-fixed",
    "title": "Dynamic Compilers",
    "section": "What do we need to keep the offset fixed?",
    "text": "What do we need to keep the offset fixed?\nif derived inherits from base, and both have a function f. the offset to f has to be the same.\nin languages where objects are hash tables, the c++ dereference becomes a hash table lookup, which is slower"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#tradeoffs-1",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#tradeoffs-1",
    "title": "Dynamic Compilers",
    "section": "tradeoffs",
    "text": "tradeoffs\nIn a dynamically typed language like python we can add or remove methods easily\nbut method calls are expensive\nwe want to make these calls cheaper"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#inline-caches-at-each-call-site",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#inline-caches-at-each-call-site",
    "title": "Dynamic Compilers",
    "section": "inline caches at each call site",
    "text": "inline caches at each call site\nthe first time we call a method, we know the type (because we are generating code at runtime)\n\n\ndef func(a,b,c):\n  for i in range(10):\n     foo(a,b,c)\n\ndef func(a,b,c):\n  for i in range(10):\n    if isinstance(a, type1)\n      body of foo  \n    else:\n      other = lookup 'foo' in the hash\n      call other(a,b,c\n      )"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#inline-caches-at-the-function-site",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#inline-caches-at-the-function-site",
    "title": "Dynamic Compilers",
    "section": "inline caches at the function site",
    "text": "inline caches at the function site\n\n\ndef func(a,b,c):\n  for i in range(10):\n     _foo(a,b,c\n\ndef _foo(a,b,c)\n  if isinstance(a, type1)\n      body of foo  \n    else:\n      other = lookup 'foo' in a\n      call other(a,b,c)\n\nis it better to do this at the call site or at the function site?"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#polymorphic-calls",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#polymorphic-calls",
    "title": "Dynamic Compilers",
    "section": "polymorphic calls",
    "text": "polymorphic calls\nif the type changes at runtime (the call to other is taken) does the optimization help?\ncould invalidate the table and rebuild it with another case"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#what-are-the-costs",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#what-are-the-costs",
    "title": "Dynamic Compilers",
    "section": "what are the costs",
    "text": "what are the costs\nfor example v8 compiler\nmonomorphic inline hit - 10 instructions\npolymorphic hit - 35 instructions for 10 types, 60 instructions for 20 types\ncache miss 1000-4000 instructions"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#value-specialization",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#value-specialization",
    "title": "Dynamic Compilers",
    "section": "value specialization",
    "text": "value specialization\nOddly many functions are called with the same arguments"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#an-example",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#an-example",
    "title": "Dynamic Compilers",
    "section": "an example",
    "text": "an example\ngiven a vector v of size n, and a parameter q find the element of v that is closest to q\n function closest(v, q, n) {\n    if (n == 0) {\n          throw \"Error\";\n    } else {\n        var i = 0;\n        var d = 0ffffffff;\n        while (i &lt; n) {\n           var nd = abs(v[i] - q);\n           if (nd &lt;= d) d = nd; \n           i++;\n        }    \n        return d;  \n      } \n}"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#the-cfg",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#the-cfg",
    "title": "Dynamic Compilers",
    "section": "the cfg",
    "text": "the cfg\nwe want to recompile this for specific v,q, and n, where we restart at the while test"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#two-entries",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#two-entries",
    "title": "Dynamic Compilers",
    "section": "two entries",
    "text": "two entries\nFirst entry is the regular starting point, second is the entry if we are currently running the loop in the interpreter\nSince we are compiling the function while in the loop we can ask the interpreter for values\n\nv == load[0]\nq = 42\nn = 100\ni = 40\nd = 0fffffff"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#dead-code-elimination",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#dead-code-elimination",
    "title": "Dynamic Compilers",
    "section": "dead code elimination",
    "text": "dead code elimination\nAfter this all calls to the function assume these arguments so no need to keep the regular entry\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nnormal_entry[\"function entry\n              v = load[0]\n              q = q = 42 \n              n = 100\n              if (n ==0) goto l1\"]\n\nl1[\"l1: throw error\"]\nl2[\" l2: i0 = 0\n     d = 0fffffff\"]\nnormal_entry --&gt; l1\nnormal_entry--&gt; l2\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(d3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nl2--&gt; l3\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nnormal_entry[\"function entry\n              v = load[0]\n              q = q = 42 \n              n = 100\n              if (n ==0) goto l1\"]\n\nl1[\"l1: throw error\"]\nl2[\" l2: i0 = 0\n     d = 0fffffff\"]\nnormal_entry --&gt; l1\nnormal_entry--&gt; l2\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(d3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nl2--&gt; l3\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(dd3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(dd3, d4)\n    if (i1 &lt; n) go to l5\"  ]\nentry_on_stack_rep[\"start replace\n                   v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl5[\"l5: t0 = 4* i\n     t1 = v[t0]\n     notinbounds(t1, n) go to l8\"]\n\nl3 --&gt; l5 \nl3--&gt; l4\nl4[\"l4: return d1\"]\nl5--&gt; l7\nl7[\" l7: nd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   goto l3\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl8[\"l8: throw boundsError\"]\nl5 --&gt; l8\nl9--&gt; l3"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#array-in-bounds-check",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#array-in-bounds-check",
    "title": "Dynamic Compilers",
    "section": "array in bounds check",
    "text": "array in bounds check\nwe can pattern match loops with bounds checks if we know the limit"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#loop-inversion",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#loop-inversion",
    "title": "Dynamic Compilers",
    "section": "loop inversion",
    "text": "loop inversion\na general while loop, loop might never run so cannot move code out of the loop\nwhile(cond){\n  ...\n}\ncan be changed into\nif (cond){\n  do {\n    ...\n  } while(cond)\n}\nfor this loop the first time around i = 40, n = 100 so the first condition is true"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#after-loop-inversion",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#after-loop-inversion",
    "title": "Dynamic Compilers",
    "section": "after loop inversion",
    "text": "after loop inversion\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(d3, d4)\" ]\nentry_on_stack_rep[\"v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl3 --&gt; l7\n\n\nl4[\"l4: return d1\"]\nl7[\" l7: l5: t0 = 4* i\nt1 = v[t0]\nnd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   if (i2 &gt; n) goto l4\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl9--&gt; l3\nl9--&gt; l4\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nl3[\"l3: i1 = phi(i2, i3)\n    d1 = phi(d3, d4)\" ]\nentry_on_stack_rep[\"v = load [0]\n                  q = 42\n                  n = 100\n                  i3 = 40\n                  d4 = offfffff\"]\nentry_on_stack_rep --&gt; l3\nl3 --&gt; l7\n\n\nl4[\"l4: return d1\"]\nl7[\" l7: l5: t0 = 4* i\nt1 = v[t0]\nnd = abs(t1, q)\n   if (nd &gt; d1) go to l9\"]\n\nl9[\"l9: d3 = phi(d1, d2)\n   i2 = i1 + 1\n   if (i2 &gt; n) goto l4\"]\nl7--&gt; l9\nl7--&gt; l6[\"l6: d2 = nd\"]\nl6--&gt; l9\nl9--&gt; l3\nl9--&gt; l4"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#results",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#results",
    "title": "Dynamic Compilers",
    "section": "results",
    "text": "results\nspecialized code is shorter and compiles faster\nsince we know that the loop goes from 42 to 100, we could unroll the loop"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#dynamic-compilation-and-pgo-profile-guided-optimization",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#dynamic-compilation-and-pgo-profile-guided-optimization",
    "title": "Dynamic Compilers",
    "section": "dynamic compilation and pgo (profile guided optimization)",
    "text": "dynamic compilation and pgo (profile guided optimization)\nworkload dependent – dynamically you only measure what you actually see – that can lead to sub-optimal choices if you make them “forever”\nIf your program has phases, where it does some kind of behavior for a while, and then shifts to another behavior afterwards, then that workload that you originally measured isn’t representative anymore!\nWhen you do PGO you’re supposed to do it on the entire program and it’s supposed to be representative, whereas when you’re tracing you’re doing that live!"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#confidence-vs-time",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#confidence-vs-time",
    "title": "Dynamic Compilers",
    "section": "confidence vs time",
    "text": "confidence vs time\nset the magic numbers high- run a long time slowly, but gather more info\nset the magic number low, quickly recompile based on less info"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#c-method-call",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#c-method-call",
    "title": "Dynamic Compilers",
    "section": "C++ method call",
    "text": "C++ method call\neach object has a pointer to a vtable, which is a table of function pointers to all the virtual functions\neach derived object has its own vtable, which has the same offsets for all the common virtual functions\nto find the code takes two dereferences\n\nfind the vtable (one per class) (one load)\nat a fixed offset (determined by the virtual function name) find the code (second load)"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#a-python-example",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#a-python-example",
    "title": "Dynamic Compilers",
    "section": "a python example",
    "text": "a python example\nmore flexible\n\nfind the hash table (one per instance)\nlookup the virtual function in the hash table\n\n\nclass Thing:\n    def __init__(self, kind):\n        self.kind = kind \n\nthing = Thing('car')\n\ndef honk(self):\n    print(f\"{self.kind} says Honk\")\n\nthing.honk = honk.__get__(thing)  ## add a method dynamically to one instance \n\nthing.honk()  ## call it \n\nhonk.__get__(thing) returns a bound method, when this method is called thing is passed as first argument"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#dynamic-chunks",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#dynamic-chunks",
    "title": "Dynamic Compilers",
    "section": "dynamic chunks",
    "text": "dynamic chunks\nSo far:\n\nrun interpreter or tier 0 compiler\ncollect statistics on call counts or branch counts\nwhen count is high enough recompile the hot functions\nspecialize the hot functions based on common values"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#the-unit-of-compilation-is-the-static-function",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#the-unit-of-compilation-is-the-static-function",
    "title": "Dynamic Compilers",
    "section": "The unit of compilation is the static function",
    "text": "The unit of compilation is the static function\nan alterative is called trace compilation\n\nrun interpreter or tier 0 compiler\ncollect the statements executed (no not collect control flow)\nthis produces a linear trace of instructions\nrecompile the trace using optimizations like value numbering\nif the next time the code executes, it takes a different path, fix things up"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#trace-compilation-0",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#trace-compilation-0",
    "title": "Dynamic Compilers",
    "section": "trace compilation 0",
    "text": "trace compilation 0\nIn a linear trace the number of assumptions you’re making accumulates as you execute [towards the end of the trace you have the most assumptions built up]\nIf you have an always-taken control flow, e.g. some virtual function call that’s always calling the same actual function, a tracing compiler will treat all back-to-back branches as one set of straight line code\nexecute this all back to back, and, whenever convenient, check whether any of those assumptions were wrong”"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#cold-path",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#cold-path",
    "title": "Dynamic Compilers",
    "section": "cold path",
    "text": "cold path\nOn the “cold path” – again, when it’s convenient undo all the inapplicable things if it turns out the branches weren’t true\nCalled a “bailout” [“bailing out” of the trace]\nAt a bailout there is new information. something you didn’t observe when you were tracing\nYou trust that everything you’ve trace is going to happen, it’s all going to well, and you’re going to be able to optimize for it\nBut then at runtime, when convenient, you’re going to check, and then bail if you were wrong, and have the rollback on the cold path\nSo the hot path, the one you’re pretty sure is going to execute, is quite optimal"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#trace-compilation-2",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#trace-compilation-2",
    "title": "Dynamic Compilers",
    "section": "trace compilation 2",
    "text": "trace compilation 2\ntracing jit: extract a hot path (not a function)\nHot paths are compiled as a single basic block, but the path might go through a call\ngamble: next execution starting at this point, go the same way, no branches leave the path\ngenerate machine code for hot paths interpret the rest of the program\nunlike specialization, tracing assumes the same path but not the same values"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#an-example-x-42",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#an-example-x-42",
    "title": "Dynamic Compilers",
    "section": "an example (x = 42)",
    "text": "an example (x = 42)\n\n\nfunction main(x){\n   y = x +1 \n   if x &lt;100 {\n      z = f(y)\n   } else {\n      z = g(y)\n   }\n   return z\n}\n\nfunction f(a){\n   return a -1 \n}\n\n\n\ny = x +1\nguard(x &lt; 100)\na = y\nz = a - 1\nreturn z\n\n\n\nguards at divergence, guards never return\noptimize assuming guards are true, ok to be slow if guard is false"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#move-guards-up",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#move-guards-up",
    "title": "Dynamic Compilers",
    "section": "move guards up",
    "text": "move guards up\nwhy is this a good idea?\n\n\nfail fast\nlonger region to optimize"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#use-local-value-numbering",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#use-local-value-numbering",
    "title": "Dynamic Compilers",
    "section": "use local value numbering",
    "text": "use local value numbering\n\n\n\nguard(x &lt; 100)\ny = x + 1\na = y\nz = a - 1\nreturn z\n\n\n\nguard(x &lt; 100)\nreturn x"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#how-do-this-in-bril",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#how-do-this-in-bril",
    "title": "Dynamic Compilers",
    "section": "how do this in Bril?",
    "text": "how do this in Bril?\n3 new operations (sort of like out-of-order instructions)\n\nspeculate - Enter a speculative execution context. No arguments.\ncommit - End the current speculative context, committing the current speculative state as the “real” state. No arguments.\nguard - Check a condition and possibly abort the current speculative context. One argument, the Boolean condition, and one label, to which control is transferred on abort."
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#speculate-extension",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#speculate-extension",
    "title": "Dynamic Compilers",
    "section": "speculate extension",
    "text": "speculate extension\nspeculative execution extension"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#example",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#example",
    "title": "Dynamic Compilers",
    "section": "example",
    "text": "example\nb: bool = const false;\nv: int = const 4; v == 4\nspeculate; v: int = const 2; v == 2 (speculate state) guard b .failed; v == 2 (speculate state) commit;\n.failed: print v; v == 4"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#implementation",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#implementation",
    "title": "Dynamic Compilers",
    "section": "implementation",
    "text": "implementation\nyou can add a tracer to an interpreter\nIn a lot of language environments you’ll have an interpreter that’s executing “op at a time”\nhook in a tracer which observes what the interpreter is doing and “make some machine code on the side” based on how the interpreter ran\nyou can implement just a subset of the operations [ed: you might call this property “compiler completeness” for your op set"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#common-bytecode-operations",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#common-bytecode-operations",
    "title": "Dynamic Compilers",
    "section": "common bytecode operations",
    "text": "common bytecode operations\nimplement only the common ones and simply end the trace when you hit one that was not implemented, because it was uncommon\nYou can build up this trace JIT-ing capability over time, because the system is built with this assumption you can bail out of the trace for whatever reason and go back to thr interpreter"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#an-example-1",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#an-example-1",
    "title": "Dynamic Compilers",
    "section": "an example",
    "text": "an example\nCould imagine making a JIT that just: Covered MULs and ADDs and could make fused/composite MUL/ADD bytecode combinations\nSpecialize that for one common type; e.g. if you have many types in your language, could support that just for integer types, or just for FP ops e.g. if it were numerical code, and then just bail if any other types showed up at runtime;\ntrace invariants: suppose traces call to other traces;\ntrace1 set of ops A, trace2 with set of ops B and we see a transfer from A to B\nmake sure that the assumptions between those two things are lining up – called trace fusion\nknow the invariants (i.e. “what must be true”) on the exit from A and the entry to B are lining up / compatible with each other"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#method-inlining",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#method-inlining",
    "title": "Dynamic Compilers",
    "section": "method inlining",
    "text": "method inlining\nIn trace compiler you just execute through methods\nInlining kind of the natural path of compilation when doing trace compilation – just linear execution where the jumps/calls/returns simply disappear"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#tail-duplication",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#tail-duplication",
    "title": "Dynamic Compilers",
    "section": "tail duplication",
    "text": "tail duplication\nit is common that multiple traces have a common tail\nfor() {\n  if op_Eq{\n     op1 \n   } else {\n      op2\n   }\n}\nop_t\nop_a\nop_i\nop_l\ntrace0:  op_eq quard\\true  op1 op_t op_a op_i op_l \n\ntrace1:              \\false op2 op_t op_a op_i op_l \ntwo traces with the same ending, could generate\none copy of the tail- with arguments showing the header\ntwo copies of the tail- frozen header"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#adding-traces-to-bril",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#adding-traces-to-bril",
    "title": "Dynamic Compilers",
    "section": "adding traces to bril",
    "text": "adding traces to bril\nHow to modify the reference interpreter (warning typescript!)\nbrili\nthere are two functions to consider\n\nevalFunc interprets a function by calling evalInstr on each instruction\nevalInstr interprets one instruction, large case statement for each instruction"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#print-instructions-as-they-execute",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#print-instructions-as-they-execute",
    "title": "Dynamic Compilers",
    "section": "print instructions as they execute",
    "text": "print instructions as they execute\n\nfigure out when to start and when to stop\nhow to print instructions (modify evalInstr by printing instructions) console.log(instr)\n\nyou have to optimize the trace and put it back"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#traces-and-users",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#traces-and-users",
    "title": "Dynamic Compilers",
    "section": "traces and users",
    "text": "traces and users\nwhen a trace works well- it looks amazing - it finds the inner loop and optimizes even through libraries\nbut users find in hard to understand what the compiler did,\na tiny source change can make a big trace change\nhard to fit in a debugger\nsecurity is a problem"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#pytorch-2.0",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#pytorch-2.0",
    "title": "Dynamic Compilers",
    "section": "pytorch 2.0",
    "text": "pytorch 2.0\nml frameworks have two modes"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#eager-mode",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#eager-mode",
    "title": "Dynamic Compilers",
    "section": "Eager Mode",
    "text": "Eager Mode\n\nPreferred by users\nEasier to use programming model\nEasy to debug\n\na + b + c executes two calls to torch.add (if they are tensors)\nno place to optimize, allows any kind of python, and any control flow\n\nPyTorch is a primarily an eager mode framework"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#graph-mode",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#graph-mode",
    "title": "Dynamic Compilers",
    "section": "Graph Mode",
    "text": "Graph Mode\n\nPreferred by backends and framework builders\nEasier to optimize with a compiler\nEasier to do automated transformations\n\nconstruct a graph with two add nodes and 3 input nodes, then execute the graph\neasy to optimize, only graph nodes allowed, no control flow\nMain optimization is fusing operations to avoid memory copies"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#how-does-the-compiler-fit",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#how-does-the-compiler-fit",
    "title": "Dynamic Compilers",
    "section": "how does the compiler fit",
    "text": "how does the compiler fit\nin Eager mode there is only a library - no compiler\nif you have a matmul followed by an activation function, it is up to the developer to notice that the memory traffic is more expensive then the activation and its up to the developer to know there is another pytorch call (2000 different calls) which does the combined operation and it is up to the developer to change the code\nif graph mode (compiler writers call this defered or late) the operations get recorded (not executed) and only get executed when we need the result"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#pytorchs-many-attempts-at-graph-modes",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#pytorchs-many-attempts-at-graph-modes",
    "title": "Dynamic Compilers",
    "section": "PyTorch’s Many Attempts at Graph Modes",
    "text": "PyTorch’s Many Attempts at Graph Modes\ntorch.jit.trace\n\nRecord + replay\nUnsound\nCan give incorrect results because it ignores Python part of program\n\ntorch. jit.script\n\nAOT parses Python into graph format\nOnly works on ~45% of real world models\nHigh effort to “TorchScript” models\nPyTorch Models Are Not Static Graphs\n\nPyTorch users write models where program graphs are impossible\nConvert tensors to native Python types (x.item(), x.tolist(), int(x), etc)\nUse other frameworks (numpy/xarray/etc) for part of their model\nData dependent Python control flow or other dynamism Exceptions, closures, generators, classes, etc\ntorch xla\ndefered execution. rather then do the graph operation, just save it and execute as late as possible\nvery slow, big performance cliffs"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#torch.compilemodel---converts-a-pytorch-eager-program-to-a-graph",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#torch.compilemodel---converts-a-pytorch-eager-program-to-a-graph",
    "title": "Dynamic Compilers",
    "section": "torch.compile(model) - converts a pytorch eager program to a graph",
    "text": "torch.compile(model) - converts a pytorch eager program to a graph\ntorch.dynamo - which dynamically captures Python code execution and creates a static computational graph.\ntorch.Inductor- compiler that optimimzes static computation graphs"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#dynamo",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#dynamo",
    "title": "Dynamic Compilers",
    "section": "dynamo",
    "text": "dynamo\nimport torch\nfrom typing import List\n\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\n\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my compiler() called with fx graph\")\n    gm.graph.print_tabular()\n    return gm\n\n@torch.compile(backend=my_compiler)\ndef toy_example(a,b):\n    x = a / (torch.abs(a)+1)\n    if b.sum() &lt; 0:\n        b  = b * -1\n    return x *b\n\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#output",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#output",
    "title": "Dynamic Compilers",
    "section": "output",
    "text": "output\n\n\ndef toy_example(a,b):\n    x = a / (torch.abs(a)+1)\n    if b.sum() &lt; 0:\n        b  = b * -1\n    return x *b\n\nopcode         name    target        args      \n------       ------  ---------    -----------\nplaceholder    l_a_    L_a_         ()         \nplaceholder    l_b_    L_b_         ()         \ncall_function  abs_1   &lt;abs&gt;        (l_a_,)   \ncall_function  add     &lt;add&gt;        (abs_1, 1) \ncall_function  x       &lt;truediv&gt;    (l_a_, add)\ncall_method    sum_1   sum          (l_b_,)   \ncall_function  lt      &lt;lt&gt;         (sum_1, 0) \noutput         output  output       ((x, lt),)\n------       ------  ---------    -----------\nplaceholder    l_b_    L_b_         ()           \nplaceholder    l_x_    L_x_         ()           \ncall_function  b       &lt;mul&gt;        (l_b_, -1)   \ncall_function  mul_1   &lt;mul&gt;        (l_x_, b)    \noutput         output  output       ((mul_1,),) \n------       ------  ---------    -----------        \nplaceholder    l_x_    L_x_         ()            \nplaceholder    l_b_    L_b_         ()            \ncall_function  mul     &lt;mul&gt;        (l_x_, l_b_)  \noutput         output  output       ((mul,),)"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#implementation-1",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#implementation-1",
    "title": "Dynamic Compilers",
    "section": "implementation",
    "text": "implementation\npython builds a frameObject (pointer to codeObject + arguments)\npasses this to eval\ncodeObject allows for extra user data and for a user function to be called between the frameObject and eval\nThis makes it easy to add a custom JIT\nsplit the function into two parts - the python part and the torch part\nThis is reused if the guards pass"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#pytorch-example-1",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#pytorch-example-1",
    "title": "Dynamic Compilers",
    "section": "pytorch example 1",
    "text": "pytorch example 1\ndef toy_example(a,b):\n    x = a / (torch.abs(a)+1)\n    if b.sum() &lt; 0:\n        b  = b * -1\n    return x *b\n\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\nsometimes the sum is negative but not always"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#implementation-2",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#implementation-2",
    "title": "Dynamic Compilers",
    "section": "implementation",
    "text": "implementation"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#graph",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#graph",
    "title": "Dynamic Compilers",
    "section": "graph",
    "text": "graph\n\n\n\n\n\n\n\n\n\nopcode\nname\ntarget\nargs\n\n\n\n\nplaceholder\nl_a_\nL_a_\n()\n\n\nplaceholder\nl_b_\nL_b_\n()\n\n\ncall_function\nabs_1\n&lt;built-in method abs of type object at 0x728736add8a0&gt;\n(l_a_,)\n\n\ncall_function\nadd\n\n(abs_1, 1)\n\n\ncall_function\nx\n\n(l_a_, add)\n\n\ncall_method\nsum_1\nsum\n(l_b_,)\n\n\ncall_function\nlt\n\n(sum_1, 0)\n\n\noutput\noutput\noutput\n((x, lt),)"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#code",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#code",
    "title": "Dynamic Compilers",
    "section": "code",
    "text": "code\ndef toy_example(a,b):\n   (x,lt) = call1(a,b)\n   if lt:\n      f1(b,x)\n   else:\n      f2(b,x)\n\ndef f1(b, x):\n   b  = b * -1\n   return x *b\n\ndef f2(b,x):\n   return x *b"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#guards",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#guards",
    "title": "Dynamic Compilers",
    "section": "guards:",
    "text": "guards:\ncheck_tensor(L[‘a’], Tensor, torch.float32, size=[10], stride=[1]) check_tensor(L[‘b’], Tensor, torch.float32, size=[10], stride=[1])\nwalk the byte code again for f1\nb  = b * -1\nreturn x *b"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#traced-graph",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#traced-graph",
    "title": "Dynamic Compilers",
    "section": "TRACED GRAPH",
    "text": "TRACED GRAPH\n\n\n\npcode n\name t\narget a\nrgs k\nwargs\n\n\n\n\nplaceholder\nl_b_\nL_b_\n()\n{}\n\n\nplaceholder\nl_x_\nL_x_\n()\n{}\n\n\ncall_function\nb\n\n(l_b_, -1)\n{}\n\n\ncall_function\nmul_1\n\n(l_x_, b)\n{}\n\n\nutput o\nutput o\nutput (\n(mul_1,),) {\n}"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#other-branch",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#other-branch",
    "title": "Dynamic Compilers",
    "section": "other branch",
    "text": "other branch\nguards: check_tensor(L['b'], torch.float32, size=[10], stride=[1]) \n        check_tensor(L['x'], torch.float32, size=[10], stride=[1]) \nTRACED GRAPH\n\n\n\npcode n\name t\narget a\nrgs k\nwarg\n\n\n\n\naceholder l_\nx_ L_\nx_ ()\n{}\n\n\n\naceholder l_\nb_ L_\nb_ ()\n{}\n\n\n\nll_function mu\nl &lt;b\nuilt-in function mul&gt; (l\nx, l_b_) {}\n\n\n\ntput ou\ntput ou\ntput ((\nmul,),) {}\n\n\n\n\ncheck_tensor(L[‘b’], torch.float32, size=[10], stride=[1]) check_tensor(L[‘x’], torch.float32, size=[10], stride=[1]) # return x *b # mp/ipykernel_1179164/26"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#linear-traces",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#linear-traces",
    "title": "Dynamic Compilers",
    "section": "linear traces",
    "text": "linear traces\nDynamo removes all control flow, if/else, loops, exceptions\nspecializes (bakes in) all non-tensor objects (numbers, strings, classes )\n\n\n\n@torch.compile\ndef fn(f,n):\n  y = x ** 2\n  if n &gt;= 0:\n    return (n +1)* y\n  else:\n    return x /y \n\nx = torch.randn(200)\nfn(x,2)\n\ndef forward(l_x_: torch.Tensor):\n   y = l_x_ ** 2\n   mul 3*x\n   return (mul,)"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#special-cases",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#special-cases",
    "title": "Dynamic Compilers",
    "section": "special cases",
    "text": "special cases\nTrace integers symbolically\nby default it specilizes on every integer in the graph but if a subsequent vall the value changes it traces symbolically but 0 or 1 are always speciaized"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#multiple-traces",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#multiple-traces",
    "title": "Dynamic Compilers",
    "section": "multiple traces",
    "text": "multiple traces\nimport torch\nfrom typing import List\n\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\n\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my compiler() called with fx graph\")\n    gm.graph.print_tabular()\n    return gm\n\n@torch.compile(backend=my_compiler)\ndef toy_example(a,b):\n    x = a / (torch.abs(a)+1)\n    if b.sum() &lt; 0:\n        b  = b * -1\n    return x *b\n\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\n\n\nimport torch\nfrom typing import List\n\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\n\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my compiler() called with fx graph\")\n    gm.graph.print_tabular()\n    return gm"
  },
  {
    "objectID": "lectures/revealjs_13_dynamic_compilers.qmd.html#implementation-3",
    "href": "lectures/revealjs_13_dynamic_compilers.qmd.html#implementation-3",
    "title": "Dynamic Compilers",
    "section": "implementation",
    "text": "implementation\npep 523, allows python function to see unevaluated frames, function + arguments\nnormally just calls the function"
  },
  {
    "objectID": "lectures/01a_performance_measurement.html",
    "href": "lectures/01a_performance_measurement.html",
    "title": "Performance and Measurement",
    "section": "",
    "text": "Producing Wrong Data Without Doing Anything Obviously Wrong! Todd Mytkowicz, Amer Diwan, Matthias Hauswirth, and Peter F. Sweeney. ASPLOS 2009.\n445 references\n\n\nMeasurement bias is significant\nChanging aspects of an experimental setup can introduce measurement bias. ​ Measurement bias is unpredictable and there are no obvious ways to avoid it. ​ Prior work in computer system evaluation does not adequately consider measurement bias. ​\nThe paper discusses two techniques for dealing with measurement bias: experimental setup randomization and causal analysis. ​\nMeasurement bias occurs for all benchmarks and architectures. ​\nMeasurement bias due to link order can significantly fluctuate conclusions. ​\nMeasurement bias due to UNIX environment size can lead to conflicting conclusions. ​\nTo avoid measurement bias, it is important to use diverse evaluation workloads, randomize the experimental setup, conduct causal analysis, and collect more information from hardware manufacturers. ​ —\n\n\nA sample blog post about this paper blog\n\n\n\nStrangely, Matrix Multiplications on GPUs Run Faster When Given “Predictable” Data!\nSIGPLAN Empirical Evaluation Guidelines\n\n\n\ndata\nA dataset with 2,185 CPUs and 2,668 GPUs to help researchers understand the development trend of CPUs and GPUs. Setup by Kaggle\n\n# library & dataset\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv('images/chip_dataset.csv')\nprint(df.head())\n\nsns.set_palette(\"pastel\")\n\nsns.violinplot(x=df[\"Vendor\"], y=np.log(df[\"Freq (MHz)\"]), hue=df['Type'])\n\n   Unnamed: 0                  Product Type Release Date  Process Size (nm)  \\\n0           0      AMD Athlon 64 3500+  CPU   2007-02-20               65.0   \n1           1         AMD Athlon 200GE  CPU   2018-09-06               14.0   \n2           2     Intel Core i5-1145G7  CPU   2020-09-02               10.0   \n3           3    Intel Xeon E5-2603 v2  CPU   2013-09-01               22.0   \n4           4  AMD Phenom II X4 980 BE  CPU   2011-05-03               45.0   \n\n   TDP (W)  Die Size (mm^2)  Transistors (million)  Freq (MHz)  Foundry  \\\n0     45.0             77.0                  122.0      2200.0  Unknown   \n1     35.0            192.0                 4800.0      3200.0  Unknown   \n2     28.0              NaN                    NaN      2600.0    Intel   \n3     80.0            160.0                 1400.0      1800.0    Intel   \n4    125.0            258.0                  758.0      3700.0  Unknown   \n\n  Vendor  FP16 GFLOPS  FP32 GFLOPS  FP64 GFLOPS  \n0    AMD          NaN          NaN          NaN  \n1    AMD          NaN          NaN          NaN  \n2  Intel          NaN          NaN          NaN  \n3  Intel          NaN          NaN          NaN  \n4    AMD          NaN          NaN          NaN  \n\n\n\n\n\n\n\n\n\napply np.log to the Freq (MHz) column. Because of wide range of values\nSet a pastel palette with sns.set_palette(“pastel”). These colors make it easier to see the parts of the violin plot\nHue Parameter: I’m using the hue parameter to differentiate between types of chips. Hue sets a color within the palette\nData Source and Preparation: Include a brief note on where the data comes from (you’ve provided a link, but a sentence or two summarizing the dataset would be helpful) and any preprocessing steps taken before visualization.\nI might want to take date into account in these plots\n\nA violin plot shows density curves. The width is the approximate frequency of data points at that value\nBest for comparing distributions\nconsider ordering the groups\nThe details\n\nthe white dot represents the median\nthe thick gray bar in the center represents the inter-quartile range\nthe thin gray line represents the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the inter-quartile range.\nOn each side of the gray line is a kernel density estimation to show the distribution shape of the data. Wider sections of the violin plot represent a higher probability that members of the population will take on the given value; the skinnier sections represent a lower probability.\n\n\n\n\n\nprint(df.describe())\n\nimport pandas as pd\n\n# Assuming df is your DataFrame and 'Release Date' is the column\ndf['Release Date'] = pd.to_datetime(df['Release Date'], errors='coerce')\ndf['Release Year'] = ((df['Release Date'].dt.year) // 5) * 5\n\n# Now df['Release Year'] contains the year extracted from 'Release Date'\n\n\n# plot a bar chart\nax = sns.barplot(x=df['Release Year'], y=df[\"TDP (W)\"], hue =df['Type'], estimator=np.mean, errorbar=(\"sd\"))\n\n        Unnamed: 0  Process Size (nm)      TDP (W)  Die Size (mm^2)  \\\ncount  4854.000000        4845.000000  4228.000000      4139.000000   \nmean   2426.500000          55.109598    81.359981       188.440445   \nstd    1401.373433          44.998676    76.807808       126.189383   \nmin       0.000000           0.000000     1.000000         1.000000   \n25%    1213.250000          22.000000    33.000000       104.000000   \n50%    2426.500000          40.000000    65.000000       148.000000   \n75%    3639.750000          90.000000   100.000000       239.000000   \nmax    4853.000000         250.000000   900.000000       826.000000   \n\n       Transistors (million)   Freq (MHz)    FP16 GFLOPS   FP32 GFLOPS  \\\ncount            4143.000000  4854.000000     536.000000   1948.000000   \nmean             1929.922279  1484.406057    8397.459851   2134.756653   \nstd              4044.891098  1066.701523   13799.551131   3898.431487   \nmin                 8.000000   100.000000      10.020000     12.800000   \n25%               154.000000   590.000000     768.800000    257.300000   \n50%               624.000000  1073.500000    2965.500000    696.000000   \n75%              1550.000000  2400.000000   10600.000000   2116.750000   \nmax             54200.000000  4700.000000  184600.000000  40000.000000   \n\n        FP64 GFLOPS  \ncount   1306.000000  \nmean     363.670511  \nstd     1145.931856  \nmin        3.600000  \n25%       38.295000  \n50%       89.280000  \n75%      220.000000  \nmax    11540.000000",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Performance and Measurement"
    ]
  },
  {
    "objectID": "lectures/01a_performance_measurement.html#example-1",
    "href": "lectures/01a_performance_measurement.html#example-1",
    "title": "Performance and Measurement",
    "section": "",
    "text": "Producing Wrong Data Without Doing Anything Obviously Wrong! Todd Mytkowicz, Amer Diwan, Matthias Hauswirth, and Peter F. Sweeney. ASPLOS 2009.\n445 references\n\n\nMeasurement bias is significant\nChanging aspects of an experimental setup can introduce measurement bias. ​ Measurement bias is unpredictable and there are no obvious ways to avoid it. ​ Prior work in computer system evaluation does not adequately consider measurement bias. ​\nThe paper discusses two techniques for dealing with measurement bias: experimental setup randomization and causal analysis. ​\nMeasurement bias occurs for all benchmarks and architectures. ​\nMeasurement bias due to link order can significantly fluctuate conclusions. ​\nMeasurement bias due to UNIX environment size can lead to conflicting conclusions. ​\nTo avoid measurement bias, it is important to use diverse evaluation workloads, randomize the experimental setup, conduct causal analysis, and collect more information from hardware manufacturers. ​ —\n\n\nA sample blog post about this paper blog",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Performance and Measurement"
    ]
  },
  {
    "objectID": "lectures/01a_performance_measurement.html#another-example",
    "href": "lectures/01a_performance_measurement.html#another-example",
    "title": "Performance and Measurement",
    "section": "",
    "text": "Strangely, Matrix Multiplications on GPUs Run Faster When Given “Predictable” Data!\nSIGPLAN Empirical Evaluation Guidelines",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Performance and Measurement"
    ]
  },
  {
    "objectID": "lectures/01a_performance_measurement.html#violin-plots",
    "href": "lectures/01a_performance_measurement.html#violin-plots",
    "title": "Performance and Measurement",
    "section": "",
    "text": "data\nA dataset with 2,185 CPUs and 2,668 GPUs to help researchers understand the development trend of CPUs and GPUs. Setup by Kaggle\n\n# library & dataset\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv('images/chip_dataset.csv')\nprint(df.head())\n\nsns.set_palette(\"pastel\")\n\nsns.violinplot(x=df[\"Vendor\"], y=np.log(df[\"Freq (MHz)\"]), hue=df['Type'])\n\n   Unnamed: 0                  Product Type Release Date  Process Size (nm)  \\\n0           0      AMD Athlon 64 3500+  CPU   2007-02-20               65.0   \n1           1         AMD Athlon 200GE  CPU   2018-09-06               14.0   \n2           2     Intel Core i5-1145G7  CPU   2020-09-02               10.0   \n3           3    Intel Xeon E5-2603 v2  CPU   2013-09-01               22.0   \n4           4  AMD Phenom II X4 980 BE  CPU   2011-05-03               45.0   \n\n   TDP (W)  Die Size (mm^2)  Transistors (million)  Freq (MHz)  Foundry  \\\n0     45.0             77.0                  122.0      2200.0  Unknown   \n1     35.0            192.0                 4800.0      3200.0  Unknown   \n2     28.0              NaN                    NaN      2600.0    Intel   \n3     80.0            160.0                 1400.0      1800.0    Intel   \n4    125.0            258.0                  758.0      3700.0  Unknown   \n\n  Vendor  FP16 GFLOPS  FP32 GFLOPS  FP64 GFLOPS  \n0    AMD          NaN          NaN          NaN  \n1    AMD          NaN          NaN          NaN  \n2  Intel          NaN          NaN          NaN  \n3  Intel          NaN          NaN          NaN  \n4    AMD          NaN          NaN          NaN  \n\n\n\n\n\n\n\n\n\napply np.log to the Freq (MHz) column. Because of wide range of values\nSet a pastel palette with sns.set_palette(“pastel”). These colors make it easier to see the parts of the violin plot\nHue Parameter: I’m using the hue parameter to differentiate between types of chips. Hue sets a color within the palette\nData Source and Preparation: Include a brief note on where the data comes from (you’ve provided a link, but a sentence or two summarizing the dataset would be helpful) and any preprocessing steps taken before visualization.\nI might want to take date into account in these plots\n\nA violin plot shows density curves. The width is the approximate frequency of data points at that value\nBest for comparing distributions\nconsider ordering the groups\nThe details\n\nthe white dot represents the median\nthe thick gray bar in the center represents the inter-quartile range\nthe thin gray line represents the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the inter-quartile range.\nOn each side of the gray line is a kernel density estimation to show the distribution shape of the data. Wider sections of the violin plot represent a higher probability that members of the population will take on the given value; the skinnier sections represent a lower probability.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Performance and Measurement"
    ]
  },
  {
    "objectID": "lectures/01a_performance_measurement.html#error-bars",
    "href": "lectures/01a_performance_measurement.html#error-bars",
    "title": "Performance and Measurement",
    "section": "",
    "text": "print(df.describe())\n\nimport pandas as pd\n\n# Assuming df is your DataFrame and 'Release Date' is the column\ndf['Release Date'] = pd.to_datetime(df['Release Date'], errors='coerce')\ndf['Release Year'] = ((df['Release Date'].dt.year) // 5) * 5\n\n# Now df['Release Year'] contains the year extracted from 'Release Date'\n\n\n# plot a bar chart\nax = sns.barplot(x=df['Release Year'], y=df[\"TDP (W)\"], hue =df['Type'], estimator=np.mean, errorbar=(\"sd\"))\n\n        Unnamed: 0  Process Size (nm)      TDP (W)  Die Size (mm^2)  \\\ncount  4854.000000        4845.000000  4228.000000      4139.000000   \nmean   2426.500000          55.109598    81.359981       188.440445   \nstd    1401.373433          44.998676    76.807808       126.189383   \nmin       0.000000           0.000000     1.000000         1.000000   \n25%    1213.250000          22.000000    33.000000       104.000000   \n50%    2426.500000          40.000000    65.000000       148.000000   \n75%    3639.750000          90.000000   100.000000       239.000000   \nmax    4853.000000         250.000000   900.000000       826.000000   \n\n       Transistors (million)   Freq (MHz)    FP16 GFLOPS   FP32 GFLOPS  \\\ncount            4143.000000  4854.000000     536.000000   1948.000000   \nmean             1929.922279  1484.406057    8397.459851   2134.756653   \nstd              4044.891098  1066.701523   13799.551131   3898.431487   \nmin                 8.000000   100.000000      10.020000     12.800000   \n25%               154.000000   590.000000     768.800000    257.300000   \n50%               624.000000  1073.500000    2965.500000    696.000000   \n75%              1550.000000  2400.000000   10600.000000   2116.750000   \nmax             54200.000000  4700.000000  184600.000000  40000.000000   \n\n        FP64 GFLOPS  \ncount   1306.000000  \nmean     363.670511  \nstd     1145.931856  \nmin        3.600000  \n25%       38.295000  \n50%       89.280000  \n75%      220.000000  \nmax    11540.000000",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Performance and Measurement"
    ]
  },
  {
    "objectID": "lectures/revealjs_01a_performance_measurement.qmd.html#example-1",
    "href": "lectures/revealjs_01a_performance_measurement.qmd.html#example-1",
    "title": "Performance and Measurement",
    "section": "example 1",
    "text": "example 1\nProducing Wrong Data Without Doing Anything Obviously Wrong! Todd Mytkowicz, Amer Diwan, Matthias Hauswirth, and Peter F. Sweeney. ASPLOS 2009.\n445 references"
  },
  {
    "objectID": "lectures/revealjs_01a_performance_measurement.qmd.html#another-example",
    "href": "lectures/revealjs_01a_performance_measurement.qmd.html#another-example",
    "title": "Performance and Measurement",
    "section": "another example",
    "text": "another example\nStrangely, Matrix Multiplications on GPUs Run Faster When Given “Predictable” Data!\nSIGPLAN Empirical Evaluation Guidelines"
  },
  {
    "objectID": "lectures/revealjs_01a_performance_measurement.qmd.html#violin-plots",
    "href": "lectures/revealjs_01a_performance_measurement.qmd.html#violin-plots",
    "title": "Performance and Measurement",
    "section": "violin plots",
    "text": "violin plots\ndata\nA dataset with 2,185 CPUs and 2,668 GPUs to help researchers understand the development trend of CPUs and GPUs. Setup by Kaggle\n\n\nCode\n# library & dataset\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv('images/chip_dataset.csv')\nprint(df.head())\n\nsns.set_palette(\"pastel\")\n\nsns.violinplot(x=df[\"Vendor\"], y=np.log(df[\"Freq (MHz)\"]), hue=df['Type'])\n\n\n   Unnamed: 0                  Product Type Release Date  Process Size (nm)  \\\n0           0      AMD Athlon 64 3500+  CPU   2007-02-20               65.0   \n1           1         AMD Athlon 200GE  CPU   2018-09-06               14.0   \n2           2     Intel Core i5-1145G7  CPU   2020-09-02               10.0   \n3           3    Intel Xeon E5-2603 v2  CPU   2013-09-01               22.0   \n4           4  AMD Phenom II X4 980 BE  CPU   2011-05-03               45.0   \n\n   TDP (W)  Die Size (mm^2)  Transistors (million)  Freq (MHz)  Foundry  \\\n0     45.0             77.0                  122.0      2200.0  Unknown   \n1     35.0            192.0                 4800.0      3200.0  Unknown   \n2     28.0              NaN                    NaN      2600.0    Intel   \n3     80.0            160.0                 1400.0      1800.0    Intel   \n4    125.0            258.0                  758.0      3700.0  Unknown   \n\n  Vendor  FP16 GFLOPS  FP32 GFLOPS  FP64 GFLOPS  \n0    AMD          NaN          NaN          NaN  \n1    AMD          NaN          NaN          NaN  \n2  Intel          NaN          NaN          NaN  \n3  Intel          NaN          NaN          NaN  \n4    AMD          NaN          NaN          NaN  \n\n\n\n\n\n\n\n\n\napply np.log to the Freq (MHz) column. Because of wide range of values\nSet a pastel palette with sns.set_palette(“pastel”). These colors make it easier to see the parts of the violin plot\nHue Parameter: I’m using the hue parameter to differentiate between types of chips. Hue sets a color within the palette\nData Source and Preparation: Include a brief note on where the data comes from (you’ve provided a link, but a sentence or two summarizing the dataset would be helpful) and any preprocessing steps taken before visualization.\nI might want to take date into account in these plots"
  },
  {
    "objectID": "lectures/revealjs_01a_performance_measurement.qmd.html#error-bars",
    "href": "lectures/revealjs_01a_performance_measurement.qmd.html#error-bars",
    "title": "Performance and Measurement",
    "section": "error bars",
    "text": "error bars\n\n\nCode\nprint(df.describe())\n\nimport pandas as pd\n\n# Assuming df is your DataFrame and 'Release Date' is the column\ndf['Release Date'] = pd.to_datetime(df['Release Date'], errors='coerce')\ndf['Release Year'] = ((df['Release Date'].dt.year) // 5) * 5\n\n# Now df['Release Year'] contains the year extracted from 'Release Date'\n\n\n# plot a bar chart\nax = sns.barplot(x=df['Release Year'], y=df[\"TDP (W)\"], hue =df['Type'], estimator=np.mean, errorbar=(\"sd\"))\n\n\n        Unnamed: 0  Process Size (nm)      TDP (W)  Die Size (mm^2)  \\\ncount  4854.000000        4845.000000  4228.000000      4139.000000   \nmean   2426.500000          55.109598    81.359981       188.440445   \nstd    1401.373433          44.998676    76.807808       126.189383   \nmin       0.000000           0.000000     1.000000         1.000000   \n25%    1213.250000          22.000000    33.000000       104.000000   \n50%    2426.500000          40.000000    65.000000       148.000000   \n75%    3639.750000          90.000000   100.000000       239.000000   \nmax    4853.000000         250.000000   900.000000       826.000000   \n\n       Transistors (million)   Freq (MHz)    FP16 GFLOPS   FP32 GFLOPS  \\\ncount            4143.000000  4854.000000     536.000000   1948.000000   \nmean             1929.922279  1484.406057    8397.459851   2134.756653   \nstd              4044.891098  1066.701523   13799.551131   3898.431487   \nmin                 8.000000   100.000000      10.020000     12.800000   \n25%               154.000000   590.000000     768.800000    257.300000   \n50%               624.000000  1073.500000    2965.500000    696.000000   \n75%              1550.000000  2400.000000   10600.000000   2116.750000   \nmax             54200.000000  4700.000000  184600.000000  40000.000000   \n\n        FP64 GFLOPS  \ncount   1306.000000  \nmean     363.670511  \nstd     1145.931856  \nmin        3.600000  \n25%       38.295000  \n50%       89.280000  \n75%      220.000000  \nmax    11540.000000"
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#reminder-hw1-due-on-friday--i-expected-some-questions",
    "href": "lectures/revealjs_03_local.qmd.html#reminder-hw1-due-on-friday--i-expected-some-questions",
    "title": "Local Analysis & Optimization",
    "section": "reminder hw1 due on Friday- I expected some questions?",
    "text": "reminder hw1 due on Friday- I expected some questions?\nand anyone who forgot hw0, still needed!"
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#llvm-control-flow-graph",
    "href": "lectures/revealjs_03_local.qmd.html#llvm-control-flow-graph",
    "title": "Local Analysis & Optimization",
    "section": "llvm control flow graph",
    "text": "llvm control flow graph\ncommands to draw a dot list of a c file from llvm\n clang -S -emit-llvm images/identity.c -o foo.ll \n opt -dot-cfg foo.ll -disable-output -enable-new-pm=0\nvoid identity(int **a, int N)\n{\n    int i, j;\n    for (i = 0; i &lt; N; i++)\n    {\n        for (j = 0; j &lt; N; j++)\n        {\n            a[i][j] = 0;\n        }\n    }\n    for (i = 0; i &lt; N; i++)\n    {\n        a[i][i] = 1;\n    }\n}\n\n\nCode\ndigraph \"CFG for 'identity' function\" {\n    label=\"CFG for 'identity' function\";\n\n    Node0x12c5490 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#88abfd70\",label=\"{%2:\\l  %3 = alloca i32**, align 8\\l  %4 = alloca i32, align 4\\l  %5 = alloca i32, align 4\\l  %6 = alloca i32, align 4\\l  store i32** %0, i32*** %3, align 8\\l  store i32 %1, i32* %4, align 4\\l  store i32 0, i32* %5, align 4\\l  br label %7\\l}\"];\n    Node0x12c5490 -&gt; Node0x12c5da0;\n    Node0x12c5da0 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%7:\\l7:                                                \\l  %8 = load i32, i32* %5, align 4\\l  %9 = load i32, i32* %4, align 4\\l  %10 = icmp slt i32 %8, %9\\l  br i1 %10, label %11, label %32\\l|{&lt;s0&gt;T|&lt;s1&gt;F}}\"];\n    Node0x12c5da0:s0 -&gt; Node0x12c5c70;\n    Node0x12c5da0:s1 -&gt; Node0x12c5f40;\n    Node0x12c5c70 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%11:\\l11:                                               \\l  store i32 0, i32* %6, align 4\\l  br label %12\\l}\"];\n    Node0x12c5c70 -&gt; Node0x12c6080;\n    Node0x12c6080 [shape=record,color=\"#b70d28ff\", style=filled, fillcolor=\"#b70d2870\",label=\"{%12:\\l12:                                               \\l  %13 = load i32, i32* %6, align 4\\l  %14 = load i32, i32* %4, align 4\\l  %15 = icmp slt i32 %13, %14\\l  br i1 %15, label %16, label %28\\l|{&lt;s0&gt;T|&lt;s1&gt;F}}\"];\n    Node0x12c6080:s0 -&gt; Node0x12c62b0;\n    Node0x12c6080:s1 -&gt; Node0x12c6300;\n    Node0x12c62b0 [shape=record,color=\"#b70d28ff\", style=filled, fillcolor=\"#b70d2870\",label=\"{%16:\\l16:                                               \\l  %17 = load i32**, i32*** %3, align 8\\l  %18 = load i32, i32* %5, align 4\\l  %19 = sext i32 %18 to i64\\l  %20 = getelementptr inbounds i32*, i32** %17, i64 %19\\l  %21 = load i32*, i32** %20, align 8\\l  %22 = load i32, i32* %6, align 4\\l  %23 = sext i32 %22 to i64\\l  %24 = getelementptr inbounds i32, i32* %21, i64 %23\\l  store i32 0, i32* %24, align 4\\l  br label %25\\l}\"];\n    Node0x12c62b0 -&gt; Node0x12c6820;\n    Node0x12c6820 [shape=record,color=\"#b70d28ff\", style=filled, fillcolor=\"#b70d2870\",label=\"{%25:\\l25:                                               \\l  %26 = load i32, i32* %6, align 4\\l  %27 = add nsw i32 %26, 1\\l  store i32 %27, i32* %6, align 4\\l  br label %12, !llvm.loop !6\\l}\"];\n    Node0x12c6820 -&gt; Node0x12c6080;\n    Node0x12c6300 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%28:\\l28:                                               \\l  br label %29\\l}\"];\n    Node0x12c6300 -&gt; Node0x12c75b0;\n    Node0x12c75b0 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%29:\\l29:                                               \\l  %30 = load i32, i32* %5, align 4\\l  %31 = add nsw i32 %30, 1\\l  store i32 %31, i32* %5, align 4\\l  br label %7, !llvm.loop !8\\l}\"];\n    Node0x12c75b0 -&gt; Node0x12c5da0;\n    Node0x12c5f40 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#88abfd70\",label=\"{%32:\\l32:                                               \\l  store i32 0, i32* %5, align 4\\l  br label %33\\l}\"];\n    Node0x12c5f40 -&gt; Node0x12c7bd0;\n    Node0x12c7bd0 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%33:\\l33:                                               \\l  %34 = load i32, i32* %5, align 4\\l  %35 = load i32, i32* %4, align 4\\l  %36 = icmp slt i32 %34, %35\\l  br i1 %36, label %37, label %49\\l|{&lt;s0&gt;T|&lt;s1&gt;F}}\"];\n    Node0x12c7bd0:s0 -&gt; Node0x12c7e00;\n    Node0x12c7bd0:s1 -&gt; Node0x12c7e50;\n    Node0x12c7e00 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%37:\\l37:                                               \\l  %38 = load i32**, i32*** %3, align 8\\l  %39 = load i32, i32* %5, align 4\\l  %40 = sext i32 %39 to i64\\l  %41 = getelementptr inbounds i32*, i32** %38, i64 %40\\l  %42 = load i32*, i32** %41, align 8\\l  %43 = load i32, i32* %5, align 4\\l  %44 = sext i32 %43 to i64\\l  %45 = getelementptr inbounds i32, i32* %42, i64 %44\\l  store i32 1, i32* %45, align 4\\l  br label %46\\l}\"];\n    Node0x12c7e00 -&gt; Node0x12c8400;\n    Node0x12c8400 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%46:\\l46:                                               \\l  %47 = load i32, i32* %5, align 4\\l  %48 = add nsw i32 %47, 1\\l  store i32 %48, i32* %5, align 4\\l  br label %33, !llvm.loop !9\\l}\"];\n    Node0x12c8400 -&gt; Node0x12c7bd0;\n    Node0x12c7e50 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#88abfd70\",label=\"{%49:\\l49:                                               \\l  ret void\\l}\"];\n}\n\n\n\n\n\n\n\nCFG for 'identity' function\n\nCFG for 'identity' function\n\n\nNode0x12c5490\n\n%2:\n %3 = alloca i32**, align 8\n %4 = alloca i32, align 4\n %5 = alloca i32, align 4\n %6 = alloca i32, align 4\n store i32** %0, i32*** %3, align 8\n store i32 %1, i32* %4, align 4\n store i32 0, i32* %5, align 4\n br label %7\n\n\n\nNode0x12c5da0\n\n%7:\n7: \n %8 = load i32, i32* %5, align 4\n %9 = load i32, i32* %4, align 4\n %10 = icmp slt i32 %8, %9\n br i1 %10, label %11, label %32\n\nT\n\nF\n\n\n\nNode0x12c5490-&gt;Node0x12c5da0\n\n\n\n\n\nNode0x12c5c70\n\n%11:\n11: \n store i32 0, i32* %6, align 4\n br label %12\n\n\n\nNode0x12c5da0:s0-&gt;Node0x12c5c70\n\n\n\n\n\nNode0x12c5f40\n\n%32:\n32: \n store i32 0, i32* %5, align 4\n br label %33\n\n\n\nNode0x12c5da0:s1-&gt;Node0x12c5f40\n\n\n\n\n\nNode0x12c6080\n\n%12:\n12: \n %13 = load i32, i32* %6, align 4\n %14 = load i32, i32* %4, align 4\n %15 = icmp slt i32 %13, %14\n br i1 %15, label %16, label %28\n\nT\n\nF\n\n\n\nNode0x12c5c70-&gt;Node0x12c6080\n\n\n\n\n\nNode0x12c7bd0\n\n%33:\n33: \n %34 = load i32, i32* %5, align 4\n %35 = load i32, i32* %4, align 4\n %36 = icmp slt i32 %34, %35\n br i1 %36, label %37, label %49\n\nT\n\nF\n\n\n\nNode0x12c5f40-&gt;Node0x12c7bd0\n\n\n\n\n\nNode0x12c62b0\n\n%16:\n16: \n %17 = load i32**, i32*** %3, align 8\n %18 = load i32, i32* %5, align 4\n %19 = sext i32 %18 to i64\n %20 = getelementptr inbounds i32*, i32** %17, i64 %19\n %21 = load i32*, i32** %20, align 8\n %22 = load i32, i32* %6, align 4\n %23 = sext i32 %22 to i64\n %24 = getelementptr inbounds i32, i32* %21, i64 %23\n store i32 0, i32* %24, align 4\n br label %25\n\n\n\nNode0x12c6080:s0-&gt;Node0x12c62b0\n\n\n\n\n\nNode0x12c6300\n\n%28:\n28: \n br label %29\n\n\n\nNode0x12c6080:s1-&gt;Node0x12c6300\n\n\n\n\n\nNode0x12c6820\n\n%25:\n25: \n %26 = load i32, i32* %6, align 4\n %27 = add nsw i32 %26, 1\n store i32 %27, i32* %6, align 4\n br label %12, !llvm.loop !6\n\n\n\nNode0x12c62b0-&gt;Node0x12c6820\n\n\n\n\n\nNode0x12c75b0\n\n%29:\n29: \n %30 = load i32, i32* %5, align 4\n %31 = add nsw i32 %30, 1\n store i32 %31, i32* %5, align 4\n br label %7, !llvm.loop !8\n\n\n\nNode0x12c6300-&gt;Node0x12c75b0\n\n\n\n\n\nNode0x12c6820-&gt;Node0x12c6080\n\n\n\n\n\nNode0x12c75b0-&gt;Node0x12c5da0\n\n\n\n\n\nNode0x12c7e00\n\n%37:\n37: \n %38 = load i32**, i32*** %3, align 8\n %39 = load i32, i32* %5, align 4\n %40 = sext i32 %39 to i64\n %41 = getelementptr inbounds i32*, i32** %38, i64 %40\n %42 = load i32*, i32** %41, align 8\n %43 = load i32, i32* %5, align 4\n %44 = sext i32 %43 to i64\n %45 = getelementptr inbounds i32, i32* %42, i64 %44\n store i32 1, i32* %45, align 4\n br label %46\n\n\n\nNode0x12c7bd0:s0-&gt;Node0x12c7e00\n\n\n\n\n\nNode0x12c7e50\n\n%49:\n49: \n ret void\n\n\n\nNode0x12c7bd0:s1-&gt;Node0x12c7e50\n\n\n\n\n\nNode0x12c8400\n\n%46:\n46: \n %47 = load i32, i32* %5, align 4\n %48 = add nsw i32 %47, 1\n store i32 %48, i32* %5, align 4\n br label %33, !llvm.loop !9\n\n\n\nNode0x12c7e00-&gt;Node0x12c8400\n\n\n\n\n\nNode0x12c8400-&gt;Node0x12c7bd0"
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#flavors-of-optimization",
    "href": "lectures/revealjs_03_local.qmd.html#flavors-of-optimization",
    "title": "Local Analysis & Optimization",
    "section": "flavors of optimization",
    "text": "flavors of optimization\nI want to separate 3 flavors of optimization.\n\nlocal meaning within one basic block\nglobal meaning within one function (not really global)\ninter-procedural over the entire program"
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#definition-1--dead-if-instruction-writes-a-variable-and-the-variable-is-never-used.",
    "href": "lectures/revealjs_03_local.qmd.html#definition-1--dead-if-instruction-writes-a-variable-and-the-variable-is-never-used.",
    "title": "Local Analysis & Optimization",
    "section": "Definition 1- Dead if instruction writes a variable and the variable is never used.",
    "text": "Definition 1- Dead if instruction writes a variable and the variable is never used.\nAn instruction that has side-effects, like a print statement does not write a variable so it never gets deleted. Labels do not write a variable so they do not get deleted as well."
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#definition-2--dead-if-instruction-writes-a-variable-and-the-variable-is-either-never-used-or-only-used-in-dead-instructions.",
    "href": "lectures/revealjs_03_local.qmd.html#definition-2--dead-if-instruction-writes-a-variable-and-the-variable-is-either-never-used-or-only-used-in-dead-instructions.",
    "title": "Local Analysis & Optimization",
    "section": "Definition 2- Dead if instruction writes a variable and the variable is either never used or only used in dead instructions.",
    "text": "Definition 2- Dead if instruction writes a variable and the variable is either never used or only used in dead instructions."
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#iterating-till-convergence",
    "href": "lectures/revealjs_03_local.qmd.html#iterating-till-convergence",
    "title": "Local Analysis & Optimization",
    "section": "iterating till convergence",
    "text": "iterating till convergence\nwhile changes:\n       run one pass of tdce above"
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#what-would-be-faster-what-is-some-pseudo-code-for-the-change",
    "href": "lectures/revealjs_03_local.qmd.html#what-would-be-faster-what-is-some-pseudo-code-for-the-change",
    "title": "Local Analysis & Optimization",
    "section": "what would be faster? What is some pseudo code for the change",
    "text": "what would be faster? What is some pseudo code for the change\n\n  find all the variables that are used in more then one block\n  for each block b \n     used = all variables used in more then one block\n     walk backwards over the instruction in the block\n     for each instruction is dest in used?\n        yes - remove dest from used, add arguments to used \n        no  - instruction is dead \n\nfinding all the variables used in more then one block might be expensive"
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#definition-an-instruction-is-dead-if-that-instruction-writes-a-variable-v-and-no-path-starting-at-that-instruction-reaches-a-use-of-v",
    "href": "lectures/revealjs_03_local.qmd.html#definition-an-instruction-is-dead-if-that-instruction-writes-a-variable-v-and-no-path-starting-at-that-instruction-reaches-a-use-of-v",
    "title": "Local Analysis & Optimization",
    "section": "Definition? An instruction is dead if that instruction writes a variable v and no path starting at that instruction reaches a use of v",
    "text": "Definition? An instruction is dead if that instruction writes a variable v and no path starting at that instruction reaches a use of v\nthis talks about paths (control flow paths)\n@main {\n  a: int = const 4;\n     br input .then .else \n  .then\n  a: int = const 200;\n  .else \n  print a;\n}"
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#for-now-we-want-to-skip-control-flow",
    "href": "lectures/revealjs_03_local.qmd.html#for-now-we-want-to-skip-control-flow",
    "title": "Local Analysis & Optimization",
    "section": "for now we want to skip control flow",
    "text": "for now we want to skip control flow"
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#definition-an-instruction-is-dead-if-that-instruction-writes-a-variable-v-and-no-path-within-the-block-starting-at-that-instruction-reaches-a-use-of-v-in-the-same-block-or-reaches-the-exit-of-the-block",
    "href": "lectures/revealjs_03_local.qmd.html#definition-an-instruction-is-dead-if-that-instruction-writes-a-variable-v-and-no-path-within-the-block-starting-at-that-instruction-reaches-a-use-of-v-in-the-same-block-or-reaches-the-exit-of-the-block",
    "title": "Local Analysis & Optimization",
    "section": "Definition: An instruction is dead if that instruction writes a variable v and no path within the block starting at that instruction reaches a use of v in the same block or reaches the exit of the block",
    "text": "Definition: An instruction is dead if that instruction writes a variable v and no path within the block starting at that instruction reaches a use of v in the same block or reaches the exit of the block"
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#testing",
    "href": "lectures/revealjs_03_local.qmd.html#testing",
    "title": "Local Analysis & Optimization",
    "section": "testing",
    "text": "testing\nbril2json &lt; bench.bril | python3 tdce.py | bril2txt\nNext, try using wc to check static code size differences:\nbril2json &lt; bench.bril | wc -l\nbril2json &lt; bench.bril | python3 tdce.py | wc -l\nThen profiling to measure dynamic instruction count: The bril interpreter has a flag -p which prints the number of dynamically executed instructions.\nHow good a measure is this for real programs?"
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#using-trunt-golden-images",
    "href": "lectures/revealjs_03_local.qmd.html#using-trunt-golden-images",
    "title": "Local Analysis & Optimization",
    "section": "using trunt (golden images)",
    "text": "using trunt (golden images)\n\nConfigure. Decide what command you want to test. Make a turnt.toml config file and put command = “mycmd {filename}” in it to pass each test file as an argument to mycmd.\nTake a snapshot. Run turnt –save foo.bril. Execute mycmd foo.bril and save the standard output into foo.out.\n\nYou might want to take a look at this output to make sure it’s what you expect\n\nTest your work. Now that you have a test in place, keep working. Use turnt *.bril to run all your tests and confirm that the output still matches.\n\nIf there’s a mismatch, you can do turnt –diff to see the changes."
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#peephole-optimizations",
    "href": "lectures/revealjs_03_local.qmd.html#peephole-optimizations",
    "title": "Local Analysis & Optimization",
    "section": "peephole optimizations",
    "text": "peephole optimizations\n1.Peephole optimizations are a category of local code optimizations.\n1. The principle is very simple: a. the optimizer analyzes sequences of instructions. –  a. only code that is within a small window of instructions is analyzed each time.\na. this window slides over the code. a. once patterns are discovered inside this window, optimizations are applied."
  },
  {
    "objectID": "lectures/revealjs_03_local.qmd.html#some-examples",
    "href": "lectures/revealjs_03_local.qmd.html#some-examples",
    "title": "Local Analysis & Optimization",
    "section": "some examples",
    "text": "some examples\nredundant loads and stores\nm = load r0 store m in r0\nbranch transformations\n’’’ if debug ==1 go to l1 go to l2 l1: l2:\ntransforms to \nif debug !=1 goto l2 l1: l2: ```\nreduction in strength\n4*x =&gt; x &lt;&lt; 2\nspecial machine idioms"
  },
  {
    "objectID": "lectures/03_local.html",
    "href": "lectures/03_local.html",
    "title": "Local Analysis & Optimization",
    "section": "",
    "text": "and anyone who forgot hw0, still needed!",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#reminder-hw1-due-on-friday--i-expected-some-questions",
    "href": "lectures/03_local.html#reminder-hw1-due-on-friday--i-expected-some-questions",
    "title": "Local Analysis & Optimization",
    "section": "",
    "text": "and anyone who forgot hw0, still needed!",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#llvm-control-flow-graph",
    "href": "lectures/03_local.html#llvm-control-flow-graph",
    "title": "Local Analysis & Optimization",
    "section": "llvm control flow graph",
    "text": "llvm control flow graph\ncommands to draw a dot list of a c file from llvm\n clang -S -emit-llvm images/identity.c -o foo.ll \n opt -dot-cfg foo.ll -disable-output -enable-new-pm=0\nvoid identity(int **a, int N)\n{\n    int i, j;\n    for (i = 0; i &lt; N; i++)\n    {\n        for (j = 0; j &lt; N; j++)\n        {\n            a[i][j] = 0;\n        }\n    }\n    for (i = 0; i &lt; N; i++)\n    {\n        a[i][i] = 1;\n    }\n}\n\ndigraph \"CFG for 'identity' function\" {\n    label=\"CFG for 'identity' function\";\n\n    Node0x12c5490 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#88abfd70\",label=\"{%2:\\l  %3 = alloca i32**, align 8\\l  %4 = alloca i32, align 4\\l  %5 = alloca i32, align 4\\l  %6 = alloca i32, align 4\\l  store i32** %0, i32*** %3, align 8\\l  store i32 %1, i32* %4, align 4\\l  store i32 0, i32* %5, align 4\\l  br label %7\\l}\"];\n    Node0x12c5490 -&gt; Node0x12c5da0;\n    Node0x12c5da0 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%7:\\l7:                                                \\l  %8 = load i32, i32* %5, align 4\\l  %9 = load i32, i32* %4, align 4\\l  %10 = icmp slt i32 %8, %9\\l  br i1 %10, label %11, label %32\\l|{&lt;s0&gt;T|&lt;s1&gt;F}}\"];\n    Node0x12c5da0:s0 -&gt; Node0x12c5c70;\n    Node0x12c5da0:s1 -&gt; Node0x12c5f40;\n    Node0x12c5c70 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%11:\\l11:                                               \\l  store i32 0, i32* %6, align 4\\l  br label %12\\l}\"];\n    Node0x12c5c70 -&gt; Node0x12c6080;\n    Node0x12c6080 [shape=record,color=\"#b70d28ff\", style=filled, fillcolor=\"#b70d2870\",label=\"{%12:\\l12:                                               \\l  %13 = load i32, i32* %6, align 4\\l  %14 = load i32, i32* %4, align 4\\l  %15 = icmp slt i32 %13, %14\\l  br i1 %15, label %16, label %28\\l|{&lt;s0&gt;T|&lt;s1&gt;F}}\"];\n    Node0x12c6080:s0 -&gt; Node0x12c62b0;\n    Node0x12c6080:s1 -&gt; Node0x12c6300;\n    Node0x12c62b0 [shape=record,color=\"#b70d28ff\", style=filled, fillcolor=\"#b70d2870\",label=\"{%16:\\l16:                                               \\l  %17 = load i32**, i32*** %3, align 8\\l  %18 = load i32, i32* %5, align 4\\l  %19 = sext i32 %18 to i64\\l  %20 = getelementptr inbounds i32*, i32** %17, i64 %19\\l  %21 = load i32*, i32** %20, align 8\\l  %22 = load i32, i32* %6, align 4\\l  %23 = sext i32 %22 to i64\\l  %24 = getelementptr inbounds i32, i32* %21, i64 %23\\l  store i32 0, i32* %24, align 4\\l  br label %25\\l}\"];\n    Node0x12c62b0 -&gt; Node0x12c6820;\n    Node0x12c6820 [shape=record,color=\"#b70d28ff\", style=filled, fillcolor=\"#b70d2870\",label=\"{%25:\\l25:                                               \\l  %26 = load i32, i32* %6, align 4\\l  %27 = add nsw i32 %26, 1\\l  store i32 %27, i32* %6, align 4\\l  br label %12, !llvm.loop !6\\l}\"];\n    Node0x12c6820 -&gt; Node0x12c6080;\n    Node0x12c6300 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%28:\\l28:                                               \\l  br label %29\\l}\"];\n    Node0x12c6300 -&gt; Node0x12c75b0;\n    Node0x12c75b0 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%29:\\l29:                                               \\l  %30 = load i32, i32* %5, align 4\\l  %31 = add nsw i32 %30, 1\\l  store i32 %31, i32* %5, align 4\\l  br label %7, !llvm.loop !8\\l}\"];\n    Node0x12c75b0 -&gt; Node0x12c5da0;\n    Node0x12c5f40 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#88abfd70\",label=\"{%32:\\l32:                                               \\l  store i32 0, i32* %5, align 4\\l  br label %33\\l}\"];\n    Node0x12c5f40 -&gt; Node0x12c7bd0;\n    Node0x12c7bd0 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%33:\\l33:                                               \\l  %34 = load i32, i32* %5, align 4\\l  %35 = load i32, i32* %4, align 4\\l  %36 = icmp slt i32 %34, %35\\l  br i1 %36, label %37, label %49\\l|{&lt;s0&gt;T|&lt;s1&gt;F}}\"];\n    Node0x12c7bd0:s0 -&gt; Node0x12c7e00;\n    Node0x12c7bd0:s1 -&gt; Node0x12c7e50;\n    Node0x12c7e00 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%37:\\l37:                                               \\l  %38 = load i32**, i32*** %3, align 8\\l  %39 = load i32, i32* %5, align 4\\l  %40 = sext i32 %39 to i64\\l  %41 = getelementptr inbounds i32*, i32** %38, i64 %40\\l  %42 = load i32*, i32** %41, align 8\\l  %43 = load i32, i32* %5, align 4\\l  %44 = sext i32 %43 to i64\\l  %45 = getelementptr inbounds i32, i32* %42, i64 %44\\l  store i32 1, i32* %45, align 4\\l  br label %46\\l}\"];\n    Node0x12c7e00 -&gt; Node0x12c8400;\n    Node0x12c8400 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#f3c7b170\",label=\"{%46:\\l46:                                               \\l  %47 = load i32, i32* %5, align 4\\l  %48 = add nsw i32 %47, 1\\l  store i32 %48, i32* %5, align 4\\l  br label %33, !llvm.loop !9\\l}\"];\n    Node0x12c8400 -&gt; Node0x12c7bd0;\n    Node0x12c7e50 [shape=record,color=\"#3d50c3ff\", style=filled, fillcolor=\"#88abfd70\",label=\"{%49:\\l49:                                               \\l  ret void\\l}\"];\n}\n\n\n\n\n\n\nCFG for 'identity' function\n\nCFG for 'identity' function\n\n\nNode0x12c5490\n\n%2:\n %3 = alloca i32**, align 8\n %4 = alloca i32, align 4\n %5 = alloca i32, align 4\n %6 = alloca i32, align 4\n store i32** %0, i32*** %3, align 8\n store i32 %1, i32* %4, align 4\n store i32 0, i32* %5, align 4\n br label %7\n\n\n\nNode0x12c5da0\n\n%7:\n7: \n %8 = load i32, i32* %5, align 4\n %9 = load i32, i32* %4, align 4\n %10 = icmp slt i32 %8, %9\n br i1 %10, label %11, label %32\n\nT\n\nF\n\n\n\nNode0x12c5490-&gt;Node0x12c5da0\n\n\n\n\n\nNode0x12c5c70\n\n%11:\n11: \n store i32 0, i32* %6, align 4\n br label %12\n\n\n\nNode0x12c5da0:s0-&gt;Node0x12c5c70\n\n\n\n\n\nNode0x12c5f40\n\n%32:\n32: \n store i32 0, i32* %5, align 4\n br label %33\n\n\n\nNode0x12c5da0:s1-&gt;Node0x12c5f40\n\n\n\n\n\nNode0x12c6080\n\n%12:\n12: \n %13 = load i32, i32* %6, align 4\n %14 = load i32, i32* %4, align 4\n %15 = icmp slt i32 %13, %14\n br i1 %15, label %16, label %28\n\nT\n\nF\n\n\n\nNode0x12c5c70-&gt;Node0x12c6080\n\n\n\n\n\nNode0x12c7bd0\n\n%33:\n33: \n %34 = load i32, i32* %5, align 4\n %35 = load i32, i32* %4, align 4\n %36 = icmp slt i32 %34, %35\n br i1 %36, label %37, label %49\n\nT\n\nF\n\n\n\nNode0x12c5f40-&gt;Node0x12c7bd0\n\n\n\n\n\nNode0x12c62b0\n\n%16:\n16: \n %17 = load i32**, i32*** %3, align 8\n %18 = load i32, i32* %5, align 4\n %19 = sext i32 %18 to i64\n %20 = getelementptr inbounds i32*, i32** %17, i64 %19\n %21 = load i32*, i32** %20, align 8\n %22 = load i32, i32* %6, align 4\n %23 = sext i32 %22 to i64\n %24 = getelementptr inbounds i32, i32* %21, i64 %23\n store i32 0, i32* %24, align 4\n br label %25\n\n\n\nNode0x12c6080:s0-&gt;Node0x12c62b0\n\n\n\n\n\nNode0x12c6300\n\n%28:\n28: \n br label %29\n\n\n\nNode0x12c6080:s1-&gt;Node0x12c6300\n\n\n\n\n\nNode0x12c6820\n\n%25:\n25: \n %26 = load i32, i32* %6, align 4\n %27 = add nsw i32 %26, 1\n store i32 %27, i32* %6, align 4\n br label %12, !llvm.loop !6\n\n\n\nNode0x12c62b0-&gt;Node0x12c6820\n\n\n\n\n\nNode0x12c75b0\n\n%29:\n29: \n %30 = load i32, i32* %5, align 4\n %31 = add nsw i32 %30, 1\n store i32 %31, i32* %5, align 4\n br label %7, !llvm.loop !8\n\n\n\nNode0x12c6300-&gt;Node0x12c75b0\n\n\n\n\n\nNode0x12c6820-&gt;Node0x12c6080\n\n\n\n\n\nNode0x12c75b0-&gt;Node0x12c5da0\n\n\n\n\n\nNode0x12c7e00\n\n%37:\n37: \n %38 = load i32**, i32*** %3, align 8\n %39 = load i32, i32* %5, align 4\n %40 = sext i32 %39 to i64\n %41 = getelementptr inbounds i32*, i32** %38, i64 %40\n %42 = load i32*, i32** %41, align 8\n %43 = load i32, i32* %5, align 4\n %44 = sext i32 %43 to i64\n %45 = getelementptr inbounds i32, i32* %42, i64 %44\n store i32 1, i32* %45, align 4\n br label %46\n\n\n\nNode0x12c7bd0:s0-&gt;Node0x12c7e00\n\n\n\n\n\nNode0x12c7e50\n\n%49:\n49: \n ret void\n\n\n\nNode0x12c7bd0:s1-&gt;Node0x12c7e50\n\n\n\n\n\nNode0x12c8400\n\n%46:\n46: \n %47 = load i32, i32* %5, align 4\n %48 = add nsw i32 %47, 1\n store i32 %48, i32* %5, align 4\n br label %33, !llvm.loop !9\n\n\n\nNode0x12c7e00-&gt;Node0x12c8400\n\n\n\n\n\nNode0x12c8400-&gt;Node0x12c7bd0",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#flavors-of-optimization",
    "href": "lectures/03_local.html#flavors-of-optimization",
    "title": "Local Analysis & Optimization",
    "section": "flavors of optimization",
    "text": "flavors of optimization\nI want to separate 3 flavors of optimization.\n\nlocal meaning within one basic block\nglobal meaning within one function (not really global)\ninter-procedural over the entire program\n\n\nUsually an optimization takes time that is more then linear in some property, For example a local optimization might take time \\(n^2\\) in the number of instructions in the block. a global optimization might take much longer, and an inter-procedural longer still. To keep compile time reasonable many compilers limit the number of global optimizations and skip inter-procedural optimizations. As a consequence many more optimizations get published but not used in production.\n\nWhen would running an optimization speedup compilation?\nFor a local optimization, instructions within a block are ordered, so it makes sense to talk about instructions coming before or after others.\nFor a global optimization, two instructions are ordered by a path from one block to another and different paths through the program give different orders.\n\nOne special case is JIT (just in time) compilers, where programs get compiled at the start of execution. GPU compilers (and java compilers) look like this. They may use run-time information to decide of recompiling a function is a good idea. This is called Hotspot compiling. Some JIT compilers use hot/cold compiling, where they only run the fancy compiler on basic blocks that are hot , i.e., execute a lot.\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\nflowchart LR\nA[application] -- offline --&gt; B[byte code/ptx]\nB --&gt; C[quick run time compiler/ finalizer]\nC --&gt; D[isa]\nB --&gt; C1[fancy compiler - only run on long running functions];\nC1 --&gt; D;\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\nflowchart LR\nA[application] -- offline --&gt; B[byte code/ptx]\nB --&gt; C[quick run time compiler/ finalizer]\nC --&gt; D[isa]\nB --&gt; C1[fancy compiler - only run on long running functions];\nC1 --&gt; D;\n\n\n\n\n\n\n\n\nWe are going to consider several versions of trivial dead code elimination. Trivial because we are going to hold off on control flow related optimizations till later. Sometimes people call this DCE or trivial DCE.\n\nFor each case, we start by defining what we mean by dead code.\nexample 1\n@main {\n  a: int = const 4;\n  b: int = const 2;\n  c: int = const 1;\n  d: int = add a b;\n  print d;\n}\nWhat instruction is dead? (meaning get the same answer if we delete the instruction) What is your definition? Is this meaning of dead code local or global?\n\nWhy would you ever have dead code in a program? One reason is that have DCE as a separate pass means other optimizations do not have to clean up.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#definition-1--dead-if-instruction-writes-a-variable-and-the-variable-is-never-used.",
    "href": "lectures/03_local.html#definition-1--dead-if-instruction-writes-a-variable-and-the-variable-is-never-used.",
    "title": "Local Analysis & Optimization",
    "section": "Definition 1- Dead if instruction writes a variable and the variable is never used.",
    "text": "Definition 1- Dead if instruction writes a variable and the variable is never used.\nAn instruction that has side-effects, like a print statement does not write a variable so it never gets deleted. Labels do not write a variable so they do not get deleted as well.\n\nWhat is the pseudo code to find dead instructions using this definition?\n. . .\nused = empty set \nfor instr in func \n   used += instr.args \nfor instd in func\n    if instr has a dest and dest in not in used \n       delete instr\n\nexample 2\n@main {\n  a: int = const 4;\n  b: int = const 2;\n  c: int = const 1;  \n  d: int = add a b;\n  e: int = add c d; \n  print d;\n}\n. . .\nThe code so far only deletes one instruction, but we would like to get rid of two. Instruction c should also be dead. How do we change the definition",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#definition-2--dead-if-instruction-writes-a-variable-and-the-variable-is-either-never-used-or-only-used-in-dead-instructions.",
    "href": "lectures/03_local.html#definition-2--dead-if-instruction-writes-a-variable-and-the-variable-is-either-never-used-or-only-used-in-dead-instructions.",
    "title": "Local Analysis & Optimization",
    "section": "Definition 2- Dead if instruction writes a variable and the variable is either never used or only used in dead instructions.",
    "text": "Definition 2- Dead if instruction writes a variable and the variable is either never used or only used in dead instructions.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#iterating-till-convergence",
    "href": "lectures/03_local.html#iterating-till-convergence",
    "title": "Local Analysis & Optimization",
    "section": "iterating till convergence",
    "text": "iterating till convergence\nwhile changes:\n       run one pass of tdce above",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#what-would-be-faster-what-is-some-pseudo-code-for-the-change",
    "href": "lectures/03_local.html#what-would-be-faster-what-is-some-pseudo-code-for-the-change",
    "title": "Local Analysis & Optimization",
    "section": "what would be faster? What is some pseudo code for the change",
    "text": "what would be faster? What is some pseudo code for the change\n. . .\n  find all the variables that are used in more then one block\n  for each block b \n     used = all variables used in more then one block\n     walk backwards over the instruction in the block\n     for each instruction is dest in used?\n        yes - remove dest from used, add arguments to used \n        no  - instruction is dead \n\nfinding all the variables used in more then one block might be expensive\n\nexample 3\n@main {\n  a: int = const 4;\n  a: int = const 200;\n  print a;\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#definition-an-instruction-is-dead-if-that-instruction-writes-a-variable-v-and-no-path-starting-at-that-instruction-reaches-a-use-of-v",
    "href": "lectures/03_local.html#definition-an-instruction-is-dead-if-that-instruction-writes-a-variable-v-and-no-path-starting-at-that-instruction-reaches-a-use-of-v",
    "title": "Local Analysis & Optimization",
    "section": "Definition? An instruction is dead if that instruction writes a variable v and no path starting at that instruction reaches a use of v",
    "text": "Definition? An instruction is dead if that instruction writes a variable v and no path starting at that instruction reaches a use of v\nthis talks about paths (control flow paths)\n@main {\n  a: int = const 4;\n     br input .then .else \n  .then\n  a: int = const 200;\n  .else \n  print a;\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#for-now-we-want-to-skip-control-flow",
    "href": "lectures/03_local.html#for-now-we-want-to-skip-control-flow",
    "title": "Local Analysis & Optimization",
    "section": "for now we want to skip control flow",
    "text": "for now we want to skip control flow",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#definition-an-instruction-is-dead-if-that-instruction-writes-a-variable-v-and-no-path-within-the-block-starting-at-that-instruction-reaches-a-use-of-v-in-the-same-block-or-reaches-the-exit-of-the-block",
    "href": "lectures/03_local.html#definition-an-instruction-is-dead-if-that-instruction-writes-a-variable-v-and-no-path-within-the-block-starting-at-that-instruction-reaches-a-use-of-v-in-the-same-block-or-reaches-the-exit-of-the-block",
    "title": "Local Analysis & Optimization",
    "section": "Definition: An instruction is dead if that instruction writes a variable v and no path within the block starting at that instruction reaches a use of v in the same block or reaches the exit of the block",
    "text": "Definition: An instruction is dead if that instruction writes a variable v and no path within the block starting at that instruction reaches a use of v in the same block or reaches the exit of the block\n\ncands are the variables that are defined but not used \nlast_def = {}  variables -&gt; instructions \nthis is a mapping variables that have been defined but not used\n\n   for instr in block:\n      each arg (use) removes arg from last def \n      if the instr has a dest \n          if the dest is in last_def, \n      add dest-&gt;instr to last def\n  \nand as you might expect, we need to iterate this till convergence\n\nCompilers often run dce more then once- why?\n\ntesting out dce\n\nprogram should get the same answer\nprogram should run less instructions\n\n\nSome test cases:\n\nsimple.bril,\nreassign.bril,\nother examples in the DCE test directory",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#testing",
    "href": "lectures/03_local.html#testing",
    "title": "Local Analysis & Optimization",
    "section": "testing",
    "text": "testing\nbril2json &lt; bench.bril | python3 tdce.py | bril2txt\nNext, try using wc to check static code size differences:\nbril2json &lt; bench.bril | wc -l\nbril2json &lt; bench.bril | python3 tdce.py | wc -l\nThen profiling to measure dynamic instruction count: The bril interpreter has a flag -p which prints the number of dynamically executed instructions.\nHow good a measure is this for real programs?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#using-trunt-golden-images",
    "href": "lectures/03_local.html#using-trunt-golden-images",
    "title": "Local Analysis & Optimization",
    "section": "using trunt (golden images)",
    "text": "using trunt (golden images)\n\nConfigure. Decide what command you want to test. Make a turnt.toml config file and put command = “mycmd {filename}” in it to pass each test file as an argument to mycmd.\nTake a snapshot. Run turnt –save foo.bril. Execute mycmd foo.bril and save the standard output into foo.out.\n\nYou might want to take a look at this output to make sure it’s what you expect\n\nTest your work. Now that you have a test in place, keep working. Use turnt *.bril to run all your tests and confirm that the output still matches.\n\nIf there’s a mismatch, you can do turnt –diff to see the changes.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#peephole-optimizations",
    "href": "lectures/03_local.html#peephole-optimizations",
    "title": "Local Analysis & Optimization",
    "section": "peephole optimizations",
    "text": "peephole optimizations\n1.Peephole optimizations are a category of local code optimizations.\n1. The principle is very simple: a. the optimizer analyzes sequences of instructions. –  a. only code that is within a small window of instructions is analyzed each time.\na. this window slides over the code. a. once patterns are discovered inside this window, optimizations are applied.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/03_local.html#some-examples",
    "href": "lectures/03_local.html#some-examples",
    "title": "Local Analysis & Optimization",
    "section": "some examples",
    "text": "some examples\n\nredundant loads and stores\nm = load r0 store m in r0\n\n\nbranch transformations\n’’’ if debug ==1 go to l1 go to l2 l1: l2:\ntransforms to \nif debug !=1 goto l2 l1: l2: ```\n\n\nreduction in strength\n4*x =&gt; x &lt;&lt; 2\n\n\nspecial machine idioms",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Local Analysis & Optimization"
    ]
  },
  {
    "objectID": "lectures/revealjs_01a2_performance_measurement.qmd.html#another-example",
    "href": "lectures/revealjs_01a2_performance_measurement.qmd.html#another-example",
    "title": "Performance and Measurement part 2",
    "section": "another example",
    "text": "another example\nStrangely, Matrix Multiplications on GPUs Run Faster When Given “Predictable” Data!\nSIGPLAN Empirical Evaluation Guidelines\nchecklist\nHere are the criteria by Berger, Blackburn, Hauswirth, and Hicks (2018):\n\nClearly stated claims\n\nExplicit Claims\nAppropriately-Scoped Claims\nAcknowledges Limitations\n\nSuitable Comparison\n\nAppropriate Baseline for Comparison\nFair Comparison\n\nPrincipled Benchmark Choice\n\nAppropriate Suite\nNon-Standard Suite(s) Justified\nApplications, Not (Just) Kernels\n\nAdequate Data Analysis\n\nSufficient Number of Trials\nAppropriate Summary Statistics\nReport Data Distribution\n\nRelevant Metrics\n\nDirect or Appropriate Proxy Metric\nMeasures All Important Effects\n\nAppropriate and Clear Experimental Design\n\nSufficient Information to Repeat\nReasonable Platform\nExplores Key Design Parameters\nOpen Loop in Workload Generator\nCross-Validation Where Needed\n\nPresentation of Results\n\nComprehensive Summary Results\nAxes Include Zero\nRatios Plotted Correctly\nAppropriate Level of Precision"
  },
  {
    "objectID": "lectures/01a2_performance_measurement.html",
    "href": "lectures/01a2_performance_measurement.html",
    "title": "Performance and Measurement part 2",
    "section": "",
    "text": "last time we looked at\nProducing Wrong Data Without Doing Anything Obviously Wrong! Todd Mytkowicz, Amer Diwan, Matthias Hauswirth, and Peter F. Sweeney. ASPLOS 2009.\n445 references\n\n\nMeasurement bias is significant\nChanging aspects of an experimental setup can introduce measurement bias. ​ Measurement bias is unpredictable and there are no obvious ways to avoid it. ​ Prior work in computer system evaluation does not adequately consider measurement bias. ​\nThe paper discusses two techniques for dealing with measurement bias: experimental setup randomization and causal analysis. ​\nMeasurement bias occurs for all benchmarks and architectures. ​\nMeasurement bias due to link order can significantly fluctuate conclusions. ​\nMeasurement bias due to UNIX environment size can lead to conflicting conclusions. ​\nTo avoid measurement bias, it is important to use diverse evaluation workloads, randomize the experimental setup, conduct causal analysis, and collect more information from hardware manufacturers. ​ —\n\n\nA sample blog post about this paper blog\n\n\nStrangely, Matrix Multiplications on GPUs Run Faster When Given “Predictable” Data!\nSIGPLAN Empirical Evaluation Guidelines\nchecklist\nHere are the criteria by Berger, Blackburn, Hauswirth, and Hicks (2018):\n\nClearly stated claims\n\nExplicit Claims\nAppropriately-Scoped Claims\nAcknowledges Limitations\n\nSuitable Comparison\n\nAppropriate Baseline for Comparison\nFair Comparison\n\nPrincipled Benchmark Choice\n\nAppropriate Suite\nNon-Standard Suite(s) Justified\nApplications, Not (Just) Kernels\n\nAdequate Data Analysis\n\nSufficient Number of Trials\nAppropriate Summary Statistics\nReport Data Distribution\n\nRelevant Metrics\n\nDirect or Appropriate Proxy Metric\nMeasures All Important Effects\n\nAppropriate and Clear Experimental Design\n\nSufficient Information to Repeat\nReasonable Platform\nExplores Key Design Parameters\nOpen Loop in Workload Generator\nCross-Validation Where Needed\n\nPresentation of Results\n\nComprehensive Summary Results\nAxes Include Zero\nRatios Plotted Correctly\nAppropriate Level of Precision",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Performance and Measurement part 2"
    ]
  },
  {
    "objectID": "lectures/01a2_performance_measurement.html#another-example",
    "href": "lectures/01a2_performance_measurement.html#another-example",
    "title": "Performance and Measurement part 2",
    "section": "",
    "text": "Strangely, Matrix Multiplications on GPUs Run Faster When Given “Predictable” Data!\nSIGPLAN Empirical Evaluation Guidelines\nchecklist\nHere are the criteria by Berger, Blackburn, Hauswirth, and Hicks (2018):\n\nClearly stated claims\n\nExplicit Claims\nAppropriately-Scoped Claims\nAcknowledges Limitations\n\nSuitable Comparison\n\nAppropriate Baseline for Comparison\nFair Comparison\n\nPrincipled Benchmark Choice\n\nAppropriate Suite\nNon-Standard Suite(s) Justified\nApplications, Not (Just) Kernels\n\nAdequate Data Analysis\n\nSufficient Number of Trials\nAppropriate Summary Statistics\nReport Data Distribution\n\nRelevant Metrics\n\nDirect or Appropriate Proxy Metric\nMeasures All Important Effects\n\nAppropriate and Clear Experimental Design\n\nSufficient Information to Repeat\nReasonable Platform\nExplores Key Design Parameters\nOpen Loop in Workload Generator\nCross-Validation Where Needed\n\nPresentation of Results\n\nComprehensive Summary Results\nAxes Include Zero\nRatios Plotted Correctly\nAppropriate Level of Precision",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Performance and Measurement part 2"
    ]
  },
  {
    "objectID": "lectures/110_whole_program.html",
    "href": "lectures/110_whole_program.html",
    "title": "11 Whole program",
    "section": "",
    "text": "Warning\n\n\n\nNot done yet\n\n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "11 Whole program"
    ]
  },
  {
    "objectID": "lectures/100_mlir.html",
    "href": "lectures/100_mlir.html",
    "title": "10 MLIR",
    "section": "",
    "text": "Warning\n\n\n\nNot done yet\n\n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "10 MLIR"
    ]
  },
  {
    "objectID": "lectures/revealjs_mlir.qmd.html#mlir-multi-level-intermediate-representation",
    "href": "lectures/revealjs_mlir.qmd.html#mlir-multi-level-intermediate-representation",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "MLIR: Multi-Level Intermediate Representation",
    "text": "MLIR: Multi-Level Intermediate Representation\nMLIR (Multi-Level Intermediate Representation) was introduced by Google in April 2019 and is designed to serve as an IR from the outset. It provides various forms:"
  },
  {
    "objectID": "lectures/revealjs_mlir.qmd.html#high-level-form",
    "href": "lectures/revealjs_mlir.qmd.html#high-level-form",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "High-level form",
    "text": "High-level form\nOps (general purpose to domain speciﬁc) on tensor types / memref types\n%patches = \"tf.reshape\"(%patches, %minus_one, %minor_dim_size)\n: (tensor&lt;? x ? x ? x ? x f32&gt;, index, index) −&gt; tensor&lt;? x ? x f32&gt;\n%mat_out = \"tf.matmul\"(%patches_ﬂat, %patches_ﬂat){transpose_a : true}\n: (tensor&lt;? x ? x f32&gt;, tensor&lt;? x ? x f32&gt;) −&gt; tensor&lt;? x ?\nx f32&gt;\n%vec_out = \"tf.reduce_sum\"(%patches_ﬂat) {axis: 0}\n: (tensor&lt;? x ? x f32&gt;) −&gt; tensor&lt;? x f32&gt;"
  },
  {
    "objectID": "lectures/revealjs_mlir.qmd.html#loop-level-mid-level-form",
    "href": "lectures/revealjs_mlir.qmd.html#loop-level-mid-level-form",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "Loop-level / mid-level form",
    "text": "Loop-level / mid-level form\naffine.for %i = 0 to 8 step 4 {\n  affine.for %j = 0 to 8 step 4 {\n    %5 = affine.load %arg0[%ii, %kk] : memref&lt;8x8xvector&lt;64xf32&gt;&gt;\n  }\n}"
  },
  {
    "objectID": "lectures/revealjs_mlir.qmd.html#low-level-form-closer-to-hardware",
    "href": "lectures/revealjs_mlir.qmd.html#low-level-form-closer-to-hardware",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "Low-level form: closer to hardware",
    "text": "Low-level form: closer to hardware\n%v1 = load %a[%i2, %i3] : memref&lt;256x64xvector&lt;16xf32&gt;&gt;\n%v3 = addf %v1, %v2 : vector&lt;16xf32&gt;\nstore %v3, %d[%i2, %i3] : memref&lt;256x64xvector&lt;16xf32&gt;&gt;"
  },
  {
    "objectID": "lectures/revealjs_mlir.qmd.html#design-principles",
    "href": "lectures/revealjs_mlir.qmd.html#design-principles",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "Design principles",
    "text": "Design principles\n\nTextual format\nAbility to represent code at multiple levels\nUniﬁed representation for all the levels\nFirst class abstractions for multi-dimensional arrays (tensors), loop nests, and more\nVery ﬂexible, extensible"
  },
  {
    "objectID": "lectures/revealjs_mlir.qmd.html#concepts",
    "href": "lectures/revealjs_mlir.qmd.html#concepts",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "concepts",
    "text": "concepts\n\nSSA, typed\nModule/Function/Block/Operation structure\nOperations can hold a “region” (a list of blocks)\n\nno phi nodes, basic blocks take arguments\n~~~{plaintext} func @condbr_simple() -&gt; (i32) { %cond = “foo”() : () -&gt; i1 %a = “bar”() : () -&gt; i32 %b = “bar”() : () -&gt; i64\n^bb1(%x : i32): %w = “foo_bar”(%x) : (i32) -&gt; i64 br ^bb2(%w: i64)\n^bb2(%y : i64): %z = “abc”(%y) : (i64) -&gt; i32 return %z : i32\n} ~~~"
  },
  {
    "objectID": "lectures/revealjs_mlir.qmd.html#operations",
    "href": "lectures/revealjs_mlir.qmd.html#operations",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "operations",
    "text": "operations\n\nalways have a name and source location\narbitrary number of ssa operands and results\nattributes - constant values\nregions"
  },
  {
    "objectID": "lectures/mlir.html",
    "href": "lectures/mlir.html",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "",
    "text": "#html: default",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Compilation Opportunities in MLIR"
    ]
  },
  {
    "objectID": "lectures/mlir.html#mlir-multi-level-intermediate-representation",
    "href": "lectures/mlir.html#mlir-multi-level-intermediate-representation",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "MLIR: Multi-Level Intermediate Representation",
    "text": "MLIR: Multi-Level Intermediate Representation\nMLIR (Multi-Level Intermediate Representation) was introduced by Google in April 2019 and is designed to serve as an IR from the outset. It provides various forms:",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Compilation Opportunities in MLIR"
    ]
  },
  {
    "objectID": "lectures/mlir.html#high-level-form",
    "href": "lectures/mlir.html#high-level-form",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "High-level form",
    "text": "High-level form\nOps (general purpose to domain speciﬁc) on tensor types / memref types\n%patches = \"tf.reshape\"(%patches, %minus_one, %minor_dim_size)\n: (tensor&lt;? x ? x ? x ? x f32&gt;, index, index) −&gt; tensor&lt;? x ? x f32&gt;\n%mat_out = \"tf.matmul\"(%patches_ﬂat, %patches_ﬂat){transpose_a : true}\n: (tensor&lt;? x ? x f32&gt;, tensor&lt;? x ? x f32&gt;) −&gt; tensor&lt;? x ?\nx f32&gt;\n%vec_out = \"tf.reduce_sum\"(%patches_ﬂat) {axis: 0}\n: (tensor&lt;? x ? x f32&gt;) −&gt; tensor&lt;? x f32&gt;",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Compilation Opportunities in MLIR"
    ]
  },
  {
    "objectID": "lectures/mlir.html#loop-level-mid-level-form",
    "href": "lectures/mlir.html#loop-level-mid-level-form",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "Loop-level / mid-level form",
    "text": "Loop-level / mid-level form\naffine.for %i = 0 to 8 step 4 {\n  affine.for %j = 0 to 8 step 4 {\n    %5 = affine.load %arg0[%ii, %kk] : memref&lt;8x8xvector&lt;64xf32&gt;&gt;\n  }\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Compilation Opportunities in MLIR"
    ]
  },
  {
    "objectID": "lectures/mlir.html#low-level-form-closer-to-hardware",
    "href": "lectures/mlir.html#low-level-form-closer-to-hardware",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "Low-level form: closer to hardware",
    "text": "Low-level form: closer to hardware\n%v1 = load %a[%i2, %i3] : memref&lt;256x64xvector&lt;16xf32&gt;&gt;\n%v3 = addf %v1, %v2 : vector&lt;16xf32&gt;\nstore %v3, %d[%i2, %i3] : memref&lt;256x64xvector&lt;16xf32&gt;&gt;",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Compilation Opportunities in MLIR"
    ]
  },
  {
    "objectID": "lectures/mlir.html#design-principles",
    "href": "lectures/mlir.html#design-principles",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "Design principles",
    "text": "Design principles\n\nTextual format\nAbility to represent code at multiple levels\nUniﬁed representation for all the levels\nFirst class abstractions for multi-dimensional arrays (tensors), loop nests, and more\nVery ﬂexible, extensible",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Compilation Opportunities in MLIR"
    ]
  },
  {
    "objectID": "lectures/mlir.html#concepts",
    "href": "lectures/mlir.html#concepts",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "concepts",
    "text": "concepts\n\nSSA, typed\nModule/Function/Block/Operation structure\nOperations can hold a “region” (a list of blocks)\n\nno phi nodes, basic blocks take arguments\n~~~{plaintext} func @condbr_simple() -&gt; (i32) { %cond = “foo”() : () -&gt; i1 %a = “bar”() : () -&gt; i32 %b = “bar”() : () -&gt; i64\n^bb1(%x : i32): %w = “foo_bar”(%x) : (i32) -&gt; i64 br ^bb2(%w: i64)\n^bb2(%y : i64): %z = “abc”(%y) : (i64) -&gt; i32 return %z : i32\n} ~~~",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Compilation Opportunities in MLIR"
    ]
  },
  {
    "objectID": "lectures/mlir.html#operations",
    "href": "lectures/mlir.html#operations",
    "title": "Polyhedral Compilation Opportunities in MLIR",
    "section": "operations",
    "text": "operations\n\nalways have a name and source location\narbitrary number of ssa operands and results\nattributes - constant values\nregions",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Compilation Opportunities in MLIR"
    ]
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#homework",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#homework",
    "title": "Compiler Overview",
    "section": "Homework",
    "text": "Homework\nMost homework\n\nwe talk about some algorithm using pseudo\nYou implement that algorithm\nYou write up a blog post explaining what happened\nIf you like up to 3 people can submit a homework\n\nSince most programming languages have a json library, you can do homework in any language you like."
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#readings",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#readings",
    "title": "Compiler Overview",
    "section": "Readings",
    "text": "Readings\nwe are going to critically read current research papers\n\nEach person leads a paper discussion (up to two people can sign up as a team to present the same paper)\nEveryone reads the paper; the leader goes over the contents pros and cons\nThe leader writes a blog post, (possibly including discussion insights )\nblog is due one week after the presentation.\nI recommend that people pair up (two people going over a paper before hand is a lot easier)\nI listed a lot of papers, but if there is a different paper you want to present let me know"
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#project",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#project",
    "title": "Compiler Overview",
    "section": "Project",
    "text": "Project\nEverybody gets to do a project, which is compiler related\n\nyou will need to get a proposal approved half way through the term\nyou submit a blog reporting on what happened\nIf you like up to 3 people can submit a project"
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#homework-0",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#homework-0",
    "title": "Compiler Overview",
    "section": "homework 0",
    "text": "homework 0\nWrite a paragraph to introduce yourself in a reply to the canvas introductions topic. Add a picture of you can. Mention a compilers topic you’d like to learn about someday, either in this class or beyond. Add your info to the canvas introductions discussion topic.\nPick a paper from the weekly schedule whose discussion you will lead. Claim the paper by opening a pull request (at the class github) for the weekly.qmd file, fill in your name in the LEADER: line. (I encurage teams of two to sign up for the same paper)\nOnce everyone has signed up, and I see which papers are covered, I’ll finalize the dates and times.\nAdd a text file containing done to Canvas assignment 0 to indicate you have done the introduction and claimed a paper\n\nFor this assignment you just need to submit a response to the canvas assignment to indicate that you are done after you write your introduction into canvas\nFor other assignments you should:\n\nWrite a blog post describing your work, and submit it via a pull request to the github page\nAdd a response to the the canvas assignment giving the name of your blog post\n\n\nMy plan is that grades, personal details and the like stay in canvas and everything else becomes public and goes on the github website."
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#early-compilers",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#early-compilers",
    "title": "Compiler Overview",
    "section": "Early Compilers",
    "text": "Early Compilers\nOriginally, a compiler was a person doing calculations.\nhidden figures"
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#fortran",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#fortran",
    "title": "Compiler Overview",
    "section": "FORTRAN",
    "text": "FORTRAN\nIn 1957, John Backus created the first commercial compiler, FORTRAN (14 people worked on it for about 4 years).\n2/3 of the cost and 90% of the time for solving a problem was coding.\nFORTRAN was provided for the IBM 1401 computer by an innovative 63-phase compiler that ran entirely in its core memory of only 8000 (six-bit) characters."
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#compiler-development-model",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#compiler-development-model",
    "title": "Compiler Overview",
    "section": "Compiler Development Model",
    "text": "Compiler Development Model\nIn these early years, the vendor development model was:\n\nbuild a new machine\ndesign a new language\nimplement the compiler\n\nVendors sometimes built compilers but often used small startup compiler companies."
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#gcc",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#gcc",
    "title": "Compiler Overview",
    "section": "gcc",
    "text": "gcc\nIn 1987, GCC was released. It formalized the IR, and was more or less open source. Within the stages, compiler writers could use any data structures but at the edges they had to use the single IR. Adding an optimization or reordering optimizations is quite hard.\nVendors could use one front end, one middle end and only need to write a new back end."
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#llvm",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#llvm",
    "title": "Compiler Overview",
    "section": "LLVM",
    "text": "LLVM\nin about 2006 LLVM (originally low level virtual machine) appeared. This changed the model to look like a library.\nThe core of LLVM is the intermediate representation (IR), a low-level programming language similar to assembly. IR is a strongly typed reduced instruction set computer (RISC) instruction set which abstracts away most details of the target."
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#bril",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#bril",
    "title": "Compiler Overview",
    "section": "bril",
    "text": "bril\nIn this course we are going to an IR call BRIL, which is a very simplified version of LLVM IR, and we are going to string passes together by using UNIX pipes.\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[TEXT_Version of BRIL]--&gt; B0[BRIL in JSON] --&gt; B1[\"new pass\"] --&gt; B2[BRIL interpreter];\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[TEXT_Version of BRIL]--&gt; B0[BRIL in JSON] --&gt; B1[\"new pass\"] --&gt; B2[BRIL interpreter];"
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#cost-of-a-compiler.",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#cost-of-a-compiler.",
    "title": "Compiler Overview",
    "section": "Cost of a compiler.",
    "text": "Cost of a compiler.\nCompilers are massive and expensive to build.\n\n\n\nCompiler\nYear Started\nDevelopers\nLines Of Code\nEst Cost $\n\n\n\n\nGCC 9.2.0\n1988\n617\n5,591,759\n425,747,279\n\n\nLLVM 8.0.1\n2001\n1,210\n6,877,489\n529,894,190\n\n\nOpenJDK 14+10\n2007\n883\n7,955,827\n616,517,789\n\n\nv8 7.8.112\n2008\n736\n3,043,793\n225,195,832\n\n\nRust 1.37.0\n2010\n2,737\n852,877\n59,109,425\n\n\nSwift\n2010\n857\n665,238\n45,535,689\n\n\nIntel Graphics 1.0.10\n2018\n149\n694,688\n46,934,626\n\n\n\nsource"
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#compiler-assumptions-how-many-are-still-true",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#compiler-assumptions-how-many-are-still-true",
    "title": "Compiler Overview",
    "section": "Compiler Assumptions (How many are still true?)",
    "text": "Compiler Assumptions (How many are still true?)\n\nThe time to compile a program should be roughly linear. So, non-linear algorithms can only be used if they work on a small part of a program.\nUsers are ok with large programs taking minutes to compile\nCompilers run on machines that are memory-limited.\nCompilers run on single-threaded machines.\nMost targets are C-like."
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#how-well-do-compilers-do",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#how-well-do-compilers-do",
    "title": "Compiler Overview",
    "section": "How well do compilers do",
    "text": "How well do compilers do\nAt the scale of data-centers, every single performance percent matters! Just take a look at Google’s (and other’s) publicly available numbers on expenditures on datacenters. We are talking about billions of dollars. A single percent improvement can mean millions of dollars from more program features or improved utilization."
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#proebstings-law",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#proebstings-law",
    "title": "Compiler Overview",
    "section": "proebsting’s law",
    "text": "proebsting’s law\nproebsting’s law\nCompiler Advances Double Computing Power Every 18 Years.\nwhile hardware computing horsepower doubles every 18 months. How would you prove this?\none attempt"
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#why-compilers-are-not-better",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#why-compilers-are-not-better",
    "title": "Compiler Overview",
    "section": "why compilers are not better",
    "text": "why compilers are not better\nTalk given by KAI’s Arch Robison\nCompile-time program optimizations are similar to poetry: more are written than actually published in commercial compilers. Hard economic reality is that many interesting optimizations have too narrow an audience to justify their cost in a general-purpose compiler and custom compilers are too expensive to write."
  },
  {
    "objectID": "lectures/revealjs_010_compiler_overview.qmd.html#effects-of-optimization",
    "href": "lectures/revealjs_010_compiler_overview.qmd.html#effects-of-optimization",
    "title": "Compiler Overview",
    "section": "effects of optimization",
    "text": "effects of optimization\nRemove performance penalty for:\n\nusing higher level constructs\nsafety checks (e.g., array bounds checks)\nwriting clean, simple code (no benefit to applying loop unrolling by hand)\nEncourage ADT’s that are as efficient as primitive types"
  },
  {
    "objectID": "lectures/010_compiler_overview.html",
    "href": "lectures/010_compiler_overview.html",
    "title": "Compiler Overview",
    "section": "",
    "text": "A compiler writer builds bridges between people and machines, and every day this task becomes more challenging.\n\nSoftware engineers want abstractions\n\nHardware engineers want efficiency\n\n\n\n\nA compiler\n\n\n\n\nMost homework\n\nwe talk about some algorithm using pseudo\nYou implement that algorithm\nYou write up a blog post explaining what happened\nIf you like up to 3 people can submit a homework\n\nSince most programming languages have a json library, you can do homework in any language you like.\n\n\n\nwe are going to critically read current research papers\n\nEach person leads a paper discussion (up to two people can sign up as a team to present the same paper)\nEveryone reads the paper; the leader goes over the contents pros and cons\nThe leader writes a blog post, (possibly including discussion insights )\nblog is due one week after the presentation.\nI recommend that people pair up (two people going over a paper before hand is a lot easier)\nI listed a lot of papers, but if there is a different paper you want to present let me know\n\n\n\n\nEverybody gets to do a project, which is compiler related\n\nyou will need to get a proposal approved half way through the term\nyou submit a blog reporting on what happened\nIf you like up to 3 people can submit a project\n\n\n\n\nWrite a paragraph to introduce yourself in a reply to the canvas introductions topic. Add a picture of you can. Mention a compilers topic you’d like to learn about someday, either in this class or beyond. Add your info to the canvas introductions discussion topic.\nPick a paper from the weekly schedule whose discussion you will lead. Claim the paper by opening a pull request (at the class github) for the weekly.qmd file, fill in your name in the LEADER: line. (I encurage teams of two to sign up for the same paper)\nOnce everyone has signed up, and I see which papers are covered, I’ll finalize the dates and times.\nAdd a text file containing done to Canvas assignment 0 to indicate you have done the introduction and claimed a paper\n\nFor this assignment you just need to submit a response to the canvas assignment to indicate that you are done after you write your introduction into canvas\nFor other assignments you should:\n\nWrite a blog post describing your work, and submit it via a pull request to the github page\nAdd a response to the the canvas assignment giving the name of your blog post\n\n\nMy plan is that grades, personal details and the like stay in canvas and everything else becomes public and goes on the github website.\n\n\n\nOriginally, a compiler was a person doing calculations.\nhidden figures\n\nIn 1952, Grace Hopper an operational link-loader, which she called a compiler. She later said, “Nobody believed that,” and that she “had a running compiler and nobody would touch it. They told me computers could only do arithmetic.”\n\n\n\nGrace Hopper\n\n\n\n\n\nIn 1957, John Backus created the first commercial compiler, FORTRAN (14 people worked on it for about 4 years).\nTheir paper is located at https://dl.acm.org/doi/10.1145/1455567.1455599.\nThe name stands for formula translation. It’s in upper case because at that time, compilers did not support lower case.\nThe FORTRAN project was begun in the summer of 1954. Its purpose was to reduce by a large factor the task of preparing scientific problems for IBM’s next large computer, the 704. If it were possible for the 704 to code problems for itself and produce as good programs as human coders (but without the errors), it was clear that large benefits could be achieved. For it was known that about two-thirds of the cost of solving most scientific and engineering problems on large computers was that of problem preparation. Furthermore, more than 90 per cent of the elapsed time for a problem was usually devoted to planning, writing, and debugging the program. In many cases the development of a general plan for solving a problem was a small job in comparison to the task of devising and coding machine procedures to carry out the plan.\nThe goal of the FORTRAN project was to enable the programmer to specify a numerical procedure using a concise language like that of mathematics and obtain automatically from this specification an efficient 704 program to carry out the procedure. It was expected that such a system would reduce the coding and debugging task to less than one-fifth of the job it had been.\nFORTRAN was provided for the IBM 1401 computer by an innovative 63-phase compiler that ran entirely in its core memory of only 8000 (six-bit) characters.\n\n\n\nIn these early years, the vendor development model was:\n\nbuild a new machine\ndesign a new language\nimplement the compiler\n\nVendors sometimes built compilers but often used small startup compiler companies.\n\nCompilers stabilized on a classic structure (using an ir intermediate language). IR is machine independent.\n\nFront end - parse the program into IR\nMiddle end - machine independent optimizations and analyses\nBack end - machine specific stuff where machine code is generated\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[Front end]--IR--&gt; B[Middle end];\nB--IR--&gt; C[Back end];\nA--IR--&gt; C;\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[Front end]--IR--&gt; B[Middle end];\nB--IR--&gt; C[Back end];\nA--IR--&gt; C;\n\n\n\n\n\n\n\nThis course focuses on stage 2 (The middle end)\nA goal of this course is to explain how to transform a program automatically, while preserving its semantics, in such a way that the new program is more efficient according to a well-defined metric.\n There are many ways to compare the performance of programs:\n\nTime\nSpace\nEnergy\n\n\n\n\nIn 1987, GCC was released. It formalized the IR, and was more or less open source. Within the stages, compiler writers could use any data structures but at the edges they had to use the single IR. Adding an optimization or reordering optimizations is quite hard.\nVendors could use one front end, one middle end and only need to write a new back end.\n\nThis ended almost all the compiler startups. Free front end, middle end.\nIn gcc the IR is somewhat C based, for instance there are pointers but there is no simple way to talk about garbage collection without hacks.\n\n\n\nin about 2006 LLVM (originally low level virtual machine) appeared. This changed the model to look like a library.\nThe core of LLVM is the intermediate representation (IR), a low-level programming language similar to assembly. IR is a strongly typed reduced instruction set computer (RISC) instruction set which abstracts away most details of the target.\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[Front end]--IR--&gt; B0[OP0] --IR--&gt; B1[OP1] --IR--&gt; B2[OPT2]--IR --&gt; BN[OPTn]--IR --&gt;C{Back end};\nA--IR--&gt; C;\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[Front end]--IR--&gt; B0[OP0] --IR--&gt; B1[OP1] --IR--&gt; B2[OPT2]--IR --&gt; BN[OPTn]--IR --&gt;C{Back end};\nA--IR--&gt; C;\n\n\n\n\n\n\n\nOptimizations form passes. A user could mix and match – run some optimizations but not others to compile a specific program. It became easy for people to add a pass. Lots of academic research, lots of experiments.\n\n\n\n\nIn this course we are going to an IR call BRIL, which is a very simplified version of LLVM IR, and we are going to string passes together by using UNIX pipes.\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[TEXT_Version of BRIL]--&gt; B0[BRIL in JSON] --&gt; B1[\"new pass\"] --&gt; B2[BRIL interpreter];\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[TEXT_Version of BRIL]--&gt; B0[BRIL in JSON] --&gt; B1[\"new pass\"] --&gt; B2[BRIL interpreter];\n\n\n\n\n\n\n\n\n\nCompilers are massive and expensive to build.\n\n\n\nCompiler\nYear Started\nDevelopers\nLines Of Code\nEst Cost $\n\n\n\n\nGCC 9.2.0\n1988\n617\n5,591,759\n425,747,279\n\n\nLLVM 8.0.1\n2001\n1,210\n6,877,489\n529,894,190\n\n\nOpenJDK 14+10\n2007\n883\n7,955,827\n616,517,789\n\n\nv8 7.8.112\n2008\n736\n3,043,793\n225,195,832\n\n\nRust 1.37.0\n2010\n2,737\n852,877\n59,109,425\n\n\nSwift\n2010\n857\n665,238\n45,535,689\n\n\nIntel Graphics 1.0.10\n2018\n149\n694,688\n46,934,626\n\n\n\nsource\n\nSome observations:\n\nProduction compilers are expensive.\nIR does not change easily.\nMuch of compiler technology is old.\nThere is a vast difference between production and student projects.\n\n\n\n\n\nThe time to compile a program should be roughly linear. So, non-linear algorithms can only be used if they work on a small part of a program.\nUsers are ok with large programs taking minutes to compile\nCompilers run on machines that are memory-limited.\nCompilers run on single-threaded machines.\nMost targets are C-like.\n\n\nSome changes since early 2000’s:\n\nIntegrated development environments. When you type a.b what has to happen?\nDSL (Domain specific languages for AI)\nMore kinds of hardware\n\n\n\n\nAt the scale of data-centers, every single performance percent matters! Just take a look at Google’s (and other’s) publicly available numbers on expenditures on datacenters. We are talking about billions of dollars. A single percent improvement can mean millions of dollars from more program features or improved utilization.\n\n\n\nproebsting’s law\nCompiler Advances Double Computing Power Every 18 Years.\nwhile hardware computing horsepower doubles every 18 months. How would you prove this?\none attempt\n\n\n\nTalk given by KAI’s Arch Robison\nCompile-time program optimizations are similar to poetry: more are written than actually published in commercial compilers. Hard economic reality is that many interesting optimizations have too narrow an audience to justify their cost in a general-purpose compiler and custom compilers are too expensive to write.\n\n\n\nRemove performance penalty for:\n\nusing higher level constructs\nsafety checks (e.g., array bounds checks)\nwriting clean, simple code (no benefit to applying loop unrolling by hand)\nEncourage ADT’s that are as efficient as primitive types\n\n\nOver time hardware has become more of a challenge for compilers, for example caches are not predictable at compile time. So compilers have to guess\nAnd hardware can ignore features of the compiler can deal with them - for example interlock",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#homework",
    "href": "lectures/010_compiler_overview.html#homework",
    "title": "Compiler Overview",
    "section": "",
    "text": "Most homework\n\nwe talk about some algorithm using pseudo\nYou implement that algorithm\nYou write up a blog post explaining what happened\nIf you like up to 3 people can submit a homework\n\nSince most programming languages have a json library, you can do homework in any language you like.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#readings",
    "href": "lectures/010_compiler_overview.html#readings",
    "title": "Compiler Overview",
    "section": "",
    "text": "we are going to critically read current research papers\n\nEach person leads a paper discussion (up to two people can sign up as a team to present the same paper)\nEveryone reads the paper; the leader goes over the contents pros and cons\nThe leader writes a blog post, (possibly including discussion insights )\nblog is due one week after the presentation.\nI recommend that people pair up (two people going over a paper before hand is a lot easier)\nI listed a lot of papers, but if there is a different paper you want to present let me know",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#project",
    "href": "lectures/010_compiler_overview.html#project",
    "title": "Compiler Overview",
    "section": "",
    "text": "Everybody gets to do a project, which is compiler related\n\nyou will need to get a proposal approved half way through the term\nyou submit a blog reporting on what happened\nIf you like up to 3 people can submit a project",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#homework-0",
    "href": "lectures/010_compiler_overview.html#homework-0",
    "title": "Compiler Overview",
    "section": "",
    "text": "Write a paragraph to introduce yourself in a reply to the canvas introductions topic. Add a picture of you can. Mention a compilers topic you’d like to learn about someday, either in this class or beyond. Add your info to the canvas introductions discussion topic.\nPick a paper from the weekly schedule whose discussion you will lead. Claim the paper by opening a pull request (at the class github) for the weekly.qmd file, fill in your name in the LEADER: line. (I encurage teams of two to sign up for the same paper)\nOnce everyone has signed up, and I see which papers are covered, I’ll finalize the dates and times.\nAdd a text file containing done to Canvas assignment 0 to indicate you have done the introduction and claimed a paper\n\nFor this assignment you just need to submit a response to the canvas assignment to indicate that you are done after you write your introduction into canvas\nFor other assignments you should:\n\nWrite a blog post describing your work, and submit it via a pull request to the github page\nAdd a response to the the canvas assignment giving the name of your blog post\n\n\nMy plan is that grades, personal details and the like stay in canvas and everything else becomes public and goes on the github website.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#early-compilers",
    "href": "lectures/010_compiler_overview.html#early-compilers",
    "title": "Compiler Overview",
    "section": "",
    "text": "Originally, a compiler was a person doing calculations.\nhidden figures\n\nIn 1952, Grace Hopper an operational link-loader, which she called a compiler. She later said, “Nobody believed that,” and that she “had a running compiler and nobody would touch it. They told me computers could only do arithmetic.”\n\n\n\nGrace Hopper",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#fortran",
    "href": "lectures/010_compiler_overview.html#fortran",
    "title": "Compiler Overview",
    "section": "",
    "text": "In 1957, John Backus created the first commercial compiler, FORTRAN (14 people worked on it for about 4 years).\nTheir paper is located at https://dl.acm.org/doi/10.1145/1455567.1455599.\nThe name stands for formula translation. It’s in upper case because at that time, compilers did not support lower case.\nThe FORTRAN project was begun in the summer of 1954. Its purpose was to reduce by a large factor the task of preparing scientific problems for IBM’s next large computer, the 704. If it were possible for the 704 to code problems for itself and produce as good programs as human coders (but without the errors), it was clear that large benefits could be achieved. For it was known that about two-thirds of the cost of solving most scientific and engineering problems on large computers was that of problem preparation. Furthermore, more than 90 per cent of the elapsed time for a problem was usually devoted to planning, writing, and debugging the program. In many cases the development of a general plan for solving a problem was a small job in comparison to the task of devising and coding machine procedures to carry out the plan.\nThe goal of the FORTRAN project was to enable the programmer to specify a numerical procedure using a concise language like that of mathematics and obtain automatically from this specification an efficient 704 program to carry out the procedure. It was expected that such a system would reduce the coding and debugging task to less than one-fifth of the job it had been.\nFORTRAN was provided for the IBM 1401 computer by an innovative 63-phase compiler that ran entirely in its core memory of only 8000 (six-bit) characters.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#compiler-development-model",
    "href": "lectures/010_compiler_overview.html#compiler-development-model",
    "title": "Compiler Overview",
    "section": "",
    "text": "In these early years, the vendor development model was:\n\nbuild a new machine\ndesign a new language\nimplement the compiler\n\nVendors sometimes built compilers but often used small startup compiler companies.\n\nCompilers stabilized on a classic structure (using an ir intermediate language). IR is machine independent.\n\nFront end - parse the program into IR\nMiddle end - machine independent optimizations and analyses\nBack end - machine specific stuff where machine code is generated\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[Front end]--IR--&gt; B[Middle end];\nB--IR--&gt; C[Back end];\nA--IR--&gt; C;\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[Front end]--IR--&gt; B[Middle end];\nB--IR--&gt; C[Back end];\nA--IR--&gt; C;\n\n\n\n\n\n\n\nThis course focuses on stage 2 (The middle end)\nA goal of this course is to explain how to transform a program automatically, while preserving its semantics, in such a way that the new program is more efficient according to a well-defined metric.\n There are many ways to compare the performance of programs:\n\nTime\nSpace\nEnergy",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#gcc",
    "href": "lectures/010_compiler_overview.html#gcc",
    "title": "Compiler Overview",
    "section": "",
    "text": "In 1987, GCC was released. It formalized the IR, and was more or less open source. Within the stages, compiler writers could use any data structures but at the edges they had to use the single IR. Adding an optimization or reordering optimizations is quite hard.\nVendors could use one front end, one middle end and only need to write a new back end.\n\nThis ended almost all the compiler startups. Free front end, middle end.\nIn gcc the IR is somewhat C based, for instance there are pointers but there is no simple way to talk about garbage collection without hacks.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#llvm",
    "href": "lectures/010_compiler_overview.html#llvm",
    "title": "Compiler Overview",
    "section": "",
    "text": "in about 2006 LLVM (originally low level virtual machine) appeared. This changed the model to look like a library.\nThe core of LLVM is the intermediate representation (IR), a low-level programming language similar to assembly. IR is a strongly typed reduced instruction set computer (RISC) instruction set which abstracts away most details of the target.\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[Front end]--IR--&gt; B0[OP0] --IR--&gt; B1[OP1] --IR--&gt; B2[OPT2]--IR --&gt; BN[OPTn]--IR --&gt;C{Back end};\nA--IR--&gt; C;\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[Front end]--IR--&gt; B0[OP0] --IR--&gt; B1[OP1] --IR--&gt; B2[OPT2]--IR --&gt; BN[OPTn]--IR --&gt;C{Back end};\nA--IR--&gt; C;\n\n\n\n\n\n\n\nOptimizations form passes. A user could mix and match – run some optimizations but not others to compile a specific program. It became easy for people to add a pass. Lots of academic research, lots of experiments.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#bril",
    "href": "lectures/010_compiler_overview.html#bril",
    "title": "Compiler Overview",
    "section": "",
    "text": "In this course we are going to an IR call BRIL, which is a very simplified version of LLVM IR, and we are going to string passes together by using UNIX pipes.\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[TEXT_Version of BRIL]--&gt; B0[BRIL in JSON] --&gt; B1[\"new pass\"] --&gt; B2[BRIL interpreter];\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[TEXT_Version of BRIL]--&gt; B0[BRIL in JSON] --&gt; B1[\"new pass\"] --&gt; B2[BRIL interpreter];",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#cost-of-a-compiler.",
    "href": "lectures/010_compiler_overview.html#cost-of-a-compiler.",
    "title": "Compiler Overview",
    "section": "",
    "text": "Compilers are massive and expensive to build.\n\n\n\nCompiler\nYear Started\nDevelopers\nLines Of Code\nEst Cost $\n\n\n\n\nGCC 9.2.0\n1988\n617\n5,591,759\n425,747,279\n\n\nLLVM 8.0.1\n2001\n1,210\n6,877,489\n529,894,190\n\n\nOpenJDK 14+10\n2007\n883\n7,955,827\n616,517,789\n\n\nv8 7.8.112\n2008\n736\n3,043,793\n225,195,832\n\n\nRust 1.37.0\n2010\n2,737\n852,877\n59,109,425\n\n\nSwift\n2010\n857\n665,238\n45,535,689\n\n\nIntel Graphics 1.0.10\n2018\n149\n694,688\n46,934,626\n\n\n\nsource\n\nSome observations:\n\nProduction compilers are expensive.\nIR does not change easily.\nMuch of compiler technology is old.\nThere is a vast difference between production and student projects.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#compiler-assumptions-how-many-are-still-true",
    "href": "lectures/010_compiler_overview.html#compiler-assumptions-how-many-are-still-true",
    "title": "Compiler Overview",
    "section": "",
    "text": "The time to compile a program should be roughly linear. So, non-linear algorithms can only be used if they work on a small part of a program.\nUsers are ok with large programs taking minutes to compile\nCompilers run on machines that are memory-limited.\nCompilers run on single-threaded machines.\nMost targets are C-like.\n\n\nSome changes since early 2000’s:\n\nIntegrated development environments. When you type a.b what has to happen?\nDSL (Domain specific languages for AI)\nMore kinds of hardware",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#how-well-do-compilers-do",
    "href": "lectures/010_compiler_overview.html#how-well-do-compilers-do",
    "title": "Compiler Overview",
    "section": "",
    "text": "At the scale of data-centers, every single performance percent matters! Just take a look at Google’s (and other’s) publicly available numbers on expenditures on datacenters. We are talking about billions of dollars. A single percent improvement can mean millions of dollars from more program features or improved utilization.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#proebstings-law",
    "href": "lectures/010_compiler_overview.html#proebstings-law",
    "title": "Compiler Overview",
    "section": "",
    "text": "proebsting’s law\nCompiler Advances Double Computing Power Every 18 Years.\nwhile hardware computing horsepower doubles every 18 months. How would you prove this?\none attempt",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#why-compilers-are-not-better",
    "href": "lectures/010_compiler_overview.html#why-compilers-are-not-better",
    "title": "Compiler Overview",
    "section": "",
    "text": "Talk given by KAI’s Arch Robison\nCompile-time program optimizations are similar to poetry: more are written than actually published in commercial compilers. Hard economic reality is that many interesting optimizations have too narrow an audience to justify their cost in a general-purpose compiler and custom compilers are too expensive to write.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/010_compiler_overview.html#effects-of-optimization",
    "href": "lectures/010_compiler_overview.html#effects-of-optimization",
    "title": "Compiler Overview",
    "section": "",
    "text": "Remove performance penalty for:\n\nusing higher level constructs\nsafety checks (e.g., array bounds checks)\nwriting clean, simple code (no benefit to applying loop unrolling by hand)\nEncourage ADT’s that are as efficient as primitive types\n\n\nOver time hardware has become more of a challenge for compilers, for example caches are not predictable at compile time. So compilers have to guess\nAnd hardware can ignore features of the compiler can deal with them - for example interlock",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Compiler Overview"
    ]
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#challenges-in-learning-about-gpus",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#challenges-in-learning-about-gpus",
    "title": "GPU Compilers",
    "section": "Challenges in Learning About GPUs",
    "text": "Challenges in Learning About GPUs\n\nArchitectural Variations\n\nAll the vendors keep changing the architecture. Different vendors build different hardware. Graphics programs run on all vendors’ hardware because vendors ship their own drivers and do finalization on the device.\n\nInconsistent Terminology\n\nVendors use different terms for the same features and reuse CPU terms somewhat differently. For instance, a “Cuda core” is not the same as a ‘CPU core’.\n\nLegacy Terminology\n\nSome terms come from the GPU’s history of graphics. For example, a shader is a program.\n\nSoftware Abstractions\n\nBig frameworks hide the details.\n\nCUDA’s Market Dominance\n\nMost explanations use CUDA terms since CUDA is the market leader. For instance, AMD uses ‘wave’ to mean more or less the same as a CUDA ‘warp’, but lots of AMD documentation uses ‘warps’.\n\nCUDA: Language and Model\n\nCUDA is both a programming language and a programming model, so you can have CUDA Fortran, CUDA Python, etc."
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#hardware-styles-nvidia-model",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#hardware-styles-nvidia-model",
    "title": "GPU Compilers",
    "section": "hardware styles NVIDIA model",
    "text": "hardware styles NVIDIA model\n\ncompute and graphics parts have same programmable parts, compute leaves out non-programmable features\ngraphics customers pay a tax for the compute instructions\nmore transistors for compute instructions\none big die"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#hardware-styles-amd-model",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#hardware-styles-amd-model",
    "title": "GPU Compilers",
    "section": "hardware styles AMD model",
    "text": "hardware styles AMD model\n\ncompute has extra instructions (no tax on graphics customers to support compute)\nchiplet model (great engineering!)\nR series is graphics, C series is compute"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#programming-model",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#programming-model",
    "title": "GPU Compilers",
    "section": "programming model",
    "text": "programming model\nboth vendors use the CUDA programming model. AMD supports a variation of the CUDA language\nmachines have multiple SIMD processors, each SIMD can be running a different instruction but each lane of a SIMD runs the same instruction\na lane of a SIMD is called a thread"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#threading-model",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#threading-model",
    "title": "GPU Compilers",
    "section": "threading model",
    "text": "threading model"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#warps-waveswavefronts",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#warps-waveswavefronts",
    "title": "GPU Compilers",
    "section": "warps/ waves/wavefronts",
    "text": "warps/ waves/wavefronts\n\nthreads are grouped together into warps.\nSize is fixed by hardware (usually 32), programmers know this and often make data set sizes a multiple of 32.\nsince all threads in a warp are running the same instruction, there is no need for explicit synchronization\nthere are a few instructions that work across a warp, - which break the model and give the compiler problems"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#thread-blocks-groups-of-warps",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#thread-blocks-groups-of-warps",
    "title": "GPU Compilers",
    "section": "thread blocks groups of warps,",
    "text": "thread blocks groups of warps,\n\nthreads within a block can access a fast scratchpad memory (called shared or LDS) - Violates the C memory model.\n\nthreads are identified by a 3d index inside a block\nthread blocks need synchronization operations.\n\nhardware schedules each block into execution units. Max block size is limited by the size of a execution unit."
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#grid-groups-of-blocks",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#grid-groups-of-blocks",
    "title": "GPU Compilers",
    "section": "grid groups of blocks,",
    "text": "grid groups of blocks,\n\n3d collection of blocks,\nusually hardware limits mean that all the threads in a grid do not run at the same time\nprogrammers need to make grids big enough to fill the hardware\nsoftware launches a grid and a program\nthreads within a grid but in different blocks do not have sync operations"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#heterogeneous-programming",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#heterogeneous-programming",
    "title": "GPU Compilers",
    "section": "Heterogeneous programming",
    "text": "Heterogeneous programming\nthe cpu is called the host\nthe gpu is called the device\ncpu launches grids and kernels to gpu\nComputations launched on the device execute asynchronously with respect to the host, and it is the user’s responsibility to synchronize"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#memory-spaces",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#memory-spaces",
    "title": "GPU Compilers",
    "section": "memory spaces",
    "text": "memory spaces\n\ncpu and gpu have different memory spaces (cpu can copy from one to the other). Some amount of combined memory (slower then not combined)\ngpu has shared/lds memory which can be accessed by threads within a block, passing an address to a different block does not work\nmain gpu memory is called global accessible by all threads\ngpu has per thread memory called local or scratch or private memory - unlike C, passing an address in local memory to another thread does not work. (under some restrictions AMD implements part of the C memory model)\na few special gpu memory types: constant, texture, surface (left over from graphics)\ngpu can treat registers as fast memory"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#hardware",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#hardware",
    "title": "GPU Compilers",
    "section": "Hardware",
    "text": "Hardware\nNVIDIA\n\nnv image level 0Each box here is hardware, an int32/fp32/fp64 can perform one operation, so here we have 32 fp32 units which can do 32 float operations in parallel as well as 16 int32 and 16 fp64 units , there are also 8 units can do loads or stores and a final special function unit that can do transcendental operations like sin/cos\nunder some conditions two instructions (a float and an int) from the same warp can execute at the same time\nI’m not going to talk much about the tensor cores\nall these units execute the same instruction (SIMT) Simple instruction multiple thread (not the same as SIMD but related )"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#one-instruction-does-vector-work",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#one-instruction-does-vector-work",
    "title": "GPU Compilers",
    "section": "one instruction does vector work",
    "text": "one instruction does vector work\nadd 32 float32 values (all coming from registers) and store the result in 32 other registers\nNotice no branch prediction, no out of order execution\ngreat at switching a warp holds the instruction, it knows which registers it owns (continuous set, so it just needs a start and length) switching to a different warp, means changing these two numbers and the pc (this is done by the dispatch unt )\nWhen we do a load, we need to wait for the result. CPU might do some kind of out of order execution, a gpu switches to another warp\nfinally we need to pick the warp to switch to, this is done by the warp scheduler (half of the hardware scheduler)"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#streaming-multiprocessors-sm",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#streaming-multiprocessors-sm",
    "title": "GPU Compilers",
    "section": "streaming multiprocessors (SM)",
    "text": "streaming multiprocessors (SM)\nnvidia packs 4 execution engines into a SM (streaming multi-processor) ands an L1 instruction cache, a special memory accelerator for tensors and 256kb l1 data cache/ shared memory block"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#pack-sms-together",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#pack-sms-together",
    "title": "GPU Compilers",
    "section": "pack sm’s together",
    "text": "pack sm’s together"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#amd",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#amd",
    "title": "GPU Compilers",
    "section": "AMD",
    "text": "AMD"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#and",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#and",
    "title": "GPU Compilers",
    "section": "and",
    "text": "and\n\nAMD has a similar block with some important differences\n\nAt the bottom of a compute core there are 4 general purpose simd engines each of length 16 and one scalar engine\n\nGiven 5 waves, this compute core can execute 4 vector instructions and one scalar instruction per clock Two instructions from the same wave never execute at the same time\nThe SIMD engines can execute different instructions\nThe simd sizes vary over different chips"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#amd-cdna",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#amd-cdna",
    "title": "GPU Compilers",
    "section": "amd CDNA",
    "text": "amd CDNA"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#packing-sms",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#packing-sms",
    "title": "GPU Compilers",
    "section": "packing sms",
    "text": "packing sms"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#simd-and-control-flow",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#simd-and-control-flow",
    "title": "GPU Compilers",
    "section": "simd and control flow",
    "text": "simd and control flow\nto execute an if then else\n\ncompute the if condition\nturn off the lanes where the condition is false\nexecute the if side\nflip the lanes\nexecute the else side\n\ntime is the sum of the times for then and the else"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#number-of-warps-in-flight",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#number-of-warps-in-flight",
    "title": "GPU Compilers",
    "section": "number of warps in flight",
    "text": "number of warps in flight\nsince an entire block has to fit on one compute unit/sm, the resources used in the block limit the number of warps on a sm,\nif a warp needs 100 registers and there are 256 vector registers on the compute unit, then two warps can run at once, compiler controls number of registers"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#latency",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#latency",
    "title": "GPU Compilers",
    "section": "latency",
    "text": "latency\ngpu is a throughput machine- how many threads finish in a unit of time not how long a single thread takes to finish\nunlike the cpu, gpu can have lots of loads in flight, time for these loads overlap so compiler tries to group loads together, but this needs extra registers\n\na warp issues a group of loads\nwarp issues a wait for loads to finish (hardware in cpu, software in gpu)\nhardware switches to another warp (if there is on), good to have a lot of warps 1, if all warps waiting for memory, alu units are idle"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#compiler-model",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#compiler-model",
    "title": "GPU Compilers",
    "section": "compiler model",
    "text": "compiler model\n\nlike a regular compiler for a scalar machine\nnew problem: registers used in warp limits number of warps in flight, so ra is different\nnew problem: control flow is more critical\nnew problem: latency means grouping loads but not to much\nnew problem: arch keeps changing"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#thread-coarsening",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#thread-coarsening",
    "title": "GPU Compilers",
    "section": "thread coarsening",
    "text": "thread coarsening\nSuppose we are computing a matrix multiply\nwe could say each thread writes one result so a 32 x 32 matrix would need 32 * 32 threads each thread reads one column and one row of the input,\nwe have a lot of reuse (redundant loads of data )\nwe could say each thread writes 4 results, so we need 1/4 of the threads each thread reads a raw and 4 columns"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#technique",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#technique",
    "title": "GPU Compilers",
    "section": "technique",
    "text": "technique\n\nmerge multiple threads so each resulting thread writes multiple outputs\ndo the redundant work once and save in registers\nuse the registers for computing all the outputs\n\nBut\n\nneeds extra registers\nincreased efficiency but reduced parallelism\n\nAfter thread coarsening, computation from merged threads can see each others results"
  },
  {
    "objectID": "lectures/revealjs_14_gpu_compilers.qmd.html#doing-this-by-hand",
    "href": "lectures/revealjs_14_gpu_compilers.qmd.html#doing-this-by-hand",
    "title": "GPU Compilers",
    "section": "doing this by hand",
    "text": "doing this by hand\nfor (atomid=0; atomid&lt;numatoms; atomid++) { \n  float dy = coory - atominfo[atomid].y; \n  float dysqpdzsq = (dy * dy) + atominfo[atomid].z; \n  float dx1 = coorx1 - atominfo[atomid].x; \n  float dx2 = coorx2 - atominfo[atomid].x; \n  float dx3 = coorx3 - atominfo[atomid].x; \n  float dx4 = coorx4 - atominfo[atomid].x; \n  energyvalx1 += atominfo[atomid].w * (1.0f / sqrtf(dx1*dx1 + dysqpdzsq));\n  energyvalx2 += atominfo[atomid].w * (1.0f / sqrtf(dx2*dx2 + dysqpdzsq)); \n  energyvalx3 += atominfo[atomid].w * (1.0f / sqrtf(dx3*dx3 + dysqpdzsq)); \n  energyvalx4 += atominfo[atomid].w * (1.0f / sqrtf(dx4*dx4 + dysqpdzsq)); } …"
  },
  {
    "objectID": "lectures/14_gpu_compilers.html",
    "href": "lectures/14_gpu_compilers.html",
    "title": "GPU Compilers",
    "section": "",
    "text": "Architectural Variations\n\nAll the vendors keep changing the architecture. Different vendors build different hardware. Graphics programs run on all vendors’ hardware because vendors ship their own drivers and do finalization on the device.\n\nInconsistent Terminology\n\nVendors use different terms for the same features and reuse CPU terms somewhat differently. For instance, a “Cuda core” is not the same as a ‘CPU core’.\n\nLegacy Terminology\n\nSome terms come from the GPU’s history of graphics. For example, a shader is a program.\n\nSoftware Abstractions\n\nBig frameworks hide the details.\n\nCUDA’s Market Dominance\n\nMost explanations use CUDA terms since CUDA is the market leader. For instance, AMD uses ‘wave’ to mean more or less the same as a CUDA ‘warp’, but lots of AMD documentation uses ‘warps’.\n\nCUDA: Language and Model\n\nCUDA is both a programming language and a programming model, so you can have CUDA Fortran, CUDA Python, etc.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#challenges-in-learning-about-gpus",
    "href": "lectures/14_gpu_compilers.html#challenges-in-learning-about-gpus",
    "title": "GPU Compilers",
    "section": "",
    "text": "Architectural Variations\n\nAll the vendors keep changing the architecture. Different vendors build different hardware. Graphics programs run on all vendors’ hardware because vendors ship their own drivers and do finalization on the device.\n\nInconsistent Terminology\n\nVendors use different terms for the same features and reuse CPU terms somewhat differently. For instance, a “Cuda core” is not the same as a ‘CPU core’.\n\nLegacy Terminology\n\nSome terms come from the GPU’s history of graphics. For example, a shader is a program.\n\nSoftware Abstractions\n\nBig frameworks hide the details.\n\nCUDA’s Market Dominance\n\nMost explanations use CUDA terms since CUDA is the market leader. For instance, AMD uses ‘wave’ to mean more or less the same as a CUDA ‘warp’, but lots of AMD documentation uses ‘warps’.\n\nCUDA: Language and Model\n\nCUDA is both a programming language and a programming model, so you can have CUDA Fortran, CUDA Python, etc.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#hardware-styles-nvidia-model",
    "href": "lectures/14_gpu_compilers.html#hardware-styles-nvidia-model",
    "title": "GPU Compilers",
    "section": "hardware styles NVIDIA model",
    "text": "hardware styles NVIDIA model\n\ncompute and graphics parts have same programmable parts, compute leaves out non-programmable features\ngraphics customers pay a tax for the compute instructions\nmore transistors for compute instructions\none big die",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#hardware-styles-amd-model",
    "href": "lectures/14_gpu_compilers.html#hardware-styles-amd-model",
    "title": "GPU Compilers",
    "section": "hardware styles AMD model",
    "text": "hardware styles AMD model\n\ncompute has extra instructions (no tax on graphics customers to support compute)\nchiplet model (great engineering!)\nR series is graphics, C series is compute",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#programming-model",
    "href": "lectures/14_gpu_compilers.html#programming-model",
    "title": "GPU Compilers",
    "section": "programming model",
    "text": "programming model\nboth vendors use the CUDA programming model. AMD supports a variation of the CUDA language\nmachines have multiple SIMD processors, each SIMD can be running a different instruction but each lane of a SIMD runs the same instruction\na lane of a SIMD is called a thread\n\nthe programming model is SIMT (single instruction multiple threads)\nUser writes a scalar program, compiler maps that program to a lane of a SIMD, many instances of the program run at once, hardware combines copies of the scalar program into warps, hardware schedules warps into the SIMD engines\nprograms are called kernels",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#threading-model",
    "href": "lectures/14_gpu_compilers.html#threading-model",
    "title": "GPU Compilers",
    "section": "threading model",
    "text": "threading model",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#warps-waveswavefronts",
    "href": "lectures/14_gpu_compilers.html#warps-waveswavefronts",
    "title": "GPU Compilers",
    "section": "warps/ waves/wavefronts",
    "text": "warps/ waves/wavefronts\n\nthreads are grouped together into warps.\nSize is fixed by hardware (usually 32), programmers know this and often make data set sizes a multiple of 32.\nsince all threads in a warp are running the same instruction, there is no need for explicit synchronization\nthere are a few instructions that work across a warp, - which break the model and give the compiler problems",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#thread-blocks-groups-of-warps",
    "href": "lectures/14_gpu_compilers.html#thread-blocks-groups-of-warps",
    "title": "GPU Compilers",
    "section": "thread blocks groups of warps,",
    "text": "thread blocks groups of warps,\n\nthreads within a block can access a fast scratchpad memory (called shared or LDS) - Violates the C memory model.\n\nthreads are identified by a 3d index inside a block\nthread blocks need synchronization operations.\n\nhardware schedules each block into execution units. Max block size is limited by the size of a execution unit.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#grid-groups-of-blocks",
    "href": "lectures/14_gpu_compilers.html#grid-groups-of-blocks",
    "title": "GPU Compilers",
    "section": "grid groups of blocks,",
    "text": "grid groups of blocks,\n\n3d collection of blocks,\nusually hardware limits mean that all the threads in a grid do not run at the same time\nprogrammers need to make grids big enough to fill the hardware\nsoftware launches a grid and a program\nthreads within a grid but in different blocks do not have sync operations",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#heterogeneous-programming",
    "href": "lectures/14_gpu_compilers.html#heterogeneous-programming",
    "title": "GPU Compilers",
    "section": "Heterogeneous programming",
    "text": "Heterogeneous programming\nthe cpu is called the host\nthe gpu is called the device\ncpu launches grids and kernels to gpu\nComputations launched on the device execute asynchronously with respect to the host, and it is the user’s responsibility to synchronize",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#memory-spaces",
    "href": "lectures/14_gpu_compilers.html#memory-spaces",
    "title": "GPU Compilers",
    "section": "memory spaces",
    "text": "memory spaces\n\ncpu and gpu have different memory spaces (cpu can copy from one to the other). Some amount of combined memory (slower then not combined)\ngpu has shared/lds memory which can be accessed by threads within a block, passing an address to a different block does not work\nmain gpu memory is called global accessible by all threads\ngpu has per thread memory called local or scratch or private memory - unlike C, passing an address in local memory to another thread does not work. (under some restrictions AMD implements part of the C memory model)\na few special gpu memory types: constant, texture, surface (left over from graphics)\ngpu can treat registers as fast memory",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#hardware",
    "href": "lectures/14_gpu_compilers.html#hardware",
    "title": "GPU Compilers",
    "section": "Hardware",
    "text": "Hardware\nNVIDIA\n\n\n\nnv image level 0\n\n\nEach box here is hardware, an int32/fp32/fp64 can perform one operation, so here we have 32 fp32 units which can do 32 float operations in parallel as well as 16 int32 and 16 fp64 units , there are also 8 units can do loads or stores and a final special function unit that can do transcendental operations like sin/cos\nunder some conditions two instructions (a float and an int) from the same warp can execute at the same time\nI’m not going to talk much about the tensor cores\nall these units execute the same instruction (SIMT) Simple instruction multiple thread (not the same as SIMD but related )",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#one-instruction-does-vector-work",
    "href": "lectures/14_gpu_compilers.html#one-instruction-does-vector-work",
    "title": "GPU Compilers",
    "section": "one instruction does vector work",
    "text": "one instruction does vector work\nadd 32 float32 values (all coming from registers) and store the result in 32 other registers\nNotice no branch prediction, no out of order execution\ngreat at switching a warp holds the instruction, it knows which registers it owns (continuous set, so it just needs a start and length) switching to a different warp, means changing these two numbers and the pc (this is done by the dispatch unt )\nWhen we do a load, we need to wait for the result. CPU might do some kind of out of order execution, a gpu switches to another warp\nfinally we need to pick the warp to switch to, this is done by the warp scheduler (half of the hardware scheduler)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#streaming-multiprocessors-sm",
    "href": "lectures/14_gpu_compilers.html#streaming-multiprocessors-sm",
    "title": "GPU Compilers",
    "section": "streaming multiprocessors (SM)",
    "text": "streaming multiprocessors (SM)\nnvidia packs 4 execution engines into a SM (streaming multi-processor) ands an L1 instruction cache, a special memory accelerator for tensors and 256kb l1 data cache/ shared memory block",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#pack-sms-together",
    "href": "lectures/14_gpu_compilers.html#pack-sms-together",
    "title": "GPU Compilers",
    "section": "pack sm’s together",
    "text": "pack sm’s together",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#amd",
    "href": "lectures/14_gpu_compilers.html#amd",
    "title": "GPU Compilers",
    "section": "AMD",
    "text": "AMD",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#and",
    "href": "lectures/14_gpu_compilers.html#and",
    "title": "GPU Compilers",
    "section": "and",
    "text": "and\n\nAMD has a similar block with some important differences\n\nAt the bottom of a compute core there are 4 general purpose simd engines each of length 16 and one scalar engine\n\nGiven 5 waves, this compute core can execute 4 vector instructions and one scalar instruction per clock Two instructions from the same wave never execute at the same time\nThe SIMD engines can execute different instructions\nThe simd sizes vary over different chips",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#amd-cdna",
    "href": "lectures/14_gpu_compilers.html#amd-cdna",
    "title": "GPU Compilers",
    "section": "amd CDNA",
    "text": "amd CDNA",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#packing-sms",
    "href": "lectures/14_gpu_compilers.html#packing-sms",
    "title": "GPU Compilers",
    "section": "packing sms",
    "text": "packing sms",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#simd-and-control-flow",
    "href": "lectures/14_gpu_compilers.html#simd-and-control-flow",
    "title": "GPU Compilers",
    "section": "simd and control flow",
    "text": "simd and control flow\nto execute an if then else\n\ncompute the if condition\nturn off the lanes where the condition is false\nexecute the if side\nflip the lanes\nexecute the else side\n\ntime is the sum of the times for then and the else",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#number-of-warps-in-flight",
    "href": "lectures/14_gpu_compilers.html#number-of-warps-in-flight",
    "title": "GPU Compilers",
    "section": "number of warps in flight",
    "text": "number of warps in flight\nsince an entire block has to fit on one compute unit/sm, the resources used in the block limit the number of warps on a sm,\nif a warp needs 100 registers and there are 256 vector registers on the compute unit, then two warps can run at once, compiler controls number of registers",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#latency",
    "href": "lectures/14_gpu_compilers.html#latency",
    "title": "GPU Compilers",
    "section": "latency",
    "text": "latency\ngpu is a throughput machine- how many threads finish in a unit of time not how long a single thread takes to finish\nunlike the cpu, gpu can have lots of loads in flight, time for these loads overlap so compiler tries to group loads together, but this needs extra registers\n\na warp issues a group of loads\nwarp issues a wait for loads to finish (hardware in cpu, software in gpu)\nhardware switches to another warp (if there is on), good to have a lot of warps 1, if all warps waiting for memory, alu units are idle",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#compiler-model",
    "href": "lectures/14_gpu_compilers.html#compiler-model",
    "title": "GPU Compilers",
    "section": "compiler model",
    "text": "compiler model\n\nlike a regular compiler for a scalar machine\nnew problem: registers used in warp limits number of warps in flight, so ra is different\nnew problem: control flow is more critical\nnew problem: latency means grouping loads but not to much\nnew problem: arch keeps changing",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#thread-coarsening",
    "href": "lectures/14_gpu_compilers.html#thread-coarsening",
    "title": "GPU Compilers",
    "section": "thread coarsening",
    "text": "thread coarsening\nSuppose we are computing a matrix multiply\nwe could say each thread writes one result so a 32 x 32 matrix would need 32 * 32 threads each thread reads one column and one row of the input,\nwe have a lot of reuse (redundant loads of data )\nwe could say each thread writes 4 results, so we need 1/4 of the threads each thread reads a raw and 4 columns",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#technique",
    "href": "lectures/14_gpu_compilers.html#technique",
    "title": "GPU Compilers",
    "section": "technique",
    "text": "technique\n\nmerge multiple threads so each resulting thread writes multiple outputs\ndo the redundant work once and save in registers\nuse the registers for computing all the outputs\n\nBut\n\nneeds extra registers\nincreased efficiency but reduced parallelism\n\nAfter thread coarsening, computation from merged threads can see each others results",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/14_gpu_compilers.html#doing-this-by-hand",
    "href": "lectures/14_gpu_compilers.html#doing-this-by-hand",
    "title": "GPU Compilers",
    "section": "doing this by hand",
    "text": "doing this by hand\nfor (atomid=0; atomid&lt;numatoms; atomid++) { \n  float dy = coory - atominfo[atomid].y; \n  float dysqpdzsq = (dy * dy) + atominfo[atomid].z; \n  float dx1 = coorx1 - atominfo[atomid].x; \n  float dx2 = coorx2 - atominfo[atomid].x; \n  float dx3 = coorx3 - atominfo[atomid].x; \n  float dx4 = coorx4 - atominfo[atomid].x; \n  energyvalx1 += atominfo[atomid].w * (1.0f / sqrtf(dx1*dx1 + dysqpdzsq));\n  energyvalx2 += atominfo[atomid].w * (1.0f / sqrtf(dx2*dx2 + dysqpdzsq)); \n  energyvalx3 += atominfo[atomid].w * (1.0f / sqrtf(dx3*dx3 + dysqpdzsq)); \n  energyvalx4 += atominfo[atomid].w * (1.0f / sqrtf(dx4*dx4 + dysqpdzsq)); } …",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "GPU Compilers"
    ]
  },
  {
    "objectID": "lectures/revealjs_05c_pre.qmd.html#partial-redundancy-elimination",
    "href": "lectures/revealjs_05c_pre.qmd.html#partial-redundancy-elimination",
    "title": "_ partial_redundancy elimination",
    "section": "partial redundancy elimination",
    "text": "partial redundancy elimination\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[a = b + c]\nb2[\" \"]\nb3[d = b + c]\nb1--&gt; b3\nb2--&gt; b3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[a = b + c]\nb2[\" \"]\nb3[d = b + c]\nb1--&gt; b3\nb2--&gt; b3\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[\"t = b + c\n    a = t\"]\nb2[t = b + c]\nb3[d =t]\nb1--&gt; b3\nb2--&gt; b3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[\"t = b + c\n    a = t\"]\nb2[t = b + c]\nb3[d =t]\nb1--&gt; b3\nb2--&gt; b3"
  },
  {
    "objectID": "lectures/revealjs_05c_pre.qmd.html#simplifications",
    "href": "lectures/revealjs_05c_pre.qmd.html#simplifications",
    "title": "_ partial_redundancy elimination",
    "section": "simplifications",
    "text": "simplifications\n\nonly going to look at one expression \\(b + c\\)\ninitially all nodes in the cfg contain at most one assignment statement\nif there is a node that has multiple successors (a branch node) and one of the successors has multiple predecessors (a join) node we have added a extra node between them"
  },
  {
    "objectID": "lectures/revealjs_05c_pre.qmd.html#down-safe",
    "href": "lectures/revealjs_05c_pre.qmd.html#down-safe",
    "title": "_ partial_redundancy elimination",
    "section": "down safe",
    "text": "down safe\nwe are moving computations earlier in the cfg\ndon’t move so far that it might not be used, or that an argument gets changed\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[\" \"]\nb2[\" \"]\nb3[b = d + e]\nb4[a = b + c]\nb5[\" \"]\nb6[d = b + c]\nb1 --&gt; b2\nb1 --&gt; b3\nb2 --&gt; b4\nb4 --&gt; b5\nb5 --&gt; b4\nb3 --&gt; b6\nb4 --&gt; exit\nb6 --&gt; exit\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[\" \"]\nb2[\" \"]\nb3[b = d + e]\nb4[a = b + c]\nb5[\" \"]\nb6[d = b + c]\nb1 --&gt; b2\nb1 --&gt; b3\nb2 --&gt; b4\nb4 --&gt; b5\nb5 --&gt; b4\nb3 --&gt; b6\nb4 --&gt; exit\nb6 --&gt; exit\n\n\n\n\n\n\n\n\ncannot move b +c to top, because b changes"
  },
  {
    "objectID": "lectures/revealjs_05c_pre.qmd.html#up-safe",
    "href": "lectures/revealjs_05c_pre.qmd.html#up-safe",
    "title": "_ partial_redundancy elimination",
    "section": "up-safe",
    "text": "up-safe\nWe can add a computation of \\(b+c\\) in any down-safe node. We want to pick a good one.\ndefine up-safe(block) (also called available) if \\(b+c\\) will be definitely used without being killed, computed on every path from entry to the block and not killed\nDo not add \\(b+c\\) to a block if the expression is available at that block\nup-safe is a second data flow problem\n\\[\nU_{\\text{safe}}(\\text{entry}) = \\text{false}\n\\]\n\\[\nU_{\\text{safe}}(n)=  \\text{trans}(n) \\cap_{p \\in \\text{preds}(n)} \\text{used}(p) \\cup \\text{U}_{\\text{safwe}}(p)\n\\]"
  },
  {
    "objectID": "lectures/revealjs_05c_pre.qmd.html#placement",
    "href": "lectures/revealjs_05c_pre.qmd.html#placement",
    "title": "_ partial_redundancy elimination",
    "section": "placement",
    "text": "placement\nwant a down-safe node, that is not up-safe\n\npick the closest to the entry (min number of computations)\npick a later node to lower register pressure\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nA[a: Down-safe]\nB[b+c  :avail]\nC[c: Down-safe]\nD[:avail,Down-safe]\nE[e: Down-safe]\nF[b+c: Down-safe]\nA--&gt;C\nB--&gt;D\nC--&gt; E\nD--&gt; E\nE--&gt; F\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nA[a: Down-safe]\nB[b+c  :avail]\nC[c: Down-safe]\nD[:avail,Down-safe]\nE[e: Down-safe]\nF[b+c: Down-safe]\nA--&gt;C\nB--&gt;D\nC--&gt; E\nD--&gt; E\nE--&gt; F\n\n\n\n\n\n\nWe could move b+c in nodes a,c or e, but e does not help"
  },
  {
    "objectID": "lectures/05c_pre.html",
    "href": "lectures/05c_pre.html",
    "title": "_ partial_redundancy elimination",
    "section": "",
    "text": "%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[a = b + c]\nb2[\" \"]\nb3[d = b + c]\nb1--&gt; b3\nb2--&gt; b3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[a = b + c]\nb2[\" \"]\nb3[d = b + c]\nb1--&gt; b3\nb2--&gt; b3\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[\"t = b + c\n    a = t\"]\nb2[t = b + c]\nb3[d =t]\nb1--&gt; b3\nb2--&gt; b3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[\"t = b + c\n    a = t\"]\nb2[t = b + c]\nb3[d =t]\nb1--&gt; b3\nb2--&gt; b3",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "_ partial_redundancy elimination"
    ]
  },
  {
    "objectID": "lectures/05c_pre.html#partial-redundancy-elimination",
    "href": "lectures/05c_pre.html#partial-redundancy-elimination",
    "title": "_ partial_redundancy elimination",
    "section": "",
    "text": "%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[a = b + c]\nb2[\" \"]\nb3[d = b + c]\nb1--&gt; b3\nb2--&gt; b3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[a = b + c]\nb2[\" \"]\nb3[d = b + c]\nb1--&gt; b3\nb2--&gt; b3\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[\"t = b + c\n    a = t\"]\nb2[t = b + c]\nb3[d =t]\nb1--&gt; b3\nb2--&gt; b3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[\"t = b + c\n    a = t\"]\nb2[t = b + c]\nb3[d =t]\nb1--&gt; b3\nb2--&gt; b3",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "_ partial_redundancy elimination"
    ]
  },
  {
    "objectID": "lectures/05c_pre.html#simplifications",
    "href": "lectures/05c_pre.html#simplifications",
    "title": "_ partial_redundancy elimination",
    "section": "simplifications",
    "text": "simplifications\n\nonly going to look at one expression \\(b + c\\)\ninitially all nodes in the cfg contain at most one assignment statement\nif there is a node that has multiple successors (a branch node) and one of the successors has multiple predecessors (a join) node we have added a extra node between them",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "_ partial_redundancy elimination"
    ]
  },
  {
    "objectID": "lectures/05c_pre.html#down-safe",
    "href": "lectures/05c_pre.html#down-safe",
    "title": "_ partial_redundancy elimination",
    "section": "down safe",
    "text": "down safe\nwe are moving computations earlier in the cfg\ndon’t move so far that it might not be used, or that an argument gets changed\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[\" \"]\nb2[\" \"]\nb3[b = d + e]\nb4[a = b + c]\nb5[\" \"]\nb6[d = b + c]\nb1 --&gt; b2\nb1 --&gt; b3\nb2 --&gt; b4\nb4 --&gt; b5\nb5 --&gt; b4\nb3 --&gt; b6\nb4 --&gt; exit\nb6 --&gt; exit\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[\" \"]\nb2[\" \"]\nb3[b = d + e]\nb4[a = b + c]\nb5[\" \"]\nb6[d = b + c]\nb1 --&gt; b2\nb1 --&gt; b3\nb2 --&gt; b4\nb4 --&gt; b5\nb5 --&gt; b4\nb3 --&gt; b6\nb4 --&gt; exit\nb6 --&gt; exit\n\n\n\n\n\n\n\n\ncannot move b +c to top, because b changes",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "_ partial_redundancy elimination"
    ]
  },
  {
    "objectID": "lectures/05c_pre.html#up-safe",
    "href": "lectures/05c_pre.html#up-safe",
    "title": "_ partial_redundancy elimination",
    "section": "up-safe",
    "text": "up-safe\nWe can add a computation of \\(b+c\\) in any down-safe node. We want to pick a good one.\ndefine up-safe(block) (also called available) if \\(b+c\\) will be definitely used without being killed, computed on every path from entry to the block and not killed\nDo not add \\(b+c\\) to a block if the expression is available at that block\nup-safe is a second data flow problem\n\\[\nU_{\\text{safe}}(\\text{entry}) = \\text{false}\n\\]\n\\[\nU_{\\text{safe}}(n)=  \\text{trans}(n) \\cap_{p \\in \\text{preds}(n)} \\text{used}(p) \\cup \\text{U}_{\\text{safwe}}(p)\n\\]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "_ partial_redundancy elimination"
    ]
  },
  {
    "objectID": "lectures/05c_pre.html#placement",
    "href": "lectures/05c_pre.html#placement",
    "title": "_ partial_redundancy elimination",
    "section": "placement",
    "text": "placement\nwant a down-safe node, that is not up-safe\n\npick the closest to the entry (min number of computations)\npick a later node to lower register pressure\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nA[a: Down-safe]\nB[b+c  :avail]\nC[c: Down-safe]\nD[:avail,Down-safe]\nE[e: Down-safe]\nF[b+c: Down-safe]\nA--&gt;C\nB--&gt;D\nC--&gt; E\nD--&gt; E\nE--&gt; F\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nA[a: Down-safe]\nB[b+c  :avail]\nC[c: Down-safe]\nD[:avail,Down-safe]\nE[e: Down-safe]\nF[b+c: Down-safe]\nA--&gt;C\nB--&gt;D\nC--&gt; E\nD--&gt; E\nE--&gt; F\n\n\n\n\n\n\nWe could move b+c in nodes a,c or e, but e does not help",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "_ partial_redundancy elimination"
    ]
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#intro-to-polyhedral-techniques",
    "href": "lectures/revealjs_09_poly.qmd.html#intro-to-polyhedral-techniques",
    "title": "Polyhedral Analysis",
    "section": "intro to polyhedral techniques",
    "text": "intro to polyhedral techniques\nThere are two kinds of polyhedral problems:\n\npolyhedral analysis - given a loop transform, does the behavior change- Is it valid?\npolyhedral scheduling - find a transform that maximizes/minimizes some property"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#base-idea",
    "href": "lectures/revealjs_09_poly.qmd.html#base-idea",
    "title": "Polyhedral Analysis",
    "section": "Base Idea",
    "text": "Base Idea\nThe base ideas -\n\nA statement in a loop might execute a lot of times. Each time it executes there is one instance of the statement. Polyhedral methods keep track of instances.\nWe can think of a program as having two parts: An algorithm, like \\[ a[i] = 3, i \\in \\{1,2,3\\}\\] and a schedule like: execute the instances in reverse order."
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#poly-steps",
    "href": "lectures/revealjs_09_poly.qmd.html#poly-steps",
    "title": "Polyhedral Analysis",
    "section": "poly steps",
    "text": "poly steps\n\nchange the program into a polyhedral notations (a set like notation)\nApply some kind of transformations to add a schedule\nGenerate code that lets the result execute on a computer"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#polyhedral-methods",
    "href": "lectures/revealjs_09_poly.qmd.html#polyhedral-methods",
    "title": "Polyhedral Analysis",
    "section": "Polyhedral Methods",
    "text": "Polyhedral Methods\n\nUse equations as an intermediate representation (IR)\nAllow reasoning about each instance\nEnsure finiteness (even if the number of instances is not)\nReduce phase ordering issues by applying multiple transformations simultaneously\nIdeal for tiling, parallelism, and cache management"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#negatives-of-polyhedral-analysis",
    "href": "lectures/revealjs_09_poly.qmd.html#negatives-of-polyhedral-analysis",
    "title": "Polyhedral Analysis",
    "section": "Negatives of Polyhedral Analysis",
    "text": "Negatives of Polyhedral Analysis\n\nOnly applies to loop nests, but we can wrap a for (i= 0; i &lt;1; i++) around a group of statements\nRequires affine array indexes, bounds, and statements\nNot applicable to loops hidden by recursion"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#an-example-changing-the-order-of-iterations",
    "href": "lectures/revealjs_09_poly.qmd.html#an-example-changing-the-order-of-iterations",
    "title": "Polyhedral Analysis",
    "section": "an example changing the order of iterations",
    "text": "an example changing the order of iterations\ncan we reverse this loop: (change the schedule so that i takes values 4,3,2,1). Does it get the same answer\nfor i = [1,2,3,4]\ns:   a[i] = a[i-1]"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#a-graphical-view",
    "href": "lectures/revealjs_09_poly.qmd.html#a-graphical-view",
    "title": "Polyhedral Analysis",
    "section": "a graphical view",
    "text": "a graphical view\n(not legal if there is a pair where the arrows go in opposite directions)\n\n\nCode\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Define the range of the loops\nn = 4  # Range for 'i' (1 to 4)\n\n# Create lists to hold the iteration points\ni_values = []\nj_values = []\n\n# Nested loops to generate iteration space\nfor i in range(1, n + 1):\n  i_values.append(i)\n  j_values.append(1)\n\n# Plotting the iteration space\nplt.figure(figsize=(6, 3))\nplt.scatter(i_values, j_values, c='blue', marker='o')\nplt.xlabel('i  loop index)')\nplt.title('Iteration Space with Data Flow')\nplt.grid(True)\nplt.gca().invert_yaxis()\n\nplt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n#plt.gca().yaxis.set_ticklabels([])\n\n# Annotate the iteration order and add arrows for data flow\nfor i in range(2, n + 1):\n    plt.annotate('',\n      xy=(i,1), xytext=(i-1,1),\n        arrowprops=dict(facecolor='black', shrink=0.05))\n\nplt.annotate(\"time left schedule\", xy=(n, 1.02), xytext=(1,1.02), arrowprops=dict(facecolor='green', shrink=0.05))\n\nplt.annotate(\"time right schedule\", xy=(1, 1.04), xytext=(n-1,1.04), arrowprops=dict(facecolor='red', shrink=0.05))\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nThere are 3 pairs that make this invalid"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#a-second-example",
    "href": "lectures/revealjs_09_poly.qmd.html#a-second-example",
    "title": "Polyhedral Analysis",
    "section": "a second example",
    "text": "a second example\nfor i in [1,2,3,4]\n  for j in [1,2,3,4]\n    a[i,j] = a[i,j-1]+ a[i-1,j]\ncan we execute this is parallel"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#definitions",
    "href": "lectures/revealjs_09_poly.qmd.html#definitions",
    "title": "Polyhedral Analysis",
    "section": "definitions",
    "text": "definitions\nan affine function\n\\[\n\\text { affine function } f(\\vec{v})=M_{f} \\vec{v}+\\vec{f}_{0}\n\\]\nwhere \\(\\vec{v}=\\left(\\begin{array}{c}v_{1} \\\\ \\vdots \\\\ v_{d}\\end{array}\\right)\\) and \\(M_{f} \\in \\mathbb{R}^{k \\times d}\\) is a matrix with \\(k\\) rows and \\(d\\) columns, \\(f_{0} \\in \\mathbb{R}^{k}\\) is a \\(k\\)-dimensional vector. In all cases, we deal with affine functions with \\(M_{f} \\in \\mathbb{Z}^{k \\times d}\\) and \\(f_{0} \\in \\mathbb{Z}^{k}\\). The domain is also a set of integers: \\(\\vec{v} \\in \\mathbb{Z}^{d}\\).\nPerfect loop nest, Imperfect loop nest. A set of nested loops is called a perfect loop nest iff all statements appearing in the nest appear inside the body of the innermost loop. Otherwise, the loop nest is called an imperfect loop nest.\nAffine loop nest. Affine loop nests are sequences of imperfectly nested loops with loop bounds and array accesses that are affine functions of outer loop variables and program parameters.\nProgram parameters or structure parameters are symbolic constants that appear in loop bounds or access functions. They very often represent the problem size. \\(N\\) and beta are the program parameters.\nfor (i=0 ; i&lt;N ; i++)\n    for (j=0 ; j&lt;N ; j++) \n        S1: A[i,j]=A[i,j]+  u1[i] * v1[j]+u2[i] * v2[j] ;\n\nfor $(k=0 ;k&lt;N; k++ )\n    for (l}=0 ; l&lt;N ; l++)\n        S2: x[k]=x[k]+beta*A[l,k] *y[l] ;\nA portion of the GEMVER kernel\nAffine spaces. A set of vectors is an affine space iff it is closed under affine combination, i.e., if \\(\\vec{x}, \\vec{y}\\) are in the space, all points lying on the line joining \\(\\vec{x}\\) and \\(\\vec{y}\\) belong to the space.\nAffine hyperplane An affine hyperplane is an \\(n-1\\) dimensional affine sub-space of an \\(n\\) dimensional space.\nIn our context, the set of all vectors \\(v \\in \\mathbb{Z}^{n}\\) such that \\(\\mathbf{h} . \\vec{v}=k\\), for \\(k \\in \\mathbb{Z}\\), forms an affine hyperplane. The set of parallel hyperplane instances correspond to different values of \\(k\\) with the row vector \\(\\mathbf{h}\\) normal to the hyperplane. Two vectors \\(\\overrightarrow{v_{1}}\\) and \\(\\overrightarrow{v_{2}}\\) lie in the same hyperplane if \\(\\mathbf{h} \\cdot \\overrightarrow{v_{1}}=\\mathbf{h} \\cdot \\overrightarrow{v_{2}}\\).\n\nAn affine hyperplane\nPolyhedron, Polytope. A polyhedron is an intersection of a finite number of half-spaces. A polytope is a bounded polyhedron.\nEach of the half-spaces provides a face to the polyhedron. Hence, the set of affine inequalities, each representing a face, can be used to compactly represent the polyhedron. If there are \\(m\\) inequalities, then the polyhedron is\n\\[\n\\left\\{\\vec{x} \\in \\mathbb{R}^{n} \\mid A \\vec{x}+\\vec{b} \\geq \\overrightarrow{0}\\right\\}\n\\]\nwhere \\(A \\in \\mathbb{R}^{m \\times n}\\) and \\(\\vec{b} \\in \\mathbb{R}^{m}\\).\n\nIn our context, we are always interested in the integer points inside a polyhedron since loop iterators typically have integer data types and traverse an integer space. The matrix \\(A\\) and \\(\\vec{b}\\) for problems we will deal with also comprise only integers. So, we always have:\n\\[\n\\begin{equation*}\n\\left\\{\\vec{x} \\in \\mathbb{Z}^{n} \\mid A \\vec{x}+\\vec{b} \\geq \\overrightarrow{0}\\right\\}\n\\end{equation*}\n\\]\nwhere \\(A \\in \\mathbb{Z}^{m \\times n}\\) and \\(\\vec{b} \\in \\mathbb{Z}^{m}\\).\nIteration vector. The iteration vector of a statement is the vector consisting of values of the indices of all loops surrounding the statement.\nLet \\(S\\) be a statement of a program. The iteration vector is denoted by \\(\\vec{i}_{S}\\). An iteration vector represents a dynamic instance of a statement appearing in a loop nest that may be nested perfectly or imperfectly.\nDomain, Index set. The set of all iteration vectors for a given statement is the domain or the index set of the statement.\nA program comprises a sequence of statements, each statement surrounded by loops in a given order. We denote the domain of a statement \\(S\\) by \\(\\mathcal{D}^{S}\\). When the loop bounds and data accesses are affine functions of outer loop indices and other program parameters, and all conditionals are statically predictable, the domain of every statement is a polyhedron as defined in (. Again, conditionals that are affine functions of outer loop indices and program parameters are statically predictable.\nEach dynamic instance of a statement \\(S\\), in a program, is identified by its iteration vector \\(\\vec{i}_{S}\\) which contains values for the indices of the loops surrounding \\(S\\), from outermost to innermost. A statement \\(S\\) is associated with a polytope \\(\\mathcal{D}^{S}\\) of dimensionality \\(m_{S}\\). Each point in the polytope is an \\(m_{S}\\)-dimensional iteration vector.\nfor (i=0 ; i&lt;N ; i++)\n    for (j=0 ; j&lt;N ; j++) \n        S1: A[i,j]=A[i,j]+  u1[i] * v1[j]+u2[i] * v2[j] ;\n\nfor $(k=0 ;k&lt;N; k++ )\n    for (l}=0 ; l&lt;N ; l++)\n        S2: x[k]=x[k]+beta*A[l,k] *y[l] ;\n\\[\n\\begin{aligned}\ni & \\geq 0 \\\\\nj & \\geq 0 \\\\\n-i+N-1 & \\geq 0 \\\\\n-j+N-1 & \\geq 0\n\\end{aligned} \\quad \\quad \\mathcal{D}^{S_{1}}:\\left(\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n-1 & 0 & 1 & -1 \\\\\n0 & -1 & 1 & -1\n\\end{array}\\right)\\left(\\begin{array}{c}\ni \\\\\nj \\\\\nN \\\\\n1\n\\end{array}\\right) \\geq 0\n\\]"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#iteration-space-as-a-set-in-matrix-multiplication",
    "href": "lectures/revealjs_09_poly.qmd.html#iteration-space-as-a-set-in-matrix-multiplication",
    "title": "Polyhedral Analysis",
    "section": "Iteration space as a set in matrix multiplication",
    "text": "Iteration space as a set in matrix multiplication\nfor (i =0 ;i &lt; M ; i++)\n for (j =0l j &lt; N; j++)\n   for (k= 0; k &lt; K, k++)\n     c[i,j] = c[i,j] + a[i,k]* B[k,j]\nIteration domain as a set:\n\\[ [M, N, K] -&gt; \\{ S[i, j, k] : 0 &lt;= i &lt; M \\and  0 &lt;= j &lt; N \\and 0 &lt;= k &lt; K; \\}\\]\n\\[ writes -&gt;  \\{ S[i, j, k] -&gt; C[i, j] \\}\\]\n\\[ reads := \\{S[i, j, k] -&gt; B[k, j], S[i, j, k] -&gt; A[i, k], S[i, j, k] -&gt; C[i, j] \\}\\]"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#dependence",
    "href": "lectures/revealjs_09_poly.qmd.html#dependence",
    "title": "Polyhedral Analysis",
    "section": "Dependence",
    "text": "Dependence\nTwo instances are dependent if they access the same location and one of them is a write.\ntrue dependence producer is a write, consumer is a read. Also called read after write to RAW, also called a flow dependence\nanti dependence write after read. WAR\n*output dependence both writes WAW"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#dependence-in-example",
    "href": "lectures/revealjs_09_poly.qmd.html#dependence-in-example",
    "title": "Polyhedral Analysis",
    "section": "dependence in example",
    "text": "dependence in example\nfor (i=0 ; i&lt;N ; i++)\n    for (j=0 ; j&lt;N ; j++) \n        S1: A[i,j]=A[i,j]+  u1[i] * v1[j]+u2[i] * v2[j] ;\n\nfor (k=0 ;k&lt;N; k++ )\n    for (l=0 ; l&lt;N ; l++)\n        S2: x[k]=x[k]+beta*A[l,k] *y[l] ;\nA[i,j] same location as A[l,k]\ni between 0 and N\nj between 0 and N\nk between 0 and N\nl between 0 and N\ni - l == 0\nj - k == 0\nDomain for statement S1"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#loop-interchange-example",
    "href": "lectures/revealjs_09_poly.qmd.html#loop-interchange-example",
    "title": "Polyhedral Analysis",
    "section": "loop interchange example",
    "text": "loop interchange example\nfor (i = 0, i &lt; 9; i++)\n  for (j = i; j &lt; 7 && j &lt; i+4; j++)\n     a[i,j] = \n\nThere is no dependence so we can reorder the loops but what are the new bounds\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Initialize the coordinates for the domain\ndomain_i = []\ndomain_j = []\n\n# Loop through the values as described in the pseudocode\nfor i in range(9):  # i = 0 to 8\n    for j in range(i, min(7, i + 4)):  # j starts from i, and is less than both 7 and i+4\n        domain_i.append(i)\n        domain_j.append(j)\n\n# Plot the domain points\nplt.scatter(domain_i, domain_j, color='blue', label='Domain Points')\n\n# Define the range for i and j for the hyperplanes\ni_values = np.arange(0, 9)\nj_values_i = i_values  # j = i\nj_values_7 = np.full_like(i_values, 7)  # j = 7\nj_values_i_plus_4 = i_values + 4  # j = i + 4\n\n# Plot the hyperplanes\nplt.plot(i_values, j_values_i, label='j = i', color='red')\nplt.plot(i_values, j_values_7, label='j = 7', color='green')\nplt.plot(i_values, j_values_i_plus_4, label='j = i + 4', color='orange')\n\n# Set plot properties\nplt.title('Domain of (i, j) with Hyperplanes')\nplt.xlabel('i')\nplt.ylabel('j')\nplt.grid(True)\nplt.xticks(range(9))\nplt.yticks(range(7))\nplt.gca().invert_yaxis()  # Invert y-axis to match matrix notation\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#reodered-loops",
    "href": "lectures/revealjs_09_poly.qmd.html#reodered-loops",
    "title": "Polyhedral Analysis",
    "section": "reodered loops",
    "text": "reodered loops\nfor (j = 0; j &lt;=6; j++)\n for (i = max(j-3,0); i &lt;= j; i++)\n   a[i,j] = 0\noutloop bounds cannot depend on i\ninner loop bounds can depend on j\nWe can read off j as a proection, for each value of j, i is a convex polygon (if the bounds were not convex this would be false)"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#fourier-motzkin-method",
    "href": "lectures/revealjs_09_poly.qmd.html#fourier-motzkin-method",
    "title": "Polyhedral Analysis",
    "section": "fourier-motzkin method",
    "text": "fourier-motzkin method\ninput:\n\nA convex polygon S on \\(x_1, x_2, ... x_n\\)\nA variable \\(X_m\\) to be eliminated\n\noutput: \\(S^1\\) a projection of S with out dimiension \\(m\\)"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#alogorithm",
    "href": "lectures/revealjs_09_poly.qmd.html#alogorithm",
    "title": "Polyhedral Analysis",
    "section": "alogorithm",
    "text": "alogorithm\n\\[ S = \\{ vec(x) | B vec(x) + vec(f) \\ge 0 \\}\\] \\[C\\] = constraints in S that involve \\(S_m\\) (coef is not zero)\n-======================="
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#how-much-of-a-limitation-is-affine-only",
    "href": "lectures/revealjs_09_poly.qmd.html#how-much-of-a-limitation-is-affine-only",
    "title": "Polyhedral Analysis",
    "section": "How much of a limitation is affine only",
    "text": "How much of a limitation is affine only\n99% of hpc loops are affine C. Bastoul, A. Cohen, S. Girbal, S. Sharma, and O. Temam. Putting polyhedral loop transformations to work. In LCPC, 2003.\nover 95% of loops in deep learning are affine Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, RaminderBajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA). IEEE, 1–12."
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#some-notation",
    "href": "lectures/revealjs_09_poly.qmd.html#some-notation",
    "title": "Polyhedral Analysis",
    "section": "some notation",
    "text": "some notation\n\\[\n\\begin{align*}\ndomain &= \\left\\{ i \\in \\mathbf{Z}^1  \\mid 1 &lt;= i &lt;= 4\\right\\} \\\\\ninstances\\  of   \\ s &= \\left\\{ s(i) \\mid 1 &lt;=i &lt;= 4\\right\\}\n\\end{align*}\n\\]\nThe domain is the set of integer values of the loop index. left and right have the same domain\nWe call the order that instances execute the schedule The schedule is the ordering of instances. this is a map of instance to time."
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#what-is-a-violated-data-dependence",
    "href": "lectures/revealjs_09_poly.qmd.html#what-is-a-violated-data-dependence",
    "title": "Polyhedral Analysis",
    "section": "what is a violated data dependence",
    "text": "what is a violated data dependence\na pair (p, c) where p produces data that c consumes and c comes before p in the new schedule\ndata dependence \\[\n\\left \\{(s(i), s(i+1)) \\mid 1 \\le i \\le 3 \\right \\}\n\\]\nset of violated data dependences \\[\n\\left \\{(s(i), s(j)) \\mid \\ newSch(j) \\ge newSch(i) \\right \\}\n\\]"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#ilp-solver-info",
    "href": "lectures/revealjs_09_poly.qmd.html#ilp-solver-info",
    "title": "Polyhedral Analysis",
    "section": "ilp solver info",
    "text": "ilp solver info\ncan an ilp solver always work?\nHilbert’s tenth problem is the tenth on the list of mathematical problems that the German mathematician David Hilbert posed in 1900. It is the challenge to provide a general algorithm that, for any given Diophantine equation (a polynomial equation with integer coefficients and a finite number of unknowns), can decide whether the equation has a solution with all unknowns taking integer values.\nsadly answer is no (found in 1970)\nBut we can do this if we limit the kinds of inequalities"
  },
  {
    "objectID": "lectures/revealjs_09_poly.qmd.html#affine-limitations",
    "href": "lectures/revealjs_09_poly.qmd.html#affine-limitations",
    "title": "Polyhedral Analysis",
    "section": "Affine limitations",
    "text": "Affine limitations\naffine functions, no multiplying unknowns, no quantifiers (for all, or exists) and is ok, or ok (if we change to two problems) just add/subtract/ multiply by a constant, can add minimize/maximize a function. division and remainder by a constant, max/min/ absolute\nMost scientific programs and ai programs fit, because array subscripts are often affine\nBut lots of other kinds of programs do not.\nThere is a lot of formal math background here, decidability in logic."
  },
  {
    "objectID": "lectures/09_poly.html",
    "href": "lectures/09_poly.html",
    "title": "Polyhedral Analysis",
    "section": "",
    "text": "There are two kinds of polyhedral problems:\n\npolyhedral analysis - given a loop transform, does the behavior change- Is it valid?\npolyhedral scheduling - find a transform that maximizes/minimizes some property",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#intro-to-polyhedral-techniques",
    "href": "lectures/09_poly.html#intro-to-polyhedral-techniques",
    "title": "Polyhedral Analysis",
    "section": "",
    "text": "There are two kinds of polyhedral problems:\n\npolyhedral analysis - given a loop transform, does the behavior change- Is it valid?\npolyhedral scheduling - find a transform that maximizes/minimizes some property",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#base-idea",
    "href": "lectures/09_poly.html#base-idea",
    "title": "Polyhedral Analysis",
    "section": "Base Idea",
    "text": "Base Idea\nThe base ideas -\n\nA statement in a loop might execute a lot of times. Each time it executes there is one instance of the statement. Polyhedral methods keep track of instances.\nWe can think of a program as having two parts: An algorithm, like \\[ a[i] = 3, i \\in \\{1,2,3\\}\\] and a schedule like: execute the instances in reverse order.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#poly-steps",
    "href": "lectures/09_poly.html#poly-steps",
    "title": "Polyhedral Analysis",
    "section": "poly steps",
    "text": "poly steps\n\nchange the program into a polyhedral notations (a set like notation)\nApply some kind of transformations to add a schedule\nGenerate code that lets the result execute on a computer",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#polyhedral-methods",
    "href": "lectures/09_poly.html#polyhedral-methods",
    "title": "Polyhedral Analysis",
    "section": "Polyhedral Methods",
    "text": "Polyhedral Methods\n\nUse equations as an intermediate representation (IR)\nAllow reasoning about each instance\nEnsure finiteness (even if the number of instances is not)\nReduce phase ordering issues by applying multiple transformations simultaneously\nIdeal for tiling, parallelism, and cache management",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#negatives-of-polyhedral-analysis",
    "href": "lectures/09_poly.html#negatives-of-polyhedral-analysis",
    "title": "Polyhedral Analysis",
    "section": "Negatives of Polyhedral Analysis",
    "text": "Negatives of Polyhedral Analysis\n\nOnly applies to loop nests, but we can wrap a for (i= 0; i &lt;1; i++) around a group of statements\nRequires affine array indexes, bounds, and statements\nNot applicable to loops hidden by recursion",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#an-example-changing-the-order-of-iterations",
    "href": "lectures/09_poly.html#an-example-changing-the-order-of-iterations",
    "title": "Polyhedral Analysis",
    "section": "an example changing the order of iterations",
    "text": "an example changing the order of iterations\ncan we reverse this loop: (change the schedule so that i takes values 4,3,2,1). Does it get the same answer\nfor i = [1,2,3,4]\ns:   a[i] = a[i-1]\n\nDo these loops do the same thing\nfor i = [1,2,3,4]               for i = [4,3,2,1]\ns:   a[i] = a[i-1]               s:   a[i] = a[i-1] \n. . .\nprogram trace (instances)\n   s(1)                               s(4)\n   s(2)                               s(3)\n   s(3)                               s(2)\n   s(4)                               s(1)\nand the data that is accessed\n          reads  writes                       reads writes\n   s(1)    a[0]   a[1]               s(4)     a[3]   a[4]\n   s(2)    a[1]   a[2]               s(3)     a[2]   a[3]\n   s(3)    a[2]   a[3]               s(2)     a[1]   a[2]\n   s(4)    a[3]   a[4]               s(1)     a[0]   a[1]\n\nA transformation is valid (legal) if it preserves the dependence relations: producers have to execute before consumers\nit is not valid if there is a pair s[i] and s[j], s[i] produces a value s[j] reads that value, s[j] is first in the new schedule",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#a-graphical-view",
    "href": "lectures/09_poly.html#a-graphical-view",
    "title": "Polyhedral Analysis",
    "section": "a graphical view",
    "text": "a graphical view\n(not legal if there is a pair where the arrows go in opposite directions)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Define the range of the loops\nn = 4  # Range for 'i' (1 to 4)\n\n# Create lists to hold the iteration points\ni_values = []\nj_values = []\n\n# Nested loops to generate iteration space\nfor i in range(1, n + 1):\n  i_values.append(i)\n  j_values.append(1)\n\n# Plotting the iteration space\nplt.figure(figsize=(6, 3))\nplt.scatter(i_values, j_values, c='blue', marker='o')\nplt.xlabel('i  loop index)')\nplt.title('Iteration Space with Data Flow')\nplt.grid(True)\nplt.gca().invert_yaxis()\n\nplt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n#plt.gca().yaxis.set_ticklabels([])\n\n# Annotate the iteration order and add arrows for data flow\nfor i in range(2, n + 1):\n    plt.annotate('',\n      xy=(i,1), xytext=(i-1,1),\n        arrowprops=dict(facecolor='black', shrink=0.05))\n\nplt.annotate(\"time left schedule\", xy=(n, 1.02), xytext=(1,1.02), arrowprops=dict(facecolor='green', shrink=0.05))\n\nplt.annotate(\"time right schedule\", xy=(1, 1.04), xytext=(n-1,1.04), arrowprops=dict(facecolor='red', shrink=0.05))\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nThere are 3 pairs that make this invalid",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#a-second-example",
    "href": "lectures/09_poly.html#a-second-example",
    "title": "Polyhedral Analysis",
    "section": "a second example",
    "text": "a second example\nfor i in [1,2,3,4]\n  for j in [1,2,3,4]\n    a[i,j] = a[i,j-1]+ a[i-1,j]\ncan we execute this is parallel\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Define the range of the loops\nn = 4  # Range for 'i' (1 to 4)\n\n# Create lists to hold the iteration points\ni_values = []\nj_values = []\n\n# Nested loops to generate iteration space\nfor i in range(1, n + 1):\n  for j in range(1, n + 1):\n    i_values.append(i)\n    j_values.append(j)\n\n# Plotting the iteration space\nplt.figure(figsize=(6, 4))\nplt.scatter(i_values, j_values, c='blue', marker='o')\nplt.xlabel('i  loop index)')\nplt.ylabel('j loop index')\nplt.title('Iteration Space with Data Flow')\nplt.grid(True)\nplt.gca().invert_yaxis()\n\nplt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\nplt.gca().yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n#plt.gca().yaxis.set_ticklabels([])\n\n# Annotate the iteration order and add arrows for data flow\nfor i in range(2, n + 1):\n  for j in range(2, n +1):\n    plt.annotate('',\n       xy=(i,j), xytext=(i-1,j),\n        arrowprops=dict(facecolor='black', shrink=0.05))\n\n    plt.annotate('',\n       xy=(i,j), xytext=(i,j-1),\n        arrowprops=dict(facecolor='black', shrink=0.05))\n\n# plt.annotate(\"time left schedule\", xy=(n, 1.02), xytext=(1,1.02), arrowprops=dict(facecolor='green', shrink=0.05))\n\n# plt.annotate(\"time right schedule\", xy=(1, 1.04), xytext=(n-1,1.04), arrowprops=dict(facecolor='red', shrink=0.05))\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\noriginal code: time goes down each column\n. . .\nrun diagonal order and could tile the loops",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#definitions",
    "href": "lectures/09_poly.html#definitions",
    "title": "Polyhedral Analysis",
    "section": "definitions",
    "text": "definitions\nan affine function\n\\[\n\\text { affine function } f(\\vec{v})=M_{f} \\vec{v}+\\vec{f}_{0}\n\\]\nwhere \\(\\vec{v}=\\left(\\begin{array}{c}v_{1} \\\\ \\vdots \\\\ v_{d}\\end{array}\\right)\\) and \\(M_{f} \\in \\mathbb{R}^{k \\times d}\\) is a matrix with \\(k\\) rows and \\(d\\) columns, \\(f_{0} \\in \\mathbb{R}^{k}\\) is a \\(k\\)-dimensional vector. In all cases, we deal with affine functions with \\(M_{f} \\in \\mathbb{Z}^{k \\times d}\\) and \\(f_{0} \\in \\mathbb{Z}^{k}\\). The domain is also a set of integers: \\(\\vec{v} \\in \\mathbb{Z}^{d}\\).\nPerfect loop nest, Imperfect loop nest. A set of nested loops is called a perfect loop nest iff all statements appearing in the nest appear inside the body of the innermost loop. Otherwise, the loop nest is called an imperfect loop nest.\nAffine loop nest. Affine loop nests are sequences of imperfectly nested loops with loop bounds and array accesses that are affine functions of outer loop variables and program parameters.\nProgram parameters or structure parameters are symbolic constants that appear in loop bounds or access functions. They very often represent the problem size. \\(N\\) and beta are the program parameters.\nfor (i=0 ; i&lt;N ; i++)\n    for (j=0 ; j&lt;N ; j++) \n        S1: A[i,j]=A[i,j]+  u1[i] * v1[j]+u2[i] * v2[j] ;\n\nfor $(k=0 ;k&lt;N; k++ )\n    for (l}=0 ; l&lt;N ; l++)\n        S2: x[k]=x[k]+beta*A[l,k] *y[l] ;\nA portion of the GEMVER kernel\nAffine spaces. A set of vectors is an affine space iff it is closed under affine combination, i.e., if \\(\\vec{x}, \\vec{y}\\) are in the space, all points lying on the line joining \\(\\vec{x}\\) and \\(\\vec{y}\\) belong to the space.\nAffine hyperplane An affine hyperplane is an \\(n-1\\) dimensional affine sub-space of an \\(n\\) dimensional space.\nIn our context, the set of all vectors \\(v \\in \\mathbb{Z}^{n}\\) such that \\(\\mathbf{h} . \\vec{v}=k\\), for \\(k \\in \\mathbb{Z}\\), forms an affine hyperplane. The set of parallel hyperplane instances correspond to different values of \\(k\\) with the row vector \\(\\mathbf{h}\\) normal to the hyperplane. Two vectors \\(\\overrightarrow{v_{1}}\\) and \\(\\overrightarrow{v_{2}}\\) lie in the same hyperplane if \\(\\mathbf{h} \\cdot \\overrightarrow{v_{1}}=\\mathbf{h} \\cdot \\overrightarrow{v_{2}}\\).\n\nAn affine hyperplane\nPolyhedron, Polytope. A polyhedron is an intersection of a finite number of half-spaces. A polytope is a bounded polyhedron.\nEach of the half-spaces provides a face to the polyhedron. Hence, the set of affine inequalities, each representing a face, can be used to compactly represent the polyhedron. If there are \\(m\\) inequalities, then the polyhedron is\n\\[\n\\left\\{\\vec{x} \\in \\mathbb{R}^{n} \\mid A \\vec{x}+\\vec{b} \\geq \\overrightarrow{0}\\right\\}\n\\]\nwhere \\(A \\in \\mathbb{R}^{m \\times n}\\) and \\(\\vec{b} \\in \\mathbb{R}^{m}\\).\n\nIn our context, we are always interested in the integer points inside a polyhedron since loop iterators typically have integer data types and traverse an integer space. The matrix \\(A\\) and \\(\\vec{b}\\) for problems we will deal with also comprise only integers. So, we always have:\n\\[\n\\begin{equation*}\n\\left\\{\\vec{x} \\in \\mathbb{Z}^{n} \\mid A \\vec{x}+\\vec{b} \\geq \\overrightarrow{0}\\right\\}\n\\end{equation*}\n\\]\nwhere \\(A \\in \\mathbb{Z}^{m \\times n}\\) and \\(\\vec{b} \\in \\mathbb{Z}^{m}\\).\nIteration vector. The iteration vector of a statement is the vector consisting of values of the indices of all loops surrounding the statement.\nLet \\(S\\) be a statement of a program. The iteration vector is denoted by \\(\\vec{i}_{S}\\). An iteration vector represents a dynamic instance of a statement appearing in a loop nest that may be nested perfectly or imperfectly.\nDomain, Index set. The set of all iteration vectors for a given statement is the domain or the index set of the statement.\nA program comprises a sequence of statements, each statement surrounded by loops in a given order. We denote the domain of a statement \\(S\\) by \\(\\mathcal{D}^{S}\\). When the loop bounds and data accesses are affine functions of outer loop indices and other program parameters, and all conditionals are statically predictable, the domain of every statement is a polyhedron as defined in (. Again, conditionals that are affine functions of outer loop indices and program parameters are statically predictable.\nEach dynamic instance of a statement \\(S\\), in a program, is identified by its iteration vector \\(\\vec{i}_{S}\\) which contains values for the indices of the loops surrounding \\(S\\), from outermost to innermost. A statement \\(S\\) is associated with a polytope \\(\\mathcal{D}^{S}\\) of dimensionality \\(m_{S}\\). Each point in the polytope is an \\(m_{S}\\)-dimensional iteration vector.\nfor (i=0 ; i&lt;N ; i++)\n    for (j=0 ; j&lt;N ; j++) \n        S1: A[i,j]=A[i,j]+  u1[i] * v1[j]+u2[i] * v2[j] ;\n\nfor $(k=0 ;k&lt;N; k++ )\n    for (l}=0 ; l&lt;N ; l++)\n        S2: x[k]=x[k]+beta*A[l,k] *y[l] ;\n\\[\n\\begin{aligned}\ni & \\geq 0 \\\\\nj & \\geq 0 \\\\\n-i+N-1 & \\geq 0 \\\\\n-j+N-1 & \\geq 0\n\\end{aligned} \\quad \\quad \\mathcal{D}^{S_{1}}:\\left(\\begin{array}{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n-1 & 0 & 1 & -1 \\\\\n0 & -1 & 1 & -1\n\\end{array}\\right)\\left(\\begin{array}{c}\ni \\\\\nj \\\\\nN \\\\\n1\n\\end{array}\\right) \\geq 0\n\\]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#iteration-space-as-a-set-in-matrix-multiplication",
    "href": "lectures/09_poly.html#iteration-space-as-a-set-in-matrix-multiplication",
    "title": "Polyhedral Analysis",
    "section": "Iteration space as a set in matrix multiplication",
    "text": "Iteration space as a set in matrix multiplication\nfor (i =0 ;i &lt; M ; i++)\n for (j =0l j &lt; N; j++)\n   for (k= 0; k &lt; K, k++)\n     c[i,j] = c[i,j] + a[i,k]* B[k,j]\nIteration domain as a set:\n\\[ [M, N, K] -&gt; \\{ S[i, j, k] : 0 &lt;= i &lt; M \\and  0 &lt;= j &lt; N \\and 0 &lt;= k &lt; K; \\}\\]\n\\[ writes -&gt;  \\{ S[i, j, k] -&gt; C[i, j] \\}\\]\n\\[ reads := \\{S[i, j, k] -&gt; B[k, j], S[i, j, k] -&gt; A[i, k], S[i, j, k] -&gt; C[i, j] \\}\\]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#dependence",
    "href": "lectures/09_poly.html#dependence",
    "title": "Polyhedral Analysis",
    "section": "Dependence",
    "text": "Dependence\nTwo instances are dependent if they access the same location and one of them is a write.\ntrue dependence producer is a write, consumer is a read. Also called read after write to RAW, also called a flow dependence\nanti dependence write after read. WAR\n*output dependence both writes WAW",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#dependence-in-example",
    "href": "lectures/09_poly.html#dependence-in-example",
    "title": "Polyhedral Analysis",
    "section": "dependence in example",
    "text": "dependence in example\nfor (i=0 ; i&lt;N ; i++)\n    for (j=0 ; j&lt;N ; j++) \n        S1: A[i,j]=A[i,j]+  u1[i] * v1[j]+u2[i] * v2[j] ;\n\nfor (k=0 ;k&lt;N; k++ )\n    for (l=0 ; l&lt;N ; l++)\n        S2: x[k]=x[k]+beta*A[l,k] *y[l] ;\nA[i,j] same location as A[l,k]\ni between 0 and N\nj between 0 and N\nk between 0 and N\nl between 0 and N\ni - l == 0\nj - k == 0\nDomain for statement S1",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#loop-interchange-example",
    "href": "lectures/09_poly.html#loop-interchange-example",
    "title": "Polyhedral Analysis",
    "section": "loop interchange example",
    "text": "loop interchange example\nfor (i = 0, i &lt; 9; i++)\n  for (j = i; j &lt; 7 && j &lt; i+4; j++)\n     a[i,j] = \n\nThere is no dependence so we can reorder the loops but what are the new bounds\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Initialize the coordinates for the domain\ndomain_i = []\ndomain_j = []\n\n# Loop through the values as described in the pseudocode\nfor i in range(9):  # i = 0 to 8\n    for j in range(i, min(7, i + 4)):  # j starts from i, and is less than both 7 and i+4\n        domain_i.append(i)\n        domain_j.append(j)\n\n# Plot the domain points\nplt.scatter(domain_i, domain_j, color='blue', label='Domain Points')\n\n# Define the range for i and j for the hyperplanes\ni_values = np.arange(0, 9)\nj_values_i = i_values  # j = i\nj_values_7 = np.full_like(i_values, 7)  # j = 7\nj_values_i_plus_4 = i_values + 4  # j = i + 4\n\n# Plot the hyperplanes\nplt.plot(i_values, j_values_i, label='j = i', color='red')\nplt.plot(i_values, j_values_7, label='j = 7', color='green')\nplt.plot(i_values, j_values_i_plus_4, label='j = i + 4', color='orange')\n\n# Set plot properties\nplt.title('Domain of (i, j) with Hyperplanes')\nplt.xlabel('i')\nplt.ylabel('j')\nplt.grid(True)\nplt.xticks(range(9))\nplt.yticks(range(7))\nplt.gca().invert_yaxis()  # Invert y-axis to match matrix notation\nplt.legend()\nplt.show()",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#reodered-loops",
    "href": "lectures/09_poly.html#reodered-loops",
    "title": "Polyhedral Analysis",
    "section": "reodered loops",
    "text": "reodered loops\nfor (j = 0; j &lt;=6; j++)\n for (i = max(j-3,0); i &lt;= j; i++)\n   a[i,j] = 0\noutloop bounds cannot depend on i\ninner loop bounds can depend on j\nWe can read off j as a proection, for each value of j, i is a convex polygon (if the bounds were not convex this would be false)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#fourier-motzkin-method",
    "href": "lectures/09_poly.html#fourier-motzkin-method",
    "title": "Polyhedral Analysis",
    "section": "fourier-motzkin method",
    "text": "fourier-motzkin method\ninput:\n\nA convex polygon S on \\(x_1, x_2, ... x_n\\)\nA variable \\(X_m\\) to be eliminated\n\noutput: \\(S^1\\) a projection of S with out dimiension \\(m\\)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#alogorithm",
    "href": "lectures/09_poly.html#alogorithm",
    "title": "Polyhedral Analysis",
    "section": "alogorithm",
    "text": "alogorithm\n\\[ S = \\{ vec(x) | B vec(x) + vec(f) \\ge 0 \\}\\] \\[C\\] = constraints in S that involve \\(S_m\\) (coef is not zero)\n-=======================",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#how-much-of-a-limitation-is-affine-only",
    "href": "lectures/09_poly.html#how-much-of-a-limitation-is-affine-only",
    "title": "Polyhedral Analysis",
    "section": "How much of a limitation is affine only",
    "text": "How much of a limitation is affine only\n99% of hpc loops are affine C. Bastoul, A. Cohen, S. Girbal, S. Sharma, and O. Temam. Putting polyhedral loop transformations to work. In LCPC, 2003.\nover 95% of loops in deep learning are affine Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, RaminderBajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017. In-datacenter performance analysis of a tensor processing unit. In 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA). IEEE, 1–12.\n\nOver the course of this, I’ll use 3 pieces of math\n\nILP integer linear programming find a set of integers that satisfies a set of inequalities and maximize something\nfourier-motzkin method\nThe affine form of Farkas Lemma (maybe)\n\n\nHere is a graph showing the data flow \n\n::: {#cc886d59 .cell execution_count=4}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Define the range of the loops\nn = 4  # Range for 'i' (1 to 4)\n\n# Create lists to hold the iteration points\ni_values = []\nj_values = []\n\n# Nested loops to generate iteration space\nfor i in range(1, n + 1):\n  i_values.append(i)\n  j_values.append(1)\n\n# Plotting the iteration space\nplt.figure(figsize=(6, 3))\nplt.scatter(i_values, j_values, c='blue', marker='o')\nplt.xlabel('i  loop index)')\nplt.title('Iteration Space with Data Flow')\nplt.grid(True)\nplt.gca().invert_yaxis()\n\nplt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\nplt.gca().yaxis.set_ticklabels([])\n\n# Annotate the iteration order and add arrows for data flow\nfor i in range(2, n + 1):\n    plt.annotate('',\n      xy=(i,1), xytext=(i-1,1),\n        arrowprops=dict(facecolor='black', shrink=0.05))\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n:::\n\nWe need to preserve order where an instance produces a value, some other instance consumes the value\nmore formally:\nThe order that statements execute is called a *schedule\neach s(i) is an instance of a single statement\ndata dependence is a mapping: \\(s(i) =&gt; s(i+1)\\)\na valid schedule does not violate data dependence\n\nobservation 1- same set of instances\nif we track each instance, we have more info but could run out of space\nwe need a more compressed ir format, polyhedral methods use sets and math",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#some-notation",
    "href": "lectures/09_poly.html#some-notation",
    "title": "Polyhedral Analysis",
    "section": "some notation",
    "text": "some notation\n\\[\n\\begin{align*}\ndomain &= \\left\\{ i \\in \\mathbf{Z}^1  \\mid 1 &lt;= i &lt;= 4\\right\\} \\\\\ninstances\\  of   \\ s &= \\left\\{ s(i) \\mid 1 &lt;=i &lt;= 4\\right\\}\n\\end{align*}\n\\]\nThe domain is the set of integer values of the loop index. left and right have the same domain\nWe call the order that instances execute the schedule The schedule is the ordering of instances. this is a map of instance to time.\n\nobservation 2- different schedules. instance -&gt; time\n\\[\n\\begin{align*}\nleft \\ schedule &= \\left\\{ s[i] \\rightarrow i \\right\\} \\\\\nright \\ schedule &= \\left\\{ s[i] \\rightarrow 5-i \\right\\}\n\\end{align*}\n\\]\ndata dependences (just the same as before but with bounds) \\[\ndependence = \\left \\{(s(i), s(i+1)) \\mid 1&lt;= i &lt;= 3 \\right \\}\n\\]\nNew schedule is legal, if it respects all data dependences, or set of violated data dependences is empty",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#what-is-a-violated-data-dependence",
    "href": "lectures/09_poly.html#what-is-a-violated-data-dependence",
    "title": "Polyhedral Analysis",
    "section": "what is a violated data dependence",
    "text": "what is a violated data dependence\na pair (p, c) where p produces data that c consumes and c comes before p in the new schedule\ndata dependence \\[\n\\left \\{(s(i), s(i+1)) \\mid 1 \\le i \\le 3 \\right \\}\n\\]\nset of violated data dependences \\[\n\\left \\{(s(i), s(j)) \\mid \\ newSch(j) \\ge newSch(i) \\right \\}\n\\]\n\nDependence is \\(s[i] -&gt; s[i+1]\\)\nset of violated data dependences \\[\n\\left \\{(s(i), s(j)) \\mid \\ newSch(j) \\ge newSch(i) \\right \\}\n\\]\n\\[\n\\left \\{ (s(i), s(i+1)) \\mid 1 \\le i \\le 3 \\land 5-i+1 \\ge 5-i  \\right \\}\n\\]\nconstrants are : \\[\n\\begin{align*}\n1&lt;= i &lt;= 3 \\\\\n5-i +1  &gt;= 5-i\n\\end{align*}\n\\]\n\nUse an ilp solver to find a solution one solution is i = 1",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#ilp-solver-info",
    "href": "lectures/09_poly.html#ilp-solver-info",
    "title": "Polyhedral Analysis",
    "section": "ilp solver info",
    "text": "ilp solver info\ncan an ilp solver always work?\nHilbert’s tenth problem is the tenth on the list of mathematical problems that the German mathematician David Hilbert posed in 1900. It is the challenge to provide a general algorithm that, for any given Diophantine equation (a polynomial equation with integer coefficients and a finite number of unknowns), can decide whether the equation has a solution with all unknowns taking integer values.\nsadly answer is no (found in 1970)\nBut we can do this if we limit the kinds of inequalities",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/09_poly.html#affine-limitations",
    "href": "lectures/09_poly.html#affine-limitations",
    "title": "Polyhedral Analysis",
    "section": "Affine limitations",
    "text": "Affine limitations\naffine functions, no multiplying unknowns, no quantifiers (for all, or exists) and is ok, or ok (if we change to two problems) just add/subtract/ multiply by a constant, can add minimize/maximize a function. division and remainder by a constant, max/min/ absolute\nMost scientific programs and ai programs fit, because array subscripts are often affine\nBut lots of other kinds of programs do not.\nThere is a lot of formal math background here, decidability in logic.\n\nilp is np-complete so it is slow, but often tractable for problems with up to several hundred variables.\nSome compiler writers feel that means can only do this for toy programs? What do you think?\n\n2 multiple dimensions\nfor (i=1; i&lt;=n; i++)\nfor (j=1; j&lt;=n; j++)\nS: A[i][j] = (A[i-1][j] + A[i][j-1]) * 0.5;\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Define the range of the loops\nn = 4  # You can change n to any desired value\n\n# Create lists to hold the iteration points\ni_values = []\nj_values = []\n\n# Nested loops to generate iteration space\nfor i in range(1, n + 1):\n    for j in range(1, n + 1):\n        i_values.append(i)\n        j_values.append(j)\n\n# Plotting the iteration space\nplt.figure(figsize=(5, 5))\nplt.scatter(i_values, j_values, c='blue', marker='o')\nplt.xlabel('i (outer loop index)')\nplt.ylabel('j (inner loop index)')\nplt.title('Data Dependence for A[i][j] = (A[i-1][j] + A[i][j-1]) * 0.5')\nplt.grid(True)\nplt.gca().invert_yaxis()\n\n# Annotate the iteration order and add arrows for data dependencies\nfor i in range(1, n + 1):\n    for j in range(1, n + 1):\n        if i &gt; 1:  # Dependency on A[i-1][j]\n            plt.annotate('',\n                         xy=(i, j), xytext=(i-1, j),\n                         arrowprops=dict(facecolor='black', shrink=0.05))\n        if j &gt; 1:  # Dependency on A[i][j-1]\n            plt.annotate('',\n                         xy=(i, j), xytext=(i, j-1),\n                         arrowprops=dict(facecolor='black', shrink=0.05))\n\n# Force the x-axis to use integer labels and remove y-axis labels\nplt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\nplt.gca().yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n#plt.gca().yaxis.set_ticklabels([])  # Turn off y-axis labels\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nThe schedule actually goes up each column (the j values) going over the columns left to right (the i column)\nnotice that this does not execute in parallel, but it could if pick a different schedule\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.ticker as ticker\n\nplt.figure(figsize=(8, 8))\ntile_size = 1\n\n# # Define parameters\n# n = 12 # Size of the matrix\n# tile_size = 2  # Size of the tile\n\n# Create lists to hold the iteration points\ni_values = []\nj_values = []\n\n# Calculate iteration points for tiles\nfor t in range(8):\n  for p in range(8):\n    i = p + t\n    j = p\n    if i &gt;= 8:\n      continue\n    if (i &gt;= 0 and  j &gt;= 0) :\n      i_values.append(i)\n      j_values.append(j)\n      if i &lt;= j:\n        continue\n      if ((t % 2) == 0) and (p % 2 ) == 0:\n          plt.gca().add_patch(   \\\n          patches.Rectangle((t,p), tile_size, tile_size,  \\\n                              linewidth=1, edgecolor='red', facecolor='none'))\n#         for i in range(ti, min(ti + tile_size, n + 1)):\n#             for j in range(tj, min(tj + tile_size, n + 1)):\n#                 if (i &gt; 1 and  j &gt; 1) :\n#                   i_values.append(i)\n#                   j_values.append(j)\n\n# # Plotting the iteration space\n\nplt.scatter(i_values, j_values, c='blue', marker='o', label='Iterations')\nplt.xlabel('i')\nplt.ylabel('j')\nplt.title('Iteration Space with Tiling and Dependencies')\n# plt.grid(True)\n#lt.gca().invert_yaxis()\n\n# # Draw a box around each tile\n# for ti in range(1, n + 1, tile_size):\n#     for tj in range(1, n + 1, tile_size):\n#         plt.gca().add_patch(\n#             patches.Rectangle((ti, tj), tile_size, tile_size,\n#                                linewidth=1, edgecolor='red', facecolor='none')\n#         )\n\n# # Draw dependencies\n# for i in range(1, n + 1):\n#     for j in range(1, n + 1):\n#         if i &gt; 1:  # Dependency on A[i-1][j]\n#             plt.annotate('',\n#                          xy=(i, j), xytext=(i-1, j),\n#                          arrowprops=dict(facecolor='black', shrink=0.05))\n#         if j &gt; 1:  # Dependency on A[i][j-1]\n#             plt.annotate('',\n#                          xy=(i, j), xytext=(i, j-1),\n#                          arrowprops=dict(facecolor='red', shrink=0.05))\n\n# # Force the x-axis and y-axis to use integer labels\n# plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n# plt.gca().yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n\n# # Show the plot\n# plt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nt p i j 0 0 0 0 1 0 1 0 1 1 0 1 t-p p 2 1 1 1 2 2 0 2\n\n## formalizing the schedule, Lexicographic ordering\n\nschedule s(i,j) -&gt; (i,j)\nstatements -&gt; vector (should be a time)\n\nHow do we interpret a vector as a time, e.g. hours, min, seconds. \n\nUsually written  as $\\gg $ Generalization of alphabetical order\n\n$$\n(i,j) \\gg (m,n) \\iff i &gt;  m \\lor (i=m \\land j&gt; n)\n$$\n\nCompare left to right if terms are equal, go to next term, or different so compare the terms\n\nNotice the or we will need to call the ilp solver more than once \n\nChecking for loop interchange \n\nfor i in [1,2,3,4] for j in [1,2,3] for j in [1,2,3] for i in [1,2,3,4] s: a(i,j) = a(i-1,j+1) a(i,j) = a(i-1,j+1)\ns(i, j) -&gt; (i,j) s(i,j)=(j,i)\n\ndata flow \n\n\n        read write\ns(1,1)  a(0,2)  a(1,1)\ns(1,2)  a(0,3)  a(1,2)\ns(1,3)  a(0,4)  a(1,3)\ns(1,4)  a(0,5)  a(1,4)\ns(2,1)  a(1,2)  a(2,1)   s(1,2)-&gt; s(2,1)\ns(2,2)  a(1,3)  a(2,2)   s(1,3)-&gt; s(2.2)\n...\n\ns(i,j) writes a value that is consumed in s(i+1, j-1)\n\n\n$$\ns(i,j) \\rightarrow s(i+1, j-1)\n$$\nconstants:\n\nDoes there exist a statement s(i,j) and a statement $s(i',j')$\nwhere in the new schedule $s(i',j')$ executes first and  data flows backward in time \n$$\n\\begin{align*}\n(i', j') \\gg (j,i)   &\\text{ $i',j'$ is first} \\\\\ni' = 1+ i            &\\text{ data\\  from \\ i+1 to $i'$}\\\\\nj' = -1 +j           &\\text{ data\\  from \\ j-1 to $j'$}\\\\\n1 \\le i \\le 4 \\\\\n1 \\le j \\le 3  \\\\\n1 \\le i' le 4 \\\\\n1 \\le j' \\leftrightarrows 3\n\\end{align*}\n$$\n\nbecause of the lexicographic order ( or) we have two ilp problems\none where $i'$ is greater then j, and one where $i'$ = j, and the other where $j'$ &gt; j\n\ni ran it through:\n\nhttps://online-optimizer.appspot.com\n\nwhich gave me a solution \n\ns(4,2) reads  s(3,3) but s(4,2) executes first \n\n## ir\n\nHow do we represent these sets in the  ir?\n\n~~~\nfor i in [0,1,2,3,4,5]\n  for j from  i to 7\n     a(i,j) = 0\n\n~~~\n\nchange the equations around so that they are ... $\\ge 0$\n\n$$\n\\begin{align*}\ni \\ge 0  &\\rightarrow  i \\ge 0 \\\\\ni \\le 5 &\\rightarrow -i + 5 \\ge 0 \\\\\nj \\ge i &\\rightarrow -i + j \\ge 0 \\\\\nj \\le 7 &\\rightarrow =j+7 \\ge 0\n\\end{align*}\n$$\n\n\nWe can split off the constraints:\n$$\nconstraints  = \\left\\{ \\vec{x} \\mid B\\vec{x} + \\vec{b} &gt;= 0\\right\\} \n$$\n\nWhere:\n$$\n\\begin{equation*}\nB = \n\\begin{bmatrix} \\begin{array}{rr}\n 1 &  0 \\\\\n-1 &  0 \\\\\n-1 &  1 \\\\\n 0 & -1\n \\end{array} \\end{bmatrix}\n\\vec{b} =\n\\begin{bmatrix}\n 0 \\\\\n 5  \\\\\n 0   \\\\\n7\n\\end{bmatrix}\n\\vec{x} =\n\\begin{bmatrix}\ni \\\\\nj \n\\end{bmatrix}\n\\end{equation*}\n$$\n\nThis also works if the loop bounds are symbolic\nfor i in [L.. U] for j from i to 7 a(i,j) = 0\n\n$$\n\\begin{equation*}\nB = \n\\begin{bmatrix} \\begin{array}{rr}\n 1 &  0 \\\\\n-1 &  0 \\\\\n-1 &  1 \\\\\n 0 & -1\n \\end{array} \\end{bmatrix}\n\\vec{b} =\n\\begin{bmatrix}\n L \\\\\n U\\\\\n0\\\\\n7\n\\end{bmatrix}\n\\end{equation*}\n$$\n\n\n## suppose we have complex loop bounds?\n\nfor i=0; i &lt; 7, i++ for j =i, j &lt; min(7, i+4), j++ a(i,j) = 0\n![alt text](plot1-7.png)\n\nshaded area is the polygon\nwhat are the loop bounds if we interchange the loops?\n\nWhat are the upper and lower bounds if we interchange the loops?\n\ninequalities\n$$\n\\begin{align*}\ni \\ge 0   & \\rightarrow  i  \\ge 0 \\\\\ni \\le 6   & \\rightarrow  -i+6 \\ge 0 \\\\\nj \\ge i   & \\rightarrow j-i \\ge 0 \\\\\nj \\le 6    & \\rightarrow  6  -j  \\ge 0 \\\\\nj \\le i+3 & \\rightarrow -j+i+3 \\ge 0 \n\\end{align*}\n$$\n\nfor j (must be constants) for j (constants and j )\n\nwe can get the j bounds by projecting onto the j axis,  next we want to remove j from the inequalities \n\n\nir constants\n\n\n\n\n\n$$\n\\begin{align*}\n\nB =\n\\begin{bmatrix} \\begin{array}{rr}\n 1 & 0 \\\\\n-1 & 0 \\\\\n-1 & 1 \\\\\n0 & -1 \\\\\n 1 & -1 \n\\end{array} \\end{bmatrix}\n\\vec{b} =\n\\begin{bmatrix}\n 0\\\\\n 6 \\\\\n0\\\\\n6\\\\\n3\n\\end{bmatrix}\n\n\n\\end{align*}\n$$\n\n\nwritten for i\n$$\n\\begin{align}\n0  \\le  & i & \\\\\n        & i &\\le 6 \\\\\n        & i & \\le j \\\\\n        & &  6  -j  \\ge 0 \\\\\nj -3 \\le & i &\n\\end{align}\n$$\n\n$ i \\le max(0, j-3) \\land  i \\le min(6,j) $\n\nwritten for j\n$$\n\\begin{align}\n       & & i \\ge 0 \\\\\n       & & i \\le 6  \\\\\ni \\le  &j & \\\\\n    &  j  & \\le 6 \\\\\n&j& \\le i+3 \n\\end{align}\n$$\n\n\nbounds for j depend on i -  We need to remove i \n\n\nmath thing #2 fourier-motzkin method\n\nhttps://people.math.carleton.ca/~kcheung/math/notes/MATH5801/02/2_1_fourier_motzkin.html\n\nGiven a set of inequalities remove one variable, (for higher dim d, need to do this multiple times)\n\nin general \nSuppose we want to remove $x_m$ we find a pair $L \\le c_1 * x_m $  and  upper bound $x_2 * x_m \\ge U$ and both c's are &gt;= 0\n\nremove x_m and add $c_2* L \\ge c_1 *U $\n\nWe start with each pair of constants\n$$\nc_1 * i &lt; U \\land\nc_2 *i &gt; L\n$$\n\nthere are 4 pairs (1,2), (1,3) , (2,5),  (3,5) \nall the c's are 1 \nfrom the ir column 1 (i column) ignore zeros, pair up plus and minus values \n\n\nWe need to eliminate i (to get the bounds for the outer loop in j)\n\nwe have 4 inequalities where i is not multiplied by zero $ j \\le 6$ \n\nwe consider each pair\n\n$$\n\\begin{align*}\n(1,2) \\rightarrow  0 &\\le 6 \\ done \\\\\n(1,3) \\rightarrow  0 &\\le j\\\\\n(2,5) \\rightarrow j-3  &\\le 6\\\\\n(3,5) \\rightarrow j-3  &\\le j \\ done \n\\end{align*}\n$$\n\nbounds for j are 0 to 6\n\n\nfor j =0 ; j &lt;= 6 , j++ for i = max(j-3,6), i &lt; j; i++&gt;\na[i,j] = 0\n\n\n## suppose we want to run the an example  in parallel\n~~~\nfor i in [1,2,3,4]\n  for j in [1,2,3, 4]\n    s: a(i,j) = a(i-1, j+1) \n~~~\n\n\n\nreorder to run in parallel get new bounds, we want to run diagonally $k= i-j$, we know the transformation that we want \nWe replace $i = k+j $\n\nfor k = ??\n   for j = ?? \n     s: a(j-k,j) = a(j-k-1, j+1)\n\n\n$$\n\\begin{align*}\n1 \\le i \\le 4 \\\\\n1 \\le j \\le 4 \\\\\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n1 \\le & k+j &\\le 4 \\\\\n1 \\le & j &\\le 4 \\\\\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n1-k \\le j \\le 4-k \\\\\n1 \\le j &lt;= 4\n\\end{align*}\n$$\n\nnow for mf\n$$\n\\begin{align*}\n1-k \\le  4-k \\\\\n1-k  \\le 4\\\\\n1 \\le 4-k \\\\\n1 \\le 4\n\\end{align*}\n$$\n\ngiving k bounds -3 to 3 \nj bound are max(1,1,k) yo min(4, 4-k)\n\n\n#  4 Scheduling \n\nUp till now we assumed we know the transformation but now we consider \ncan we find a schedule that is good in some way\n\nan example \nfor i in [0,1,2,3,4,5] P: a(i) = input(i) +1 for j in [0,12,3,4,5] c: b(j) = a(j) +2\n\nThis is not quite a loop nest - unless we picture an outer loop running one time.\n\n\n\nThe new schedule has to be affine (restriction)\nschedule is a map from instances to time \n$$\nP(i) = sp*i + dp \\\\ \nC(j) = sc*j + dc \n$$\n\noptimization pick sp, dp, sc, dc that optimizes the locality.\n\n\nconstraint is for all i,j, p(i) writes data that c(j) reads, and i,j are in bounds\n\n\nFor any legal schedule:\n\n\n$$\n\\forall i,j \\mid 0 \\le i \\le 5 \\land 0 \\le j \\le 5 \\land i = j \n\\land sp * i + dp  \\le sc * j + dc\n$$\n\n\ndifferent values of sp, dp, sc, and dc give different schedules.  \nSince all these schedules are affine - None on them is the \"best\".\nSome non-affine schedule might be better.\n\nThis is not suitable for ilp, because of the  $ sp * i $\n\nSuppose we want to select one of the schedules that minimizes a cost function:\n$$\nw(i,j)  = sc * j + dc - sp * i + dp \n$$\n\nw is the time the value is needed\n\nHow we select a legal schedule that has the smallest w?\n\n\n\nmath fact 3 \n\nThe affine form of Farkas Lemma\nAn affine function non-negative everywhere inside a polygon \niff its non-negative at all the vertices \n\n$$\ngiven: \n\\forall \\vec{x} \\mid \\left\\{ \\vec{x} \\mid B\\vec{x} + \\vec{b} &gt;= 0\\right\\} \nS^T \\vec{x}+\\vec{d} \\ge 0\n$$\nif and only if \n$$\n\\exists p_0, \\vec{p} \\ge 0 \\mid \\forall \\vec{x}\\mid S^T \\vec{x} + \\vec{d} = (\\vec{p})^t(B\\vec{x}+\\vec{d}) + p_0\n$$\n\nWhat does this mean and how does it help?\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Arc\n\nfig, ax = plt.subplots(figsize=(5,5))\n\n# Define the range of the loops\nn = 4  # Range for 'i' (1 to 4)\n\n# Create lists to hold the iteration points\ni_values = []\nj_values = []\n\n# Nested loops to generate iteration space\nfor i in range(1, n + 1):\n    i_values.append(i)\n    j_values.append(1)\n\n    arc = Arc(xy=(i+.5, 1), width=1, height=1, angle=0, theta1=0, theta2=180, color='red', lw=2)\n    ax.add_patch(arc)\n\nax.set_ylim(0,2)\n\n# Plotting the iteration space\n\nplt.scatter(i_values, j_values, c='blue', marker='o')\nplt.xlabel('i (outer loop index)')\nplt.ylabel('j (inner loop index)')\nplt.title('Iteration Space with Data Flow')\nplt.grid(True)\nplt.gca().invert_yaxis()\n\n# # Annotate the iteration order and add arrows for data flow\n# for i in range(1, n + 1):\n#     for j in range(2, m + 1):  # Start from j=2 since j-1 needs to exist\n#         plt.annotate('',\n#                      xy=(i, j), xytext=(i, j-1),\n#                      arrowprops=dict(facecolor='black', shrink=0.05))\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Polyhedral Analysis"
    ]
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#memory-operations",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#memory-operations",
    "title": "memory consistancy",
    "section": "memory operations",
    "text": "memory operations\nThe C semantics assume that (within a single thread) all loads and stores stay in order. That is is not allowed to re-order a store past a load of the same address.\nin ssa each argument of an instruction is a pointer to the source instruction. These edges force serialization of the code.\nWe want to apply this to loads and stores this will make ordering explicit\nIn Static Single Assignment (SSA) form, memory tokens, representing stores or loads to memory, are typically handled by introducing memory state variables\n\nload: dest = load addrs, memory_token\nstore memory_token = store value, address, memory_token\ncalls to functions that might modify memory also need to read and write memory tokens\n\ntreat a store as though it created a new copy of memory\nwe can use phi functions on memory tokens\nMaintaining Correct Memory Order: By tracking memory states explicitly in SSA form (through these memory tokens and versioning), SSA ensures that memory operations respect the correct order, even if the control flow of the program is complex. This helps compilers optimize code by making memory dependencies explicit.\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nstore1\nload1\nload2\nstore2\nload4\nload3\nstore1--&gt;load1\nstore1--&gt;load2\nstore1 --&gt; store2\nstore2 --&gt; load3\nstore2--&gt; load4\nstore2--&gt; exit\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nstore1\nload1\nload2\nstore2\nload4\nload3\nstore1--&gt;load1\nstore1--&gt;load2\nstore1 --&gt; store2\nstore2 --&gt; load3\nstore2--&gt; load4\nstore2--&gt; exit\n\n\n\n\n\n\nOptimize loads/stores\nwalk backwards - load from store\n\nif we can prove the load address is the same as the store address- remove the load\nif we can prove the load address is different move the load up a store\notherwise go on"
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#multi-threaded-programs",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#multi-threaded-programs",
    "title": "memory consistancy",
    "section": "multi-threaded programs",
    "text": "multi-threaded programs\nCompilers started out assuming targets are single threaded. What optimizations change for multi-threaded code? How do users tell compiler that the target is multi-threaded?"
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#shared-memory-multi-threading",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#shared-memory-multi-threading",
    "title": "memory consistancy",
    "section": "shared memory multi-threading",
    "text": "shared memory multi-threading\nThe most common parallel system is\n\nA single big memory\nmultiple threads address that memory"
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#what-is-sequential-consistency-sq",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#what-is-sequential-consistency-sq",
    "title": "memory consistancy",
    "section": "what is sequential consistency SQ",
    "text": "what is sequential consistency SQ\nProgram Order is Maintained Within Threads:\nOperations (reads and writes) appear to occur in the order they are issued by each individual thread. If a thread performs a write followed by a read, the read cannot appear to happen before the write in the execution.\nGlobal Order of Operations Across Threads:\nAll threads see the effects of memory operations in the same sequential order. Every thread agrees on the order of reads and writes, though the specific order is not predefined—it just needs to be consistent across all threads. Interleaving of Operations:\nThe execution can be viewed as an interleaving of instructions from all threads. However, the interleaving must follow the program order within each thread.\nno real machine/compiler implements this"
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#compiler-effects",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#compiler-effects",
    "title": "memory consistancy",
    "section": "compiler effects",
    "text": "compiler effects\nCompiler transformations that break multi-thread sequential consistency (SC) often reorder or optimize instructions in ways that do not respect the original program order seen by other threads. These transformations can lead to subtle bugs in multithreaded programs where the expected interleaving of operations is violated."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#loadstore-reordering",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#loadstore-reordering",
    "title": "memory consistancy",
    "section": "Load/Store Reordering",
    "text": "Load/Store Reordering\nTransformation: Compilers might reorder loads and stores to improve performance. Violation: In a multi-threaded environment, this can lead to a situation where one thread sees stale or unexpected data. Example:\nCopy code\n// Thread 1\nx = 1;     // Store\nr1 = y;    // Load\n\n// Thread 2\ny = 1;     // Store\nr2 = x;    // Load\nUnder sequential consistency, if thread 1’s x = 1 happens before thread 2’s r2 = x, then thread 2 should observe r2 == 1. But reordering could result in thread 2 reading x as 0."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#common-subexpression-elimination-cse",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#common-subexpression-elimination-cse",
    "title": "memory consistancy",
    "section": "Common Subexpression Elimination (CSE)",
    "text": "Common Subexpression Elimination (CSE)\nTransformation: If a variable or expression is computed multiple times, the compiler may optimize by reusing the result of an earlier computation. Violation: This assumes that no other thread modifies shared variables between these uses. Example:\n// Original code\nr1 = x;\nr2 = x;\n\n// Transformed code (CSE applied)\ntemp = x;\nr1 = temp;\nr2 = temp;\nIf x is modified by another thread between the two reads, the transformed code will incorrectly assume the value of x hasn’t changed."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#dead-code-elimination-dce",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#dead-code-elimination-dce",
    "title": "memory consistancy",
    "section": "Dead Code Elimination (DCE)",
    "text": "Dead Code Elimination (DCE)\nTransformation: The compiler may remove stores to variables that are not subsequently read in the same thread. Violation: If the variable is shared and accessed by other threads, removing the store could lead to unexpected behavior. Example:\n// Original code\nx = 1;\n\n// Transformed code (DCE applied)\n// x = 1 is removed because x is not used locally If another thread reads x, it expects the store to have happened, but DCE breaks this assumption."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#speculative-execution-out-of-order-execution",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#speculative-execution-out-of-order-execution",
    "title": "memory consistancy",
    "section": "Speculative Execution (Out-of-Order Execution)",
    "text": "Speculative Execution (Out-of-Order Execution)\nTransformation: Compilers (or hardware) may execute instructions speculatively, assuming certain branches are likely to be taken. Violation: This can cause out-of-order writes or reads visible to other threads, breaking SC. Example:\nif (flag) {\n    r1 = x;\n}\nIf the compiler speculatively reads x before knowing the value of flag, another thread’s write to x might be missed or observed out-of-order."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#loop-invariant-code-motion",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#loop-invariant-code-motion",
    "title": "memory consistancy",
    "section": "Loop Invariant Code Motion",
    "text": "Loop Invariant Code Motion\nTransformation: The compiler moves computations that are invariant inside a loop to outside the loop. Violation: If these computations involve shared variables modified by other threads within the loop, moving them outside could make the code see stale values. Example:\n// Original code\nwhile (condition) {\n    r = shared_variable;\n}\n\n// Transformed code (Loop Invariant Code Motion)\ntemp = shared_variable;\nwhile (condition) {\n    r = temp;\n}\nIf shared_variable is updated by another thread, the transformed code might keep using the old value.\nRegister Allocation (Caching Shared Variables in Registers)\nTransformation: Compilers can keep a shared variable in a register for efficiency rather than repeatedly loading it from memory. Violation: If another thread modifies that shared variable in memory, the compiler’s register optimization would cause the thread to read stale data. Example:\nwhile (flag == 0) {\n    // busy-wait\n}\nIf flag is cached in a register, updates to flag by another thread in memory won’t be reflected, breaking SC."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#instruction-fusion-combining-loadsstores",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#instruction-fusion-combining-loadsstores",
    "title": "memory consistancy",
    "section": "Instruction Fusion (Combining Loads/Stores)",
    "text": "Instruction Fusion (Combining Loads/Stores)\nTransformation: The compiler may combine consecutive memory accesses into one, such as merging adjacent stores into a single store or combining two loads. Violation: If other threads expect these loads or stores to happen separately, they might see an inconsistent view of memory. Example:\n// Original code\nx = 1;\ny = 2;\n\n// Transformed code (store fusion)\n// x and y are stored together in a single transaction\nA thread expecting x and y to be updated separately might observe an inconsistent state if this transformation is applied."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#volatile",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#volatile",
    "title": "memory consistancy",
    "section": "volatile",
    "text": "volatile\nvolatile turns off some optimizations, so compiler writers don’t like it. lots of bugs in compilers around volatile.\nspec is confusing, developers think it means something not in the spec.\nBest for memory mapped hardware, not good for inter thread communication.\nMemory is allowed to tear. No synchronization.\nundefined: bitfields, unions, pre/post increment(not atomic), big objects (more then 1 transfer operation)\nvolatile is like const: does not apply in constructors/destructors and it does not effect call conventions or returns\nint i,j;\nvolatile int vol;\ni = vol = j;\nvolatile int buffer_ready; \nchar buffer[BUF_SIZE]; \nvoid buffer_init() { \n    int i;\n    for (i=0; i&lt;BUF_SIZE; i++) \n        buffer[i] = 0; \n    buffer_ready = 1; \n    }\n    // ok to move buffer assignments below buffer_ready \nIn C++ using volatile for inter thread communication is a race, and is not a valid program, so the compiler is free to treat it as any way it wants."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#thread-libraries",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#thread-libraries",
    "title": "memory consistancy",
    "section": "thread libraries",
    "text": "thread libraries\nstart out assuming single threaded, add a threads library like pthreads\nmultiple threads could access shared memory simultaneously, leading to race conditions, inconsistent data, and undefined behavior.\nModern CPUs and compilers perform optimizations like instruction reordering, which can break assumptions about the order of memory operations in multithreaded programs.\nMultithreaded code is harder to test because race conditions and bugs might only manifest under certain timing conditions.\nDebugging multithreaded programs is more difficult due to the unpredictable nature of thread execution and interactions.\nSome optimizations might reorder instructions in a way that is incompatible with multithreading, introducing subtle bugs or performance regressions.\nCaching, prefetching, or other memory optimizations need to account for the fact that multiple threads may be accessing the same memory, which a simple thread library does not handle."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#using-libraries",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#using-libraries",
    "title": "memory consistancy",
    "section": "using libraries",
    "text": "using libraries\n\nFunctions such as pthread mutex lock() that are guaranteed by the standard to “synchronize memory” include hardware instructions (“memory barriers”) that prevent hardware reordering of memory operations around the call\nTo prevent the compiler from moving memory operations around calls to functions such as pthread mutex lock(), they are essentially treated as calls to opaque functions, about which the compiler has no information.\n\nThe compiler effectively assumes that pthread mutex lock() may read or write any global variable. Thus a memory reference cannot simply be moved across the call. This approach also ensures that transitive calls, e.g. a call to a function f() which then calls pthread mutex lock(), are handled in the same way more or less appropriately, i.e. memory operations are not moved across the call to f() either, whether or not the entire user program is being analyzed at once."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#adding-multi-threading-to-user-explaining-the-intent",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#adding-multi-threading-to-user-explaining-the-intent",
    "title": "memory consistancy",
    "section": "adding multi-threading to user explaining the intent",
    "text": "adding multi-threading to user explaining the intent\nc++/c added atomics\nAtomic operations are operations that are completed as a single, uninterruptible action. No other thread can observe a partial update or interfere with the operation.\nThese operations ensure that read-modify-write sequences are safe without needing explicit locks."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#an-example-1",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#an-example-1",
    "title": "memory consistancy",
    "section": "an example",
    "text": "an example\n#include &lt;atomic&gt;\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n\n// Global spinlock using atomic_flag\nstd::atomic_flag lock = ATOMIC_FLAG_INIT;\n\nvoid enter_critical_section() {\n    // Busy-wait (spin) until the lock is acquired\n    while (lock.test_and_set(std::memory_order_acquire)) {\n        // Spin and wait for the lock to become available\n    }\n}\n\nvoid leave_critical_section() {\n    // Release the lock\n    lock.clear(std::memory_order_release);\n}\n\n// Shared resource\nint shared_counter = 0;\n\nvoid critical_section_task(int num_increments) {\n    for (int i = 0; i &lt; num_increments; ++i) {\n        enter_critical_section();\n        // Begin critical section\n        ++shared_counter;\n        // End critical section\n        leave_critical_section();\n    }\n}"
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#load-acquire-needs-special-hardware",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#load-acquire-needs-special-hardware",
    "title": "memory consistancy",
    "section": "load acquire (needs special hardware )",
    "text": "load acquire (needs special hardware )\nused by default with atomics but not used for non-atomics\nall memory reads and writes after the load operation cannot be moved before the load. This ensures that after acquiring the value, any operations that depend on this value (like accessing shared data) will see consistent and up-to-date memory.\na one way fence - nothing can move up"
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#write-release-needs-special-hardware",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#write-release-needs-special-hardware",
    "title": "memory consistancy",
    "section": "write release (needs special hardware )",
    "text": "write release (needs special hardware )\nprevents the compiler or processor from reordering any memory operations (reads or writes) that appear before the release store. This guarantees that all operations that modify shared data before the release are visible to other threads that subsequently perform an acquire operation.\nalso a one way fence - nothing can move down\nload.acquire - \nloads and stores on non-atomics  - compiler picks the order for these operations \nstore.release"
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#using-atomics",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#using-atomics",
    "title": "memory consistancy",
    "section": "using atomics",
    "text": "using atomics\nAll operations appear to occur in a single total order that is consistent across all threads. This means that the results of operations are predictable and consistent as if all operations were executed in some sequential order.\nlimits the hardware and compiler because it prevents reordering"
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#data-race-free",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#data-race-free",
    "title": "memory consistancy",
    "section": "Data Race Free",
    "text": "Data Race Free\nData Race Free (DRF) means that a program is free from data races, which occur when:\n\nTwo or more threads access the same variable concurrently.\nAt least one of the accesses is a write.\n\nThere is no synchronization mechanism (like mutexes or atomic operations) to control the access. In a data race-free program, every shared variable is accessed in a way that ensures predictable results. C++ provides various synchronization primitives (such as mutexes and atomic types) to help developers write DRF code.\nAll shared variables must be accessed using synchronization to prevent concurrent threads from modifying shared data simultaneously without coordination."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#an-example-2",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#an-example-2",
    "title": "memory consistancy",
    "section": "an example",
    "text": "an example\n#include &lt;iostream&gt;\n#include &lt;atomic&gt;\n#include &lt;thread&gt;\n\nint shared_counter1 = 0;                  // First non-atomic shared variable\nint shared_counter2 = 0;                  // Second non-atomic shared variable\nstd::atomic&lt;bool&gt; lock_flag(false);       // Atomic flag to control access\n\nvoid safe_increment() {\n    for (int i = 0; i &lt; 1000; ++i) {\n        // Spin until the lock is acquired\n        while (lock_flag.exchange(true)) {\n            // Busy-wait (spin) until the lock is free\n        }\n\n        // Critical section: update the non-atomic shared variables\n        ++shared_counter1;\n        ++shared_counter2;\n\n        // Release the lock\n        lock_flag.store(false);\n    }\n}"
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#language-rules",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#language-rules",
    "title": "memory consistancy",
    "section": "language rules",
    "text": "language rules\nC and C++\ndo not define what happens in the presence of data races. If a program has data races (e.g., multiple threads concurrently reading and writing to the same variable without synchronization), the behavior is considered undefined. This means that the program may produce unexpected results, crash, or behave inconsistently across different executions or platforms.\nJava\ntries to define what happens but definition is very complex and maybe inconsistent\nRust\nCompile-Time Guarantees: Rust’s ownership and borrowing system prevents data races at compile time. If a program is not DRF, the Rust compiler will typically refuse to compile it, enforcing memory safety guarantees."
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#can-the-compiler-add-a-race-to-a-drf-program",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#can-the-compiler-add-a-race-to-a-drf-program",
    "title": "memory consistancy",
    "section": "can the compiler add a race to a drf program",
    "text": "can the compiler add a race to a drf program\nnew rule, compiler cannot add a write to a shared variable\nif (x ==1) y++\n\nto \ny++\nif (x!=1) y--"
  },
  {
    "objectID": "lectures/revealjs_mem_consistancy.qmd.html#how-does-this-effect-hardware",
    "href": "lectures/revealjs_mem_consistancy.qmd.html#how-does-this-effect-hardware",
    "title": "memory consistancy",
    "section": "how does this effect hardware?",
    "text": "how does this effect hardware?\nstruct { char a; char b; char c; char d;} s;\ns.a = 1\ns.c = 3\n\ncan a compiler do \nchar temp[4] = s // load 32 bits \ntemp[0] = 1\ntemp[2] = 3\ns = temp\nnot allowed - reads/writes b and d, so compiler incorrectly added writes\noptions are either have byte addressable hardware, or pad so that each char gets 32 bits\nVendors forced to add 8 byte loads/stores"
  },
  {
    "objectID": "lectures/mem_consistancy.html",
    "href": "lectures/mem_consistancy.html",
    "title": "memory consistancy",
    "section": "",
    "text": "The C semantics assume that (within a single thread) all loads and stores stay in order. That is is not allowed to re-order a store past a load of the same address.\nin ssa each argument of an instruction is a pointer to the source instruction. These edges force serialization of the code.\nWe want to apply this to loads and stores this will make ordering explicit\nIn Static Single Assignment (SSA) form, memory tokens, representing stores or loads to memory, are typically handled by introducing memory state variables\n\nload: dest = load addrs, memory_token\nstore memory_token = store value, address, memory_token\ncalls to functions that might modify memory also need to read and write memory tokens\n\ntreat a store as though it created a new copy of memory\nwe can use phi functions on memory tokens\nMaintaining Correct Memory Order: By tracking memory states explicitly in SSA form (through these memory tokens and versioning), SSA ensures that memory operations respect the correct order, even if the control flow of the program is complex. This helps compilers optimize code by making memory dependencies explicit.\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nstore1\nload1\nload2\nstore2\nload4\nload3\nstore1--&gt;load1\nstore1--&gt;load2\nstore1 --&gt; store2\nstore2 --&gt; load3\nstore2--&gt; load4\nstore2--&gt; exit\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nstore1\nload1\nload2\nstore2\nload4\nload3\nstore1--&gt;load1\nstore1--&gt;load2\nstore1 --&gt; store2\nstore2 --&gt; load3\nstore2--&gt; load4\nstore2--&gt; exit\n\n\n\n\n\n\nOptimize loads/stores\nwalk backwards - load from store\n\nif we can prove the load address is the same as the store address- remove the load\nif we can prove the load address is different move the load up a store\notherwise go on",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#memory-operations",
    "href": "lectures/mem_consistancy.html#memory-operations",
    "title": "memory consistancy",
    "section": "",
    "text": "The C semantics assume that (within a single thread) all loads and stores stay in order. That is is not allowed to re-order a store past a load of the same address.\nin ssa each argument of an instruction is a pointer to the source instruction. These edges force serialization of the code.\nWe want to apply this to loads and stores this will make ordering explicit\nIn Static Single Assignment (SSA) form, memory tokens, representing stores or loads to memory, are typically handled by introducing memory state variables\n\nload: dest = load addrs, memory_token\nstore memory_token = store value, address, memory_token\ncalls to functions that might modify memory also need to read and write memory tokens\n\ntreat a store as though it created a new copy of memory\nwe can use phi functions on memory tokens\nMaintaining Correct Memory Order: By tracking memory states explicitly in SSA form (through these memory tokens and versioning), SSA ensures that memory operations respect the correct order, even if the control flow of the program is complex. This helps compilers optimize code by making memory dependencies explicit.\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nstore1\nload1\nload2\nstore2\nload4\nload3\nstore1--&gt;load1\nstore1--&gt;load2\nstore1 --&gt; store2\nstore2 --&gt; load3\nstore2--&gt; load4\nstore2--&gt; exit\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nstore1\nload1\nload2\nstore2\nload4\nload3\nstore1--&gt;load1\nstore1--&gt;load2\nstore1 --&gt; store2\nstore2 --&gt; load3\nstore2--&gt; load4\nstore2--&gt; exit\n\n\n\n\n\n\nOptimize loads/stores\nwalk backwards - load from store\n\nif we can prove the load address is the same as the store address- remove the load\nif we can prove the load address is different move the load up a store\notherwise go on",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#multi-threaded-programs",
    "href": "lectures/mem_consistancy.html#multi-threaded-programs",
    "title": "memory consistancy",
    "section": "multi-threaded programs",
    "text": "multi-threaded programs\nCompilers started out assuming targets are single threaded. What optimizations change for multi-threaded code? How do users tell compiler that the target is multi-threaded?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#shared-memory-multi-threading",
    "href": "lectures/mem_consistancy.html#shared-memory-multi-threading",
    "title": "memory consistancy",
    "section": "shared memory multi-threading",
    "text": "shared memory multi-threading\nThe most common parallel system is\n\nA single big memory\nmultiple threads address that memory",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#what-is-sequential-consistency-sq",
    "href": "lectures/mem_consistancy.html#what-is-sequential-consistency-sq",
    "title": "memory consistancy",
    "section": "what is sequential consistency SQ",
    "text": "what is sequential consistency SQ\nProgram Order is Maintained Within Threads:\nOperations (reads and writes) appear to occur in the order they are issued by each individual thread. If a thread performs a write followed by a read, the read cannot appear to happen before the write in the execution.\nGlobal Order of Operations Across Threads:\nAll threads see the effects of memory operations in the same sequential order. Every thread agrees on the order of reads and writes, though the specific order is not predefined—it just needs to be consistent across all threads. Interleaving of Operations:\nThe execution can be viewed as an interleaving of instructions from all threads. However, the interleaving must follow the program order within each thread.\nno real machine/compiler implements this",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#compiler-effects",
    "href": "lectures/mem_consistancy.html#compiler-effects",
    "title": "memory consistancy",
    "section": "compiler effects",
    "text": "compiler effects\nCompiler transformations that break multi-thread sequential consistency (SC) often reorder or optimize instructions in ways that do not respect the original program order seen by other threads. These transformations can lead to subtle bugs in multithreaded programs where the expected interleaving of operations is violated.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#loadstore-reordering",
    "href": "lectures/mem_consistancy.html#loadstore-reordering",
    "title": "memory consistancy",
    "section": "Load/Store Reordering",
    "text": "Load/Store Reordering\nTransformation: Compilers might reorder loads and stores to improve performance. Violation: In a multi-threaded environment, this can lead to a situation where one thread sees stale or unexpected data. Example:\nCopy code\n// Thread 1\nx = 1;     // Store\nr1 = y;    // Load\n\n// Thread 2\ny = 1;     // Store\nr2 = x;    // Load\nUnder sequential consistency, if thread 1’s x = 1 happens before thread 2’s r2 = x, then thread 2 should observe r2 == 1. But reordering could result in thread 2 reading x as 0.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#common-subexpression-elimination-cse",
    "href": "lectures/mem_consistancy.html#common-subexpression-elimination-cse",
    "title": "memory consistancy",
    "section": "Common Subexpression Elimination (CSE)",
    "text": "Common Subexpression Elimination (CSE)\nTransformation: If a variable or expression is computed multiple times, the compiler may optimize by reusing the result of an earlier computation. Violation: This assumes that no other thread modifies shared variables between these uses. Example:\n// Original code\nr1 = x;\nr2 = x;\n\n// Transformed code (CSE applied)\ntemp = x;\nr1 = temp;\nr2 = temp;\nIf x is modified by another thread between the two reads, the transformed code will incorrectly assume the value of x hasn’t changed.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#dead-code-elimination-dce",
    "href": "lectures/mem_consistancy.html#dead-code-elimination-dce",
    "title": "memory consistancy",
    "section": "Dead Code Elimination (DCE)",
    "text": "Dead Code Elimination (DCE)\nTransformation: The compiler may remove stores to variables that are not subsequently read in the same thread. Violation: If the variable is shared and accessed by other threads, removing the store could lead to unexpected behavior. Example:\n// Original code\nx = 1;\n\n// Transformed code (DCE applied)\n// x = 1 is removed because x is not used locally If another thread reads x, it expects the store to have happened, but DCE breaks this assumption.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#speculative-execution-out-of-order-execution",
    "href": "lectures/mem_consistancy.html#speculative-execution-out-of-order-execution",
    "title": "memory consistancy",
    "section": "Speculative Execution (Out-of-Order Execution)",
    "text": "Speculative Execution (Out-of-Order Execution)\nTransformation: Compilers (or hardware) may execute instructions speculatively, assuming certain branches are likely to be taken. Violation: This can cause out-of-order writes or reads visible to other threads, breaking SC. Example:\nif (flag) {\n    r1 = x;\n}\nIf the compiler speculatively reads x before knowing the value of flag, another thread’s write to x might be missed or observed out-of-order.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#loop-invariant-code-motion",
    "href": "lectures/mem_consistancy.html#loop-invariant-code-motion",
    "title": "memory consistancy",
    "section": "Loop Invariant Code Motion",
    "text": "Loop Invariant Code Motion\nTransformation: The compiler moves computations that are invariant inside a loop to outside the loop. Violation: If these computations involve shared variables modified by other threads within the loop, moving them outside could make the code see stale values. Example:\n// Original code\nwhile (condition) {\n    r = shared_variable;\n}\n\n// Transformed code (Loop Invariant Code Motion)\ntemp = shared_variable;\nwhile (condition) {\n    r = temp;\n}\nIf shared_variable is updated by another thread, the transformed code might keep using the old value.\n\nRegister Allocation (Caching Shared Variables in Registers)\nTransformation: Compilers can keep a shared variable in a register for efficiency rather than repeatedly loading it from memory. Violation: If another thread modifies that shared variable in memory, the compiler’s register optimization would cause the thread to read stale data. Example:\nwhile (flag == 0) {\n    // busy-wait\n}\nIf flag is cached in a register, updates to flag by another thread in memory won’t be reflected, breaking SC.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#instruction-fusion-combining-loadsstores",
    "href": "lectures/mem_consistancy.html#instruction-fusion-combining-loadsstores",
    "title": "memory consistancy",
    "section": "Instruction Fusion (Combining Loads/Stores)",
    "text": "Instruction Fusion (Combining Loads/Stores)\nTransformation: The compiler may combine consecutive memory accesses into one, such as merging adjacent stores into a single store or combining two loads. Violation: If other threads expect these loads or stores to happen separately, they might see an inconsistent view of memory. Example:\n// Original code\nx = 1;\ny = 2;\n\n// Transformed code (store fusion)\n// x and y are stored together in a single transaction\nA thread expecting x and y to be updated separately might observe an inconsistent state if this transformation is applied.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#volatile",
    "href": "lectures/mem_consistancy.html#volatile",
    "title": "memory consistancy",
    "section": "volatile",
    "text": "volatile\nvolatile turns off some optimizations, so compiler writers don’t like it. lots of bugs in compilers around volatile.\nspec is confusing, developers think it means something not in the spec.\nBest for memory mapped hardware, not good for inter thread communication.\nMemory is allowed to tear. No synchronization.\nundefined: bitfields, unions, pre/post increment(not atomic), big objects (more then 1 transfer operation)\nvolatile is like const: does not apply in constructors/destructors and it does not effect call conventions or returns\nint i,j;\nvolatile int vol;\ni = vol = j;\nvolatile int buffer_ready; \nchar buffer[BUF_SIZE]; \nvoid buffer_init() { \n    int i;\n    for (i=0; i&lt;BUF_SIZE; i++) \n        buffer[i] = 0; \n    buffer_ready = 1; \n    }\n    // ok to move buffer assignments below buffer_ready \nIn C++ using volatile for inter thread communication is a race, and is not a valid program, so the compiler is free to treat it as any way it wants.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#thread-libraries",
    "href": "lectures/mem_consistancy.html#thread-libraries",
    "title": "memory consistancy",
    "section": "thread libraries",
    "text": "thread libraries\nstart out assuming single threaded, add a threads library like pthreads\nmultiple threads could access shared memory simultaneously, leading to race conditions, inconsistent data, and undefined behavior.\nModern CPUs and compilers perform optimizations like instruction reordering, which can break assumptions about the order of memory operations in multithreaded programs.\nMultithreaded code is harder to test because race conditions and bugs might only manifest under certain timing conditions.\nDebugging multithreaded programs is more difficult due to the unpredictable nature of thread execution and interactions.\nSome optimizations might reorder instructions in a way that is incompatible with multithreading, introducing subtle bugs or performance regressions.\nCaching, prefetching, or other memory optimizations need to account for the fact that multiple threads may be accessing the same memory, which a simple thread library does not handle.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#using-libraries",
    "href": "lectures/mem_consistancy.html#using-libraries",
    "title": "memory consistancy",
    "section": "using libraries",
    "text": "using libraries\n\nFunctions such as pthread mutex lock() that are guaranteed by the standard to “synchronize memory” include hardware instructions (“memory barriers”) that prevent hardware reordering of memory operations around the call\nTo prevent the compiler from moving memory operations around calls to functions such as pthread mutex lock(), they are essentially treated as calls to opaque functions, about which the compiler has no information.\n\nThe compiler effectively assumes that pthread mutex lock() may read or write any global variable. Thus a memory reference cannot simply be moved across the call. This approach also ensures that transitive calls, e.g. a call to a function f() which then calls pthread mutex lock(), are handled in the same way more or less appropriately, i.e. memory operations are not moved across the call to f() either, whether or not the entire user program is being analyzed at once.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#adding-multi-threading-to-user-explaining-the-intent",
    "href": "lectures/mem_consistancy.html#adding-multi-threading-to-user-explaining-the-intent",
    "title": "memory consistancy",
    "section": "adding multi-threading to user explaining the intent",
    "text": "adding multi-threading to user explaining the intent\nc++/c added atomics\nAtomic operations are operations that are completed as a single, uninterruptible action. No other thread can observe a partial update or interfere with the operation.\nThese operations ensure that read-modify-write sequences are safe without needing explicit locks.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#an-example-1",
    "href": "lectures/mem_consistancy.html#an-example-1",
    "title": "memory consistancy",
    "section": "an example",
    "text": "an example\n#include &lt;atomic&gt;\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n\n// Global spinlock using atomic_flag\nstd::atomic_flag lock = ATOMIC_FLAG_INIT;\n\nvoid enter_critical_section() {\n    // Busy-wait (spin) until the lock is acquired\n    while (lock.test_and_set(std::memory_order_acquire)) {\n        // Spin and wait for the lock to become available\n    }\n}\n\nvoid leave_critical_section() {\n    // Release the lock\n    lock.clear(std::memory_order_release);\n}\n\n// Shared resource\nint shared_counter = 0;\n\nvoid critical_section_task(int num_increments) {\n    for (int i = 0; i &lt; num_increments; ++i) {\n        enter_critical_section();\n        // Begin critical section\n        ++shared_counter;\n        // End critical section\n        leave_critical_section();\n    }\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#load-acquire-needs-special-hardware",
    "href": "lectures/mem_consistancy.html#load-acquire-needs-special-hardware",
    "title": "memory consistancy",
    "section": "load acquire (needs special hardware )",
    "text": "load acquire (needs special hardware )\nused by default with atomics but not used for non-atomics\nall memory reads and writes after the load operation cannot be moved before the load. This ensures that after acquiring the value, any operations that depend on this value (like accessing shared data) will see consistent and up-to-date memory.\na one way fence - nothing can move up",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#write-release-needs-special-hardware",
    "href": "lectures/mem_consistancy.html#write-release-needs-special-hardware",
    "title": "memory consistancy",
    "section": "write release (needs special hardware )",
    "text": "write release (needs special hardware )\nprevents the compiler or processor from reordering any memory operations (reads or writes) that appear before the release store. This guarantees that all operations that modify shared data before the release are visible to other threads that subsequently perform an acquire operation.\nalso a one way fence - nothing can move down\nload.acquire - \nloads and stores on non-atomics  - compiler picks the order for these operations \nstore.release",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#using-atomics",
    "href": "lectures/mem_consistancy.html#using-atomics",
    "title": "memory consistancy",
    "section": "using atomics",
    "text": "using atomics\nAll operations appear to occur in a single total order that is consistent across all threads. This means that the results of operations are predictable and consistent as if all operations were executed in some sequential order.\nlimits the hardware and compiler because it prevents reordering",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#data-race-free",
    "href": "lectures/mem_consistancy.html#data-race-free",
    "title": "memory consistancy",
    "section": "Data Race Free",
    "text": "Data Race Free\nData Race Free (DRF) means that a program is free from data races, which occur when:\n\nTwo or more threads access the same variable concurrently.\nAt least one of the accesses is a write.\n\nThere is no synchronization mechanism (like mutexes or atomic operations) to control the access. In a data race-free program, every shared variable is accessed in a way that ensures predictable results. C++ provides various synchronization primitives (such as mutexes and atomic types) to help developers write DRF code.\nAll shared variables must be accessed using synchronization to prevent concurrent threads from modifying shared data simultaneously without coordination.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#an-example-2",
    "href": "lectures/mem_consistancy.html#an-example-2",
    "title": "memory consistancy",
    "section": "an example",
    "text": "an example\n#include &lt;iostream&gt;\n#include &lt;atomic&gt;\n#include &lt;thread&gt;\n\nint shared_counter1 = 0;                  // First non-atomic shared variable\nint shared_counter2 = 0;                  // Second non-atomic shared variable\nstd::atomic&lt;bool&gt; lock_flag(false);       // Atomic flag to control access\n\nvoid safe_increment() {\n    for (int i = 0; i &lt; 1000; ++i) {\n        // Spin until the lock is acquired\n        while (lock_flag.exchange(true)) {\n            // Busy-wait (spin) until the lock is free\n        }\n\n        // Critical section: update the non-atomic shared variables\n        ++shared_counter1;\n        ++shared_counter2;\n\n        // Release the lock\n        lock_flag.store(false);\n    }\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#language-rules",
    "href": "lectures/mem_consistancy.html#language-rules",
    "title": "memory consistancy",
    "section": "language rules",
    "text": "language rules\nC and C++\ndo not define what happens in the presence of data races. If a program has data races (e.g., multiple threads concurrently reading and writing to the same variable without synchronization), the behavior is considered undefined. This means that the program may produce unexpected results, crash, or behave inconsistently across different executions or platforms.\nJava\ntries to define what happens but definition is very complex and maybe inconsistent\nRust\nCompile-Time Guarantees: Rust’s ownership and borrowing system prevents data races at compile time. If a program is not DRF, the Rust compiler will typically refuse to compile it, enforcing memory safety guarantees.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#can-the-compiler-add-a-race-to-a-drf-program",
    "href": "lectures/mem_consistancy.html#can-the-compiler-add-a-race-to-a-drf-program",
    "title": "memory consistancy",
    "section": "can the compiler add a race to a drf program",
    "text": "can the compiler add a race to a drf program\nnew rule, compiler cannot add a write to a shared variable\nif (x ==1) y++\n\nto \ny++\nif (x!=1) y--",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/mem_consistancy.html#how-does-this-effect-hardware",
    "href": "lectures/mem_consistancy.html#how-does-this-effect-hardware",
    "title": "memory consistancy",
    "section": "how does this effect hardware?",
    "text": "how does this effect hardware?\nstruct { char a; char b; char c; char d;} s;\ns.a = 1\ns.c = 3\n\ncan a compiler do \nchar temp[4] = s // load 32 bits \ntemp[0] = 1\ntemp[2] = 3\ns = temp\nnot allowed - reads/writes b and d, so compiler incorrectly added writes\noptions are either have byte addressable hardware, or pad so that each char gets 32 bits\nVendors forced to add 8 byte loads/stores",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "memory consistancy"
    ]
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#in-and-out",
    "href": "lectures/revealjs_04_data_flow.qmd.html#in-and-out",
    "title": "Data Flow",
    "section": "IN and OUT",
    "text": "IN and OUT"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#equations",
    "href": "lectures/revealjs_04_data_flow.qmd.html#equations",
    "title": "Data Flow",
    "section": "EQUATIONS",
    "text": "EQUATIONS"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#liveness-example",
    "href": "lectures/revealjs_04_data_flow.qmd.html#liveness-example",
    "title": "Data Flow",
    "section": "Liveness Example",
    "text": "Liveness Example"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#summary-by-basic-blocks",
    "href": "lectures/revealjs_04_data_flow.qmd.html#summary-by-basic-blocks",
    "title": "Data Flow",
    "section": "Summary by basic blocks",
    "text": "Summary by basic blocks\nThe dataflow equations used for a given basic block b and exiting block final in live variable analysis:\n\\(\\operatorname{GEN}[b]\\) - The set of variables that are used in b before any assignment in the same basic block.\n\\(\\operatorname{KILL}[b]\\) - The set of variables that are assigned a value in b\nThe in-state of a block is the set of variables that are live at the start of the block. Its out-state is the set of variables that are live at the end of it. The out-state is the union of the in-states of the block’s successors. The transfer function of a statement is applied by making the variables that are written dead, then making the variables that are read live."
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#equations-1",
    "href": "lectures/revealjs_04_data_flow.qmd.html#equations-1",
    "title": "Data Flow",
    "section": "equations",
    "text": "equations\n$\n\\[\\begin{aligned}\n  & \\operatorname{IN}[b]=\\operatorname{GEN}[b] \\cup\\left(\\operatorname{OUT}[b]-\\operatorname{KILL}[s]\\right) \\\\\n  & \\operatorname{OUT}[\\text { final }]=\\emptyset \\\\\n  & \\operatorname{OUT}[b]=\\bigcup_{p \\in s u c c[b]} \\operatorname{IN}[p] \\\\\n  & \\operatorname{GEN}\\left[b: y \\leftarrow f\\left(x_1, \\cdots, x_n\\right)\\right]=\\left\\{x_1, \\ldots, x_n\\right\\} \\\\\n  & \\operatorname{KILL}\\left[b: y \\leftarrow f\\left(x_1, \\cdots, x_n\\right)\\right]=\\{y\\}\n\n\\end{aligned}\\]\n$"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#an-example",
    "href": "lectures/revealjs_04_data_flow.qmd.html#an-example",
    "title": "Data Flow",
    "section": "an example",
    "text": "an example\nb1: \na = 3\nb = 5\nd = 4\nx = 100\nif a &gt; b then \n\n   b2: \n    c = a + b\n    d = 2 \n\nb3: \n   c = 4\n   return b*d +c\n\\(\\operatorname{GEN}[b]\\) - The set of variables that are used in b before any assignment in the same basic block.\n\\(\\operatorname{KILL}[b]\\) - The set of variables that are assigned a value in b"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#processing",
    "href": "lectures/revealjs_04_data_flow.qmd.html#processing",
    "title": "Data Flow",
    "section": "processing",
    "text": "processing\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nb1--&gt; b2\nb1 --&gt; b3\nb2--&gt; b3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nb1--&gt; b2\nb1 --&gt; b3\nb2--&gt; b3\n\n\n\n\n\n\nGEN[b1] = []         kill[b1] = [a,b,d,x]\nGEN[b2] = [a,b]      kill[b2] = [c,d]\nGEN[b3] = [b,d]      Kill[b3] = [c]\nblock OUT       IN   Next IN        worklist  \nb3    []        []    [b,d]           b1,b2\nb1    [b,d]     []    []              b2\nb2    [b,d]     []    [a,b]           b1\nb1    [a,b,d]   []    []              empty"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#frameworks",
    "href": "lectures/revealjs_04_data_flow.qmd.html#frameworks",
    "title": "Data Flow",
    "section": "frameworks",
    "text": "frameworks"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#common-properties-direction",
    "href": "lectures/revealjs_04_data_flow.qmd.html#common-properties-direction",
    "title": "Data Flow",
    "section": "common properties Direction",
    "text": "common properties Direction\nDirection\n\n\nbackward\n\nliveness\nvery busy expressions\n\nOUT is a function of the IN of successors\n\nforward\n\nreaching Defs\nAvailable Expressions\n\nIN is a function of the OUT of Preds"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#common-properties-operation",
    "href": "lectures/revealjs_04_data_flow.qmd.html#common-properties-operation",
    "title": "Data Flow",
    "section": "common properties Operation",
    "text": "common properties Operation\n\n\nMay union\n\nLiveness\nReaching defs\n\nmerge using intersection\n\nmust\n\nvery busy expressions\nAvailable Expressions\n\nmerge using union"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#transfer-functions-with-a-block-or-for-one-statement",
    "href": "lectures/revealjs_04_data_flow.qmd.html#transfer-functions-with-a-block-or-for-one-statement",
    "title": "Data Flow",
    "section": "transfer functions with a block or for one statement",
    "text": "transfer functions with a block or for one statement\nForward\n\\[ \\text{OUT}_b = f_b(\\text{IN}_b) \\]\nBackward\n\\[ \\text{IN}_b = f_b(\\text{OUT}_b) \\]\nliveness IN = (OUT-def) union (args)\nVery busy expressions IN = (OUT - exprs(def)) union (this expr)"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#an-example-1",
    "href": "lectures/revealjs_04_data_flow.qmd.html#an-example-1",
    "title": "Data Flow",
    "section": "an example",
    "text": "an example\nif b1 \n    while b2 { x = a1}\nelse \n    while b3 { x = a2}\nx = a3\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nb1[\"p1: if b1\"]\nb2[\"p2: use b3\"]\nb3[\"p3: x = a2\"]\nb4[\"p4: use b2\"]\nb5[\"p5: x = a1\"]\nb6[\"p6: x = a3\"]\nb1--&gt; b4\nb1 --&gt; b2\n\n\nb2--&gt; b6\nb2--&gt; b3\n\nb3--&gt; b2\nb5--&gt; b4\n\nb4--&gt; b5\nb4--&gt; b6\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nb1[\"p1: if b1\"]\nb2[\"p2: use b3\"]\nb3[\"p3: x = a2\"]\nb4[\"p4: use b2\"]\nb5[\"p5: x = a1\"]\nb6[\"p6: x = a3\"]\nb1--&gt; b4\nb1 --&gt; b2\n\n\nb2--&gt; b6\nb2--&gt; b3\n\nb3--&gt; b2\nb5--&gt; b4\n\nb4--&gt; b5\nb4--&gt; b6\n\n\n\n\n\n\n\nreaching defs - a definition of a variable v at pv reaches a point p if there is a path from pv tp p and v is not redefined along the path"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#equations-2",
    "href": "lectures/revealjs_04_data_flow.qmd.html#equations-2",
    "title": "Data Flow",
    "section": "equations",
    "text": "equations\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n\ngraph TD;\nb1[\"p1: if b1\"]\nb2[\"p2: use b3\"]\nb3[\"p3: x = a2\"]\nb4[\"p4: use b2\"]\nb5[\"p5: x = a1\"]\nb6[\"p6: x = a3\"]\nb1--&gt; b4\nb1 --&gt; b2\n\n\nb2--&gt; b6\nb2--&gt; b3\n\nb3--&gt; b2\nb5--&gt; b4\n\nb4--&gt; b5\nb4--&gt; b6\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n\ngraph TD;\nb1[\"p1: if b1\"]\nb2[\"p2: use b3\"]\nb3[\"p3: x = a2\"]\nb4[\"p4: use b2\"]\nb5[\"p5: x = a1\"]\nb6[\"p6: x = a3\"]\nb1--&gt; b4\nb1 --&gt; b2\n\n\nb2--&gt; b6\nb2--&gt; b3\n\nb3--&gt; b2\nb5--&gt; b4\n\nb4--&gt; b5\nb4--&gt; b6\n\n\n\n\n\n\n\n\n\n\\[ \\small \\text{IN}_p = \\bigcup \\text{OUT}_{ps}, ps \\in pred(p)\\]\nIN[1] = empty\nIN[2] = OUT[1] union OUT[3]\nIN[3] = OUT[2]\nIN[4] = OUT[1] union OUT[5]\nIN[5] = OUT[4]\nIN[6] = OUT[2] union OUT[4]\n\n\n\\[ \\small \\text{OUT}_p = (\\text{IN)}_p - defs(v)) \\cup \\{ (p,v) \\}  \\]\nOUT[1] = IN[1]\nOUT[2] = IN[2]\nOUT[3] = (IN[3] -{3,5,6}) union {3}\nOUT[4] = IN[4]\nOUT[5] = (IN[5] - {3,5,6}) union {5}\nOUT[6] = (IN[6] - {3,5,6}) union {6}"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#complexity",
    "href": "lectures/revealjs_04_data_flow.qmd.html#complexity",
    "title": "Data Flow",
    "section": "complexity",
    "text": "complexity"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#graph-of-equations",
    "href": "lectures/revealjs_04_data_flow.qmd.html#graph-of-equations",
    "title": "Data Flow",
    "section": "graph of equations",
    "text": "graph of equations\n\n\nIN[1] = empty\nIN[2] = OUT[1] union OUT[3]\nIN[3] = OUT[2]\nIN[4] = OUT[1] union OUT[5]\nIN[5] = OUT[4]\nIN[6] = OUT[2] union OUT[4]\nOUT[1] = IN[1]\nOUT[2] = IN[2]\nOUT[3] = (IN[3] -{3,5,6}) union {3}\nOUT[4] = IN[4]\nOUT[5] = (IN[5] - {3,5,6}) union {5}\nOUT[6] = (IN[6] - {3,5,6}) union {6}\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB;\nIN1--&gt;OUT1\nOUT1--&gt;IN2\nOUT1--&gt;IN4\nOUT3--&gt; IN2\nOUT2--&gt; IN3\nOUT5--&gt; IN4\nOUT4--&gt; IN5\nOUT2--&gt; IN6\nOUT4--&gt; IN6\nIN2--&gt; OUT2\nIN3--&gt; OUT3\nIN4--&gt; OUT4\nIN5--&gt; OUT5\nIN6--&gt; OUT6\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB;\nIN1--&gt;OUT1\nOUT1--&gt;IN2\nOUT1--&gt;IN4\nOUT3--&gt; IN2\nOUT2--&gt; IN3\nOUT5--&gt; IN4\nOUT4--&gt; IN5\nOUT2--&gt; IN6\nOUT4--&gt; IN6\nIN2--&gt; OUT2\nIN3--&gt; OUT3\nIN4--&gt; OUT4\nIN5--&gt; OUT5\nIN6--&gt; OUT6"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#reverse-postorder",
    "href": "lectures/revealjs_04_data_flow.qmd.html#reverse-postorder",
    "title": "Data Flow",
    "section": "Reverse Postorder",
    "text": "Reverse Postorder\nvisit successors first (need an ordering)"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#order",
    "href": "lectures/revealjs_04_data_flow.qmd.html#order",
    "title": "Data Flow",
    "section": "order",
    "text": "order\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB;\nIN1--&gt;OUT1\nOUT1--&gt;IN2\nOUT1--&gt;IN4\nOUT3--&gt; IN2\nOUT2--&gt; IN3\nOUT5--&gt; IN4\nOUT4--&gt; IN5\nOUT2--&gt; IN6\nOUT4--&gt; IN6\nIN2--&gt; OUT2\nIN3--&gt; OUT3\nIN4--&gt; OUT4\nIN5--&gt; OUT5\nIN6--&gt; OUT6\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB;\nIN1--&gt;OUT1\nOUT1--&gt;IN2\nOUT1--&gt;IN4\nOUT3--&gt; IN2\nOUT2--&gt; IN3\nOUT5--&gt; IN4\nOUT4--&gt; IN5\nOUT2--&gt; IN6\nOUT4--&gt; IN6\nIN2--&gt; OUT2\nIN3--&gt; OUT3\nIN4--&gt; OUT4\nIN5--&gt; OUT5\nIN6--&gt; OUT6\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[b1-1]--&gt; b4[b4-4]\nb1--&gt; b2[b2-2]\nb4--&gt; b6[b6-5]\nb4--&gt; b5[b5-6]\nb2--&gt; b3[b3-3]\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[b1-1]--&gt; b4[b4-4]\nb1--&gt; b2[b2-2]\nb4--&gt; b6[b6-5]\nb4--&gt; b5[b5-6]\nb2--&gt; b3[b3-3]\n\n\n\n\n\n\n\norder b5 b6 b3 b3 b2 b1"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#implement",
    "href": "lectures/revealjs_04_data_flow.qmd.html#implement",
    "title": "Data Flow",
    "section": "implement",
    "text": "implement\nkeep two data structures\n\nC current list\nP set of pending lists\n\ninitially C is a reverse post order sort of the nodes\nprocess each element of C\nwhen we find a changed add it to P\nWhen C is empty, sort P in reverse post order and move to C"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#representing-sets",
    "href": "lectures/revealjs_04_data_flow.qmd.html#representing-sets",
    "title": "Data Flow",
    "section": "representing sets",
    "text": "representing sets\nwe keep doing union and intersection for sets, which are sparse\ncompilers generally use bit vectors"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#pseudo-code",
    "href": "lectures/revealjs_04_data_flow.qmd.html#pseudo-code",
    "title": "Data Flow",
    "section": "pseudo code",
    "text": "pseudo code\n// Initialize\nfor all CFG nodes n in N,\n    OUT[n] = emptyset; // can optimize by OUT[n] = GEN[n];\n\n// put all nodes into the changed set\n// N is all nodes in graph,\nChanged = N;\n\n// Iterate \nwhile (Changed != emptyset)\n{\n    choose a node n in Changed;\n    // remove it from the changed set\n    Changed = Changed -{ n };\n\n    // init IN[n] to be empty\n    IN[n] = emptyset;\n  \n    // calculate IN[n] from predecessors' OUT[p]\n    for all nodes p in predecessors(n)\n         IN[n] = IN[n] Union OUT[p];\n\n    oldout = OUT[n]; // save old OUT[n]\n    \n    // update OUT[n] using transfer function f_n ()\n    OUT[n] = GEN[n] Union (IN[n] -KILL[n]);\n\n    // any change to OUT[n] compared to previous value?\n    if (OUT[n] changed) // compare oldout vs. OUT[n]\n    {    \n        // if yes, put all successors of n into the changed set\n        for all nodes s in successors(n)\n             Changed = Changed U { s };\n    }\n}"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#loops",
    "href": "lectures/revealjs_04_data_flow.qmd.html#loops",
    "title": "Data Flow",
    "section": "loops",
    "text": "loops\nThis algorithm has no problems with loops!"
  },
  {
    "objectID": "lectures/revealjs_04_data_flow.qmd.html#homework-3",
    "href": "lectures/revealjs_04_data_flow.qmd.html#homework-3",
    "title": "Data Flow",
    "section": "homework 3",
    "text": "homework 3\nImplement one data flow analysis - For Bonus points make it generic so that the same code supports multiple analysis. As always, think about how to test it. use a simple ordering- Not necessary to use reverse post order\nas always think about testing"
  },
  {
    "objectID": "lectures/04_data_flow.html",
    "href": "lectures/04_data_flow.html",
    "title": "Data Flow",
    "section": "",
    "text": "The material in these slides have been taken from Lecture Notes in Static Analysis” (Sec.6), by Michael I. Schwartzbach, “Principles of Program Analysis”, Chapter 6, by Niesen et al, and from Miachel Schwartzbach’s “Lecture notes in Static Analysis”, Chapter 6, First Section.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#in-and-out",
    "href": "lectures/04_data_flow.html#in-and-out",
    "title": "Data Flow",
    "section": "IN and OUT",
    "text": "IN and OUT",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#equations",
    "href": "lectures/04_data_flow.html#equations",
    "title": "Data Flow",
    "section": "EQUATIONS",
    "text": "EQUATIONS",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#liveness-example",
    "href": "lectures/04_data_flow.html#liveness-example",
    "title": "Data Flow",
    "section": "Liveness Example",
    "text": "Liveness Example",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#summary-by-basic-blocks",
    "href": "lectures/04_data_flow.html#summary-by-basic-blocks",
    "title": "Data Flow",
    "section": "Summary by basic blocks",
    "text": "Summary by basic blocks\nThe dataflow equations used for a given basic block b and exiting block final in live variable analysis:\n\\(\\operatorname{GEN}[b]\\) - The set of variables that are used in b before any assignment in the same basic block.\n\\(\\operatorname{KILL}[b]\\) - The set of variables that are assigned a value in b\nThe in-state of a block is the set of variables that are live at the start of the block. Its out-state is the set of variables that are live at the end of it. The out-state is the union of the in-states of the block’s successors. The transfer function of a statement is applied by making the variables that are written dead, then making the variables that are read live.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#equations-1",
    "href": "lectures/04_data_flow.html#equations-1",
    "title": "Data Flow",
    "section": "equations",
    "text": "equations\n$\n\\[\\begin{aligned}\n  & \\operatorname{IN}[b]=\\operatorname{GEN}[b] \\cup\\left(\\operatorname{OUT}[b]-\\operatorname{KILL}[s]\\right) \\\\\n  & \\operatorname{OUT}[\\text { final }]=\\emptyset \\\\\n  & \\operatorname{OUT}[b]=\\bigcup_{p \\in s u c c[b]} \\operatorname{IN}[p] \\\\\n  & \\operatorname{GEN}\\left[b: y \\leftarrow f\\left(x_1, \\cdots, x_n\\right)\\right]=\\left\\{x_1, \\ldots, x_n\\right\\} \\\\\n  & \\operatorname{KILL}\\left[b: y \\leftarrow f\\left(x_1, \\cdots, x_n\\right)\\right]=\\{y\\}\n\n\\end{aligned}\\]\n$",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#an-example",
    "href": "lectures/04_data_flow.html#an-example",
    "title": "Data Flow",
    "section": "an example",
    "text": "an example\nb1: \na = 3\nb = 5\nd = 4\nx = 100\nif a &gt; b then \n\n   b2: \n    c = a + b\n    d = 2 \n\nb3: \n   c = 4\n   return b*d +c\n\\(\\operatorname{GEN}[b]\\) - The set of variables that are used in b before any assignment in the same basic block.\n\\(\\operatorname{KILL}[b]\\) - The set of variables that are assigned a value in b\n\nGEN[b1] = []         kill[b1] = [a,b,d,x]\nGEN[b2] = [a,b]      kill[b2] = [c,d]\nGEN[b3] = [b,d]      Kill[b3] = [c]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#processing",
    "href": "lectures/04_data_flow.html#processing",
    "title": "Data Flow",
    "section": "processing",
    "text": "processing\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nb1--&gt; b2\nb1 --&gt; b3\nb2--&gt; b3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nb1--&gt; b2\nb1 --&gt; b3\nb2--&gt; b3\n\n\n\n\n\n\nGEN[b1] = []         kill[b1] = [a,b,d,x]\nGEN[b2] = [a,b]      kill[b2] = [c,d]\nGEN[b3] = [b,d]      Kill[b3] = [c]\nblock OUT       IN   Next IN        worklist  \nb3    []        []    [b,d]           b1,b2\nb1    [b,d]     []    []              b2\nb2    [b,d]     []    [a,b]           b1\nb1    [a,b,d]   []    []              empty",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#frameworks",
    "href": "lectures/04_data_flow.html#frameworks",
    "title": "Data Flow",
    "section": "frameworks",
    "text": "frameworks",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#common-properties-direction",
    "href": "lectures/04_data_flow.html#common-properties-direction",
    "title": "Data Flow",
    "section": "common properties Direction",
    "text": "common properties Direction\nDirection\n\n\nbackward\n\nliveness\nvery busy expressions\n\nOUT is a function of the IN of successors\n\nforward\n\nreaching Defs\nAvailable Expressions\n\nIN is a function of the OUT of Preds",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#common-properties-operation",
    "href": "lectures/04_data_flow.html#common-properties-operation",
    "title": "Data Flow",
    "section": "common properties Operation",
    "text": "common properties Operation\n\n\nMay union\n\nLiveness\nReaching defs\n\nmerge using intersection\n\nmust\n\nvery busy expressions\nAvailable Expressions\n\nmerge using union",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#transfer-functions-with-a-block-or-for-one-statement",
    "href": "lectures/04_data_flow.html#transfer-functions-with-a-block-or-for-one-statement",
    "title": "Data Flow",
    "section": "transfer functions with a block or for one statement",
    "text": "transfer functions with a block or for one statement\nForward\n\\[ \\text{OUT}_b = f_b(\\text{IN}_b) \\]\nBackward\n\\[ \\text{IN}_b = f_b(\\text{OUT}_b) \\]\nliveness IN = (OUT-def) union (args)\nVery busy expressions IN = (OUT - exprs(def)) union (this expr)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#an-example-1",
    "href": "lectures/04_data_flow.html#an-example-1",
    "title": "Data Flow",
    "section": "an example",
    "text": "an example\nif b1 \n    while b2 { x = a1}\nelse \n    while b3 { x = a2}\nx = a3\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nb1[\"p1: if b1\"]\nb2[\"p2: use b3\"]\nb3[\"p3: x = a2\"]\nb4[\"p4: use b2\"]\nb5[\"p5: x = a1\"]\nb6[\"p6: x = a3\"]\nb1--&gt; b4\nb1 --&gt; b2\n\n\nb2--&gt; b6\nb2--&gt; b3\n\nb3--&gt; b2\nb5--&gt; b4\n\nb4--&gt; b5\nb4--&gt; b6\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nb1[\"p1: if b1\"]\nb2[\"p2: use b3\"]\nb3[\"p3: x = a2\"]\nb4[\"p4: use b2\"]\nb5[\"p5: x = a1\"]\nb6[\"p6: x = a3\"]\nb1--&gt; b4\nb1 --&gt; b2\n\n\nb2--&gt; b6\nb2--&gt; b3\n\nb3--&gt; b2\nb5--&gt; b4\n\nb4--&gt; b5\nb4--&gt; b6\n\n\n\n\n\n\n\nreaching defs - a definition of a variable v at pv reaches a point p if there is a path from pv tp p and v is not redefined along the path",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#equations-2",
    "href": "lectures/04_data_flow.html#equations-2",
    "title": "Data Flow",
    "section": "equations",
    "text": "equations\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n\ngraph TD;\nb1[\"p1: if b1\"]\nb2[\"p2: use b3\"]\nb3[\"p3: x = a2\"]\nb4[\"p4: use b2\"]\nb5[\"p5: x = a1\"]\nb6[\"p6: x = a3\"]\nb1--&gt; b4\nb1 --&gt; b2\n\n\nb2--&gt; b6\nb2--&gt; b3\n\nb3--&gt; b2\nb5--&gt; b4\n\nb4--&gt; b5\nb4--&gt; b6\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n\ngraph TD;\nb1[\"p1: if b1\"]\nb2[\"p2: use b3\"]\nb3[\"p3: x = a2\"]\nb4[\"p4: use b2\"]\nb5[\"p5: x = a1\"]\nb6[\"p6: x = a3\"]\nb1--&gt; b4\nb1 --&gt; b2\n\n\nb2--&gt; b6\nb2--&gt; b3\n\nb3--&gt; b2\nb5--&gt; b4\n\nb4--&gt; b5\nb4--&gt; b6\n\n\n\n\n\n\n\n\n\n\\[ \\small \\text{IN}_p = \\bigcup \\text{OUT}_{ps}, ps \\in pred(p)\\]\nIN[1] = empty\nIN[2] = OUT[1] union OUT[3]\nIN[3] = OUT[2]\nIN[4] = OUT[1] union OUT[5]\nIN[5] = OUT[4]\nIN[6] = OUT[2] union OUT[4]\n\n\n\\[ \\small \\text{OUT}_p = (\\text{IN)}_p - defs(v)) \\cup \\{ (p,v) \\}  \\]\nOUT[1] = IN[1]\nOUT[2] = IN[2]\nOUT[3] = (IN[3] -{3,5,6}) union {3}\nOUT[4] = IN[4]\nOUT[5] = (IN[5] - {3,5,6}) union {5}\nOUT[6] = (IN[6] - {3,5,6}) union {6}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#complexity",
    "href": "lectures/04_data_flow.html#complexity",
    "title": "Data Flow",
    "section": "complexity",
    "text": "complexity",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#graph-of-equations",
    "href": "lectures/04_data_flow.html#graph-of-equations",
    "title": "Data Flow",
    "section": "graph of equations",
    "text": "graph of equations\n\n\nIN[1] = empty\nIN[2] = OUT[1] union OUT[3]\nIN[3] = OUT[2]\nIN[4] = OUT[1] union OUT[5]\nIN[5] = OUT[4]\nIN[6] = OUT[2] union OUT[4]\nOUT[1] = IN[1]\nOUT[2] = IN[2]\nOUT[3] = (IN[3] -{3,5,6}) union {3}\nOUT[4] = IN[4]\nOUT[5] = (IN[5] - {3,5,6}) union {5}\nOUT[6] = (IN[6] - {3,5,6}) union {6}\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB;\nIN1--&gt;OUT1\nOUT1--&gt;IN2\nOUT1--&gt;IN4\nOUT3--&gt; IN2\nOUT2--&gt; IN3\nOUT5--&gt; IN4\nOUT4--&gt; IN5\nOUT2--&gt; IN6\nOUT4--&gt; IN6\nIN2--&gt; OUT2\nIN3--&gt; OUT3\nIN4--&gt; OUT4\nIN5--&gt; OUT5\nIN6--&gt; OUT6\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB;\nIN1--&gt;OUT1\nOUT1--&gt;IN2\nOUT1--&gt;IN4\nOUT3--&gt; IN2\nOUT2--&gt; IN3\nOUT5--&gt; IN4\nOUT4--&gt; IN5\nOUT2--&gt; IN6\nOUT4--&gt; IN6\nIN2--&gt; OUT2\nIN3--&gt; OUT3\nIN4--&gt; OUT4\nIN5--&gt; OUT5\nIN6--&gt; OUT6",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#reverse-postorder",
    "href": "lectures/04_data_flow.html#reverse-postorder",
    "title": "Data Flow",
    "section": "Reverse Postorder",
    "text": "Reverse Postorder\nvisit successors first (need an ordering)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#order",
    "href": "lectures/04_data_flow.html#order",
    "title": "Data Flow",
    "section": "order",
    "text": "order\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB;\nIN1--&gt;OUT1\nOUT1--&gt;IN2\nOUT1--&gt;IN4\nOUT3--&gt; IN2\nOUT2--&gt; IN3\nOUT5--&gt; IN4\nOUT4--&gt; IN5\nOUT2--&gt; IN6\nOUT4--&gt; IN6\nIN2--&gt; OUT2\nIN3--&gt; OUT3\nIN4--&gt; OUT4\nIN5--&gt; OUT5\nIN6--&gt; OUT6\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB;\nIN1--&gt;OUT1\nOUT1--&gt;IN2\nOUT1--&gt;IN4\nOUT3--&gt; IN2\nOUT2--&gt; IN3\nOUT5--&gt; IN4\nOUT4--&gt; IN5\nOUT2--&gt; IN6\nOUT4--&gt; IN6\nIN2--&gt; OUT2\nIN3--&gt; OUT3\nIN4--&gt; OUT4\nIN5--&gt; OUT5\nIN6--&gt; OUT6\n\n\n\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[b1-1]--&gt; b4[b4-4]\nb1--&gt; b2[b2-2]\nb4--&gt; b6[b6-5]\nb4--&gt; b5[b5-6]\nb2--&gt; b3[b3-3]\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nb1[b1-1]--&gt; b4[b4-4]\nb1--&gt; b2[b2-2]\nb4--&gt; b6[b6-5]\nb4--&gt; b5[b5-6]\nb2--&gt; b3[b3-3]\n\n\n\n\n\n\n\norder b5 b6 b3 b3 b2 b1",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#implement",
    "href": "lectures/04_data_flow.html#implement",
    "title": "Data Flow",
    "section": "implement",
    "text": "implement\nkeep two data structures\n\nC current list\nP set of pending lists\n\ninitially C is a reverse post order sort of the nodes\nprocess each element of C\nwhen we find a changed add it to P\nWhen C is empty, sort P in reverse post order and move to C",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#representing-sets",
    "href": "lectures/04_data_flow.html#representing-sets",
    "title": "Data Flow",
    "section": "representing sets",
    "text": "representing sets\nwe keep doing union and intersection for sets, which are sparse\ncompilers generally use bit vectors",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#pseudo-code",
    "href": "lectures/04_data_flow.html#pseudo-code",
    "title": "Data Flow",
    "section": "pseudo code",
    "text": "pseudo code\n// Initialize\nfor all CFG nodes n in N,\n    OUT[n] = emptyset; // can optimize by OUT[n] = GEN[n];\n\n// put all nodes into the changed set\n// N is all nodes in graph,\nChanged = N;\n\n// Iterate \nwhile (Changed != emptyset)\n{\n    choose a node n in Changed;\n    // remove it from the changed set\n    Changed = Changed -{ n };\n\n    // init IN[n] to be empty\n    IN[n] = emptyset;\n  \n    // calculate IN[n] from predecessors' OUT[p]\n    for all nodes p in predecessors(n)\n         IN[n] = IN[n] Union OUT[p];\n\n    oldout = OUT[n]; // save old OUT[n]\n    \n    // update OUT[n] using transfer function f_n ()\n    OUT[n] = GEN[n] Union (IN[n] -KILL[n]);\n\n    // any change to OUT[n] compared to previous value?\n    if (OUT[n] changed) // compare oldout vs. OUT[n]\n    {    \n        // if yes, put all successors of n into the changed set\n        for all nodes s in successors(n)\n             Changed = Changed U { s };\n    }\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#loops",
    "href": "lectures/04_data_flow.html#loops",
    "title": "Data Flow",
    "section": "loops",
    "text": "loops\nThis algorithm has no problems with loops!",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/04_data_flow.html#homework-3",
    "href": "lectures/04_data_flow.html#homework-3",
    "title": "Data Flow",
    "section": "homework 3",
    "text": "homework 3\nImplement one data flow analysis - For Bonus points make it generic so that the same code supports multiple analysis. As always, think about how to test it. use a simple ordering- Not necessary to use reverse post order\nas always think about testing",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Data Flow"
    ]
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#what-are-classic-loop-optimizations",
    "href": "lectures/revealjs_05b_licm.qmd.html#what-are-classic-loop-optimizations",
    "title": "classic loop optimizations",
    "section": "What are classic loop optimizations?",
    "text": "What are classic loop optimizations?\n\nLoop Invariant Code Motion\nInduction Variable Recognition\nStrength Reduction\nLinear Test Replacement\nLoop Unrolling"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#loop-invariant-code-motion",
    "href": "lectures/revealjs_05b_licm.qmd.html#loop-invariant-code-motion",
    "title": "classic loop optimizations",
    "section": "Loop Invariant Code Motion",
    "text": "Loop Invariant Code Motion\nrecall natural loops\n\nstrongly connected region in the cfg\none entry point (dominates all the nodes in the loop)"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#pre-steps---reshape-the-cfg",
    "href": "lectures/revealjs_05b_licm.qmd.html#pre-steps---reshape-the-cfg",
    "title": "classic loop optimizations",
    "section": "pre steps - reshape the cfg",
    "text": "pre steps - reshape the cfg\n\nfind the natural loops\nadd pre-header\n\nif we are going to move code we often need to add a special basic block which is called a landing pad or a pre-header create a new block b. change all the preds of the loop header to point to the pre-header, add an edge from b to the loop header"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#pre-header-can-change-phi-nodes",
    "href": "lectures/revealjs_05b_licm.qmd.html#pre-header-can-change-phi-nodes",
    "title": "classic loop optimizations",
    "section": "pre-header can change phi nodes",
    "text": "pre-header can change phi nodes\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA[\"x1 = 3\"]\nB1[\"y1 = 4\"]\nB2[\"y2 = 5\"]\nC[\"x2=phi(x1,x3)\\ny3=phi(y1,y2,y3)\\nz1=x2*x3\\nq1=y3*y3\\nw1=y3+2\"]\nD[\"w2=w1+5\"]\nE[\"w3=phi(w1,w2)\\np1=w3+y3\\nx3=x2+1\\nq2=q1+1\"]\nA--&gt; B1\nA--&gt; B2\nB1--&gt; C\nB2--&gt; C\nC--&gt;D\nD--&gt; E\nC--&gt; E\nE--&gt; C\nE--&gt; After\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA[\"x1 = 3\"]\nB1[\"y1 = 4\"]\nB2[\"y2 = 5\"]\nC[\"x2=phi(x1,x3)\\ny3=phi(y1,y2,y3)\\nz1=x2*x3\\nq1=y3*y3\\nw1=y3+2\"]\nD[\"w2=w1+5\"]\nE[\"w3=phi(w1,w2)\\np1=w3+y3\\nx3=x2+1\\nq2=q1+1\"]\nA--&gt; B1\nA--&gt; B2\nB1--&gt; C\nB2--&gt; C\nC--&gt;D\nD--&gt; E\nC--&gt; E\nE--&gt; C\nE--&gt; After\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA[\"x1 = 3\"]\nB1[\"y1 = 4\"]\nB2[\"y2 = 5\"]\nLP[\"y3=phi(y1,y2)\"]\nC[\"x2=phi(x1,x3)\\nz1=x2*x3\\nq1=y3*y3\\nw1=y3+2\"]\nD[\"w2=w1+5\"]\nE[\"w3=phi(w1,w2)\\np1=w3+y3\\nx3=x2+1\\nq2=q1+1\"]\nA--&gt; B1\nA--&gt; B2\nB1--&gt;LP --&gt; C\nB2--&gt;LP \nC--&gt;D\nD--&gt; E\nC--&gt; E\nE--&gt; C\nE--&gt; After\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA[\"x1 = 3\"]\nB1[\"y1 = 4\"]\nB2[\"y2 = 5\"]\nLP[\"y3=phi(y1,y2)\"]\nC[\"x2=phi(x1,x3)\\nz1=x2*x3\\nq1=y3*y3\\nw1=y3+2\"]\nD[\"w2=w1+5\"]\nE[\"w3=phi(w1,w2)\\np1=w3+y3\\nx3=x2+1\\nq2=q1+1\"]\nA--&gt; B1\nA--&gt; B2\nB1--&gt;LP --&gt; C\nB2--&gt;LP \nC--&gt;D\nD--&gt; E\nC--&gt; E\nE--&gt; C\nE--&gt; After"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#while-loop-may-not-execute-we-can-restructure-into-a-do-while",
    "href": "lectures/revealjs_05b_licm.qmd.html#while-loop-may-not-execute-we-can-restructure-into-a-do-while",
    "title": "classic loop optimizations",
    "section": "while loop may not execute, we can restructure into a do-while",
    "text": "while loop may not execute, we can restructure into a do-while\n\n\nwhile(e) {\n  s(j) \n}\n\nif (e) {\n  t  = j loopinv \n  do {\n    s\n  } while(e)\n}"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#check-for-zero-trip-count",
    "href": "lectures/revealjs_05b_licm.qmd.html#check-for-zero-trip-count",
    "title": "classic loop optimizations",
    "section": "check for zero trip count",
    "text": "check for zero trip count\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nA;\nB[\"if e\"]\nS\nendloop\nnext\nA--&gt; B\nB--&gt; next\nB--&gt; S\nS--&gt; endloop\nendloop --&gt; B\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nA;\nB[\"if e\"]\nS\nendloop\nnext\nA--&gt; B\nB--&gt; next\nB--&gt; S\nS--&gt; endloop\nendloop --&gt; B\n\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\npretest[\"if e\"]\npretest--&gt;next \ndo\ns\nendloop\nposttest[\"if e\"]\nnext\npretest--&gt;do\ndo --&gt; s\ns--&gt; endloop\nendloop--&gt; posttest\nposttest --&gt; do\nposttest --&gt; next\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\npretest[\"if e\"]\npretest--&gt;next \ndo\ns\nendloop\nposttest[\"if e\"]\nnext\npretest--&gt;do\ndo --&gt; s\ns--&gt; endloop\nendloop--&gt; posttest\nposttest --&gt; do\nposttest --&gt; next\n\n\n\n\n\n\n\n\nIn ssa a variable is loop invariant if it does not have a phi function at the header of the loop, or it is defined by a phi function and all the inputs are loop constants\nSSA If we find a loop-invariant computation in SSA form, then we just move it out of the loop to a block before the loop. When moving a (side effect-free) SSA loop-invariant computation to a previous position, nothing can go wrong, because the value it computes cannot be overwritten later and the values it depends on cannot have been changed"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#licm",
    "href": "lectures/revealjs_05b_licm.qmd.html#licm",
    "title": "classic loop optimizations",
    "section": "licm",
    "text": "licm\nLoop invariant code motion recognizes computations in loop that produce the same value on each iteration and moves them out of the loop."
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#a-very-common-case-for-this-is-matrix-addressing",
    "href": "lectures/revealjs_05b_licm.qmd.html#a-very-common-case-for-this-is-matrix-addressing",
    "title": "classic loop optimizations",
    "section": "A very common case for this is matrix addressing",
    "text": "A very common case for this is matrix addressing\na[i,j] might expand to to \\(i*4*\\operatorname{stride_{a}} + j *4\\)\nfor j \n  a[i,j] = f(a[i,j+1])\nturns into\na = \nb = \nresult = 0\nfor (){\n    result += a*b\n}"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#when-is-ok-to-move-a-computation",
    "href": "lectures/revealjs_05b_licm.qmd.html#when-is-ok-to-move-a-computation",
    "title": "classic loop optimizations",
    "section": "when is ok to move a computation",
    "text": "when is ok to move a computation\n\nno side effects - cannot move alloc 10 outside of loop;\nin non ssa, computation d dominates all loop exits where d is live\nin non ssa only one def of d in the loop\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\npre[\"d = 0\"]\nL1[\"if i &gt; n\" ]\nloop1[\"i = i +1\"]\nd[\"d = a op b\"]\nuse[\"  =d \"]\nnext[\" = d\"]\n\npre--&gt; L1\nL1--&gt; loop1\nloop1--&gt; d\nd--&gt; use \nuse --&gt; L1\nL1 --&gt; next\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\npre[\"d = 0\"]\nL1[\"if i &gt; n\" ]\nloop1[\"i = i +1\"]\nd[\"d = a op b\"]\nuse[\"  =d \"]\nnext[\" = d\"]\n\npre--&gt; L1\nL1--&gt; loop1\nloop1--&gt; d\nd--&gt; use \nuse --&gt; L1\nL1 --&gt; next\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\npre[\"d = 0\"]\nL1[\"i = i +1\\n d= a op b\\n use d\"]\nL2[\"d = 2\\n use d\"]\nL3[\"if (i &lt; n)\"]\npre--&gt; L1\nL1--&gt; L2\nL2--&gt; L3\nL3--&gt; after\nL3 --&gt; L1\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\npre[\"d = 0\"]\nL1[\"i = i +1\\n d= a op b\\n use d\"]\nL2[\"d = 2\\n use d\"]\nL3[\"if (i &lt; n)\"]\npre--&gt; L1\nL1--&gt; L2\nL2--&gt; L3\nL3--&gt; after\nL3 --&gt; L1"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#find-loop-invariant-instructions",
    "href": "lectures/revealjs_05b_licm.qmd.html#find-loop-invariant-instructions",
    "title": "classic loop optimizations",
    "section": "find loop invariant instructions,",
    "text": "find loop invariant instructions,"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#naturally-iterative",
    "href": "lectures/revealjs_05b_licm.qmd.html#naturally-iterative",
    "title": "classic loop optimizations",
    "section": "naturally iterative",
    "text": "naturally iterative\niterate to convergence\n for each instr in  the loop\n   see if it is loop invar \n   if it is - move it"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#nested-loops",
    "href": "lectures/revealjs_05b_licm.qmd.html#nested-loops",
    "title": "classic loop optimizations",
    "section": "nested loops",
    "text": "nested loops\nwe want to process inner loops first\n\nadd all the pre-headers\nwalk the dominator tree in reverse post order - saving all the loop headers"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#def-of-loop-invariant-for-an-instruction-d-op-ab",
    "href": "lectures/revealjs_05b_licm.qmd.html#def-of-loop-invariant-for-an-instruction-d-op-ab",
    "title": "classic loop optimizations",
    "section": "def of loop invariant for an instruction d = op a,b",
    "text": "def of loop invariant for an instruction d = op a,b\n\na,b are constants or,\na,b defined outside the loop\na,b are loop invariants\n\nin SSA form if we find a loop invariant instruction we can always move it into the pre-header, because the value it writes is never rewritten, and the values that it depends on come from outside the loop"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#test-at-the-bottom---loop-always-executes-at-least-once",
    "href": "lectures/revealjs_05b_licm.qmd.html#test-at-the-bottom---loop-always-executes-at-least-once",
    "title": "classic loop optimizations",
    "section": "test at the bottom - loop always executes at least once",
    "text": "test at the bottom - loop always executes at least once\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nl0[\"l0:\"]\npre[\"preheader\"]\nl1[\"l1: i = i +1\"]\nd1[\"d1 = a ⊕ b\"]\nd2[\"   = d1\"]\nl0--&gt;pre\npre --&gt; l1\nl1--&gt; d1\nd1--&gt; d2\nd3[\"(i &lt; N) goto L1\"]\nd2--&gt; d3\nd3--&gt; l1\nd3--&gt; Next\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nl0[\"l0:\"]\npre[\"preheader\"]\nl1[\"l1: i = i +1\"]\nd1[\"d1 = a ⊕ b\"]\nd2[\"   = d1\"]\nl0--&gt;pre\npre --&gt; l1\nl1--&gt; d1\nd1--&gt; d2\nd3[\"(i &lt; N) goto L1\"]\nd2--&gt; d3\nd3--&gt; l1\nd3--&gt; Next\n\n\n\n\n\n\ncan move d"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#test-at-the-top",
    "href": "lectures/revealjs_05b_licm.qmd.html#test-at-the-top",
    "title": "classic loop optimizations",
    "section": "test at the top",
    "text": "test at the top\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nl0[\"l0:d = 0\"]\npre[\"preheader\"]\nl1[\"if (i&gt;=N) goto L2 \\nl1: i = i +1\"]\nl0--&gt;pre\npre --&gt; l1\nd1[\"d1 = a ⊕ b\"]\nd2[\"   = d1\"]\nl1--&gt; d1\nd1--&gt; d2\nd2--&gt; l1\nl1--&gt; l2\nl2--&gt; next\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nl0[\"l0:d = 0\"]\npre[\"preheader\"]\nl1[\"if (i&gt;=N) goto L2 \\nl1: i = i +1\"]\nl0--&gt;pre\npre --&gt; l1\nd1[\"d1 = a ⊕ b\"]\nd2[\"   = d1\"]\nl1--&gt; d1\nd1--&gt; d2\nd2--&gt; l1\nl1--&gt; l2\nl2--&gt; next\n\n\n\n\n\n\n\nwe can always convert into\n\nloop test at top \n\n\nif (test) {\n  preheader\n  loop test at bottom\n}"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#induction-variable-elimination",
    "href": "lectures/revealjs_05b_licm.qmd.html#induction-variable-elimination",
    "title": "classic loop optimizations",
    "section": "induction variable elimination",
    "text": "induction variable elimination\nfor (int i = 0; i &lt; 100; ++1){\n    f(a[i])\n}\ncalculate a[i] as: &a[0] + 4 * i in every loop iteration, but the values at each step only differ by 4\nWe want to change the multiply and add to an add"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#transformation",
    "href": "lectures/revealjs_05b_licm.qmd.html#transformation",
    "title": "classic loop optimizations",
    "section": "Transformation",
    "text": "Transformation\n\na_i = &a[0] before the loop\na_i = a_i + 4 (add the stride) in every iteration\nthe only remaining use of i is the test i &lt; 100, which could become a_i &lt; &a[0] + 4*100 (which is loop invariant)"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#step-1",
    "href": "lectures/revealjs_05b_licm.qmd.html#step-1",
    "title": "classic loop optimizations",
    "section": "step 1",
    "text": "step 1\n\nfind basic induction variables i = i + e, where e is loop invariant\n\nwhat does this look like in ssa\nfor each instruction d = c +- loop invariant see if there is a strongly connected graph in the ssa edges that only has adds and subtracts of loop invariant expressions"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#step-2-find-auxiliary-induction-variables",
    "href": "lectures/revealjs_05b_licm.qmd.html#step-2-find-auxiliary-induction-variables",
    "title": "classic loop optimizations",
    "section": "Step 2 find auxiliary induction variables",
    "text": "Step 2 find auxiliary induction variables\nj = basic_ind * loop inv + loop invar\nfor (int i = 0; i &lt; n; i++) {\n     j = 2*i + 1;     // Y \n     k = -i;          // Y \n     l = 2*i*i + 1;   // N \n     c = c + 5;       // Y* \n}"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#step-3",
    "href": "lectures/revealjs_05b_licm.qmd.html#step-3",
    "title": "classic loop optimizations",
    "section": "step 3",
    "text": "step 3\nreplace auxiliary induction variables (derived ) by new variables without the multiply"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#step4",
    "href": "lectures/revealjs_05b_licm.qmd.html#step4",
    "title": "classic loop optimizations",
    "section": "step4",
    "text": "step4\nif the only remaining use of the induction variable is the termination test, change the test to use the new variable\nsum = 0\nfor (i = 1, i &lt; 100; i++) {\n  sum = sum + a[i -1]\n}\nin SSA form:\n   sum0 = 0\n   i0 = 1\nL: sum1 = phi(sum0, sum2)\n   i1 = phi(i0, i2)\n   t10 = i1 -1 \n   t20 = t10 * 4\n   t30 = t20 + &a\n   t40 = load t30\n   sum2 = sum1 + t40\n   i2 = i1 + 1\n   if (i2 &lt;= 100)go to l\n\ni is a basic induction variable\nt10 is a aux induction variable\nt20 is an aux induction variable\nt30 is an aux induction variable\n\nt3 has a use in the load\nt3 = t20 + &a ==&gt; t10 * 4 + &a ==&gt; (i1-1)* 4+ &a\nt3 = 4* i1 + &a - 4\n   sum0 = 0\n   i0 = 1\n   t50 = &a -4  // initial value \nL: sum1 = phi(sum0, sum2)\n   i1 = phi(i0, i2)\n   t51 = phi(t50, t52)\n   //t10 = i1 -1 \n   //t20 = t10 * 4\n   //t30 = t20 + &a\n   t40 = load t50\n   sum2 = sum1 + t40\n   i2 = i1 + 1\n   t52 = t50 + 4\n   if (i2 &lt;= 100)go to l\n   sum0 = 0\n   i0 = 1\n   t50 = &a -4  // initial value \nL: sum1 = phi(sum0, sum2)\n   // i1 = phi(i0, i2)\n   t51 = phi(t50, t52)\n   //t10 = i1 -1 \n   //t20 = t10 * 4\n   //t30 = t20 + &a\n   t40 = load t50\n   sum2 = sum1 + t40\n   //i2 = i1 + 1\n   t52 = t50 + 4\n   if (t52 &lt;= 396 + &a )go to l"
  },
  {
    "objectID": "lectures/revealjs_05b_licm.qmd.html#loop-un-switching",
    "href": "lectures/revealjs_05b_licm.qmd.html#loop-un-switching",
    "title": "classic loop optimizations",
    "section": "loop un-switching",
    "text": "loop un-switching\nfor (int i = 0 ; i &lt; 100; ++1){\n    if (c) {  // c is loop invariant \n        f(i)\n    } else {\n        g(i)\n    }\n}\nlook for special patterns and replace\nif (c) {  // c is loop invariant \n   for (int i = 0 ; i &lt; 100; ++1){\n        f(i)\n    } \n}else {\n    for (int i = 0 ; i &lt; 100; ++1){\n        g(i)\n    }\n}\nThis is often done before vectorization\nloop fusion\nfor (i = 0; i &lt; 100 ; ++){\n s0:   b[i] = f(a[i])\n}\nfor (i = 0; i &lt; 100 ; ++){\n s1:   c[i] = f(b[i])\n}\n\nwhen is it legal to do this?\nWhen can we get rid of the b array?\n\nThere is also an optimization that goes the other way split a loop so that each statement becomes a separate loop incase we could run as vectors"
  },
  {
    "objectID": "lectures/05b_licm.html",
    "href": "lectures/05b_licm.html",
    "title": "classic loop optimizations",
    "section": "",
    "text": "Loop optimizations are important because",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#what-are-classic-loop-optimizations",
    "href": "lectures/05b_licm.html#what-are-classic-loop-optimizations",
    "title": "classic loop optimizations",
    "section": "What are classic loop optimizations?",
    "text": "What are classic loop optimizations?\n\nLoop Invariant Code Motion\nInduction Variable Recognition\nStrength Reduction\nLinear Test Replacement\nLoop Unrolling",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#loop-invariant-code-motion",
    "href": "lectures/05b_licm.html#loop-invariant-code-motion",
    "title": "classic loop optimizations",
    "section": "Loop Invariant Code Motion",
    "text": "Loop Invariant Code Motion\nrecall natural loops\n\nstrongly connected region in the cfg\none entry point (dominates all the nodes in the loop)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#pre-steps---reshape-the-cfg",
    "href": "lectures/05b_licm.html#pre-steps---reshape-the-cfg",
    "title": "classic loop optimizations",
    "section": "pre steps - reshape the cfg",
    "text": "pre steps - reshape the cfg\n\nfind the natural loops\nadd pre-header\n\nif we are going to move code we often need to add a special basic block which is called a landing pad or a pre-header create a new block b. change all the preds of the loop header to point to the pre-header, add an edge from b to the loop header",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#pre-header-can-change-phi-nodes",
    "href": "lectures/05b_licm.html#pre-header-can-change-phi-nodes",
    "title": "classic loop optimizations",
    "section": "pre-header can change phi nodes",
    "text": "pre-header can change phi nodes\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA[\"x1 = 3\"]\nB1[\"y1 = 4\"]\nB2[\"y2 = 5\"]\nC[\"x2=phi(x1,x3)\\ny3=phi(y1,y2,y3)\\nz1=x2*x3\\nq1=y3*y3\\nw1=y3+2\"]\nD[\"w2=w1+5\"]\nE[\"w3=phi(w1,w2)\\np1=w3+y3\\nx3=x2+1\\nq2=q1+1\"]\nA--&gt; B1\nA--&gt; B2\nB1--&gt; C\nB2--&gt; C\nC--&gt;D\nD--&gt; E\nC--&gt; E\nE--&gt; C\nE--&gt; After\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA[\"x1 = 3\"]\nB1[\"y1 = 4\"]\nB2[\"y2 = 5\"]\nC[\"x2=phi(x1,x3)\\ny3=phi(y1,y2,y3)\\nz1=x2*x3\\nq1=y3*y3\\nw1=y3+2\"]\nD[\"w2=w1+5\"]\nE[\"w3=phi(w1,w2)\\np1=w3+y3\\nx3=x2+1\\nq2=q1+1\"]\nA--&gt; B1\nA--&gt; B2\nB1--&gt; C\nB2--&gt; C\nC--&gt;D\nD--&gt; E\nC--&gt; E\nE--&gt; C\nE--&gt; After\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA[\"x1 = 3\"]\nB1[\"y1 = 4\"]\nB2[\"y2 = 5\"]\nLP[\"y3=phi(y1,y2)\"]\nC[\"x2=phi(x1,x3)\\nz1=x2*x3\\nq1=y3*y3\\nw1=y3+2\"]\nD[\"w2=w1+5\"]\nE[\"w3=phi(w1,w2)\\np1=w3+y3\\nx3=x2+1\\nq2=q1+1\"]\nA--&gt; B1\nA--&gt; B2\nB1--&gt;LP --&gt; C\nB2--&gt;LP \nC--&gt;D\nD--&gt; E\nC--&gt; E\nE--&gt; C\nE--&gt; After\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA[\"x1 = 3\"]\nB1[\"y1 = 4\"]\nB2[\"y2 = 5\"]\nLP[\"y3=phi(y1,y2)\"]\nC[\"x2=phi(x1,x3)\\nz1=x2*x3\\nq1=y3*y3\\nw1=y3+2\"]\nD[\"w2=w1+5\"]\nE[\"w3=phi(w1,w2)\\np1=w3+y3\\nx3=x2+1\\nq2=q1+1\"]\nA--&gt; B1\nA--&gt; B2\nB1--&gt;LP --&gt; C\nB2--&gt;LP \nC--&gt;D\nD--&gt; E\nC--&gt; E\nE--&gt; C\nE--&gt; After",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#while-loop-may-not-execute-we-can-restructure-into-a-do-while",
    "href": "lectures/05b_licm.html#while-loop-may-not-execute-we-can-restructure-into-a-do-while",
    "title": "classic loop optimizations",
    "section": "while loop may not execute, we can restructure into a do-while",
    "text": "while loop may not execute, we can restructure into a do-while\n\n\nwhile(e) {\n  s(j) \n}\n\nif (e) {\n  t  = j loopinv \n  do {\n    s\n  } while(e)\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#check-for-zero-trip-count",
    "href": "lectures/05b_licm.html#check-for-zero-trip-count",
    "title": "classic loop optimizations",
    "section": "check for zero trip count",
    "text": "check for zero trip count\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nA;\nB[\"if e\"]\nS\nendloop\nnext\nA--&gt; B\nB--&gt; next\nB--&gt; S\nS--&gt; endloop\nendloop --&gt; B\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nA;\nB[\"if e\"]\nS\nendloop\nnext\nA--&gt; B\nB--&gt; next\nB--&gt; S\nS--&gt; endloop\nendloop --&gt; B\n\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\npretest[\"if e\"]\npretest--&gt;next \ndo\ns\nendloop\nposttest[\"if e\"]\nnext\npretest--&gt;do\ndo --&gt; s\ns--&gt; endloop\nendloop--&gt; posttest\nposttest --&gt; do\nposttest --&gt; next\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\npretest[\"if e\"]\npretest--&gt;next \ndo\ns\nendloop\nposttest[\"if e\"]\nnext\npretest--&gt;do\ndo --&gt; s\ns--&gt; endloop\nendloop--&gt; posttest\nposttest --&gt; do\nposttest --&gt; next\n\n\n\n\n\n\n\n\n\nIn ssa a variable is loop invariant if it does not have a phi function at the header of the loop, or it is defined by a phi function and all the inputs are loop constants\nSSA If we find a loop-invariant computation in SSA form, then we just move it out of the loop to a block before the loop. When moving a (side effect-free) SSA loop-invariant computation to a previous position, nothing can go wrong, because the value it computes cannot be overwritten later and the values it depends on cannot have been changed",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#licm",
    "href": "lectures/05b_licm.html#licm",
    "title": "classic loop optimizations",
    "section": "licm",
    "text": "licm\nLoop invariant code motion recognizes computations in loop that produce the same value on each iteration and moves them out of the loop.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#a-very-common-case-for-this-is-matrix-addressing",
    "href": "lectures/05b_licm.html#a-very-common-case-for-this-is-matrix-addressing",
    "title": "classic loop optimizations",
    "section": "A very common case for this is matrix addressing",
    "text": "A very common case for this is matrix addressing\na[i,j] might expand to to \\(i*4*\\operatorname{stride_{a}} + j *4\\)\nfor j \n  a[i,j] = f(a[i,j+1])\nturns into\na = \nb = \nresult = 0\nfor (){\n    result += a*b\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#when-is-ok-to-move-a-computation",
    "href": "lectures/05b_licm.html#when-is-ok-to-move-a-computation",
    "title": "classic loop optimizations",
    "section": "when is ok to move a computation",
    "text": "when is ok to move a computation\n\nno side effects - cannot move alloc 10 outside of loop;\nin non ssa, computation d dominates all loop exits where d is live\nin non ssa only one def of d in the loop\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\npre[\"d = 0\"]\nL1[\"if i &gt; n\" ]\nloop1[\"i = i +1\"]\nd[\"d = a op b\"]\nuse[\"  =d \"]\nnext[\" = d\"]\n\npre--&gt; L1\nL1--&gt; loop1\nloop1--&gt; d\nd--&gt; use \nuse --&gt; L1\nL1 --&gt; next\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\npre[\"d = 0\"]\nL1[\"if i &gt; n\" ]\nloop1[\"i = i +1\"]\nd[\"d = a op b\"]\nuse[\"  =d \"]\nnext[\" = d\"]\n\npre--&gt; L1\nL1--&gt; loop1\nloop1--&gt; d\nd--&gt; use \nuse --&gt; L1\nL1 --&gt; next\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\npre[\"d = 0\"]\nL1[\"i = i +1\\n d= a op b\\n use d\"]\nL2[\"d = 2\\n use d\"]\nL3[\"if (i &lt; n)\"]\npre--&gt; L1\nL1--&gt; L2\nL2--&gt; L3\nL3--&gt; after\nL3 --&gt; L1\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\npre[\"d = 0\"]\nL1[\"i = i +1\\n d= a op b\\n use d\"]\nL2[\"d = 2\\n use d\"]\nL3[\"if (i &lt; n)\"]\npre--&gt; L1\nL1--&gt; L2\nL2--&gt; L3\nL3--&gt; after\nL3 --&gt; L1",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#find-loop-invariant-instructions",
    "href": "lectures/05b_licm.html#find-loop-invariant-instructions",
    "title": "classic loop optimizations",
    "section": "find loop invariant instructions,",
    "text": "find loop invariant instructions,",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#naturally-iterative",
    "href": "lectures/05b_licm.html#naturally-iterative",
    "title": "classic loop optimizations",
    "section": "naturally iterative",
    "text": "naturally iterative\niterate to convergence\n for each instr in  the loop\n   see if it is loop invar \n   if it is - move it",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#nested-loops",
    "href": "lectures/05b_licm.html#nested-loops",
    "title": "classic loop optimizations",
    "section": "nested loops",
    "text": "nested loops\nwe want to process inner loops first\n\nadd all the pre-headers\nwalk the dominator tree in reverse post order - saving all the loop headers",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#def-of-loop-invariant-for-an-instruction-d-op-ab",
    "href": "lectures/05b_licm.html#def-of-loop-invariant-for-an-instruction-d-op-ab",
    "title": "classic loop optimizations",
    "section": "def of loop invariant for an instruction d = op a,b",
    "text": "def of loop invariant for an instruction d = op a,b\n\na,b are constants or,\na,b defined outside the loop\na,b are loop invariants\n\nin SSA form if we find a loop invariant instruction we can always move it into the pre-header, because the value it writes is never rewritten, and the values that it depends on come from outside the loop",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#test-at-the-bottom---loop-always-executes-at-least-once",
    "href": "lectures/05b_licm.html#test-at-the-bottom---loop-always-executes-at-least-once",
    "title": "classic loop optimizations",
    "section": "test at the bottom - loop always executes at least once",
    "text": "test at the bottom - loop always executes at least once\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nl0[\"l0:\"]\npre[\"preheader\"]\nl1[\"l1: i = i +1\"]\nd1[\"d1 = a ⊕ b\"]\nd2[\"   = d1\"]\nl0--&gt;pre\npre --&gt; l1\nl1--&gt; d1\nd1--&gt; d2\nd3[\"(i &lt; N) goto L1\"]\nd2--&gt; d3\nd3--&gt; l1\nd3--&gt; Next\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nl0[\"l0:\"]\npre[\"preheader\"]\nl1[\"l1: i = i +1\"]\nd1[\"d1 = a ⊕ b\"]\nd2[\"   = d1\"]\nl0--&gt;pre\npre --&gt; l1\nl1--&gt; d1\nd1--&gt; d2\nd3[\"(i &lt; N) goto L1\"]\nd2--&gt; d3\nd3--&gt; l1\nd3--&gt; Next\n\n\n\n\n\n\ncan move d",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#test-at-the-top",
    "href": "lectures/05b_licm.html#test-at-the-top",
    "title": "classic loop optimizations",
    "section": "test at the top",
    "text": "test at the top\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nl0[\"l0:d = 0\"]\npre[\"preheader\"]\nl1[\"if (i&gt;=N) goto L2 \\nl1: i = i +1\"]\nl0--&gt;pre\npre --&gt; l1\nd1[\"d1 = a ⊕ b\"]\nd2[\"   = d1\"]\nl1--&gt; d1\nd1--&gt; d2\nd2--&gt; l1\nl1--&gt; l2\nl2--&gt; next\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nl0[\"l0:d = 0\"]\npre[\"preheader\"]\nl1[\"if (i&gt;=N) goto L2 \\nl1: i = i +1\"]\nl0--&gt;pre\npre --&gt; l1\nd1[\"d1 = a ⊕ b\"]\nd2[\"   = d1\"]\nl1--&gt; d1\nd1--&gt; d2\nd2--&gt; l1\nl1--&gt; l2\nl2--&gt; next\n\n\n\n\n\n\n\nwe can always convert into\n\nloop test at top \n\n\nif (test) {\n  preheader\n  loop test at bottom\n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#induction-variable-elimination",
    "href": "lectures/05b_licm.html#induction-variable-elimination",
    "title": "classic loop optimizations",
    "section": "induction variable elimination",
    "text": "induction variable elimination\nfor (int i = 0; i &lt; 100; ++1){\n    f(a[i])\n}\ncalculate a[i] as: &a[0] + 4 * i in every loop iteration, but the values at each step only differ by 4\nWe want to change the multiply and add to an add",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#transformation",
    "href": "lectures/05b_licm.html#transformation",
    "title": "classic loop optimizations",
    "section": "Transformation",
    "text": "Transformation\n\na_i = &a[0] before the loop\na_i = a_i + 4 (add the stride) in every iteration\nthe only remaining use of i is the test i &lt; 100, which could become a_i &lt; &a[0] + 4*100 (which is loop invariant)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#step-1",
    "href": "lectures/05b_licm.html#step-1",
    "title": "classic loop optimizations",
    "section": "step 1",
    "text": "step 1\n\nfind basic induction variables i = i + e, where e is loop invariant\n\nwhat does this look like in ssa\nfor each instruction d = c +- loop invariant see if there is a strongly connected graph in the ssa edges that only has adds and subtracts of loop invariant expressions",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#step-2-find-auxiliary-induction-variables",
    "href": "lectures/05b_licm.html#step-2-find-auxiliary-induction-variables",
    "title": "classic loop optimizations",
    "section": "Step 2 find auxiliary induction variables",
    "text": "Step 2 find auxiliary induction variables\nj = basic_ind * loop inv + loop invar\nfor (int i = 0; i &lt; n; i++) {\n     j = 2*i + 1;     // Y \n     k = -i;          // Y \n     l = 2*i*i + 1;   // N \n     c = c + 5;       // Y* \n}",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#step-3",
    "href": "lectures/05b_licm.html#step-3",
    "title": "classic loop optimizations",
    "section": "step 3",
    "text": "step 3\nreplace auxiliary induction variables (derived ) by new variables without the multiply",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#step4",
    "href": "lectures/05b_licm.html#step4",
    "title": "classic loop optimizations",
    "section": "step4",
    "text": "step4\nif the only remaining use of the induction variable is the termination test, change the test to use the new variable\nsum = 0\nfor (i = 1, i &lt; 100; i++) {\n  sum = sum + a[i -1]\n}\nin SSA form:\n   sum0 = 0\n   i0 = 1\nL: sum1 = phi(sum0, sum2)\n   i1 = phi(i0, i2)\n   t10 = i1 -1 \n   t20 = t10 * 4\n   t30 = t20 + &a\n   t40 = load t30\n   sum2 = sum1 + t40\n   i2 = i1 + 1\n   if (i2 &lt;= 100)go to l\n\ni is a basic induction variable\nt10 is a aux induction variable\nt20 is an aux induction variable\nt30 is an aux induction variable\n\nt3 has a use in the load\nt3 = t20 + &a ==&gt; t10 * 4 + &a ==&gt; (i1-1)* 4+ &a\nt3 = 4* i1 + &a - 4\n   sum0 = 0\n   i0 = 1\n   t50 = &a -4  // initial value \nL: sum1 = phi(sum0, sum2)\n   i1 = phi(i0, i2)\n   t51 = phi(t50, t52)\n   //t10 = i1 -1 \n   //t20 = t10 * 4\n   //t30 = t20 + &a\n   t40 = load t50\n   sum2 = sum1 + t40\n   i2 = i1 + 1\n   t52 = t50 + 4\n   if (i2 &lt;= 100)go to l\n   sum0 = 0\n   i0 = 1\n   t50 = &a -4  // initial value \nL: sum1 = phi(sum0, sum2)\n   // i1 = phi(i0, i2)\n   t51 = phi(t50, t52)\n   //t10 = i1 -1 \n   //t20 = t10 * 4\n   //t30 = t20 + &a\n   t40 = load t50\n   sum2 = sum1 + t40\n   //i2 = i1 + 1\n   t52 = t50 + 4\n   if (t52 &lt;= 396 + &a )go to l",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/05b_licm.html#loop-un-switching",
    "href": "lectures/05b_licm.html#loop-un-switching",
    "title": "classic loop optimizations",
    "section": "loop un-switching",
    "text": "loop un-switching\nfor (int i = 0 ; i &lt; 100; ++1){\n    if (c) {  // c is loop invariant \n        f(i)\n    } else {\n        g(i)\n    }\n}\nlook for special patterns and replace\nif (c) {  // c is loop invariant \n   for (int i = 0 ; i &lt; 100; ++1){\n        f(i)\n    } \n}else {\n    for (int i = 0 ; i &lt; 100; ++1){\n        g(i)\n    }\n}\nThis is often done before vectorization\nloop fusion\nfor (i = 0; i &lt; 100 ; ++){\n s0:   b[i] = f(a[i])\n}\nfor (i = 0; i &lt; 100 ; ++){\n s1:   c[i] = f(b[i])\n}\n\nwhen is it legal to do this?\nWhen can we get rid of the b array?\n\nThere is also an optimization that goes the other way split a loop so that each statement becomes a separate loop incase we could run as vectors",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "classic loop optimizations"
    ]
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#static-single-assignment-ssa",
    "href": "lectures/revealjs_06_ssa.qmd.html#static-single-assignment-ssa",
    "title": "Static Single Assignment",
    "section": "Static single assignment (SSA)",
    "text": "Static single assignment (SSA)\nA variable in a program can have multiple definitions. In Bril definitions are instructions which compute values. Up till now we have been thinking about analysis which look at variables (names) but a different way to look at this is based on values, If we think of instructions calculating values, and uses being uses of values we can picture a graph called the data flow graph showing how values move through a program"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#ssa",
    "href": "lectures/revealjs_06_ssa.qmd.html#ssa",
    "title": "Static Single Assignment",
    "section": "ssa",
    "text": "ssa\nin SSA we change our IR so that every variable has exactly one definition in the program (each variable is assigned only once). The name SSA means statically there is only a single assignment per variable."
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#the-ssa-philosophy",
    "href": "lectures/revealjs_06_ssa.qmd.html#the-ssa-philosophy",
    "title": "Static Single Assignment",
    "section": "The SSA Philosophy",
    "text": "The SSA Philosophy\nIn addition to a language form, SSA is also a philosophy! It can fundamentally change the way you think about programs. In the SSA philosophy:\n\ndefinitions == variables\ninstructions == values\narguments == data flow graph edges"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#an-example",
    "href": "lectures/revealjs_06_ssa.qmd.html#an-example",
    "title": "Static Single Assignment",
    "section": "an example",
    "text": "an example\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nB0[\"0: i = 0\n    1: s = 0\"]\nB1[\"2: x = m\n    3: s = s + x\n    4: i = i +4\n    5: if i &lt; n go to B0\"]\nB0 --&gt; B1\nB1 --&gt; B1\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nB0[\"0: i = 0\n    1: s = 0\"]\nB1[\"2: x = m\n    3: s = s + x\n    4: i = i +4\n    5: if i &lt; n go to B0\"]\nB0 --&gt; B1\nB1 --&gt; B1\n\n\n\n\n\n\nvariable i has two static assignments 0 and 4, so this program is not in SSA\nVariable s has two static assignments, x has one static assignment but x has lots of dynamic assignments (when the program executes)"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#straight-line-code",
    "href": "lectures/revealjs_06_ssa.qmd.html#straight-line-code",
    "title": "Static Single Assignment",
    "section": "straight line code",
    "text": "straight line code\nWe call a program without branches a piece of straight line code.\n    @main {\n      a: int = const 4;\n      b: int = const 2;\n      a: int = add a b;\n      b: int = add a b;\n      print b;\n    }\n\nIts easy to see how to convert straight line code into ssa\n    @main {\n      a.1: int = const 4;\n      b.1: int = const 2;\n      a.2: int = add a.1 b.1;\n      b.2: int = add a.2 b.1;\n      print b.2;\n    }"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#pseudo-code-for-one-basic-block",
    "href": "lectures/revealjs_06_ssa.qmd.html#pseudo-code-for-one-basic-block",
    "title": "Static Single Assignment",
    "section": "pseudo code for one basic block",
    "text": "pseudo code for one basic block\nfor each variable a: \n    Count[a] = 0 \n    Stack[a] = [0]\n\nrename_basic_block(B): \n    for each instruction S in block B:\n        for each use of a argument x in S: \n            i = top(Stack[x]) \n            replace the use of x with x_i\n            \n        for each variable a that S defines (a dest)\n            count[a] = Count[a] + 1 \n            i = Count[a]             \n            push i onto Stack[a]             \n            replace definition of a with a_i\nWe don’t need the stack here but we will need it later."
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#phi-nodes",
    "href": "lectures/revealjs_06_ssa.qmd.html#phi-nodes",
    "title": "Static Single Assignment",
    "section": "phi-Nodes",
    "text": "phi-Nodes\nJust renaming assignments will quickly run into problems. Consider this program:\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nB0[\".b0\n    a: int = const 47;\n    br cond .left .right;\"]\nleft[\"a: int = add a a;\n    jmp .exit;\"]\nright[\"a: int = mul a a;\n        jmp .exit;\"]\nexit[\"print a;\"]\nB0 --&gt; left\nB0 --&gt; right\nleft --&gt; exit\nright --&gt; exit\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nB0[\".b0\n    a: int = const 47;\n    br cond .left .right;\"]\nleft[\"a: int = add a a;\n    jmp .exit;\"]\nright[\"a: int = mul a a;\n        jmp .exit;\"]\nexit[\"print a;\"]\nB0 --&gt; left\nB0 --&gt; right\nleft --&gt; exit\nright --&gt; exit\n\n\n\n\n\n\nWhich “version” of a should we use in the print statement?"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#phi-nodes-1",
    "href": "lectures/revealjs_06_ssa.qmd.html#phi-nodes-1",
    "title": "Static Single Assignment",
    "section": "phi nodes",
    "text": "phi nodes\nTo match the expressiveness of unrestricted programs, SSA adds a new kind of instruction: a phi-node.\nphi-nodes are flow-sensitive copy instructions: they get a value from one of several variables, depending on which incoming CFG edge was most recently taken to get to them."
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#phi-nodes-in-bril",
    "href": "lectures/revealjs_06_ssa.qmd.html#phi-nodes-in-bril",
    "title": "Static Single Assignment",
    "section": "phi nodes in Bril",
    "text": "phi nodes in Bril\nIn Bril, a phi-node appears as a phi instruction:\na.4: int = phi .left a.2 .right a.3;\nThe phi instruction chooses between any number of variables, and it picks between them based on labels. If the program most recently executed a basic block with the given label, then the phi instruction takes its value from the corresponding variable."
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#back-to-the-example",
    "href": "lectures/revealjs_06_ssa.qmd.html#back-to-the-example",
    "title": "Static Single Assignment",
    "section": "back to the example",
    "text": "back to the example\nYou can write the above program in SSA like this:\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nB0[\".b0\n    a: int = const 47;\n    br cond .left .right;\"]\nleft[\"a: int = add a a;\n    jmp .exit;\"]\nright[\"a: int = mul a a;\n        jmp .exit;\"]\nexit[\"print a;\"]\nB0 --&gt; left\nB0 --&gt; right\nleft --&gt; exit\nright --&gt; exit\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nB0[\".b0\n    a: int = const 47;\n    br cond .left .right;\"]\nleft[\"a: int = add a a;\n    jmp .exit;\"]\nright[\"a: int = mul a a;\n        jmp .exit;\"]\nexit[\"print a;\"]\nB0 --&gt; left\nB0 --&gt; right\nleft --&gt; exit\nright --&gt; exit\n\n\n\n\n\n\n\n    @main(cond: bool) {\n    .entry:\n        a.1: int = const 47;\n        br cond .left .right;\n    .left:\n        a.2: int = add a.1 a.1;\n        jmp .exit;\n    .right:\n        a.3: int = mul a.1 a.1;\n        jmp .exit;\n    .exit:\n        a.4: int = phi .left a.2 .right a.3;\n        print a.4;\n    }"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#bril-in-ssa",
    "href": "lectures/revealjs_06_ssa.qmd.html#bril-in-ssa",
    "title": "Static Single Assignment",
    "section": "Bril in SSA",
    "text": "Bril in SSA\nBril has an SSA extension It adds support for a phi instruction. Beyond that, SSA form is just a restriction on the normal expressiveness of Bril—if you solemnly promise never to assign statically to the same variable twice, you are writing “SSA Bril.”\nThe reference interpreter has built-in support for phi, so you can execute your SSA-form Bril programs without fuss."
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#converting-to-ssa---very-simple-scheme",
    "href": "lectures/revealjs_06_ssa.qmd.html#converting-to-ssa---very-simple-scheme",
    "title": "Static Single Assignment",
    "section": "Converting to SSA - Very simple scheme",
    "text": "Converting to SSA - Very simple scheme\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph LR\nX[\"Block X\n   a = \n   b = \n   if s &gt; b\"]\nY[\"Block Y\n  b = a\"]\nZ[\"Block Z\nret b\"]\nX --&gt; Y\nY--&gt; Z\nX --&gt; Z\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph LR\nX[\"Block X\n   a = \n   b = \n   if s &gt; b\"]\nY[\"Block Y\n  b = a\"]\nZ[\"Block Z\nret b\"]\nX --&gt; Y\nY--&gt; Z\nX --&gt; Z\n\n\n\n\n\n\nWhere do we need phi-functions?\nWhich variables\n\nphi At the merge (join) node\nvariable b"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#conditions",
    "href": "lectures/revealjs_06_ssa.qmd.html#conditions",
    "title": "Static Single Assignment",
    "section": "conditions",
    "text": "conditions\nconditions: phi-function for variable b at node z\n\nThere is a block x containing a definition of b\nThere is a block y (with y ≠ x) containing a definition of b\nThere is a nonempty path Pxz of edges from x to z\nThere is a nonempty path Pyz of edges from y to z\nPaths Pxz and Pyz do not have any node in common other than z, and…\nThe node z does not appear within both Pxz and Pyz prior to the end, though it may appear in one or the other."
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#scheme-part-2",
    "href": "lectures/revealjs_06_ssa.qmd.html#scheme-part-2",
    "title": "Static Single Assignment",
    "section": "scheme part 2",
    "text": "scheme part 2\nthis is iterative since when we add a phi, we are creating a new defintion, which may add new phi-functions\nWhen we find nodes X,Y,Z that match these steps and z does not contain a phi function for b, insert a phi\nWhile really expensive this will work"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#diagram",
    "href": "lectures/revealjs_06_ssa.qmd.html#diagram",
    "title": "Static Single Assignment",
    "section": "diagram",
    "text": "diagram\nusing dash for path\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\na[\"x:x=\"]\nb[\"y:x=\"]\nc[join]\nd\na-.-&gt; c\nb-.-&gt; c\nc-.-&gt;d\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\na[\"x:x=\"]\nb[\"y:x=\"]\nc[join]\nd\na-.-&gt; c\nb-.-&gt; c\nc-.-&gt;d \n\n\n\n\n\n\nWe could have complex flow - including loops on the paths"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#reminder-dominators",
    "href": "lectures/revealjs_06_ssa.qmd.html#reminder-dominators",
    "title": "Static Single Assignment",
    "section": "reminder dominators",
    "text": "reminder dominators\nwhile dom is changing \nfor vertex in cfg \n   dom[vertex] ="
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#fast-methods-for-inserting-phis",
    "href": "lectures/revealjs_06_ssa.qmd.html#fast-methods-for-inserting-phis",
    "title": "Static Single Assignment",
    "section": "fast methods for inserting phi’s",
    "text": "fast methods for inserting phi’s\nThe method for this has two steps\n\ninsert phi instructions where needed (do not add subscripts yet)\nin a second pass insert all the numbers"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#to-ssa",
    "href": "lectures/revealjs_06_ssa.qmd.html#to-ssa",
    "title": "Static Single Assignment",
    "section": "to ssa",
    "text": "to ssa\nTo convert to SSA, we want to insert phi-nodes whenever there are distinct paths containing distinct definitions of a variable. We don’t need phi-nodes in places that are dominated by a definition of the variable. So what’s a way to know when control reachable from a definition is not dominated by that definition?\nThe dominance frontier!"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#recall-the-dominance-frontier",
    "href": "lectures/revealjs_06_ssa.qmd.html#recall-the-dominance-frontier",
    "title": "Static Single Assignment",
    "section": "recall the dominance frontier",
    "text": "recall the dominance frontier\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA--&gt;B \nA--&gt; F\nB--&gt;C\nB--&gt; D\nC--&gt; E\nD--&gt; E\nE--&gt; F\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA--&gt;B \nA--&gt; F\nB--&gt;C\nB--&gt; D\nC--&gt; E\nD--&gt; E\nE--&gt; F\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nA--&gt; B\nA--&gt; F\nB--&gt; C\nB--&gt;D\nB--&gt; E\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nA--&gt; B\nA--&gt; F\nB--&gt; C\nB--&gt;D\nB--&gt; E\n\n\n\n\n\n\n\n\n\n\nblock\nA\nB\nC\nD\nE\n\n\nfrontier\nempty\nF\nE\nE\nF"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#a-block-can-be-in-its-own-dom-frontier",
    "href": "lectures/revealjs_06_ssa.qmd.html#a-block-can-be-in-its-own-dom-frontier",
    "title": "Static Single Assignment",
    "section": "a block can be in its own dom frontier",
    "text": "a block can be in its own dom frontier\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA--&gt; B\nB--&gt; A\nB--&gt; C\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA--&gt; B\nB--&gt; A\nB--&gt; C\n\n\n\n\n\n\nwhy: A dom B, but B does not dom A. so A is in the dom frontier of A"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#an-almost-linear-method",
    "href": "lectures/revealjs_06_ssa.qmd.html#an-almost-linear-method",
    "title": "Static Single Assignment",
    "section": "an almost linear method",
    "text": "an almost linear method\nWe do it in two steps.\n\ninsert phi-nodes:\nrename variables:"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#placing-phi-functions",
    "href": "lectures/revealjs_06_ssa.qmd.html#placing-phi-functions",
    "title": "Static Single Assignment",
    "section": "placing Phi functions",
    "text": "placing Phi functions\nlet b be a block with a def of a variable v, if b has multiple defs of v, use the last one\nWhat is the first block following v that can be reached by a different def of v\nin blocks dominated by b, b’s def must have been executed, (other defs of v in a dominated block may overwrite it)\nwe need to place a phi function for b at the start of all blocks in the dom frontier of b.\nafter we add phi functions to S where S = df(b) we have more defs, so we need to add phi’s in the dom frontier of all the blocks in S"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#example-1",
    "href": "lectures/revealjs_06_ssa.qmd.html#example-1",
    "title": "Static Single Assignment",
    "section": "example 1",
    "text": "example 1\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nN[\"v=1\"]\nM\nM2[\"v=2\"]\nZ[\"M\"]\nN--&gt; M\nM--&gt;Z\nM2--&gt; Z\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nN[\"v=1\"]\nM\nM2[\"v=2\"]\nZ[\"M\"]\nN--&gt; M\nM--&gt;Z\nM2--&gt; Z\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nN[\"v1=1\"]\nM\nM2[\"v2=2\"]\nZ[\"M:v3= phi(v1,v2)\"]\nN--&gt; M\nM--&gt;Z\nM2--&gt; Z\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nN[\"v1=1\"]\nM\nM2[\"v2=2\"]\nZ[\"M:v3= phi(v1,v2)\"]\nN--&gt; M\nM--&gt;Z\nM2--&gt; Z"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#a-loop",
    "href": "lectures/revealjs_06_ssa.qmd.html#a-loop",
    "title": "Static Single Assignment",
    "section": "a loop",
    "text": "a loop\n::: {.columns}\n::: {.column}\n\n\nCode\ngraph TD\n V[\"v=init\"]\n Z{\"z:v = v+1\"}\n V --&gt; Z\n Z --&gt;Z\n\n\n\n\n\n graph TD\n V[\"v=init\"]\n Z{\"z:v = v+1\"}\n V --&gt; Z\n Z --&gt;Z\n\n\n\n\n\n\n:::\n::: {.column}\n\n\nCode\ngraph TD\n V[\"v1=init\"]\n Z{\"z:v2 = phi(v1,v3)\\nv3 = v2+1\"}\n V --&gt; Z\n Z --&gt;Z\n\n\n\n\n\n graph TD\n V[\"v1=init\"]\n Z{\"z:v2 = phi(v1,v3)\\nv3 = v2+1\"}\n V --&gt; Z\n Z --&gt;Z\n\n\n\n\n\n\n:::\n:::"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#iterative-placement",
    "href": "lectures/revealjs_06_ssa.qmd.html#iterative-placement",
    "title": "Static Single Assignment",
    "section": "iterative placement",
    "text": "iterative placement\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nB1[v=1]\nB2[v=2]\nB3\nB4[v=3]\nB5\nB1--&gt; B3\nB2--&gt; B3\nB3--&gt; B5\nB4--&gt; B5\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nB1[v=1]\nB2[v=2]\nB3\nB4[v=3]\nB5\nB1--&gt; B3\nB2--&gt; B3\nB3--&gt; B5\nB4--&gt; B5\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nB1[v1=1]\nB2[v2=2]\nB3[\"v3=phi(v1,v2)\"]\nB4[v4=3]\nB5[\"v5=phi(v3,v4)\"]\nB1--&gt; B3\nB2--&gt; B3\nB3--&gt; B5\nB4--&gt; B5\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nB1[v1=1]\nB2[v2=2]\nB3[\"v3=phi(v1,v2)\"]\nB4[v4=3]\nB5[\"v5=phi(v3,v4)\"]\nB1--&gt; B3\nB2--&gt; B3\nB3--&gt; B5\nB4--&gt; B5"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#phi-placement",
    "href": "lectures/revealjs_06_ssa.qmd.html#phi-placement",
    "title": "Static Single Assignment",
    "section": "phi placement",
    "text": "phi placement\nfor each block b in the cfg \n  for each var v defined in b\n    add block to the set defs(v)  ## blocks that contain an assignment to v \n\n  W = Defs[v]\n    while W is not empty\n      remove a node n from w\n         for block in DF[n]:  # Dominance frontier.\n           Add a phi-node to block,\n             unless we have done so already.\n           Add block to W (because it now writes to v),\n             unless it's already in there."
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#an-example-1",
    "href": "lectures/revealjs_06_ssa.qmd.html#an-example-1",
    "title": "Static Single Assignment",
    "section": "an example",
    "text": "an example\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nN1[\"1:x = 1\"]\nN2[\"2:\"]\nN3[\"3:x= 2\"]\nN4[\"4:\"]\nN5[\"5:x=3\"]\nN6[6:x=4]\nN7[7:]\nN1--&gt; N2\nN1--&gt; N3\nN2--&gt; N4\nN3--&gt; N4\nN4--&gt; N5\nN5--&gt;  N4\nN5--&gt; N6\nN6--&gt; N7\nN6--&gt; N5\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nN1[\"1:x = 1\"]\nN2[\"2:\"]\nN3[\"3:x= 2\"]\nN4[\"4:\"]\nN5[\"5:x=3\"]\nN6[6:x=4]\nN7[7:]\nN1--&gt; N2\nN1--&gt; N3\nN2--&gt; N4\nN3--&gt; N4\nN4--&gt; N5\nN5--&gt;  N4\nN5--&gt; N6\nN6--&gt; N7\nN6--&gt; N5\n\n\n\n\n\n\n\n\ninitially w = {1,3,5,6}\nprocess DF(1) = empty\nprocess DF(3) = 4, add 4 to w and add a phi function for x to 4\nprocess DF(5) = 4,5 no need to add 5 to w, add phi for x to 5\nprocess DF(6) = 5\nprocess DF(4) = 4\n\n\nadd phi’s to blocks 4 and 5"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#rename-variables",
    "href": "lectures/revealjs_06_ssa.qmd.html#rename-variables",
    "title": "Static Single Assignment",
    "section": "rename variables:",
    "text": "rename variables:\n# allocate a stack and a counter for each variable\nfor each V a variable \n  c[v] = 0\n  s[v] = empty stack\n  search(entry)\n\nsearch(n):\n  for each instr i in n:\n     if instr is not a phi\n         replace every variable in the rhs of instr by vi where i = top(s[v])\n         if instr has a dest v\n           i = C(v)\n            replace v by new vi, push i onto s[v]\n            increment c[v]\n\n  for each y a successor of n\n     j = which pred (y,n)\n     for each phi function pinstr in Y replace the jth opernmad of pinstr by vi where \n        i = top(s(v)\n  \n  for each Y a child of n in the dominator tree \n      call search(Y)"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#an-example-2",
    "href": "lectures/revealjs_06_ssa.qmd.html#an-example-2",
    "title": "Static Single Assignment",
    "section": "an example",
    "text": "an example\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\n\nL5--&gt; L9\nL7 --&gt; L9\nL9 --&gt; L3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\n\nL5--&gt; L9\nL7 --&gt; L9\nL9 --&gt; L3"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#what-is-the-dominator-tree",
    "href": "lectures/revealjs_06_ssa.qmd.html#what-is-the-dominator-tree",
    "title": "Static Single Assignment",
    "section": "What is the dominator tree?",
    "text": "What is the dominator tree?\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 -.-&gt; L9\nL9 --&gt; L3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 -.-&gt; L9\nL9 --&gt; L3\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB \nL0 --&gt; L3 \nL3--&gt; L4\nL3 --&gt; L10\nL4--&gt; L5 \nL4 --&gt; L9\nL4 --&gt; L7\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB \nL0 --&gt; L3 \nL3--&gt; L4\nL3 --&gt; L10\nL4--&gt; L5 \nL4 --&gt; L9\nL4 --&gt; L7"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#dominance-frontiers",
    "href": "lectures/revealjs_06_ssa.qmd.html#dominance-frontiers",
    "title": "Static Single Assignment",
    "section": "dominance frontiers",
    "text": "dominance frontiers\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3 \n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 -.-&gt; L9\nL9 --&gt; L3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3 \n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 -.-&gt; L9\nL9 --&gt; L3\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB \nL0 --&gt; L3 \nL3--&gt; L4\nL3 --&gt; L10\nL4--&gt; L5 \nL4 --&gt; L9\nL4 --&gt; L7\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB \nL0 --&gt; L3 \nL3--&gt; L4\nL3 --&gt; L10\nL4--&gt; L5 \nL4 --&gt; L9\nL4 --&gt; L7"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#add-phi-nodes",
    "href": "lectures/revealjs_06_ssa.qmd.html#add-phi-nodes",
    "title": "Static Single Assignment",
    "section": "add phi nodes",
    "text": "add phi nodes\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 --&gt; L9\nL9 --&gt; L3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 --&gt; L9\nL9 --&gt; L3\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"j = phi(j,j) \n    k = phi(k,k) \n   L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[\"j = phi(j,j)\n   k = phi(k,k)\n  L9: goto l3\"]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 --&gt; L9\nL9 --&gt; L3\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"j = phi(j,j) \n    k = phi(k,k) \n   L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[\"j = phi(j,j)\n   k = phi(k,k)\n  L9: goto l3\"]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 --&gt; L9\nL9 --&gt; L3"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#the-arity-of-phi-functions",
    "href": "lectures/revealjs_06_ssa.qmd.html#the-arity-of-phi-functions",
    "title": "Static Single Assignment",
    "section": "The arity of phi-functions",
    "text": "The arity of phi-functions\nCould we have a phi-function in a node with only one predecessor?\ncould we have a phi-function wit more then two arguments?\nThis algorithm computes what is called minimal SSA form which is not so mimimal since it can leave dead assignments\ndoing dead code elimination pruned ssa form"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#getting-out-of-ssa",
    "href": "lectures/revealjs_06_ssa.qmd.html#getting-out-of-ssa",
    "title": "Static Single Assignment",
    "section": "Getting out of ssa",
    "text": "Getting out of ssa\nCompilers that use the SSA form usually contain a step, before the generation of actual assembly code, in which phi functions are replaced by ordinary instructions. Normally these instructions are simple copies."
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#an-example-3",
    "href": "lectures/revealjs_06_ssa.qmd.html#an-example-3",
    "title": "Static Single Assignment",
    "section": "an example",
    "text": "an example\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n A0[\"io =\n     j0 = \n     k0 =\"]\nA1[\"i1 =\n   j1 =\n   k1 = \"]\nA2[\"i2 = phi(i0, i1)\n   j2 = phi(j0, j1)\n   k2 = phi(k0, k1)\n   ...\n    = i2\n    = j2 \n    = k2\"]\n\n    A0 --&gt; A2\n    A1--&gt; A2\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n A0[\"io =\n     j0 = \n     k0 =\"]\nA1[\"i1 =\n   j1 =\n   k1 = \"]\nA2[\"i2 = phi(i0, i1)\n   j2 = phi(j0, j1)\n   k2 = phi(k0, k1)\n   ...\n    = i2\n    = j2 \n    = k2\"]\n\n    A0 --&gt; A2\n    A1--&gt; A2\n\n\n\n\n\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n B0[\"io =\n     j0 = \n     k0 =\"]\nB1[\"i1 =\n   j1 =\n   k1 = \"]\nB2[\"\n   ...\n    = i2\n    = j2 \n    = k2\"]\n    B0 --\"i2 = i0\n       j2 = j0\n       k2 = k0\"--&gt; B2\n    B1 --\"i2 = i1\n          j2 = j1\n          k2 = k1\"--&gt; B2\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n B0[\"io =\n     j0 = \n     k0 =\"]\nB1[\"i1 =\n   j1 =\n   k1 = \"]\nB2[\"\n   ...\n    = i2\n    = j2 \n    = k2\"]\n    B0 --\"i2 = i0\n       j2 = j0\n       k2 = k0\"--&gt; B2\n    B1 --\"i2 = i1\n          j2 = j1\n          k2 = k1\"--&gt; B2\n\n\n\n\n\n\n\nwe cannot put instructions on edges, but we can add to prev block"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#critical-edges",
    "href": "lectures/revealjs_06_ssa.qmd.html#critical-edges",
    "title": "Static Single Assignment",
    "section": "critical edges",
    "text": "critical edges\n\n\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nA0[\"L1:\n   a0 =\n   b0 =\n   if A0 &gt; b0\"]\nA1[\"b1 = a0\"]\nA2[\"l2:\nb2 = phi(b1,b0)\"]\nA0 --&gt; A1\nA1 --&gt; A2\nA0 --&gt; A2\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nA0[\"L1:\n   a0 =\n   b0 =\n   if A0 &gt; b0\"]\nA1[\"b1 = a0\"]\nA2[\"l2:\nb2 = phi(b1,b0)\"]\nA0 --&gt; A1\nA1 --&gt; A2\nA0 --&gt; A2\n\n\n\n\n\n\n\nb2 = b0?\n\nThe placement of the copy b2 = b0 is not simple, because the edge that links L2 to L5 is critical. A critical edge connects a block with multiple successors to a block with multiple predecessors. This should remind you of adding a preheader to a loop"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#critical-edge-splitting",
    "href": "lectures/revealjs_06_ssa.qmd.html#critical-edge-splitting",
    "title": "Static Single Assignment",
    "section": "critical edge splitting",
    "text": "critical edge splitting\nWe can solve this problem by doing critical edge splitting. This CFG transformation consists in adding an empty basic block (empty, except by – perhaps – a goto statement) between each pair of blocks connected by a critical edge."
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#making-use-of-ssa-form",
    "href": "lectures/revealjs_06_ssa.qmd.html#making-use-of-ssa-form",
    "title": "Static Single Assignment",
    "section": "making use of ssa form",
    "text": "making use of ssa form\nOur previous analyses always used a (variable, program point), but in ssa these are the same"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#dead-code-elimination-in-ssa",
    "href": "lectures/revealjs_06_ssa.qmd.html#dead-code-elimination-in-ssa",
    "title": "Static Single Assignment",
    "section": "dead code elimination in ssa",
    "text": "dead code elimination in ssa\nwhile there is some variable v with no uses and the statement that defines v has no other side effects, delete the statement that defines v from the program.\nwe need a counter for each variable (or each instruction)\nwalk the program once increment the counter each time the variable is used\nwhile there exists v, such that counter[v] = 0 remove the instruction that defined v, e.g., “v = E for each variable x used in E decrement counter[x]"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#sparse-constant-prop",
    "href": "lectures/revealjs_06_ssa.qmd.html#sparse-constant-prop",
    "title": "Static Single Assignment",
    "section": "sparse constant prop",
    "text": "sparse constant prop\nwe define a partial order on constats, any &gt; all constants &gt; undefined and define the intersection of two states as the common parent\nwith each variable we have an abstract state (like a value number)\nv  = const c   ==&gt; v state is const \n\nv = id q      ==&gt; v state is the state of  q \n\nv = v0 op v1  ==&gt; if both are constants v = c0 op c1\n\n             ==&gt; if one is any, v's state is any\n\nv = phi(v0,..vn) ==&gt; v's state is the intersection of the states of v0,..,vn"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#what-order-do-we-process-nodes",
    "href": "lectures/revealjs_06_ssa.qmd.html#what-order-do-we-process-nodes",
    "title": "Static Single Assignment",
    "section": "What order do we process nodes?",
    "text": "What order do we process nodes?\nbecause the program is in ssa form we can do the nodes in dominator tree order, then before processing any instruction that is not a phi, we will have processed all the arguments"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#walking-the-dominator-tree-b0---b1",
    "href": "lectures/revealjs_06_ssa.qmd.html#walking-the-dominator-tree-b0---b1",
    "title": "Static Single Assignment",
    "section": "walking the dominator tree b0 -> b1",
    "text": "walking the dominator tree b0 -&gt; b1\n\n\nB0: x0  = input \n    a0 = 1 \n    c0 = a0 +10\n    if a0 &lt; c0 go to b1\n\nB1: a1 phi(a0,a2 )\n    b0 = x0 * a1\n    print b0 \n    a2 = a1 +1 \n    go to b1\n\nB0:\nx0 - any \na0 - 1 \nc0 - 11 (folding the constant)\na0 &lt; c0  skip\nB1:\na1 -  1 (only one input defined)\nb0  - any\na2 -  2\nupdate the uses of a2 - the phi\na1 -  any \n\nupdate the uses of a1 \nno change"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#converting-from-ssa",
    "href": "lectures/revealjs_06_ssa.qmd.html#converting-from-ssa",
    "title": "Static Single Assignment",
    "section": "Converting from SSA",
    "text": "Converting from SSA\nEventually, we need to convert out of SSA form to generate efficient code for real machines that don’t have phi-nodes and do have finite space for variable storage."
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#basic-algorithm",
    "href": "lectures/revealjs_06_ssa.qmd.html#basic-algorithm",
    "title": "Static Single Assignment",
    "section": "basic algorithm",
    "text": "basic algorithm\nThe basic algorithm is pretty straightforward. If you have a phi-node:\nv = phi .l1 x .l2 y;\nThen there must be assignments to x and y (recursively) preceding this statement in the CFG.\nThe paths from x to the phi-containing block and from y to the same block must “converge” at that block. So insert code into the phi-containing block’s immediate predecessors along each of those two paths: one that does v = id x and one that does v = id y. Then you can delete the phi instruction."
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#extra-copies",
    "href": "lectures/revealjs_06_ssa.qmd.html#extra-copies",
    "title": "Static Single Assignment",
    "section": "extra copies",
    "text": "extra copies\nThis basic approach can introduce some redundant copying. (Take a look at the code it generates after you implement it!) Non-SSA copy propagation optimization can work well as a post-processing step. For a more extensive take on how to translate out of SSA efficiently, see “Revisiting Out-of-SSA Translation for Correctness, Code Quality, and Efficiency” by Boissinot et al."
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#overlap",
    "href": "lectures/revealjs_06_ssa.qmd.html#overlap",
    "title": "Static Single Assignment",
    "section": "overlap",
    "text": "overlap\nits possible that an optimization can give overlapping phi-functions\nb0 \n  x1 = 1\n  y1 = 2\nB1 \nx2 = phi(x1,x3)\ny2 = phi(y1, y3)\n  z = x2\n  x3 = y2\n  y3= z\n  if() go to b1"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#optimize-it",
    "href": "lectures/revealjs_06_ssa.qmd.html#optimize-it",
    "title": "Static Single Assignment",
    "section": "optimize it",
    "text": "optimize it\nb0 \n  x1 = 1\n  y1 = 2\nB1 \nx2 = phi(x1, y2)\ny2 = phi(y1, x2)\n  if() go to b1"
  },
  {
    "objectID": "lectures/revealjs_06_ssa.qmd.html#lost-the-temp-this-is-called-the-swap-problem",
    "href": "lectures/revealjs_06_ssa.qmd.html#lost-the-temp-this-is-called-the-swap-problem",
    "title": "Static Single Assignment",
    "section": "lost the temp (this is called the swap problem)",
    "text": "lost the temp (this is called the swap problem)\nif we add copies x2 = y3 y2 = x2 (uses the wrong value of x2)\nphi nodes execute all at once - not one at a time\nSome SSA slides from Todd Mowry at CMU"
  },
  {
    "objectID": "lectures/06_ssa.html",
    "href": "lectures/06_ssa.html",
    "title": "Static Single Assignment",
    "section": "",
    "text": "A variable in a program can have multiple definitions. In Bril definitions are instructions which compute values. Up till now we have been thinking about analysis which look at variables (names) but a different way to look at this is based on values, If we think of instructions calculating values, and uses being uses of values we can picture a graph called the data flow graph showing how values move through a program",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#static-single-assignment-ssa",
    "href": "lectures/06_ssa.html#static-single-assignment-ssa",
    "title": "Static Single Assignment",
    "section": "",
    "text": "A variable in a program can have multiple definitions. In Bril definitions are instructions which compute values. Up till now we have been thinking about analysis which look at variables (names) but a different way to look at this is based on values, If we think of instructions calculating values, and uses being uses of values we can picture a graph called the data flow graph showing how values move through a program",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#ssa",
    "href": "lectures/06_ssa.html#ssa",
    "title": "Static Single Assignment",
    "section": "ssa",
    "text": "ssa\nin SSA we change our IR so that every variable has exactly one definition in the program (each variable is assigned only once). The name SSA means statically there is only a single assignment per variable.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#the-ssa-philosophy",
    "href": "lectures/06_ssa.html#the-ssa-philosophy",
    "title": "Static Single Assignment",
    "section": "The SSA Philosophy",
    "text": "The SSA Philosophy\nIn addition to a language form, SSA is also a philosophy! It can fundamentally change the way you think about programs. In the SSA philosophy:\n\ndefinitions == variables\ninstructions == values\narguments == data flow graph edges\n\n\nIn LLVM, for example, instructions do not refer to argument variables by name—an argument is a pointer to defining instruction.\nStatic means in the text, not in the execution.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#an-example",
    "href": "lectures/06_ssa.html#an-example",
    "title": "Static Single Assignment",
    "section": "an example",
    "text": "an example\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nB0[\"0: i = 0\n    1: s = 0\"]\nB1[\"2: x = m\n    3: s = s + x\n    4: i = i +4\n    5: if i &lt; n go to B0\"]\nB0 --&gt; B1\nB1 --&gt; B1\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nB0[\"0: i = 0\n    1: s = 0\"]\nB1[\"2: x = m\n    3: s = s + x\n    4: i = i +4\n    5: if i &lt; n go to B0\"]\nB0 --&gt; B1\nB1 --&gt; B1\n\n\n\n\n\n\nvariable i has two static assignments 0 and 4, so this program is not in SSA\nVariable s has two static assignments, x has one static assignment but x has lots of dynamic assignments (when the program executes)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#straight-line-code",
    "href": "lectures/06_ssa.html#straight-line-code",
    "title": "Static Single Assignment",
    "section": "straight line code",
    "text": "straight line code\nWe call a program without branches a piece of straight line code.\n    @main {\n      a: int = const 4;\n      b: int = const 2;\n      a: int = add a b;\n      b: int = add a b;\n      print b;\n    }\n. . .\nIts easy to see how to convert straight line code into ssa\n    @main {\n      a.1: int = const 4;\n      b.1: int = const 2;\n      a.2: int = add a.1 b.1;\n      b.2: int = add a.2 b.1;\n      print b.2;\n    }",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#pseudo-code-for-one-basic-block",
    "href": "lectures/06_ssa.html#pseudo-code-for-one-basic-block",
    "title": "Static Single Assignment",
    "section": "pseudo code for one basic block",
    "text": "pseudo code for one basic block\nfor each variable a: \n    Count[a] = 0 \n    Stack[a] = [0]\n\nrename_basic_block(B): \n    for each instruction S in block B:\n        for each use of a argument x in S: \n            i = top(Stack[x]) \n            replace the use of x with x_i\n            \n        for each variable a that S defines (a dest)\n            count[a] = Count[a] + 1 \n            i = Count[a]             \n            push i onto Stack[a]             \n            replace definition of a with a_i\nWe don’t need the stack here but we will need it later.\n\nOf course, things will get a little more complicated when there is control flow. And because real machines are not SSA, using separate variables (i.e., memory locations and registers) for everything is bound to be inefficient.\nThe idea in SSA is to convert general programs into SSA form, do all our optimization there, and then convert back to a standard mutating form before we generate backend code.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#phi-nodes",
    "href": "lectures/06_ssa.html#phi-nodes",
    "title": "Static Single Assignment",
    "section": "phi-Nodes",
    "text": "phi-Nodes\nJust renaming assignments will quickly run into problems. Consider this program:\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nB0[\".b0\n    a: int = const 47;\n    br cond .left .right;\"]\nleft[\"a: int = add a a;\n    jmp .exit;\"]\nright[\"a: int = mul a a;\n        jmp .exit;\"]\nexit[\"print a;\"]\nB0 --&gt; left\nB0 --&gt; right\nleft --&gt; exit\nright --&gt; exit\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nB0[\".b0\n    a: int = const 47;\n    br cond .left .right;\"]\nleft[\"a: int = add a a;\n    jmp .exit;\"]\nright[\"a: int = mul a a;\n        jmp .exit;\"]\nexit[\"print a;\"]\nB0 --&gt; left\nB0 --&gt; right\nleft --&gt; exit\nright --&gt; exit\n\n\n\n\n\n\nWhich “version” of a should we use in the print statement?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#phi-nodes-1",
    "href": "lectures/06_ssa.html#phi-nodes-1",
    "title": "Static Single Assignment",
    "section": "phi nodes",
    "text": "phi nodes\nTo match the expressiveness of unrestricted programs, SSA adds a new kind of instruction: a phi-node.\nphi-nodes are flow-sensitive copy instructions: they get a value from one of several variables, depending on which incoming CFG edge was most recently taken to get to them.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#phi-nodes-in-bril",
    "href": "lectures/06_ssa.html#phi-nodes-in-bril",
    "title": "Static Single Assignment",
    "section": "phi nodes in Bril",
    "text": "phi nodes in Bril\nIn Bril, a phi-node appears as a phi instruction:\na.4: int = phi .left a.2 .right a.3;\nThe phi instruction chooses between any number of variables, and it picks between them based on labels. If the program most recently executed a basic block with the given label, then the phi instruction takes its value from the corresponding variable.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#back-to-the-example",
    "href": "lectures/06_ssa.html#back-to-the-example",
    "title": "Static Single Assignment",
    "section": "back to the example",
    "text": "back to the example\nYou can write the above program in SSA like this:\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nB0[\".b0\n    a: int = const 47;\n    br cond .left .right;\"]\nleft[\"a: int = add a a;\n    jmp .exit;\"]\nright[\"a: int = mul a a;\n        jmp .exit;\"]\nexit[\"print a;\"]\nB0 --&gt; left\nB0 --&gt; right\nleft --&gt; exit\nright --&gt; exit\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\nB0[\".b0\n    a: int = const 47;\n    br cond .left .right;\"]\nleft[\"a: int = add a a;\n    jmp .exit;\"]\nright[\"a: int = mul a a;\n        jmp .exit;\"]\nexit[\"print a;\"]\nB0 --&gt; left\nB0 --&gt; right\nleft --&gt; exit\nright --&gt; exit\n\n\n\n\n\n\n\n    @main(cond: bool) {\n    .entry:\n        a.1: int = const 47;\n        br cond .left .right;\n    .left:\n        a.2: int = add a.1 a.1;\n        jmp .exit;\n    .right:\n        a.3: int = mul a.1 a.1;\n        jmp .exit;\n    .exit:\n        a.4: int = phi .left a.2 .right a.3;\n        print a.4;\n    }",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#bril-in-ssa",
    "href": "lectures/06_ssa.html#bril-in-ssa",
    "title": "Static Single Assignment",
    "section": "Bril in SSA",
    "text": "Bril in SSA\nBril has an SSA extension It adds support for a phi instruction. Beyond that, SSA form is just a restriction on the normal expressiveness of Bril—if you solemnly promise never to assign statically to the same variable twice, you are writing “SSA Bril.”\nThe reference interpreter has built-in support for phi, so you can execute your SSA-form Bril programs without fuss.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#converting-to-ssa---very-simple-scheme",
    "href": "lectures/06_ssa.html#converting-to-ssa---very-simple-scheme",
    "title": "Static Single Assignment",
    "section": "Converting to SSA - Very simple scheme",
    "text": "Converting to SSA - Very simple scheme\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph LR\nX[\"Block X\n   a = \n   b = \n   if s &gt; b\"]\nY[\"Block Y\n  b = a\"]\nZ[\"Block Z\nret b\"]\nX --&gt; Y\nY--&gt; Z\nX --&gt; Z\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph LR\nX[\"Block X\n   a = \n   b = \n   if s &gt; b\"]\nY[\"Block Y\n  b = a\"]\nZ[\"Block Z\nret b\"]\nX --&gt; Y\nY--&gt; Z\nX --&gt; Z\n\n\n\n\n\n\nWhere do we need phi-functions?\nWhich variables\n. . .\nphi At the merge (join) node\nvariable b",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#conditions",
    "href": "lectures/06_ssa.html#conditions",
    "title": "Static Single Assignment",
    "section": "conditions",
    "text": "conditions\nconditions: phi-function for variable b at node z\n\nThere is a block x containing a definition of b\nThere is a block y (with y ≠ x) containing a definition of b\nThere is a nonempty path Pxz of edges from x to z\nThere is a nonempty path Pyz of edges from y to z\nPaths Pxz and Pyz do not have any node in common other than z, and…\nThe node z does not appear within both Pxz and Pyz prior to the end, though it may appear in one or the other.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#scheme-part-2",
    "href": "lectures/06_ssa.html#scheme-part-2",
    "title": "Static Single Assignment",
    "section": "scheme part 2",
    "text": "scheme part 2\nthis is iterative since when we add a phi, we are creating a new defintion, which may add new phi-functions\nWhen we find nodes X,Y,Z that match these steps and z does not contain a phi function for b, insert a phi\nWhile really expensive this will work",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#diagram",
    "href": "lectures/06_ssa.html#diagram",
    "title": "Static Single Assignment",
    "section": "diagram",
    "text": "diagram\nusing dash for path\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\na[\"x:x=\"]\nb[\"y:x=\"]\nc[join]\nd\na-.-&gt; c\nb-.-&gt; c\nc-.-&gt;d\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\na[\"x:x=\"]\nb[\"y:x=\"]\nc[join]\nd\na-.-&gt; c\nb-.-&gt; c\nc-.-&gt;d \n\n\n\n\n\n\nWe could have complex flow - including loops on the paths",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#reminder-dominators",
    "href": "lectures/06_ssa.html#reminder-dominators",
    "title": "Static Single Assignment",
    "section": "reminder dominators",
    "text": "reminder dominators\nwhile dom is changing \nfor vertex in cfg \n   dom[vertex] =\n\nwhile dom is changing \nfor vertex in cfg \n   dom[vertex] =  {vertex} + ...\n\nif b has multiple preds, and a dominates all of them, a dom b\nwhile dom is changing \nfor vertex in cfg \n   dom[vertex] =  {vertex} + Intersection( dom(p) for p a pred of vertex)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#fast-methods-for-inserting-phis",
    "href": "lectures/06_ssa.html#fast-methods-for-inserting-phis",
    "title": "Static Single Assignment",
    "section": "fast methods for inserting phi’s",
    "text": "fast methods for inserting phi’s\nThe method for this has two steps\n\ninsert phi instructions where needed (do not add subscripts yet)\nin a second pass insert all the numbers",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#to-ssa",
    "href": "lectures/06_ssa.html#to-ssa",
    "title": "Static Single Assignment",
    "section": "to ssa",
    "text": "to ssa\nTo convert to SSA, we want to insert phi-nodes whenever there are distinct paths containing distinct definitions of a variable. We don’t need phi-nodes in places that are dominated by a definition of the variable. So what’s a way to know when control reachable from a definition is not dominated by that definition?\nThe dominance frontier!",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#recall-the-dominance-frontier",
    "href": "lectures/06_ssa.html#recall-the-dominance-frontier",
    "title": "Static Single Assignment",
    "section": "recall the dominance frontier",
    "text": "recall the dominance frontier\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA--&gt;B \nA--&gt; F\nB--&gt;C\nB--&gt; D\nC--&gt; E\nD--&gt; E\nE--&gt; F\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA--&gt;B \nA--&gt; F\nB--&gt;C\nB--&gt; D\nC--&gt; E\nD--&gt; E\nE--&gt; F\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nA--&gt; B\nA--&gt; F\nB--&gt; C\nB--&gt;D\nB--&gt; E\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nA--&gt; B\nA--&gt; F\nB--&gt; C\nB--&gt;D\nB--&gt; E\n\n\n\n\n\n\n\n\n\n\n\nblock\nA\nB\nC\nD\nE\n\n\nfrontier\nempty\nF\nE\nE\nF",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#a-block-can-be-in-its-own-dom-frontier",
    "href": "lectures/06_ssa.html#a-block-can-be-in-its-own-dom-frontier",
    "title": "Static Single Assignment",
    "section": "a block can be in its own dom frontier",
    "text": "a block can be in its own dom frontier\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA--&gt; B\nB--&gt; A\nB--&gt; C\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nA--&gt; B\nB--&gt; A\nB--&gt; C\n\n\n\n\n\n\nwhy: A dom B, but B does not dom A. so A is in the dom frontier of A",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#an-almost-linear-method",
    "href": "lectures/06_ssa.html#an-almost-linear-method",
    "title": "Static Single Assignment",
    "section": "an almost linear method",
    "text": "an almost linear method\nWe do it in two steps.\n\ninsert phi-nodes:\nrename variables:",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#placing-phi-functions",
    "href": "lectures/06_ssa.html#placing-phi-functions",
    "title": "Static Single Assignment",
    "section": "placing Phi functions",
    "text": "placing Phi functions\nlet b be a block with a def of a variable v, if b has multiple defs of v, use the last one\nWhat is the first block following v that can be reached by a different def of v\nin blocks dominated by b, b’s def must have been executed, (other defs of v in a dominated block may overwrite it)\nwe need to place a phi function for b at the start of all blocks in the dom frontier of b.\nafter we add phi functions to S where S = df(b) we have more defs, so we need to add phi’s in the dom frontier of all the blocks in S",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#example-1",
    "href": "lectures/06_ssa.html#example-1",
    "title": "Static Single Assignment",
    "section": "example 1",
    "text": "example 1\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nN[\"v=1\"]\nM\nM2[\"v=2\"]\nZ[\"M\"]\nN--&gt; M\nM--&gt;Z\nM2--&gt; Z\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nN[\"v=1\"]\nM\nM2[\"v=2\"]\nZ[\"M\"]\nN--&gt; M\nM--&gt;Z\nM2--&gt; Z\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nN[\"v1=1\"]\nM\nM2[\"v2=2\"]\nZ[\"M:v3= phi(v1,v2)\"]\nN--&gt; M\nM--&gt;Z\nM2--&gt; Z\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nN[\"v1=1\"]\nM\nM2[\"v2=2\"]\nZ[\"M:v3= phi(v1,v2)\"]\nN--&gt; M\nM--&gt;Z\nM2--&gt; Z",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#a-loop",
    "href": "lectures/06_ssa.html#a-loop",
    "title": "Static Single Assignment",
    "section": "a loop",
    "text": "a loop\n::: {.columns}\n::: {.column}\n\ngraph TD\n V[\"v=init\"]\n Z{\"z:v = v+1\"}\n V --&gt; Z\n Z --&gt;Z\n\n\n\n\n graph TD\n V[\"v=init\"]\n Z{\"z:v = v+1\"}\n V --&gt; Z\n Z --&gt;Z\n\n\n\n\n\n\n:::\n::: {.column}\n\ngraph TD\n V[\"v1=init\"]\n Z{\"z:v2 = phi(v1,v3)\\nv3 = v2+1\"}\n V --&gt; Z\n Z --&gt;Z\n\n\n\n\n graph TD\n V[\"v1=init\"]\n Z{\"z:v2 = phi(v1,v3)\\nv3 = v2+1\"}\n V --&gt; Z\n Z --&gt;Z\n\n\n\n\n\n\n:::\n:::",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#iterative-placement",
    "href": "lectures/06_ssa.html#iterative-placement",
    "title": "Static Single Assignment",
    "section": "iterative placement",
    "text": "iterative placement\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nB1[v=1]\nB2[v=2]\nB3\nB4[v=3]\nB5\nB1--&gt; B3\nB2--&gt; B3\nB3--&gt; B5\nB4--&gt; B5\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nB1[v=1]\nB2[v=2]\nB3\nB4[v=3]\nB5\nB1--&gt; B3\nB2--&gt; B3\nB3--&gt; B5\nB4--&gt; B5\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nB1[v1=1]\nB2[v2=2]\nB3[\"v3=phi(v1,v2)\"]\nB4[v4=3]\nB5[\"v5=phi(v3,v4)\"]\nB1--&gt; B3\nB2--&gt; B3\nB3--&gt; B5\nB4--&gt; B5\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD\nB1[v1=1]\nB2[v2=2]\nB3[\"v3=phi(v1,v2)\"]\nB4[v4=3]\nB5[\"v5=phi(v3,v4)\"]\nB1--&gt; B3\nB2--&gt; B3\nB3--&gt; B5\nB4--&gt; B5",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#phi-placement",
    "href": "lectures/06_ssa.html#phi-placement",
    "title": "Static Single Assignment",
    "section": "phi placement",
    "text": "phi placement\nfor each block b in the cfg \n  for each var v defined in b\n    add block to the set defs(v)  ## blocks that contain an assignment to v \n\n  W = Defs[v]\n    while W is not empty\n      remove a node n from w\n         for block in DF[n]:  # Dominance frontier.\n           Add a phi-node to block,\n             unless we have done so already.\n           Add block to W (because it now writes to v),\n             unless it's already in there.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#an-example-1",
    "href": "lectures/06_ssa.html#an-example-1",
    "title": "Static Single Assignment",
    "section": "an example",
    "text": "an example\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nN1[\"1:x = 1\"]\nN2[\"2:\"]\nN3[\"3:x= 2\"]\nN4[\"4:\"]\nN5[\"5:x=3\"]\nN6[6:x=4]\nN7[7:]\nN1--&gt; N2\nN1--&gt; N3\nN2--&gt; N4\nN3--&gt; N4\nN4--&gt; N5\nN5--&gt;  N4\nN5--&gt; N6\nN6--&gt; N7\nN6--&gt; N5\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB\nN1[\"1:x = 1\"]\nN2[\"2:\"]\nN3[\"3:x= 2\"]\nN4[\"4:\"]\nN5[\"5:x=3\"]\nN6[6:x=4]\nN7[7:]\nN1--&gt; N2\nN1--&gt; N3\nN2--&gt; N4\nN3--&gt; N4\nN4--&gt; N5\nN5--&gt;  N4\nN5--&gt; N6\nN6--&gt; N7\nN6--&gt; N5\n\n\n\n\n\n\n\n\ninitially w = {1,3,5,6}\nprocess DF(1) = empty\nprocess DF(3) = 4, add 4 to w and add a phi function for x to 4\nprocess DF(5) = 4,5 no need to add 5 to w, add phi for x to 5\nprocess DF(6) = 5\nprocess DF(4) = 4\n\n\n\nadd phi’s to blocks 4 and 5",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#rename-variables",
    "href": "lectures/06_ssa.html#rename-variables",
    "title": "Static Single Assignment",
    "section": "rename variables:",
    "text": "rename variables:\n# allocate a stack and a counter for each variable\nfor each V a variable \n  c[v] = 0\n  s[v] = empty stack\n  search(entry)\n\nsearch(n):\n  for each instr i in n:\n     if instr is not a phi\n         replace every variable in the rhs of instr by vi where i = top(s[v])\n         if instr has a dest v\n           i = C(v)\n            replace v by new vi, push i onto s[v]\n            increment c[v]\n\n  for each y a successor of n\n     j = which pred (y,n)\n     for each phi function pinstr in Y replace the jth opernmad of pinstr by vi where \n        i = top(s(v)\n  \n  for each Y a child of n in the dominator tree \n      call search(Y)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#an-example-2",
    "href": "lectures/06_ssa.html#an-example-2",
    "title": "Static Single Assignment",
    "section": "an example",
    "text": "an example\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\n\nL5--&gt; L9\nL7 --&gt; L9\nL9 --&gt; L3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\n\nL5--&gt; L9\nL7 --&gt; L9\nL9 --&gt; L3",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#what-is-the-dominator-tree",
    "href": "lectures/06_ssa.html#what-is-the-dominator-tree",
    "title": "Static Single Assignment",
    "section": "What is the dominator tree?",
    "text": "What is the dominator tree?\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 -.-&gt; L9\nL9 --&gt; L3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 -.-&gt; L9\nL9 --&gt; L3\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB \nL0 --&gt; L3 \nL3--&gt; L4\nL3 --&gt; L10\nL4--&gt; L5 \nL4 --&gt; L9\nL4 --&gt; L7\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB \nL0 --&gt; L3 \nL3--&gt; L4\nL3 --&gt; L10\nL4--&gt; L5 \nL4 --&gt; L9\nL4 --&gt; L7",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#dominance-frontiers",
    "href": "lectures/06_ssa.html#dominance-frontiers",
    "title": "Static Single Assignment",
    "section": "dominance frontiers",
    "text": "dominance frontiers\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3 \n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 -.-&gt; L9\nL9 --&gt; L3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3 \n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 -.-&gt; L9\nL9 --&gt; L3\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB \nL0 --&gt; L3 \nL3--&gt; L4\nL3 --&gt; L10\nL4--&gt; L5 \nL4 --&gt; L9\nL4 --&gt; L7\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TB \nL0 --&gt; L3 \nL3--&gt; L4\nL3 --&gt; L10\nL4--&gt; L5 \nL4 --&gt; L9\nL4 --&gt; L7",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#add-phi-nodes",
    "href": "lectures/06_ssa.html#add-phi-nodes",
    "title": "Static Single Assignment",
    "section": "add phi nodes",
    "text": "add phi nodes\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 --&gt; L9\nL9 --&gt; L3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[L9: goto l3]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 --&gt; L9\nL9 --&gt; L3\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"j = phi(j,j) \n    k = phi(k,k) \n   L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[\"j = phi(j,j)\n   k = phi(k,k)\n  L9: goto l3\"]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 --&gt; L9\nL9 --&gt; L3\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nL0[\"L0: i = 1\n   L1: j = 1\n   L2: k = 0\"]\n\nL3[\"j = phi(j,j) \n    k = phi(k,k) \n   L3: if j &lt;20 go to l4 else l10\"]\n\nL4[\"l4: if j &lt; 20 goto l7 else l5\"]\n\nL5[\"l5: j = i\n   l6: k = k +1\"]\n\n\nL7[\"l7: j = k\n  l8: k = k +2\"]\n\nL9[\"j = phi(j,j)\n   k = phi(k,k)\n  L9: goto l3\"]\n\nL10[\"l10: ret j\"]\n\nL0--&gt; L3\n\nL3--&gt; L4\nL3 --&gt; L10\n\nL4 --&gt; L5\nL4 --&gt; L7\nL5 --&gt; L9\nL7 --&gt; L9\nL9 --&gt; L3",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#the-arity-of-phi-functions",
    "href": "lectures/06_ssa.html#the-arity-of-phi-functions",
    "title": "Static Single Assignment",
    "section": "The arity of phi-functions",
    "text": "The arity of phi-functions\nCould we have a phi-function in a node with only one predecessor?\ncould we have a phi-function wit more then two arguments?\nThis algorithm computes what is called minimal SSA form which is not so mimimal since it can leave dead assignments\ndoing dead code elimination pruned ssa form",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#getting-out-of-ssa",
    "href": "lectures/06_ssa.html#getting-out-of-ssa",
    "title": "Static Single Assignment",
    "section": "Getting out of ssa",
    "text": "Getting out of ssa\nCompilers that use the SSA form usually contain a step, before the generation of actual assembly code, in which phi functions are replaced by ordinary instructions. Normally these instructions are simple copies.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#an-example-3",
    "href": "lectures/06_ssa.html#an-example-3",
    "title": "Static Single Assignment",
    "section": "an example",
    "text": "an example\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n A0[\"io =\n     j0 = \n     k0 =\"]\nA1[\"i1 =\n   j1 =\n   k1 = \"]\nA2[\"i2 = phi(i0, i1)\n   j2 = phi(j0, j1)\n   k2 = phi(k0, k1)\n   ...\n    = i2\n    = j2 \n    = k2\"]\n\n    A0 --&gt; A2\n    A1--&gt; A2\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n A0[\"io =\n     j0 = \n     k0 =\"]\nA1[\"i1 =\n   j1 =\n   k1 = \"]\nA2[\"i2 = phi(i0, i1)\n   j2 = phi(j0, j1)\n   k2 = phi(k0, k1)\n   ...\n    = i2\n    = j2 \n    = k2\"]\n\n    A0 --&gt; A2\n    A1--&gt; A2\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n B0[\"io =\n     j0 = \n     k0 =\"]\nB1[\"i1 =\n   j1 =\n   k1 = \"]\nB2[\"\n   ...\n    = i2\n    = j2 \n    = k2\"]\n    B0 --\"i2 = i0\n       j2 = j0\n       k2 = k0\"--&gt; B2\n    B1 --\"i2 = i1\n          j2 = j1\n          k2 = k1\"--&gt; B2\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TD\n B0[\"io =\n     j0 = \n     k0 =\"]\nB1[\"i1 =\n   j1 =\n   k1 = \"]\nB2[\"\n   ...\n    = i2\n    = j2 \n    = k2\"]\n    B0 --\"i2 = i0\n       j2 = j0\n       k2 = k0\"--&gt; B2\n    B1 --\"i2 = i1\n          j2 = j1\n          k2 = k1\"--&gt; B2\n\n\n\n\n\n\n\n\nwe cannot put instructions on edges, but we can add to prev block",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#critical-edges",
    "href": "lectures/06_ssa.html#critical-edges",
    "title": "Static Single Assignment",
    "section": "critical edges",
    "text": "critical edges\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nA0[\"L1:\n   a0 =\n   b0 =\n   if A0 &gt; b0\"]\nA1[\"b1 = a0\"]\nA2[\"l2:\nb2 = phi(b1,b0)\"]\nA0 --&gt; A1\nA1 --&gt; A2\nA0 --&gt; A2\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\ngraph TB\nA0[\"L1:\n   a0 =\n   b0 =\n   if A0 &gt; b0\"]\nA1[\"b1 = a0\"]\nA2[\"l2:\nb2 = phi(b1,b0)\"]\nA0 --&gt; A1\nA1 --&gt; A2\nA0 --&gt; A2\n\n\n\n\n\n\n\nb2 = b0?\n\n\nThe placement of the copy b2 = b0 is not simple, because the edge that links L2 to L5 is critical. A critical edge connects a block with multiple successors to a block with multiple predecessors. This should remind you of adding a preheader to a loop",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#critical-edge-splitting",
    "href": "lectures/06_ssa.html#critical-edge-splitting",
    "title": "Static Single Assignment",
    "section": "critical edge splitting",
    "text": "critical edge splitting\nWe can solve this problem by doing critical edge splitting. This CFG transformation consists in adding an empty basic block (empty, except by – perhaps – a goto statement) between each pair of blocks connected by a critical edge.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#making-use-of-ssa-form",
    "href": "lectures/06_ssa.html#making-use-of-ssa-form",
    "title": "Static Single Assignment",
    "section": "making use of ssa form",
    "text": "making use of ssa form\nOur previous analyses always used a (variable, program point), but in ssa these are the same",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#dead-code-elimination-in-ssa",
    "href": "lectures/06_ssa.html#dead-code-elimination-in-ssa",
    "title": "Static Single Assignment",
    "section": "dead code elimination in ssa",
    "text": "dead code elimination in ssa\nwhile there is some variable v with no uses and the statement that defines v has no other side effects, delete the statement that defines v from the program.\nwe need a counter for each variable (or each instruction)\nwalk the program once increment the counter each time the variable is used\nwhile there exists v, such that counter[v] = 0 remove the instruction that defined v, e.g., “v = E for each variable x used in E decrement counter[x]",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#sparse-constant-prop",
    "href": "lectures/06_ssa.html#sparse-constant-prop",
    "title": "Static Single Assignment",
    "section": "sparse constant prop",
    "text": "sparse constant prop\nwe define a partial order on constats, any &gt; all constants &gt; undefined and define the intersection of two states as the common parent\nwith each variable we have an abstract state (like a value number)\nv  = const c   ==&gt; v state is const \n\nv = id q      ==&gt; v state is the state of  q \n\nv = v0 op v1  ==&gt; if both are constants v = c0 op c1\n\n             ==&gt; if one is any, v's state is any\n\nv = phi(v0,..vn) ==&gt; v's state is the intersection of the states of v0,..,vn",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#what-order-do-we-process-nodes",
    "href": "lectures/06_ssa.html#what-order-do-we-process-nodes",
    "title": "Static Single Assignment",
    "section": "What order do we process nodes?",
    "text": "What order do we process nodes?\nbecause the program is in ssa form we can do the nodes in dominator tree order, then before processing any instruction that is not a phi, we will have processed all the arguments\n\nB0: x0  = input \n    a0 = 1 \n    c0 = a0 +10\n    if a0 &lt; c0 go to b1\n\nB1: a1 phi(a1,a2 )\n    b0 = x0 * a1\n    print b0 \n    a2 = a1 +1 \n    go to b1",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#walking-the-dominator-tree-b0---b1",
    "href": "lectures/06_ssa.html#walking-the-dominator-tree-b0---b1",
    "title": "Static Single Assignment",
    "section": "walking the dominator tree b0 -> b1",
    "text": "walking the dominator tree b0 -&gt; b1\n\n\nB0: x0  = input \n    a0 = 1 \n    c0 = a0 +10\n    if a0 &lt; c0 go to b1\n\nB1: a1 phi(a0,a2 )\n    b0 = x0 * a1\n    print b0 \n    a2 = a1 +1 \n    go to b1\n\nB0:\nx0 - any \na0 - 1 \nc0 - 11 (folding the constant)\na0 &lt; c0  skip\nB1:\na1 -  1 (only one input defined)\nb0  - any\na2 -  2\nupdate the uses of a2 - the phi\na1 -  any \n\nupdate the uses of a1 \nno change",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#converting-from-ssa",
    "href": "lectures/06_ssa.html#converting-from-ssa",
    "title": "Static Single Assignment",
    "section": "Converting from SSA",
    "text": "Converting from SSA\nEventually, we need to convert out of SSA form to generate efficient code for real machines that don’t have phi-nodes and do have finite space for variable storage.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#basic-algorithm",
    "href": "lectures/06_ssa.html#basic-algorithm",
    "title": "Static Single Assignment",
    "section": "basic algorithm",
    "text": "basic algorithm\nThe basic algorithm is pretty straightforward. If you have a phi-node:\nv = phi .l1 x .l2 y;\nThen there must be assignments to x and y (recursively) preceding this statement in the CFG.\nThe paths from x to the phi-containing block and from y to the same block must “converge” at that block. So insert code into the phi-containing block’s immediate predecessors along each of those two paths: one that does v = id x and one that does v = id y. Then you can delete the phi instruction.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#extra-copies",
    "href": "lectures/06_ssa.html#extra-copies",
    "title": "Static Single Assignment",
    "section": "extra copies",
    "text": "extra copies\nThis basic approach can introduce some redundant copying. (Take a look at the code it generates after you implement it!) Non-SSA copy propagation optimization can work well as a post-processing step. For a more extensive take on how to translate out of SSA efficiently, see “Revisiting Out-of-SSA Translation for Correctness, Code Quality, and Efficiency” by Boissinot et al.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#overlap",
    "href": "lectures/06_ssa.html#overlap",
    "title": "Static Single Assignment",
    "section": "overlap",
    "text": "overlap\nits possible that an optimization can give overlapping phi-functions\nb0 \n  x1 = 1\n  y1 = 2\nB1 \nx2 = phi(x1,x3)\ny2 = phi(y1, y3)\n  z = x2\n  x3 = y2\n  y3= z\n  if() go to b1",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#optimize-it",
    "href": "lectures/06_ssa.html#optimize-it",
    "title": "Static Single Assignment",
    "section": "optimize it",
    "text": "optimize it\nb0 \n  x1 = 1\n  y1 = 2\nB1 \nx2 = phi(x1, y2)\ny2 = phi(y1, x2)\n  if() go to b1",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/06_ssa.html#lost-the-temp-this-is-called-the-swap-problem",
    "href": "lectures/06_ssa.html#lost-the-temp-this-is-called-the-swap-problem",
    "title": "Static Single Assignment",
    "section": "lost the temp (this is called the swap problem)",
    "text": "lost the temp (this is called the swap problem)\nif we add copies x2 = y3 y2 = x2 (uses the wrong value of x2)\nphi nodes execute all at once - not one at a time\nSome SSA slides from Todd Mowry at CMU",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Static Single Assignment"
    ]
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#terms",
    "href": "lectures/revealjs_register_allocation.qmd.html#terms",
    "title": "Register Allocation",
    "section": "terms",
    "text": "terms\n\nThe task of determining the register in which each variable will be stored is known as register assignment.\nIf a variable must be stored in memory, it is referred to as a spill. Spilling involves identifying which variables need to be mapped to memory.\nIf the same register can be assigned to two variables related by a move instruction, the move can be eliminated. This optimization is called coalescing."
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#more-terms",
    "href": "lectures/revealjs_register_allocation.qmd.html#more-terms",
    "title": "Register Allocation",
    "section": "more terms",
    "text": "more terms\n\nGPU performance often improves when fewer registers are used.\nVariables in Bril are virtual registers. After assignment, they become physical registers.\n\nRegister allocators often have to manage constraints. For example, a function argument may need to be placed in a specific physical register."
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#formal-limits",
    "href": "lectures/revealjs_register_allocation.qmd.html#formal-limits",
    "title": "Register Allocation",
    "section": "Formal Limits",
    "text": "Formal Limits\nRegister allocation is NP complete. Given a program P and K registers, is there an assignment where each variable gets a register and all simultaneously live variables get different registers\nGregory Chaitin showed that if we have a graph that we want to paint with K colors, such that adjacent vertices get different colors we can construct a program where the program can be allocated with K registers iff the graph can be colored with K colors"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#interference-graph",
    "href": "lectures/revealjs_register_allocation.qmd.html#interference-graph",
    "title": "Register Allocation",
    "section": "Interference Graph",
    "text": "Interference Graph\nChaitin used the interference graph. One vertex for each variable, and edge between variables that are simultaneously live.\nTwo variables that interfere cannot be in the same register"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#allocation-and-liveness",
    "href": "lectures/revealjs_register_allocation.qmd.html#allocation-and-liveness",
    "title": "Register Allocation",
    "section": "Allocation and Liveness",
    "text": "Allocation and Liveness\nIf two variables are alive at the same point, and they have different values, they have to be assigned different registers\nApproximate this by ignoring “have different values” - Different registers if alive at the same point. (id is special)\nMaxLive is the max number of values live at the same point\nMinReg is the min number of registers we need\nminReg &gt;= MaxLive"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#an-example",
    "href": "lectures/revealjs_register_allocation.qmd.html#an-example",
    "title": "Register Allocation",
    "section": "an example",
    "text": "an example\n\nWhat is the maximum number of variables alive at any program point?\nWhat is the interference graph of this program?\n\ndraw it?"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#interference-graph-1",
    "href": "lectures/revealjs_register_allocation.qmd.html#interference-graph-1",
    "title": "Register Allocation",
    "section": "interference graph",
    "text": "interference graph\n\nMaxLive = 2 Can we compile this with 2 registers? - Need 3\ndraw it?\nThe interference graph is a pentagon, needed 3 registers.\nA pentagon is the smallest graph whose chromatic number (number of colors needed 3 ) is less the maximum clique (2)"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#ssa-form",
    "href": "lectures/revealjs_register_allocation.qmd.html#ssa-form",
    "title": "Register Allocation",
    "section": "SSA Form",
    "text": "SSA Form"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#with-liveness",
    "href": "lectures/revealjs_register_allocation.qmd.html#with-liveness",
    "title": "Register Allocation",
    "section": "with liveness",
    "text": "with liveness"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#ssa-with-interference-graph",
    "href": "lectures/revealjs_register_allocation.qmd.html#ssa-with-interference-graph",
    "title": "Register Allocation",
    "section": "ssa with interference graph",
    "text": "ssa with interference graph"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#register-allocation",
    "href": "lectures/revealjs_register_allocation.qmd.html#register-allocation",
    "title": "Register Allocation",
    "section": "register allocation",
    "text": "register allocation"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#swaps-via-xor",
    "href": "lectures/revealjs_register_allocation.qmd.html#swaps-via-xor",
    "title": "Register Allocation",
    "section": "swaps via xor",
    "text": "swaps via xor"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#final-code",
    "href": "lectures/revealjs_register_allocation.qmd.html#final-code",
    "title": "Register Allocation",
    "section": "final code",
    "text": "final code"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#ssa-based-register-allocation",
    "href": "lectures/revealjs_register_allocation.qmd.html#ssa-based-register-allocation",
    "title": "Register Allocation",
    "section": "ssa based register allocation",
    "text": "ssa based register allocation\nWe have been able to compile the SSA-form program with less registers than the minimum that the original program requires.\nTwo claims\n\nThe SSA-form program will never require more registers than the original program.\nAnd we can find the minimum number of registers that the SSA-form program needs in polynomial time."
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#setting-up-the-colors",
    "href": "lectures/revealjs_register_allocation.qmd.html#setting-up-the-colors",
    "title": "Register Allocation",
    "section": "setting up the colors",
    "text": "setting up the colors\nsuppose we have an ordering of the vertices, where the neighbors of a node to the left of the node in the ordering from a clique. If there are K such neighbors we need K+1 colors"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#an-example-1",
    "href": "lectures/revealjs_register_allocation.qmd.html#an-example-1",
    "title": "Register Allocation",
    "section": "an example",
    "text": "an example\n\n\nCode\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn1--&gt; n2 --&gt; n3 --&gt;  n4 --&gt; n5 --&gt; n6--&gt; n1\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn1--&gt; n2 --&gt; n3 --&gt;  n4 --&gt; n5 --&gt; n6--&gt; n1\n\n\n\n\n\n\ndraw it\ngiven this order - it is simple to pick the colors\nonce we have an order , we can greedy color the nodes. When we get to the n’th node, all the neighbors are in a clique and are colored, so just pick one\nmaybe try not to clobber a copy\nall nodes in the clique need different colors\nIn a chordal graph the size of the largest clique equals the chromatic number\nif we find the point in the program with max live variables, we know the chromatic number"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#how-do-we-get-the-order",
    "href": "lectures/revealjs_register_allocation.qmd.html#how-do-we-get-the-order",
    "title": "Register Allocation",
    "section": "how do we get the order",
    "text": "how do we get the order\n\ngive each node number\ninitially each node gets count of zero 1 pick an unordered node with max count\nput that node in the front of the list, mark that node ordered\nincrement each neighbor by 1"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#dominance-trees",
    "href": "lectures/revealjs_register_allocation.qmd.html#dominance-trees",
    "title": "Register Allocation",
    "section": "dominance trees",
    "text": "dominance trees\nWhat is the dominance tree of this program?"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#dominance-tree",
    "href": "lectures/revealjs_register_allocation.qmd.html#dominance-tree",
    "title": "Register Allocation",
    "section": "dominance tree",
    "text": "dominance tree"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#dom-sub-trees",
    "href": "lectures/revealjs_register_allocation.qmd.html#dom-sub-trees",
    "title": "Register Allocation",
    "section": "dom sub trees",
    "text": "dom sub trees"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#chordal-graphs-triangular-graphs",
    "href": "lectures/revealjs_register_allocation.qmd.html#chordal-graphs-triangular-graphs",
    "title": "Register Allocation",
    "section": "Chordal Graphs (triangular graphs)",
    "text": "Chordal Graphs (triangular graphs)\n\nintersection graph of subtrees\nA graph is chordal if each of its cycles of four or more nodes has a chord, which is an edge joining two nodes that are not adjacent in the cycle.\nif each of its cycles of four or more nodes has a chord, which is an edge joining two nodes that are not adjacent in the cycle. An equivalent definition is that any chord free cycles have at most three nodes."
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#examples-of-chordal-graphs",
    "href": "lectures/revealjs_register_allocation.qmd.html#examples-of-chordal-graphs",
    "title": "Register Allocation",
    "section": "examples of chordal graphs",
    "text": "examples of chordal graphs\ndraw am example of a cord graph"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#orderings",
    "href": "lectures/revealjs_register_allocation.qmd.html#orderings",
    "title": "Register Allocation",
    "section": "orderings",
    "text": "orderings\nwe number the vertices of G\nv0,v1,v2,…., vi, …\nconsider vi all the neighbors to the left are a clique (all connected )"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#running-example",
    "href": "lectures/revealjs_register_allocation.qmd.html#running-example",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#running-example-1",
    "href": "lectures/revealjs_register_allocation.qmd.html#running-example-1",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#running-example-2",
    "href": "lectures/revealjs_register_allocation.qmd.html#running-example-2",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#running-example-3",
    "href": "lectures/revealjs_register_allocation.qmd.html#running-example-3",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#running-example-4",
    "href": "lectures/revealjs_register_allocation.qmd.html#running-example-4",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#running-example-5",
    "href": "lectures/revealjs_register_allocation.qmd.html#running-example-5",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#running-example-6",
    "href": "lectures/revealjs_register_allocation.qmd.html#running-example-6",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#running-example-7",
    "href": "lectures/revealjs_register_allocation.qmd.html#running-example-7",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#coloring",
    "href": "lectures/revealjs_register_allocation.qmd.html#coloring",
    "title": "Register Allocation",
    "section": "coloring",
    "text": "coloring\nonce we have an order (the reverse order above), we can greedy color the nodes. When we get to the n’th node, all the neighbors are in a clique and are colored\nall nodes in the clique need different colors\nIn a chordal graph the size of the largest clique equals the chromatic number\nif we find the point in the program with max live variables, we know the chromatic number"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#spilling",
    "href": "lectures/revealjs_register_allocation.qmd.html#spilling",
    "title": "Register Allocation",
    "section": "spilling",
    "text": "spilling\nif we ever have a program point where the number of live variables is &gt; MaxRegs we will have to spill - so do it here"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#spilling-1",
    "href": "lectures/revealjs_register_allocation.qmd.html#spilling-1",
    "title": "Register Allocation",
    "section": "spilling",
    "text": "spilling"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#coalescing",
    "href": "lectures/revealjs_register_allocation.qmd.html#coalescing",
    "title": "Register Allocation",
    "section": "coalescing",
    "text": "coalescing\nif we assign both sides of a copy to the same register, we can eliminate the copy.\ninput: L list of copy instructions, G=(V,E), K\noutput: updated graph G'\n\nG' = G\nfor all x=y in L\n   sx is the set of colors in the neighborhood of x\n   sy is the set of colors in the neighborood of y\n   let c be a color &lt; K that not in either set \n   add xy a new node xy is ajacent to all node in the union of neighborhoods \n   remove x and y from G'\nxy is a merge of x and y"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#how-do-we-know-that-ssa-graphs-are-chordal",
    "href": "lectures/revealjs_register_allocation.qmd.html#how-do-we-know-that-ssa-graphs-are-chordal",
    "title": "Register Allocation",
    "section": "how do we know that ssa graphs are chordal",
    "text": "how do we know that ssa graphs are chordal"
  },
  {
    "objectID": "lectures/revealjs_register_allocation.qmd.html#dominance-and-interference-thm-1",
    "href": "lectures/revealjs_register_allocation.qmd.html#dominance-and-interference-thm-1",
    "title": "Register Allocation",
    "section": "dominance and interference Thm 1",
    "text": "dominance and interference Thm 1\nIn a strict ssa form the definition of a variable dominates all the uses\nlemma1 : if two variables interfere then the def of one dominates the def of the other\nlemma2 if two variables a and b interfere and Da &lt; Db, then a is live at Db\nlemma3 if u,v,w are variables u-v interfere and v-w interfere and u-w do not if Du &lt; Dv then Dv &lt; Dw\nthm: the interference graph of an ssa form program is chordal"
  },
  {
    "objectID": "lectures/register_allocation.html",
    "href": "lectures/register_allocation.html",
    "title": "Register Allocation",
    "section": "",
    "text": "Register allocation is the process of determining storage locations to the values used in a program.\nThese values can either be stored in registers or in memory.\nRegisters provide fast access but are limited in number.\nMemory has much higher latency and slower access speeds.\nA good register allocation strategy keeps frequently accessed variables in registers to maximize performance.\nmuch of the material for these slides comes from fernando",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#terms",
    "href": "lectures/register_allocation.html#terms",
    "title": "Register Allocation",
    "section": "terms",
    "text": "terms\n\nThe task of determining the register in which each variable will be stored is known as register assignment.\nIf a variable must be stored in memory, it is referred to as a spill. Spilling involves identifying which variables need to be mapped to memory.\nIf the same register can be assigned to two variables related by a move instruction, the move can be eliminated. This optimization is called coalescing.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#more-terms",
    "href": "lectures/register_allocation.html#more-terms",
    "title": "Register Allocation",
    "section": "more terms",
    "text": "more terms\n\nGPU performance often improves when fewer registers are used.\nVariables in Bril are virtual registers. After assignment, they become physical registers.\n\nRegister allocators often have to manage constraints. For example, a function argument may need to be placed in a specific physical register.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#formal-limits",
    "href": "lectures/register_allocation.html#formal-limits",
    "title": "Register Allocation",
    "section": "Formal Limits",
    "text": "Formal Limits\nRegister allocation is NP complete. Given a program P and K registers, is there an assignment where each variable gets a register and all simultaneously live variables get different registers\nGregory Chaitin showed that if we have a graph that we want to paint with K colors, such that adjacent vertices get different colors we can construct a program where the program can be allocated with K registers iff the graph can be colored with K colors",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#interference-graph",
    "href": "lectures/register_allocation.html#interference-graph",
    "title": "Register Allocation",
    "section": "Interference Graph",
    "text": "Interference Graph\nChaitin used the interference graph. One vertex for each variable, and edge between variables that are simultaneously live.\nTwo variables that interfere cannot be in the same register",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#allocation-and-liveness",
    "href": "lectures/register_allocation.html#allocation-and-liveness",
    "title": "Register Allocation",
    "section": "Allocation and Liveness",
    "text": "Allocation and Liveness\nIf two variables are alive at the same point, and they have different values, they have to be assigned different registers\nApproximate this by ignoring “have different values” - Different registers if alive at the same point. (id is special)\nMaxLive is the max number of values live at the same point\nMinReg is the min number of registers we need\nminReg &gt;= MaxLive",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#an-example",
    "href": "lectures/register_allocation.html#an-example",
    "title": "Register Allocation",
    "section": "an example",
    "text": "an example\n\nWhat is the maximum number of variables alive at any program point?\nWhat is the interference graph of this program?\n\ndraw it?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#interference-graph-1",
    "href": "lectures/register_allocation.html#interference-graph-1",
    "title": "Register Allocation",
    "section": "interference graph",
    "text": "interference graph\n\nMaxLive = 2 Can we compile this with 2 registers? - Need 3\ndraw it?\nThe interference graph is a pentagon, needed 3 registers.\nA pentagon is the smallest graph whose chromatic number (number of colors needed 3 ) is less the maximum clique (2)",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#ssa-form",
    "href": "lectures/register_allocation.html#ssa-form",
    "title": "Register Allocation",
    "section": "SSA Form",
    "text": "SSA Form",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#with-liveness",
    "href": "lectures/register_allocation.html#with-liveness",
    "title": "Register Allocation",
    "section": "with liveness",
    "text": "with liveness",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#ssa-with-interference-graph",
    "href": "lectures/register_allocation.html#ssa-with-interference-graph",
    "title": "Register Allocation",
    "section": "ssa with interference graph",
    "text": "ssa with interference graph",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#register-allocation",
    "href": "lectures/register_allocation.html#register-allocation",
    "title": "Register Allocation",
    "section": "register allocation",
    "text": "register allocation",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#swaps-via-xor",
    "href": "lectures/register_allocation.html#swaps-via-xor",
    "title": "Register Allocation",
    "section": "swaps via xor",
    "text": "swaps via xor",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#final-code",
    "href": "lectures/register_allocation.html#final-code",
    "title": "Register Allocation",
    "section": "final code",
    "text": "final code",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#ssa-based-register-allocation",
    "href": "lectures/register_allocation.html#ssa-based-register-allocation",
    "title": "Register Allocation",
    "section": "ssa based register allocation",
    "text": "ssa based register allocation\nWe have been able to compile the SSA-form program with less registers than the minimum that the original program requires.\nTwo claims\n\nThe SSA-form program will never require more registers than the original program.\nAnd we can find the minimum number of registers that the SSA-form program needs in polynomial time.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#setting-up-the-colors",
    "href": "lectures/register_allocation.html#setting-up-the-colors",
    "title": "Register Allocation",
    "section": "setting up the colors",
    "text": "setting up the colors\nsuppose we have an ordering of the vertices, where the neighbors of a node to the left of the node in the ordering from a clique. If there are K such neighbors we need K+1 colors",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#an-example-1",
    "href": "lectures/register_allocation.html#an-example-1",
    "title": "Register Allocation",
    "section": "an example",
    "text": "an example\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn1--&gt; n2 --&gt; n3 --&gt;  n4 --&gt; n5 --&gt; n6--&gt; n1\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\nn1--&gt; n2 --&gt; n3 --&gt;  n4 --&gt; n5 --&gt; n6--&gt; n1\n\n\n\n\n\n\ndraw it\ngiven this order - it is simple to pick the colors\nonce we have an order , we can greedy color the nodes. When we get to the n’th node, all the neighbors are in a clique and are colored, so just pick one\nmaybe try not to clobber a copy\nall nodes in the clique need different colors\nIn a chordal graph the size of the largest clique equals the chromatic number\nif we find the point in the program with max live variables, we know the chromatic number",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#how-do-we-get-the-order",
    "href": "lectures/register_allocation.html#how-do-we-get-the-order",
    "title": "Register Allocation",
    "section": "how do we get the order",
    "text": "how do we get the order\n\ngive each node number\ninitially each node gets count of zero 1 pick an unordered node with max count\nput that node in the front of the list, mark that node ordered\nincrement each neighbor by 1",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#dominance-trees",
    "href": "lectures/register_allocation.html#dominance-trees",
    "title": "Register Allocation",
    "section": "dominance trees",
    "text": "dominance trees\nWhat is the dominance tree of this program?",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#dominance-tree",
    "href": "lectures/register_allocation.html#dominance-tree",
    "title": "Register Allocation",
    "section": "dominance tree",
    "text": "dominance tree",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#dom-sub-trees",
    "href": "lectures/register_allocation.html#dom-sub-trees",
    "title": "Register Allocation",
    "section": "dom sub trees",
    "text": "dom sub trees",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#chordal-graphs-triangular-graphs",
    "href": "lectures/register_allocation.html#chordal-graphs-triangular-graphs",
    "title": "Register Allocation",
    "section": "Chordal Graphs (triangular graphs)",
    "text": "Chordal Graphs (triangular graphs)\n\nintersection graph of subtrees\nA graph is chordal if each of its cycles of four or more nodes has a chord, which is an edge joining two nodes that are not adjacent in the cycle.\nif each of its cycles of four or more nodes has a chord, which is an edge joining two nodes that are not adjacent in the cycle. An equivalent definition is that any chord free cycles have at most three nodes.",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#examples-of-chordal-graphs",
    "href": "lectures/register_allocation.html#examples-of-chordal-graphs",
    "title": "Register Allocation",
    "section": "examples of chordal graphs",
    "text": "examples of chordal graphs\ndraw am example of a cord graph",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#orderings",
    "href": "lectures/register_allocation.html#orderings",
    "title": "Register Allocation",
    "section": "orderings",
    "text": "orderings\nwe number the vertices of G\nv0,v1,v2,…., vi, …\nconsider vi all the neighbors to the left are a clique (all connected )",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#running-example",
    "href": "lectures/register_allocation.html#running-example",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#running-example-1",
    "href": "lectures/register_allocation.html#running-example-1",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#running-example-2",
    "href": "lectures/register_allocation.html#running-example-2",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#running-example-3",
    "href": "lectures/register_allocation.html#running-example-3",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#running-example-4",
    "href": "lectures/register_allocation.html#running-example-4",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#running-example-5",
    "href": "lectures/register_allocation.html#running-example-5",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#running-example-6",
    "href": "lectures/register_allocation.html#running-example-6",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#running-example-7",
    "href": "lectures/register_allocation.html#running-example-7",
    "title": "Register Allocation",
    "section": "running example",
    "text": "running example",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#coloring",
    "href": "lectures/register_allocation.html#coloring",
    "title": "Register Allocation",
    "section": "coloring",
    "text": "coloring\nonce we have an order (the reverse order above), we can greedy color the nodes. When we get to the n’th node, all the neighbors are in a clique and are colored\nall nodes in the clique need different colors\nIn a chordal graph the size of the largest clique equals the chromatic number\nif we find the point in the program with max live variables, we know the chromatic number",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#spilling",
    "href": "lectures/register_allocation.html#spilling",
    "title": "Register Allocation",
    "section": "spilling",
    "text": "spilling\nif we ever have a program point where the number of live variables is &gt; MaxRegs we will have to spill - so do it here",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#spilling-1",
    "href": "lectures/register_allocation.html#spilling-1",
    "title": "Register Allocation",
    "section": "spilling",
    "text": "spilling",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#coalescing",
    "href": "lectures/register_allocation.html#coalescing",
    "title": "Register Allocation",
    "section": "coalescing",
    "text": "coalescing\nif we assign both sides of a copy to the same register, we can eliminate the copy.\ninput: L list of copy instructions, G=(V,E), K\noutput: updated graph G'\n\nG' = G\nfor all x=y in L\n   sx is the set of colors in the neighborhood of x\n   sy is the set of colors in the neighborood of y\n   let c be a color &lt; K that not in either set \n   add xy a new node xy is ajacent to all node in the union of neighborhoods \n   remove x and y from G'\nxy is a merge of x and y",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#how-do-we-know-that-ssa-graphs-are-chordal",
    "href": "lectures/register_allocation.html#how-do-we-know-that-ssa-graphs-are-chordal",
    "title": "Register Allocation",
    "section": "how do we know that ssa graphs are chordal",
    "text": "how do we know that ssa graphs are chordal",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "lectures/register_allocation.html#dominance-and-interference-thm-1",
    "href": "lectures/register_allocation.html#dominance-and-interference-thm-1",
    "title": "Register Allocation",
    "section": "dominance and interference Thm 1",
    "text": "dominance and interference Thm 1\nIn a strict ssa form the definition of a variable dominates all the uses\nlemma1 : if two variables interfere then the def of one dominates the def of the other\nlemma2 if two variables a and b interfere and Da &lt; Db, then a is live at Db\nlemma3 if u,v,w are variables u-v interfere and v-w interfere and u-w do not if Du &lt; Dv then Dv &lt; Dw\nthm: the interference graph of an ssa form program is chordal",
    "crumbs": [
      "EECS 7398",
      "Lectures",
      "Register Allocation"
    ]
  },
  {
    "objectID": "weekly.html",
    "href": "weekly.html",
    "title": "EECS7398 Weekly Schedule fa 2024",
    "section": "",
    "text": "Since the is the first time this course is offered.\nThis is a tentative schedule.\nThe papers listed here are suggestions, if there is a different paper you would like to present, send me a link so that I can approve it.\nEither 1 or 2 people can sign up for a paper. Once everyone has done so, I’ll schedule which paper gets which date.\n\n\n\n\n\n\n\n\n\n\nsession\nDate\ntopic\ndue\ndiscussions\n\n\n\n\n1\nFriday Sept 6\nCompiler Overview\n\ndiscussion\n\n\n\n\nPerformance Measurement-1\n\n\n\n\n2\nTuesday Sept 10\nPerformance Measurement-2\n\ndiscussion\n\n\n3\nFriday Sept 13\nRepresenting programs\nhw0\ndiscussion\n\n\n\n\nBril\n\ndiscussion\n\n\n4\nTuesday Sept 17\nLocal analysis and optimization\n\ndiscussion\n\n\n5\nFriday Sept 20\nLocal Value Numbering\nhw1\n\n\n\n6\nTuesday Sept 24\nlvn + data flow\n\n\n\n\n7\nFriday Sept 27\nData flow continued\n\n\n\n\n8\nTuesday Oct 1\nData flow + graphs\n\n\n\n\n9\nFriday Oct 4\nStatic single assignment\n\n\n\n\n10\nTuesday Oct 8\nregister allocation\n\n\n\n\n11\nFriday Oct 11\nNorm unavailable - guest lecture over zoom\nhw3\n\n\n\n\nguest lecture\n\n\n\n\n\n\n\n\n\n\n12\nTuesday Oct 15\nreading\n\n\n\n\n\nLeader: Aymane Jerari The MLIR Transform Dialect\n\n\n\n\n\n\n\n\n\n\n13\nFriday Oct 18\nGPU Compilers\nhw4\n\n\n\n14\nTuesday Oct 22\nLLVM\n\n\n\n\n\nproject proposal due\n\n\n\n\n\n\n\n\n\n\n15\nFriday Oct 25\nreading\n\n\n\n\n\nLeader: Oscar Kellner Retargeting and Respecializing GPU Workloads for Performance Portability\n\n\n\n\n\n\n\n\n\n\n16\nTuesday Oct 29\nreading\n\n\n\n\n\nLeader: Sana Anvari Large Language Models for Compiler Optimization\n\n\n\n\n\n\n\n\n\n\n17\nFriday Nov 1\ninstruction scheduling\nhw5\n\n\n\n18\nTuesday Nov 5\nreading\n\n\n\n\n\nLeader: Qucheng Jiang An MLIR-based Compiler Flow for System-Level Design and Hardware Acceleration\n\n\n\n\n\n\n\n\n\n\n19\nFriday Nov 8\ndivergence\n\n\n\n\n20\nTuesday Nov 12\nreading\n\n\n\n\n\nLeader: Yashaswini Makaram ProGraML: Graph-based Deep Learning for Program Optimization and Analysis\n\n\n\n\n\n\n\n\n\n\n21\nFriday Nov 15\nDynamic compilers 1\n\n\n\n\n22\nTuesday Nov 19\nreading\n\n\n\n\n\nLeaders: Rohit Anandakumar and Sharmila Sivalingam Generating GPU Compiler Heuristics using Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n23\nFriday Nov 22\nDynamic compilers\n\n\n\n\n24\nTuesday Nov 26\nreading\n\n\n\n\n\nLeader: Michael Maurer Energy-Aware Tile Size Selection for Affine Programs on GPUs\n\n\n\n\n\n\n\n\n\n\nNov 29\nThanksgiving\n\n\n\n\n\n26\nTuesday Dec 3\nreading\n\n\n\n\n\nLeader: Matin Raayai: Dr.Jit: A Just-In-Time Compiler for Differentiable Rendering\n\n\n\n\n\n\n\n\n\n\n27\nFriday Dec 6\nai in compilers\n\n\n\n\n\nPapers\nextra topics\nra checking, add more about ai\n\nextra papers\nTVM: An Automated End-to-End Optimizing Compiler for Deep Learning](https://www.usenix.org/conference/osdi18/presentation/chen) paper\nleader: SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly\nLeader:ACPO: AI-Enabled Compiler-Driven Program Optimization\nleader: Adaptive Online Context-Sensitive Inlining\nleader: Threaded Code Variations and Optimizations\nleader:Learning Compiler Pass Orders using Coreset and Normalized Value Prediction\nleader: Learning to Optimize Tensor Programs\nleader: End-to-end Deep Learning of Optimization Heuristics\nleader: Compiler Fuzzing through Deep Learning\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Weekly Schedule"
    ]
  },
  {
    "objectID": "blogs/samples/junk.html",
    "href": "blogs/samples/junk.html",
    "title": "sample blog",
    "section": "",
    "text": "this is a sample blog!\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Samples",
      "sample blog"
    ]
  },
  {
    "objectID": "blogs/Sana/10-11-2024-HW3-SanaTaghipourAnvari.html",
    "href": "blogs/Sana/10-11-2024-HW3-SanaTaghipourAnvari.html",
    "title": "Homework3 - data flow",
    "section": "",
    "text": "Link for the code: cfgnode",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework3 - data flow"
    ]
  },
  {
    "objectID": "blogs/Sana/10-11-2024-HW3-SanaTaghipourAnvari.html#explanation-of-the-code",
    "href": "blogs/Sana/10-11-2024-HW3-SanaTaghipourAnvari.html#explanation-of-the-code",
    "title": "Homework3 - data flow",
    "section": "Explanation of the code",
    "text": "Explanation of the code\nI implemented a generic data flow analysis framework that supports multiple analyses by designing a reusable structure for different types of data flow problems. The code includes:\nThis code implements a liveness analysis tool for Bril programs. It builds a control flow graph (CFG) from the program’s instructions, computes the in and out sets for each basic block based on variable usage (using the gen and kill sets), and then prints the liveness information for each block. The analysis helps track which variables are live at various points in the program.\n\nimport json\nimport sys\nfrom collections import defaultdict\n\nclass BrilLivenessAnalyzer:\n    def __init__(self):\n        self.cfg = defaultdict(lambda: {'instrs': [], 'succ': set(), 'pred': set()})\n        self.live_in = defaultdict(set)\n        self.live_out = defaultdict(set)\n\n    def analyze(self, program):\n        for function in program['functions']:\n            self.analyze_function(function)\n\n    def analyze_function(self, function):\n        self.build_cfg(function['instrs'])\n        self.compute_liveness()\n        self.print_results()\n\n    def build_cfg(self, instrs):\n        self.cfg.clear()\n        current_block = 'entry'\n\n        for i, instr in enumerate(instrs):\n            if 'label' in instr:\n                current_block = instr['label']\n            self.cfg[current_block]['instrs'].append(instr)\n\n            if instr.get('op') in ['jmp', 'br']:\n                if instr['op'] == 'jmp':\n                    target = instr['labels'][0]\n                    self.cfg[current_block]['succ'].add(target)\n                    self.cfg[target]['pred'].add(current_block)\n                elif instr['op'] == 'br':\n                    for label in instr['labels']:\n                        self.cfg[current_block]['succ'].add(label)\n                        self.cfg[label]['pred'].add(current_block)\n            elif instr.get('op') == 'ret':\n                pass \n            else:\n                if i + 1 &lt; len(instrs) and 'label' in instrs[i + 1]:\n                    next_block = instrs[i + 1]['label']\n                    self.cfg[current_block]['succ'].add(next_block)\n                    self.cfg[next_block]['pred'].add(current_block)\n\n    def compute_liveness(self):\n        changed = True\n        while changed:\n            changed = False\n            for block in self.cfg:\n                old_in = self.live_in[block].copy()\n                old_out = self.live_out[block].copy()\n\n                kill = set()\n                gen = set()\n\n                for instr in self.cfg[block]['instrs']:\n                    if 'dest' in instr:\n                        kill.add(instr['dest'])  \n                    if 'args' in instr:\n                        for arg in instr['args']:\n                            if arg not in kill:\n                                gen.add(arg)  \n\n                #  OUT{P} = Union of IN{Psuccessor}\n                self.live_out[block] = set()\n                for succ in self.cfg[block]['succ']:\n                    self.live_out[block] |= self.live_in[succ]\n\n                #  IN{P} = (OUT{P} - Kill{P}) U Gen{P}\n                self.live_in[block] = (self.live_out[block] - kill) | gen\n\n                if old_in != self.live_in[block] or old_out != self.live_out[block]:\n                    changed = True\n\n        self.live_in['entry'] = set()\n\n    def print_results(self):\n        for block in self.cfg:\n            print(f\"Block: {block}\")\n            print(f\"  In:  {sorted(self.live_in[block])}\")\n            print(f\"  Out: {sorted(self.live_out[block])}\")\n            print()\n\ndef main():\n    json_input = '''\n    {\n      \"functions\": [\n        {\n          \"name\": \"main\",\n          \"instrs\": [\n            { \"op\": \"const\", \"type\": \"int\", \"dest\": \"a\", \"value\": 3 },\n            { \"op\": \"const\", \"type\": \"int\", \"dest\": \"b\", \"value\": 5 },\n            { \"op\": \"const\", \"type\": \"int\", \"dest\": \"d\", \"value\": 4 },\n            { \"op\": \"const\", \"type\": \"int\", \"dest\": \"x\", \"value\": 100 },\n            { \"op\": \"br\", \"args\": [\"gt\", \"a\", \"b\"], \"labels\": [\"b2\", \"b3\"] },\n\n            { \"label\": \"b2\" },\n            { \"op\": \"add\", \"type\": \"int\", \"dest\": \"c\", \"args\": [\"a\", \"b\"] },\n            { \"op\": \"const\", \"type\": \"int\", \"dest\": \"d\", \"value\": 2 },\n            { \"op\": \"jmp\", \"labels\": [\"done\"] },\n\n            { \"label\": \"b3\" },\n            { \"op\": \"const\", \"type\": \"int\", \"dest\": \"c\", \"value\": 4 },\n\n            { \"label\": \"done\" },\n            { \"op\": \"mul\", \"type\": \"int\", \"dest\": \"result\", \"args\": [\"b\", \"d\"] },\n            { \"op\": \"add\", \"type\": \"int\", \"dest\": \"result\", \"args\": [\"result\", \"c\"] },\n            { \"op\": \"ret\", \"args\": [\"result\"] }\n          ]\n        }\n      ]\n    }\n    '''\n\n    program = json.loads(json_input)\n    \n    analyzer = BrilLivenessAnalyzer()\n    analyzer.analyze(program)\n\nif __name__ == \"__main__\":\n    main()\n\nBlock: entry\n  In:  []\n  Out: ['a', 'b', 'd']\n\nBlock: b2\n  In:  ['a', 'b']\n  Out: ['b', 'c', 'd']\n\nBlock: b3\n  In:  ['b', 'd']\n  Out: ['b', 'c', 'd']\n\nBlock: done\n  In:  ['b', 'c', 'd']\n  Out: []",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework3 - data flow"
    ]
  },
  {
    "objectID": "blogs/Sana/10-11-2024-HW3-SanaTaghipourAnvari.html#how-we-testing-our-implementation",
    "href": "blogs/Sana/10-11-2024-HW3-SanaTaghipourAnvari.html#how-we-testing-our-implementation",
    "title": "Homework3 - data flow",
    "section": "How we testing our implementation",
    "text": "How we testing our implementation\n# Bril Code (Block by Block)\n\n# Block: entry\nconst a: int = 3;\nconst b: int = 5;\nconst d: int = 4;\nconst x: int = 100;\nbr gt a b .b2 .b3;\n\n# Block: b2\nb2:\n  add c: int = a + b;\n  const d: int = 2;\n  jmp .done;\n\n# Block: b3\nb3:\n  const c: int = 4;\n\n# Block: done\ndone:\n  mul result: int = b * d;\n  add result: int = result + c;\n  ret result;",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework3 - data flow"
    ]
  },
  {
    "objectID": "blogs/Sana/10-11-2024-HW3-SanaTaghipourAnvari.html#conclusion",
    "href": "blogs/Sana/10-11-2024-HW3-SanaTaghipourAnvari.html#conclusion",
    "title": "Homework3 - data flow",
    "section": "Conclusion:",
    "text": "Conclusion:\nThe results generated by the liveness analysis are correct based on the Bril code provided. Each block correctly identifies the variables that are live before and after it, based on the definitions and uses of those variables within the block and across the program.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework3 - data flow"
    ]
  },
  {
    "objectID": "blogs/Sana/10-11-2024-HW3-SanaTaghipourAnvari.html#hardest-part-of-the-task-and-how-we-addressed-it",
    "href": "blogs/Sana/10-11-2024-HW3-SanaTaghipourAnvari.html#hardest-part-of-the-task-and-how-we-addressed-it",
    "title": "Homework3 - data flow",
    "section": "Hardest Part of the Task and How We Addressed It",
    "text": "Hardest Part of the Task and How We Addressed It\nThe hardest part was ensuring that the liveness analysis results were correctly generalized without hardcoding specific variables. The challenge was to accurately compute the gen and kill sets for each block and propagate liveness across blocks.\n\nHow We Addressed It:\n\nGeneralized Propagation: We avoided manual exclusions and relied on computing gen and kill sets to ensure only the necessary variables appeared in the In and Out sets.\nFixed-Point Iteration: Liveness information was iteratively propagated until no changes occurred, ensuring accurate results.\nValidation: We compared the results for each block with expected outcomes to ensure correctness.\n\nThis approach ensured a generalized and correct liveness analysis.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework3 - data flow"
    ]
  },
  {
    "objectID": "blogs/Sana/11-1-2024-HW5-SanaTaghipourAnvari.html",
    "href": "blogs/Sana/11-1-2024-HW5-SanaTaghipourAnvari.html",
    "title": "Homework5 - LLVM",
    "section": "",
    "text": "link for the implementation: llvm-pass-allocsize",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework5 - LLVM"
    ]
  },
  {
    "objectID": "blogs/Sana/11-1-2024-HW5-SanaTaghipourAnvari.html#explanation-of-the-code",
    "href": "blogs/Sana/11-1-2024-HW5-SanaTaghipourAnvari.html#explanation-of-the-code",
    "title": "Homework5 - LLVM",
    "section": "Explanation of the code",
    "text": "Explanation of the code\nFor this homework, I tried to implement AllocSizePass which is an LLVM pass that identifies alloca instructions within a function (representing stack allocations) and inserts a runtime printf statement after each allocation. This printf call outputs the size of each allocation in bytes, allowing users to observe the memory usage of each stack allocation during program execution. The pass traverses each function in the LLVM module, calculating the size of each allocation based on the data layout, then injects instrumentation code to print these sizes.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework5 - LLVM"
    ]
  },
  {
    "objectID": "blogs/Sana/11-1-2024-HW5-SanaTaghipourAnvari.html#how-it-is-tested",
    "href": "blogs/Sana/11-1-2024-HW5-SanaTaghipourAnvari.html#how-it-is-tested",
    "title": "Homework5 - LLVM",
    "section": "How It Is Tested",
    "text": "How It Is Tested\nThe pass was tested by:\n\nCompiling a C Program: A C program with multiple allocations was compiled with the pass plugin enabled, which allowed the pass to analyze and instrument the code.\nRuntime Execution: After compilation, the generated binary was executed. The output was inspected for runtime printf statements that reported the size of each allocation. This confirmed that the pass inserted the necessary instrumentation and altered the program’s runtime behavior as expected. The expected output included Allocation size: X bytes statements printed for each allocation, verifying that the pass correctly identified and instrumented allocations.\n\n#include &lt;stdio.h&gt;\n\nint calculate(int a, int b) {\n    int result;\n    if (a &gt; b) {\n        result = a - b;\n    } else {\n        result = a + b;\n    }\n    return result;\n}\n\nint main() {\n    int x[10];\n    int sum = 0;\n\n    for (int i = 0; i &lt; 10; i++) {\n        x[i] = i * 2;\n        sum += x[i];\n    }\n\n    int result1 = calculate(sum, 15);\n    int result2 = calculate(sum, 25);\n\n    printf(\"Result 1: %d\\n\", result1);\n    printf(\"Result 2: %d\\n\", result2);\n\n    return 0;\n}\nOutput:\nclang -fpass-plugin=build/alloc_size_pass.dylib test.c -o test_alloc_out \nFunction: calculate\n  Basic Block: %2\n    Allocation of type i32 with size: 4 bytes\n    Allocation of type i32 with size: 4 bytes\n    Allocation of type i32 with size: 4 bytes\n  Basic Block: %9\n  Basic Block: %13\n  Basic Block: %17\nFunction: main\n  Basic Block: %0\n    Allocation of type i32 with size: 4 bytes\n    Allocation of type [10 x i32] with size: 40 bytes\n    Allocation of type i32 with size: 4 bytes\n    Allocation of type i32 with size: 4 bytes\n    Allocation of type i32 with size: 4 bytes\n    Allocation of type i32 with size: 4 bytes\n  Basic Block: %7\n  Basic Block: %10\n  Basic Block: %22\n  Basic Block: %25\nFunction: printf\n./test_alloc_out \nAllocation size: 4 bytes\nAllocation size: 40 bytes\nAllocation size: 4 bytes\nAllocation size: 4 bytes\nAllocation size: 4 bytes\nAllocation size: 4 bytes\nAllocation size: 4 bytes\nAllocation size: 4 bytes\nAllocation size: 4 bytes\nAllocation size: 4 bytes\nAllocation size: 4 bytes\nAllocation size: 4 bytes\nResult 1: 75\nResult 2: 65",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework5 - LLVM"
    ]
  },
  {
    "objectID": "blogs/Sana/11-1-2024-HW5-SanaTaghipourAnvari.html#what-was-challenging-during-implementation",
    "href": "blogs/Sana/11-1-2024-HW5-SanaTaghipourAnvari.html#what-was-challenging-during-implementation",
    "title": "Homework5 - LLVM",
    "section": "What Was Challenging During Implementation",
    "text": "What Was Challenging During Implementation\nAdding a printf call required defining printf in the LLVM IR, creating or referencing a global format string, and ensuring correct argument types. Missteps in the function type, format string handling, or argument casting could lead to compilation or runtime errors. Solution: I used getOrInsertFunction to declare printf, which ensured that the function was only added if it didn’t already exist. To handle arguments, I used IRBuilder for consistent type casting and format string handling, carefully matching printf’s arguments in LLVM IR.\nThis project specifically targets alloca instructions, if we want the pass to also detect other types of allocations, we should probably identify calls to other available allocation instructions as well for this to become a comprehensive project.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework5 - LLVM"
    ]
  },
  {
    "objectID": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html",
    "href": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html",
    "title": "Homework2 - local optimizations",
    "section": "",
    "text": "All codes are here: lvn_dce_project",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework2 - local optimizations"
    ]
  },
  {
    "objectID": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#explanation-of-the-code",
    "href": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#explanation-of-the-code",
    "title": "Homework2 - local optimizations",
    "section": "Explanation of the Code",
    "text": "Explanation of the Code\nThe code I wrote for this question implements a trivial dead code elimination (DCE) optimization for Bril programs. It works by iterating through the instructions in reverse order, maintaining a set of used variables. Instructions that define variables not in this set are eliminated. The algorithm adds variables used as arguments to the set and removes variables when their defining instruction is processed. This approach ensures that instructions defining unused variables are removed while preserving the program’s essential structure and functionality.\nOutput of the code with Example 1 as the input:\n\nimport json\nimport copy\n\n\ndef tdce(func):\n    used_vars = set()\n    instructions = func['instrs']\n    new_instructions = []\n\n    for instr in reversed(instructions):\n        if 'dest' in instr:\n            if instr['dest'] not in used_vars:\n                continue  # skip this instruction as its destination is never used\n            used_vars.remove(instr['dest'])\n        \n        if 'args' in instr:\n            used_vars.update(instr['args'])\n        \n        new_instructions.append(instr)\n    \n    func['instrs'] = list(reversed(new_instructions))\n    return func\n\ndef main(input_str):\n    prog = json.loads(example1)\n    \n    for func in prog['functions']:\n        func = tdce(func)\n    \n    return json.dumps(prog, indent=2)\n\n# Example 1 as a JSON string (after bril2json conversion)\nexample1 = '''\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"a\", \"value\": 4 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"b\", \"value\": 2 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"c\", \"value\": 1 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"d\", \"args\": [\"a\", \"b\"] },\n        { \"op\": \"print\", \"args\": [\"d\"] }\n      ]\n    }\n  ]\n}\n'''\n\nprint(\"Original program:\")\nprint(example1)\nprint(\"\\nOptimized program:\")\noptimized_prog = main(example1)\nprint(optimized_prog)\n\nOriginal program:\n\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"a\", \"value\": 4 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"b\", \"value\": 2 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"c\", \"value\": 1 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"d\", \"args\": [\"a\", \"b\"] },\n        { \"op\": \"print\", \"args\": [\"d\"] }\n      ]\n    }\n  ]\n}\n\n\nOptimized program:\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"a\",\n          \"value\": 4\n        },\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"b\",\n          \"value\": 2\n        },\n        {\n          \"op\": \"add\",\n          \"type\": \"int\",\n          \"dest\": \"d\",\n          \"args\": [\n            \"a\",\n            \"b\"\n          ]\n        },\n        {\n          \"op\": \"print\",\n          \"args\": [\n            \"d\"\n          ]\n        }\n      ]\n    }\n  ]\n}",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework2 - local optimizations"
    ]
  },
  {
    "objectID": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#testing-the-code",
    "href": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#testing-the-code",
    "title": "Homework2 - local optimizations",
    "section": "Testing the Code",
    "text": "Testing the Code\nAs we can see in above output, line 3 ‘c: int = const 1;’ is removed because c is assigned and never used. we can also further test the code with a bigger input:\nexample2:\n@main {\n  # Variables with different usage patterns\n  a: int = const 4;\n  b: int = const 2;\n  c: int = const 1;  # This is dead code\n  d: int = add a b;\n  e: int = add c d;  # 'c' is used here, but 'e' is never used\n  \n  # Reassignment\n  a: int = const 10;\n  a: int = const 200;  # This overwrites the previous 'a'\n  \n  # Prints to mark usage\n  print a;\n  print d;\n}\n\nimport json\nimport copy\n\n\ndef tdce(func):\n    used_vars = set()\n    instructions = func['instrs']\n    new_instructions = []\n\n    for instr in reversed(instructions):\n        if 'dest' in instr:\n            if instr['dest'] not in used_vars:\n                continue  # skip this instruction as its destination is never used\n            used_vars.remove(instr['dest'])\n        \n        if 'args' in instr:\n            used_vars.update(instr['args'])\n        \n        new_instructions.append(instr)\n    \n    func['instrs'] = list(reversed(new_instructions))\n    return func\n\ndef main(input_str):\n    prog = json.loads(example2)\n    \n    for func in prog['functions']:\n        func = tdce(func)\n    \n    return json.dumps(prog, indent=2)\n\n# Example 2 as a JSON string (after bril2json conversion)\nexample2 = '''\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"a\", \"value\": 4 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"b\", \"value\": 2 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"c\", \"value\": 1 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"e\", \"args\": [\"c\", \"d\"] },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"d\", \"args\": [\"a\", \"b\"] },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"a\", \"value\": 10 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"a\", \"value\": 200 },\n        { \"op\": \"print\", \"args\": [\"a\"] },\n        { \"op\": \"print\", \"args\": [\"d\"] }\n      ]\n    }\n  ]\n}\n'''\n\nprint(\"Original program:\")\nprint(example2)\nprint(\"\\nOptimized program:\")\noptimized_prog = main(example2)\nprint(optimized_prog)\n\nOriginal program:\n\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"a\", \"value\": 4 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"b\", \"value\": 2 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"c\", \"value\": 1 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"e\", \"args\": [\"c\", \"d\"] },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"d\", \"args\": [\"a\", \"b\"] },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"a\", \"value\": 10 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"a\", \"value\": 200 },\n        { \"op\": \"print\", \"args\": [\"a\"] },\n        { \"op\": \"print\", \"args\": [\"d\"] }\n      ]\n    }\n  ]\n}\n\n\nOptimized program:\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"a\",\n          \"value\": 4\n        },\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"b\",\n          \"value\": 2\n        },\n        {\n          \"op\": \"add\",\n          \"type\": \"int\",\n          \"dest\": \"d\",\n          \"args\": [\n            \"a\",\n            \"b\"\n          ]\n        },\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"a\",\n          \"value\": 200\n        },\n        {\n          \"op\": \"print\",\n          \"args\": [\n            \"a\"\n          ]\n        },\n        {\n          \"op\": \"print\",\n          \"args\": [\n            \"d\"\n          ]\n        }\n      ]\n    }\n  ]\n}\n\n\nAs we expect, we can see that line 3 ise removed, because c is assigned and used in line 5 for defining e but e is never used so these two lines are both dead code, also a is reassigned in line 6 and thus removed.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework2 - local optimizations"
    ]
  },
  {
    "objectID": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#challenges-faced",
    "href": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#challenges-faced",
    "title": "Homework2 - local optimizations",
    "section": "Challenges Faced",
    "text": "Challenges Faced\nOne of the challenges that I can think of in this question, is that for large and complex programs, the reverse iteration and set operations might become a performance bottleneck. Addressing these challenges would involve extending the algorithm, careful consideration of Bril’s semantics, and developing more sophisticated testing strategies.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework2 - local optimizations"
    ]
  },
  {
    "objectID": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#part-2",
    "href": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#part-2",
    "title": "Homework2 - local optimizations",
    "section": "Part 2",
    "text": "Part 2",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework2 - local optimizations"
    ]
  },
  {
    "objectID": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#explanation-of-the-code-1",
    "href": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#explanation-of-the-code-1",
    "title": "Homework2 - local optimizations",
    "section": "Explanation of the Code",
    "text": "Explanation of the Code\nThe lvn code implements Local Value Numbering to optimize Bril programs by eliminating redundant computations within basic blocks. It processes each instruction, assigns value numbers to expressions based on their operations and operands, and maintains tables to track these values. When it encounters a redundant computation, it eliminates it by reusing the previously computed result, effectively reducing the number of instructions and optimizing the code.\nExample input:\n@main() {\n    a: int = const 4;\n    b: int = const 2;\n    sum1: int = add a b;\n    sum2: int = add a b;\n    prod: int = mul sum1 sum2;\n    print prod;\n}\n\nimport json\nimport sys\n\n\ndef lvn(func):\n    new_instrs = []\n    value_table = {}  \n    var_table = {}   \n\n    for instr in func['instrs']:\n        if 'op' in instr:\n            if instr['op'] == 'const':\n                value = instr['value']\n                value_num = get_value_number(value_table, ('const', value))\n                value_table[value_num] = ('const', value, instr['dest'])\n                var_table[instr['dest']] = value_num\n                new_instrs.append(instr)\n            elif instr['op'] == 'print':\n                new_instr = instr.copy()\n                if 'args' in new_instr:\n                    new_instr['args'] = [value_table[var_table[arg]][2] for arg in new_instr['args']]\n                new_instrs.append(new_instr)\n            else:\n                args = [var_table.get(arg, arg) for arg in instr.get('args', [])]\n                value_num = get_value_number(value_table, (instr['op'], tuple(args)))\n\n                if value_num in value_table:\n                    # Redundant computation found here!\n                    canonical_op, canonical_args, canonical_var = value_table[value_num]\n                    var_table[instr['dest']] = value_num\n                else:\n                    new_instr = instr.copy()\n                    new_instr['args'] = [value_table[arg][2] if arg in value_table else arg for arg in args]\n                    value_table[value_num] = (instr['op'], tuple(args), instr['dest'])\n                    var_table[instr['dest']] = value_num\n                    new_instrs.append(new_instr)\n        else:\n            new_instrs.append(instr)\n\n    func['instrs'] = new_instrs\n    return func\n\n       \ndef get_value_number(value_table, key):\n    for num, (op, args, var) in value_table.items():\n        if op == key[0] and args == key[1]:\n            return num\n    return len(value_table)\n\ndef main():\n    try:\n        bril_input = json.loads(example1)\n        for func in bril_input['functions']:\n            lvn(func)\n        json.dump(bril_input, sys.stdout, indent=2)\n        sys.stdout.flush()\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\", file=sys.stderr)\n        sys.exit(1)\n\n\n# Example 1 as a JSON string (after bril2json conversion)\nexample1 = '''\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"a\", \"value\": 4 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"b\", \"value\": 2 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"sum1\", \"args\": [\"a\", \"b\"] },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"sum2\", \"args\": [\"a\", \"b\"] },\n        { \"op\": \"mul\", \"type\": \"int\", \"dest\": \"prod\", \"args\": [\"sum1\", \"sum2\"] },\n        { \"op\": \"print\", \"args\": [\"prod\"] }\n      ]\n    }\n  ]\n}\n\n'''\n\nif __name__ == '__main__':\n    main()\n\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"a\",\n          \"value\": 4\n        },\n        {\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"dest\": \"b\",\n          \"value\": 2\n        },\n        {\n          \"op\": \"add\",\n          \"type\": \"int\",\n          \"dest\": \"sum1\",\n          \"args\": [\n            \"a\",\n            \"b\"\n          ]\n        },\n        {\n          \"op\": \"mul\",\n          \"type\": \"int\",\n          \"dest\": \"prod\",\n          \"args\": [\n            \"sum1\",\n            \"sum1\"\n          ]\n        },\n        {\n          \"op\": \"print\",\n          \"args\": [\n            \"prod\"\n          ]\n        }\n      ]\n    }\n  ]\n}\n\n\nAs we can see in the output above, line 5 has been changed to prod: int = mul sum1 sum1;",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework2 - local optimizations"
    ]
  },
  {
    "objectID": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#testing-the-code-1",
    "href": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#testing-the-code-1",
    "title": "Homework2 - local optimizations",
    "section": "Testing the Code",
    "text": "Testing the Code\nFor testing the correctness of the code, I used brench.py code in bril, and for that we need to have a brench.toml file (or a configuration file for brench.py) configures the Brench tool to run a Bril benchmark through four different pipelines: baseline (no optimization), DCE only, LVN only, and LVN followed by DCE. It then extracts the total number of dynamic instructions executed for each run, allowing us to compare the effectiveness of these optimizations. We use brench.toml with python3 ../brench/brench.py brench.toml &gt; results.csv\nexample test:\n@main {\n    a: int = const 1;\n    b: int = const 2;\n\n    c: int = add a b;\n    d: int = add a b;    # Redundant computation\n    e: int = add b a;    # Redundant due to commutativity\n    f: int = mul c d;\n    g: int = mul c e;    # Redundant computation\n\n    sum1: int = add a b; # Same as c, d, and e\n    sum2: int = add a b; # Same as c, d, e, and sum1\n    prod: int = mul sum1 sum2; # Uses two identical values\n\n    h: int = sub f g;    # Dead code: computed but never used\n    i: int = add a a;    # Dead code: computed but never used\n\n    print f;\n    print g;\n    print prod;\n}\nresults.csv:\nbenchmark,run,result\ntest_lvn,baseline,15\ntest_lvn,dce,13\ntest_lvn,lvn,11\ntest_lvn,lvn_dce,9\nBased on this analysis, let’s count the instructions that should remain after each optimization:\nBaseline: 15 instructions (all original instructions)\nDCE: 13 instructions (removes h and i)\nLVN: 11 instructions (keeps a, b, c, f, prod, and the three print statements)\nLVN + DCE: 9 instructions (same as LVN, but also removes h and i)\nTherefore, We can confirm the correctness of the implementations.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework2 - local optimizations"
    ]
  },
  {
    "objectID": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#challenges",
    "href": "blogs/Sana/09-27-2024-HW2-SanaTaghipourAnvari.html#challenges",
    "title": "Homework2 - local optimizations",
    "section": "Challenges",
    "text": "Challenges\nThe main challenges in lvn.py include handling commutative operations (like recognizing that add a b and add b a are equivalent) and ensuring that value numbering accurately tracks and replaces redundant computations without altering the program’s correct behavior.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sana",
      "Homework2 - local optimizations"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html",
    "title": "Paper Presentation",
    "section": "",
    "text": "Context: GPU compilers optimize programs for GPU hardware.\nProblem: Manual creation of heuristic rules is labor-intensive and requires experts.\nSolution Introduced: A new framework uses off-policy deep reinforcement learning to automate heuristic rule generation.\nObjective: Enhance frame rates of graphics applications.\nResilience: The new heuristics remain effective through compiler updates, reducing the need for frequent retraining.\nResults: The framework matches or exceeds traditional heuristics in 98% of benchmarks, improving frame rates by 1.6% to 15.8%.\n\n\n\n\n\nLet’s break down the concepts of “black box” and “glass box” paradigms in compiler autotuning frameworks:\n\n“Black Box” Paradigm:\n\nDefinition: In this approach, optimization is done from the outside. The framework uses pragmas (directives) and optimization flags to tweak the code for better performance.\nLearning Agent: The agent responsible for learning and improving the performance (like a human expert or a computational model) is separate from the compiler itself.\nLimitations: When new code is introduced, this external learning agent might not be available or integrated with the compiler, which can limit how well-tuned the optimizations are.\n\n“Glass Box” Paradigm:\n\nDefinition: This approach integrates the learning agent directly into the compiler. The agent has visibility into the internal workings of the compiler and can interact with the code more deeply.\nAdvantages: Because the learning agent is part of the compiler, it can continue to optimize new code introduced during production. This means a well-trained solution remains effective and adaptable without needing external adjustments.\n\n\n\n\n\n\n\nOptimal Heuristic Settings: Finding the best optimization settings for compilers could theoretically be done by trial-and-error across all possibilities. However, this is impractical due to the extensive time and computational resources required, especially for complex graphics benchmarks.\nChallenges: Graphics benchmarks have long runtimes. Production compilers are frequently updated, making manual tuning continuously laborious.\nStandard Approaches:\n\nExpert-Driven Autotuning: Heuristics (optimization rules) are hand-tuned by experts over selected benchmarks. This requires significant expertise and might miss complex patterns.\nMachine Learning-Based Autotuning: Machine learning can automatically learn complex, non-linear optimization functions in high-dimensional spaces.\n\nIdeal for Compiler Autotuning: It can handle the non-linear nature and local minima challenges of optimizing heuristics across different hardware.\nBenefits of ML-Based Autotuning: ML models can generalize better to new programs and can be re-tuned more quickly for new hardware with a well-labeled dataset.\n“Glass Box” Framework: In this approach, the ML models that are learned are integrated within the compiler itself, allowing them to control the optimizations directly.\n\n\n\n\n\nShaders and Their Role:\n\nShaders: Programs in graphics applications that render frames by projecting 3D shapes onto a 2D display and determining each pixel’s color and opacity.\nLanguages and Resources: Written in high-level languages, using resources like multi-dimensional arrays.\n\nCompilation Process:\n\nFront-End Compiler: Converts shaders into device-independent byte code.\nDriver Translation: Translates byte code into machine-dependent byte code.\nBack-End Compiler: Converts byte code into the instruction set architecture (ISA) specific to the GPU.\n\nExecution: ISA and dynamic resource information are combined and sent to the GPU for execution. Multiple shader pipelines can run asynchronously.\nChallenges and Heuristics:\n\nDynamic Information: Efforts to use dynamic information for optimization often fail as the back-end compiler lacks resource knowledge during compile time.\nSimplified Heuristics: Leads to assumptions that speeding up individual programs will improve the entire application.\n\n\n\n\n\nFocus on Supervised Learning: Most machine learning (ML)-based autotuning frameworks use supervised learning to create predictive models based on labeled data.\nPerformance and Training: Supervised learning has achieved top performance in compiler autotuning by using inputs derived from the program code and hardware characteristics, and labels based on performance measurements like execution times or frame rates.\nQuality of Training Data: The success of supervised learning depends heavily on the quality of the training dataset. Poor-quality or corrupted data can lead to models that don’t perform well in real-world scenarios.\nChallenges in GPU Compiler Autotuning: For real-world graphics applications, obtaining accurate performance labels is challenging due to the non-deterministic and multi-threaded nature of GPU executions, making performance measurement computationally intensive and often impractical.\n\n\n\n\n\nReinforcement Learning Basics: Unlike supervised learning (SL), RL doesn’t need pre-determined labels. Instead, it uses a reward signal from the environment.\nPolicy: The decision function that maps states to actions. In deep reinforcement learning (DRL), this is modeled using a deep neural network (DNN).\n\nObjective: To learn a policy (π) that maximizes cumulative expected rewards over a series of states, known as a trajectory (τ).\n\nTraining Process: RL involves trial-and-error, with reward signals guiding the updates to parameters. The “state, action, reward” tuples are generated through interactions between the policy and the environment, forming a feedback loop.\nApplications: RL is typically used for sequential decision-making problems but can also achieve high performance in tasks like classification or detection. Unlike SL, which trains on large static datasets, RL collects data dynamically throughout the training process.\nCompiler Autotuning with DRL:\n\nPrevious works have applied DRL to compiler autotuning.\nStates: Derived from characteristics of program code.\nActions: Applied code optimizations or heuristic settings.\nRewards: Based on performance measurements.\n\n\n\n\n\nWhat is Q-Learning?: Q-Learning is a reinforcement learning (RL) strategy that helps an agent learn how to act optimally by evaluating the quality of different actions in different states, known as state-action pairs.\nQ-Table: In discrete action spaces, the evaluations (or values) of state-action pairs are stored in a Q-table. Each entry represents the expected cumulative reward (discounted by a factor γ) for taking a certain action in a certain state and following a specific policy π.\nOptimal Policy: The goal is to find the optimal policy (π*) that maximizes expected cumulative reward over time. This is achieved by updating the Q-table through trial-and-error interactions with the environment. When following the optimal policy, the agent chooses actions that maximize the expected reward from an initial state s0.\nConvergence: Q-Learning is proven to converge to the optimal values of state-action pairs with probability 1, given that all state-action pairs are sampled repeatedly. Over time, the Q-table reflects the best possible actions for every state.\nExpected Cumulative Discounted Reward: The values in the Q-table, denoted as Q*(s, a), represent the expected cumulative reward, discounted by time, when following the optimal policy through a trajectory τ of size T.\n\n\n\n\n\n\nOn-Policy DRL:\n- Standard DRL algorithms require frequent interaction with a stationary environment, using collected data only once to update internal parameters, making them sample inefficient. - Research shows that on-policy algorithms are more simulation-bound compared to off-policy counterparts. - Relying solely on on-policy DRL can bottleneck GPU compiler autotuning due to long graphic benchmark runtimes and frequent compiler updates.\nOff-Policy DRL:\n- Off-policy DRL learns from previously collected data without needing frequent interaction with the environment, suitable for GPU compiler auto-tuning. - This approach allows decoupling data collection and model training, leveraging existing performance automation workflows to prepare offline datasets. - However, off-policy methods face instability without corrective feedback and require careful hyperparameter tuning. - To address these issues, a Q-learning-based strategy with a controlled feedback loop between data collection and model training is proposed.\n\n\n\nThe objective here will be to identify those heuristic settings that achieve the highest possible frame rate improvements for shaders, while deploying stable compiler heuristics in light of frequent code changes.\n\nEnvironment: Non-stationary system: static hardware with evolving compiler revisions over time.\nStates: Extracted from the IR of the shader, which changes according to the compiler updates.\nActions: Heuristic settings used at compile time. Optimal actions result in maximum increases in frame rates.\nRewards: Observed frame rate improvements when performing an action on states.\nPolicy: A DNN-based decision function which is trained to maximize the expected frame rates in a non-stationary environment.\n\n\n\n\nThe author propose a Q-learning-based off-policy RL strategy to generate GPU compiler heuristics.\n\nTraining Objective: Optimize a Q-table to maximize expected frame rate improvements for heuristic actions applied to given states.\n\n• Inference Objective: Fit a DNN-based decision policy to approximate the optimal policy by minimizing divergence between them. - Integration: The trained inference model will serve as a heuristic decision function integrated into the compiler. - Policy Differentiation: During training, iterate on the decision policy while the behavior policy is frozen at inference time for stable deployment.\n\n\n\nThe framework addresses the high cost of GPU graphics benchmarks with a robust and generalizable auto-tuning pipeline comprising three modules in a feedback loop: continuous integration, data collection, and model training.\n\nContinuous Integration (CI): Deploys the behavior policy as a heuristic in the latest production compiler.\nData Collection: Gathers performance metrics and IRs from the graphics benchmarks.\nModel Training: Updates Q-table and trains the decision policy to approximate the optimal policy for maximum expected frame rates across the application suite. Starting with a randomly initialized decision policy, the framework iteratively refines the model to generate stable and efficient GPU compiler heuristics.\n\n\n\n\nContinuous integration means developers’ source code revisions are incorporated into a shared mainline many times a day. For the generated code, this tempo translates to significant changes in a production compiler. Unlike traditional ML-based autotuning based on frozen snapshots of compilers, our approach integrates the behavior policy into a continuously updated compiler.\n\n\n\nThe compiler inference engine applies the behavior policy as a heuristic to the IR before machine-dependent optimizations.\n\nState Analysis: The state, derived from the incoming IR, is analyzed by the model to determine the optimal heuristic setting.\nContinuous Integration: The policy of behavior is compiled into the most recent compiler for each run of performance.\nData Collection: Every trial collects the static features of the IR, applied heuristic settings, and observed frame rates from the benchmark suite.\n\nStatic features extracted pre-inference are used to build the state to apply heuristics in a time and memory-efficient and reproducible manner.\n\n\n\nA stable policy generalizing well relies on a meaningful reward signal that encourages desired behavior. In a multi-threaded environment, the performance of one optimized shader sometimes degrades in others due to shared resource competition. The reward signal is defined by the normalized change in frame rate compared to the global default compiler actions. Given a baseline frame rate F0​ and observed frame rate F, the reward is the relative speed-up, F/F0​. But as compilers improve, the performance measurements from the past become outdated.\n\n\n\nRepresentation of source code for machine learning can be static or dynamic. Dynamic techniques depend on performance counters, which provide a compact summary at runtime, but are expensive to collect and unavailable at compile time. Static techniques, like those in natural language processing, directly extract features from the source code, which is resource-intensive. We extract the features at compile time from the compiler IR in the form of fast, machine-independent features, such as total instructions, basic blocks, and memory operations to meet the constraints of production compilers. This yields a fixed-length feature vector of 44 attributes, consuming merely 176 bytes at 32-bit floating point precision.\n\n\n\n\nOff-Policy DRL Algorithms: Reuse data collected asynchronously, allowing training without bottlenecks from data collection. This is crucial in production systems, where the compiler and data continuously evolve. We implement a Q-learning strategy for stable heuristics in GPU compiler autotuning.\n\n\n\n\n\nQ-Table and Probabilistic Policy:\n\nThe Q-table represents the expected reward for each state-action pair over a set of applications. It simplifies to the expected performance of applying a heuristic action given a shader’s state, modeled as a single time-step Markov Decision Process (MDP).\nThe optimal policy selects the action that maximizes the probability of achieving the best performance for a given state. Alternatively, the policy can be derived using the Boltzmann softmax operator to balance exploration and exploitation, controlled by the temperature parameter.\n\nQ-Table Updates and Staleness Discounting:\n\nAs the compiler evolves, older performance measurements may become stale. Q-table updates include a discount factor, diminishing the weight of older data based on elapsed time.\nFor existing state-action pairs, the Q-value is updated as a weighted average of the prior value and the newly observed reward, modulated by a learning rate. For unseen states, the Q-value initializes to the observed reward.\n\nHyperparameter Tuning and Policy Behavior:\n\nThe temperature parameter in the softmax operator adjusts the trade-off between exploration and exploitation.\nThe learning rate and discount factor are tunable hyperparameters, ensuring adaptability to changing compiler behavior and iterative data collection.\n\n\n\n\n\n\nDynamic Q-Table and Generalization Issue: Compiler updates change the intermediate representation (IR), creating a non-stationary environment (EEE) and expanding the Q-table with new state-action pairs. This makes the Q-table too large and impractical while lacking the ability to generalize to unseen states.\nModel-Based Policy Approximation and Iterative Learning: To address these issues, train decision policy to approximates the optimal decision policy by minimizing the KL divergence between the empirical Q-table policy and the model’s output. This model generalizes to new states and is periodically deployed to guide compiler decisions, with new performance data updating the Q-table in an iterative DRL feedback loop.\n\n\n    Algorithm 1: RL-based GPU Compiler Autotuning\n    Result: (N)\n    Randomly initialize ;\n    Initialize an empty Q-table;\n    while i &lt; numIterations do\n        Copy learned parameters of decision policy to behaviour policy (i);\n        Integrate behaviour policy into the latest compiler;\n        Get states and performance measurements over A;\n        Update (or expand) the Q-table based on the observed performance measurements;\n        Convert the empirical Q-table to probabilities optimal decision policy(sa);\n        Train decision policy(a|s) to approximate optimal decision policy(sa);\n    end\n\n\n\n\n\n\nOff-Policy DRL Algorithms: Reuse data collected asynchronously, allowing training without bottlenecks from data collection. This is crucial in production systems, where the compiler and data continuously evolve. We implement a Q-learning strategy for stable heuristics in GPU compiler autotuning.\n\n\n\n\n\nQ-Table and Probabilistic Policy:\n\nThe Q-table represents the expected reward for each state-action pair over a set of applications. It simplifies to the expected performance of applying a heuristic action given a shader’s state, modeled as a single time-step Markov Decision Process (MDP).\nThe optimal policy selects the action that maximizes the probability of achieving the best performance for a given state. Alternatively, the policy can be derived using the Boltzmann softmax operator to balance exploration and exploitation, controlled by the temperature parameter.\n\nQ-Table Updates and Staleness Discounting:\n\nAs the compiler evolves, older performance measurements may become stale. Q-table updates include a discount factor, diminishing the weight of older data based on elapsed time.\nFor existing state-action pairs, the Q-value is updated as a weighted average of the prior value and the newly observed reward, modulated by a learning rate. For unseen states, the Q-value initializes to the observed reward.\n\nHyperparameter Tuning and Policy Behavior:\n\nThe temperature parameter in the softmax operator adjusts the trade-off between exploration and exploitation.\nThe learning rate and discount factor are tunable hyperparameters, ensuring adaptability to changing compiler behavior and iterative data collection. ### Adapting to Production Compiler Development\n\n\n\nDynamic Environment: Production-level compilers often change rapidly, resulting in a dynamic and non-stationary environment, with constantly evolving conditions and code.\nNon-Stationary Environment: Defined by target hardware and continuously updated production compilers, leading to variability and complexity.\nImpact on IR Instructions: Percentage change in intermediate representation (IR) instructions for a shader can vary by up to 50% over a year, indicating significant code structure changes.\nGrowth of Q-Table: The rapid pace of software development leads to an increase in the state space, causing the Q-table to grow. To manage growth and memory overhead, a DNN decision policy (πθ) is trained to approximate the optimal policy (π∗).\nPolicy Deployment: The trained DNN policy (πθ) is periodically deployed as a practical policy (πβ), maintaining an efficient and updated policy without excessive memory requirements of a growing Q-table.\n\n\n\n\nWavefront Sizes: AMD RDNA™ graphics architecture supports two wavefront sizes: wave32 (32 work-items) and wave64 (64 work-items). The optimal wavefront size depends on dynamic shader properties such as divergence and memory access patterns.\nBandwidth Considerations:\n\nWave64: Can offer better performance if there is sufficient bandwidth, allowing for more simultaneous memory accesses without causing cache misses.\nWave32: Reduces bandwidth strain by spreading out memory accesses over time, beneficial when multiple shaders are running concurrently.\n\nDynamic Environment: System bandwidth and other resources are dynamic and shared among concurrently running shaders. The best wavefront size depends on the real-time execution environment, which the compiler doesn’t know at compile time.\n\nRL-Based Autotuning Framework: Improves frame rates by selecting the optimal wavefront size for each shader at compile time.\nState Representation: The state of each shader is represented as a fixed-length vector of static features from the compiler’s intermediate representation (IR).\nAction Space: Limited to two options: wave32 and wave64.\nReward Signal: Based on changes in frame rate compared to default behavior.\n\nPolicy and Implementation: The decision policy (πθ) is a lightweight, 3-layer feed-forward classifier with less than 20 KB of learnable parameters. The learned parameters are periodically integrated into the compiler as the behavior policy (πβ).\nExperimental Results: The RL framework was tested on the AMD Radeon™ 6800 XT and achieved frame rates matching or surpassing 98% of graphics benchmarks. Performance improvements ranged from an average increase of 1.6% to a maximum of 15.8%. The model converged in only 45 iterations per benchmark application. Experiments were conducted using over 150 graphics benchmarks, each with an average of 230 unique shaders.\n\n\n\n\n\nGeneralization: The model needs to generalize well to handle new code generated by the frequently updated compiler IR. A well-trained model should perform effectively without needing frequent updates to its learned parameters.\nStability Metric: Stability is measured using the inverse of the rate of statistically significant regressions. This metric shows how often the model’s performance does not decline significantly. A 1-tailed t-test with a p-value of 5% is used to determine statistical significance.\nShader Wavefront Size Selection: The histogram in Figure 8 illustrates changes in frame rate when the RL-based compiler heuristic is used for selecting shader wavefront size on the AMD Radeon™ RX 5700 XT, demonstrating how the RL-based method impacts performance across various graphics applications.\nStability Over Time: Stability is tracked over a year of production compiler updates without retraining the deployed model. A value of 100% indicates no statistically significant performance regressions in benchmarks when using the DNN heuristic compared to the default compiler behavior.\n\n\n\n\n\nTransfer Learning: Instead of starting from scratch, the researchers use transfer learning to optimize the wavefront execution mode for a different GPU (the AMD Radeon™ RX 5700 XT), utilizing the final Q-table and behavior policy (πβ) from previous experiments as the starting point.\nEfficiency and Performance: With transfer learning, the model needed only 10 iterations over the set of graphics benchmarks to match or surpass previous frame rates in 94.4% of the benchmarks. Improvements included increases of up to 10.3% and an average improvement of 1.5%.\nBenchmarking: Experiments were conducted using the AMD production graphics compiler on over 270 graphics benchmarks, each with an average of 230 unique shaders. Figure 8 in the document provides a histogram of these results, showing the distribution of observed changes in frame rates.\n\n\n\n\n\n\nFramework Development: The authors developed a GPU compiler autotuning framework using off-policy deep reinforcement learning (DRL) to generate heuristics that improve frame rates in graphics applications.\nContinuous Integration and Q-Learning: The framework combines continuous integration (CI) with Q-Learning to find optimal heuristic settings that maximize frame rate improvements across various graphics benchmarks.\nDeployment: By considering the rapid changes in software development, the trained models can be deployed as stable heuristics in evolving production compilers.\nGeneralized Gains: The framework demonstrates generalized performance gains across a large suite of graphics benchmarks and different GPUs.\n\n\n\n\n\nExplore Static Counters and Dynamic Properties: Investigate the relationship between the set of static counters and the dynamic properties that the neural network has learned to account for.\nExtend to Continuous Action Spaces: Aim to extend the framework to domains with continuous action spaces using techniques from deep Q-Learning.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#abstract",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#abstract",
    "title": "Paper Presentation",
    "section": "",
    "text": "Context: GPU compilers optimize programs for GPU hardware.\nProblem: Manual creation of heuristic rules is labor-intensive and requires experts.\nSolution Introduced: A new framework uses off-policy deep reinforcement learning to automate heuristic rule generation.\nObjective: Enhance frame rates of graphics applications.\nResilience: The new heuristics remain effective through compiler updates, reducing the need for frequent retraining.\nResults: The framework matches or exceeds traditional heuristics in 98% of benchmarks, improving frame rates by 1.6% to 15.8%.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#introduction",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#introduction",
    "title": "Paper Presentation",
    "section": "",
    "text": "Let’s break down the concepts of “black box” and “glass box” paradigms in compiler autotuning frameworks:\n\n“Black Box” Paradigm:\n\nDefinition: In this approach, optimization is done from the outside. The framework uses pragmas (directives) and optimization flags to tweak the code for better performance.\nLearning Agent: The agent responsible for learning and improving the performance (like a human expert or a computational model) is separate from the compiler itself.\nLimitations: When new code is introduced, this external learning agent might not be available or integrated with the compiler, which can limit how well-tuned the optimizations are.\n\n“Glass Box” Paradigm:\n\nDefinition: This approach integrates the learning agent directly into the compiler. The agent has visibility into the internal workings of the compiler and can interact with the code more deeply.\nAdvantages: Because the learning agent is part of the compiler, it can continue to optimize new code introduced during production. This means a well-trained solution remains effective and adaptable without needing external adjustments.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#background",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#background",
    "title": "Paper Presentation",
    "section": "",
    "text": "Optimal Heuristic Settings: Finding the best optimization settings for compilers could theoretically be done by trial-and-error across all possibilities. However, this is impractical due to the extensive time and computational resources required, especially for complex graphics benchmarks.\nChallenges: Graphics benchmarks have long runtimes. Production compilers are frequently updated, making manual tuning continuously laborious.\nStandard Approaches:\n\nExpert-Driven Autotuning: Heuristics (optimization rules) are hand-tuned by experts over selected benchmarks. This requires significant expertise and might miss complex patterns.\nMachine Learning-Based Autotuning: Machine learning can automatically learn complex, non-linear optimization functions in high-dimensional spaces.\n\nIdeal for Compiler Autotuning: It can handle the non-linear nature and local minima challenges of optimizing heuristics across different hardware.\nBenefits of ML-Based Autotuning: ML models can generalize better to new programs and can be re-tuned more quickly for new hardware with a well-labeled dataset.\n“Glass Box” Framework: In this approach, the ML models that are learned are integrated within the compiler itself, allowing them to control the optimizations directly.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#production-gpu-compiler-autotuning",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#production-gpu-compiler-autotuning",
    "title": "Paper Presentation",
    "section": "",
    "text": "Shaders and Their Role:\n\nShaders: Programs in graphics applications that render frames by projecting 3D shapes onto a 2D display and determining each pixel’s color and opacity.\nLanguages and Resources: Written in high-level languages, using resources like multi-dimensional arrays.\n\nCompilation Process:\n\nFront-End Compiler: Converts shaders into device-independent byte code.\nDriver Translation: Translates byte code into machine-dependent byte code.\nBack-End Compiler: Converts byte code into the instruction set architecture (ISA) specific to the GPU.\n\nExecution: ISA and dynamic resource information are combined and sent to the GPU for execution. Multiple shader pipelines can run asynchronously.\nChallenges and Heuristics:\n\nDynamic Information: Efforts to use dynamic information for optimization often fail as the back-end compiler lacks resource knowledge during compile time.\nSimplified Heuristics: Leads to assumptions that speeding up individual programs will improve the entire application.\n\n\n\n\n\nFocus on Supervised Learning: Most machine learning (ML)-based autotuning frameworks use supervised learning to create predictive models based on labeled data.\nPerformance and Training: Supervised learning has achieved top performance in compiler autotuning by using inputs derived from the program code and hardware characteristics, and labels based on performance measurements like execution times or frame rates.\nQuality of Training Data: The success of supervised learning depends heavily on the quality of the training dataset. Poor-quality or corrupted data can lead to models that don’t perform well in real-world scenarios.\nChallenges in GPU Compiler Autotuning: For real-world graphics applications, obtaining accurate performance labels is challenging due to the non-deterministic and multi-threaded nature of GPU executions, making performance measurement computationally intensive and often impractical.\n\n\n\n\n\nReinforcement Learning Basics: Unlike supervised learning (SL), RL doesn’t need pre-determined labels. Instead, it uses a reward signal from the environment.\nPolicy: The decision function that maps states to actions. In deep reinforcement learning (DRL), this is modeled using a deep neural network (DNN).\n\nObjective: To learn a policy (π) that maximizes cumulative expected rewards over a series of states, known as a trajectory (τ).\n\nTraining Process: RL involves trial-and-error, with reward signals guiding the updates to parameters. The “state, action, reward” tuples are generated through interactions between the policy and the environment, forming a feedback loop.\nApplications: RL is typically used for sequential decision-making problems but can also achieve high performance in tasks like classification or detection. Unlike SL, which trains on large static datasets, RL collects data dynamically throughout the training process.\nCompiler Autotuning with DRL:\n\nPrevious works have applied DRL to compiler autotuning.\nStates: Derived from characteristics of program code.\nActions: Applied code optimizations or heuristic settings.\nRewards: Based on performance measurements.\n\n\n\n\n\nWhat is Q-Learning?: Q-Learning is a reinforcement learning (RL) strategy that helps an agent learn how to act optimally by evaluating the quality of different actions in different states, known as state-action pairs.\nQ-Table: In discrete action spaces, the evaluations (or values) of state-action pairs are stored in a Q-table. Each entry represents the expected cumulative reward (discounted by a factor γ) for taking a certain action in a certain state and following a specific policy π.\nOptimal Policy: The goal is to find the optimal policy (π*) that maximizes expected cumulative reward over time. This is achieved by updating the Q-table through trial-and-error interactions with the environment. When following the optimal policy, the agent chooses actions that maximize the expected reward from an initial state s0.\nConvergence: Q-Learning is proven to converge to the optimal values of state-action pairs with probability 1, given that all state-action pairs are sampled repeatedly. Over time, the Q-table reflects the best possible actions for every state.\nExpected Cumulative Discounted Reward: The values in the Q-table, denoted as Q*(s, a), represent the expected cumulative reward, discounted by time, when following the optimal policy through a trajectory τ of size T.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#on-policy-vs.-off-policy-drl-for-gpu-compiler-autotuning",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#on-policy-vs.-off-policy-drl-for-gpu-compiler-autotuning",
    "title": "Paper Presentation",
    "section": "",
    "text": "On-Policy DRL:\n- Standard DRL algorithms require frequent interaction with a stationary environment, using collected data only once to update internal parameters, making them sample inefficient. - Research shows that on-policy algorithms are more simulation-bound compared to off-policy counterparts. - Relying solely on on-policy DRL can bottleneck GPU compiler autotuning due to long graphic benchmark runtimes and frequent compiler updates.\nOff-Policy DRL:\n- Off-policy DRL learns from previously collected data without needing frequent interaction with the environment, suitable for GPU compiler auto-tuning. - This approach allows decoupling data collection and model training, leveraging existing performance automation workflows to prepare offline datasets. - However, off-policy methods face instability without corrective feedback and require careful hyperparameter tuning. - To address these issues, a Q-learning-based strategy with a controlled feedback loop between data collection and model training is proposed.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#problem-formulation-for-rl-based-gpu-compiler-autotuning",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#problem-formulation-for-rl-based-gpu-compiler-autotuning",
    "title": "Paper Presentation",
    "section": "",
    "text": "The objective here will be to identify those heuristic settings that achieve the highest possible frame rate improvements for shaders, while deploying stable compiler heuristics in light of frequent code changes.\n\nEnvironment: Non-stationary system: static hardware with evolving compiler revisions over time.\nStates: Extracted from the IR of the shader, which changes according to the compiler updates.\nActions: Heuristic settings used at compile time. Optimal actions result in maximum increases in frame rates.\nRewards: Observed frame rate improvements when performing an action on states.\nPolicy: A DNN-based decision function which is trained to maximize the expected frame rates in a non-stationary environment.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#solution-overview",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#solution-overview",
    "title": "Paper Presentation",
    "section": "",
    "text": "The author propose a Q-learning-based off-policy RL strategy to generate GPU compiler heuristics.\n\nTraining Objective: Optimize a Q-table to maximize expected frame rate improvements for heuristic actions applied to given states.\n\n• Inference Objective: Fit a DNN-based decision policy to approximate the optimal policy by minimizing divergence between them. - Integration: The trained inference model will serve as a heuristic decision function integrated into the compiler. - Policy Differentiation: During training, iterate on the decision policy while the behavior policy is frozen at inference time for stable deployment.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#rl-based-gpu-compiler-auto-tuning",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#rl-based-gpu-compiler-auto-tuning",
    "title": "Paper Presentation",
    "section": "",
    "text": "The framework addresses the high cost of GPU graphics benchmarks with a robust and generalizable auto-tuning pipeline comprising three modules in a feedback loop: continuous integration, data collection, and model training.\n\nContinuous Integration (CI): Deploys the behavior policy as a heuristic in the latest production compiler.\nData Collection: Gathers performance metrics and IRs from the graphics benchmarks.\nModel Training: Updates Q-table and trains the decision policy to approximate the optimal policy for maximum expected frame rates across the application suite. Starting with a randomly initialized decision policy, the framework iteratively refines the model to generate stable and efficient GPU compiler heuristics.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#continuous-integration",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#continuous-integration",
    "title": "Paper Presentation",
    "section": "",
    "text": "Continuous integration means developers’ source code revisions are incorporated into a shared mainline many times a day. For the generated code, this tempo translates to significant changes in a production compiler. Unlike traditional ML-based autotuning based on frozen snapshots of compilers, our approach integrates the behavior policy into a continuously updated compiler.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#inference-engine",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#inference-engine",
    "title": "Paper Presentation",
    "section": "",
    "text": "The compiler inference engine applies the behavior policy as a heuristic to the IR before machine-dependent optimizations.\n\nState Analysis: The state, derived from the incoming IR, is analyzed by the model to determine the optimal heuristic setting.\nContinuous Integration: The policy of behavior is compiled into the most recent compiler for each run of performance.\nData Collection: Every trial collects the static features of the IR, applied heuristic settings, and observed frame rates from the benchmark suite.\n\nStatic features extracted pre-inference are used to build the state to apply heuristics in a time and memory-efficient and reproducible manner.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#robust-reward-signals",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#robust-reward-signals",
    "title": "Paper Presentation",
    "section": "",
    "text": "A stable policy generalizing well relies on a meaningful reward signal that encourages desired behavior. In a multi-threaded environment, the performance of one optimized shader sometimes degrades in others due to shared resource competition. The reward signal is defined by the normalized change in frame rate compared to the global default compiler actions. Given a baseline frame rate F0​ and observed frame rate F, the reward is the relative speed-up, F/F0​. But as compilers improve, the performance measurements from the past become outdated.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#data-collection",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#data-collection",
    "title": "Paper Presentation",
    "section": "",
    "text": "Representation of source code for machine learning can be static or dynamic. Dynamic techniques depend on performance counters, which provide a compact summary at runtime, but are expensive to collect and unavailable at compile time. Static techniques, like those in natural language processing, directly extract features from the source code, which is resource-intensive. We extract the features at compile time from the compiler IR in the form of fast, machine-independent features, such as total instructions, basic blocks, and memory operations to meet the constraints of production compilers. This yields a fixed-length feature vector of 44 attributes, consuming merely 176 bytes at 32-bit floating point precision.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#model-training",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#model-training",
    "title": "Paper Presentation",
    "section": "",
    "text": "Off-Policy DRL Algorithms: Reuse data collected asynchronously, allowing training without bottlenecks from data collection. This is crucial in production systems, where the compiler and data continuously evolve. We implement a Q-learning strategy for stable heuristics in GPU compiler autotuning.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#q-learning-for-gpu-compiler-autotuning",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#q-learning-for-gpu-compiler-autotuning",
    "title": "Paper Presentation",
    "section": "",
    "text": "Q-Table and Probabilistic Policy:\n\nThe Q-table represents the expected reward for each state-action pair over a set of applications. It simplifies to the expected performance of applying a heuristic action given a shader’s state, modeled as a single time-step Markov Decision Process (MDP).\nThe optimal policy selects the action that maximizes the probability of achieving the best performance for a given state. Alternatively, the policy can be derived using the Boltzmann softmax operator to balance exploration and exploitation, controlled by the temperature parameter.\n\nQ-Table Updates and Staleness Discounting:\n\nAs the compiler evolves, older performance measurements may become stale. Q-table updates include a discount factor, diminishing the weight of older data based on elapsed time.\nFor existing state-action pairs, the Q-value is updated as a weighted average of the prior value and the newly observed reward, modulated by a learning rate. For unseen states, the Q-value initializes to the observed reward.\n\nHyperparameter Tuning and Policy Behavior:\n\nThe temperature parameter in the softmax operator adjusts the trade-off between exploration and exploitation.\nThe learning rate and discount factor are tunable hyperparameters, ensuring adaptability to changing compiler behavior and iterative data collection.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#approximate-optimal-policy",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#approximate-optimal-policy",
    "title": "Paper Presentation",
    "section": "",
    "text": "Dynamic Q-Table and Generalization Issue: Compiler updates change the intermediate representation (IR), creating a non-stationary environment (EEE) and expanding the Q-table with new state-action pairs. This makes the Q-table too large and impractical while lacking the ability to generalize to unseen states.\nModel-Based Policy Approximation and Iterative Learning: To address these issues, train decision policy to approximates the optimal decision policy by minimizing the KL divergence between the empirical Q-table policy and the model’s output. This model generalizes to new states and is periodically deployed to guide compiler decisions, with new performance data updating the Q-table in an iterative DRL feedback loop.\n\n\n    Algorithm 1: RL-based GPU Compiler Autotuning\n    Result: (N)\n    Randomly initialize ;\n    Initialize an empty Q-table;\n    while i &lt; numIterations do\n        Copy learned parameters of decision policy to behaviour policy (i);\n        Integrate behaviour policy into the latest compiler;\n        Get states and performance measurements over A;\n        Update (or expand) the Q-table based on the observed performance measurements;\n        Convert the empirical Q-table to probabilities optimal decision policy(sa);\n        Train decision policy(a|s) to approximate optimal decision policy(sa);\n    end",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#model-training-1",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#model-training-1",
    "title": "Paper Presentation",
    "section": "",
    "text": "Off-Policy DRL Algorithms: Reuse data collected asynchronously, allowing training without bottlenecks from data collection. This is crucial in production systems, where the compiler and data continuously evolve. We implement a Q-learning strategy for stable heuristics in GPU compiler autotuning.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#q-learning-for-gpu-compiler-autotuning-1",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#q-learning-for-gpu-compiler-autotuning-1",
    "title": "Paper Presentation",
    "section": "",
    "text": "Q-Table and Probabilistic Policy:\n\nThe Q-table represents the expected reward for each state-action pair over a set of applications. It simplifies to the expected performance of applying a heuristic action given a shader’s state, modeled as a single time-step Markov Decision Process (MDP).\nThe optimal policy selects the action that maximizes the probability of achieving the best performance for a given state. Alternatively, the policy can be derived using the Boltzmann softmax operator to balance exploration and exploitation, controlled by the temperature parameter.\n\nQ-Table Updates and Staleness Discounting:\n\nAs the compiler evolves, older performance measurements may become stale. Q-table updates include a discount factor, diminishing the weight of older data based on elapsed time.\nFor existing state-action pairs, the Q-value is updated as a weighted average of the prior value and the newly observed reward, modulated by a learning rate. For unseen states, the Q-value initializes to the observed reward.\n\nHyperparameter Tuning and Policy Behavior:\n\nThe temperature parameter in the softmax operator adjusts the trade-off between exploration and exploitation.\nThe learning rate and discount factor are tunable hyperparameters, ensuring adaptability to changing compiler behavior and iterative data collection. ### Adapting to Production Compiler Development\n\n\n\nDynamic Environment: Production-level compilers often change rapidly, resulting in a dynamic and non-stationary environment, with constantly evolving conditions and code.\nNon-Stationary Environment: Defined by target hardware and continuously updated production compilers, leading to variability and complexity.\nImpact on IR Instructions: Percentage change in intermediate representation (IR) instructions for a shader can vary by up to 50% over a year, indicating significant code structure changes.\nGrowth of Q-Table: The rapid pace of software development leads to an increase in the state space, causing the Q-table to grow. To manage growth and memory overhead, a DNN decision policy (πθ) is trained to approximate the optimal policy (π∗).\nPolicy Deployment: The trained DNN policy (πθ) is periodically deployed as a practical policy (πβ), maintaining an efficient and updated policy without excessive memory requirements of a growing Q-table.\n\n\n\n\nWavefront Sizes: AMD RDNA™ graphics architecture supports two wavefront sizes: wave32 (32 work-items) and wave64 (64 work-items). The optimal wavefront size depends on dynamic shader properties such as divergence and memory access patterns.\nBandwidth Considerations:\n\nWave64: Can offer better performance if there is sufficient bandwidth, allowing for more simultaneous memory accesses without causing cache misses.\nWave32: Reduces bandwidth strain by spreading out memory accesses over time, beneficial when multiple shaders are running concurrently.\n\nDynamic Environment: System bandwidth and other resources are dynamic and shared among concurrently running shaders. The best wavefront size depends on the real-time execution environment, which the compiler doesn’t know at compile time.\n\nRL-Based Autotuning Framework: Improves frame rates by selecting the optimal wavefront size for each shader at compile time.\nState Representation: The state of each shader is represented as a fixed-length vector of static features from the compiler’s intermediate representation (IR).\nAction Space: Limited to two options: wave32 and wave64.\nReward Signal: Based on changes in frame rate compared to default behavior.\n\nPolicy and Implementation: The decision policy (πθ) is a lightweight, 3-layer feed-forward classifier with less than 20 KB of learnable parameters. The learned parameters are periodically integrated into the compiler as the behavior policy (πβ).\nExperimental Results: The RL framework was tested on the AMD Radeon™ 6800 XT and achieved frame rates matching or surpassing 98% of graphics benchmarks. Performance improvements ranged from an average increase of 1.6% to a maximum of 15.8%. The model converged in only 45 iterations per benchmark application. Experiments were conducted using over 150 graphics benchmarks, each with an average of 230 unique shaders.\n\n\n\n\n\nGeneralization: The model needs to generalize well to handle new code generated by the frequently updated compiler IR. A well-trained model should perform effectively without needing frequent updates to its learned parameters.\nStability Metric: Stability is measured using the inverse of the rate of statistically significant regressions. This metric shows how often the model’s performance does not decline significantly. A 1-tailed t-test with a p-value of 5% is used to determine statistical significance.\nShader Wavefront Size Selection: The histogram in Figure 8 illustrates changes in frame rate when the RL-based compiler heuristic is used for selecting shader wavefront size on the AMD Radeon™ RX 5700 XT, demonstrating how the RL-based method impacts performance across various graphics applications.\nStability Over Time: Stability is tracked over a year of production compiler updates without retraining the deployed model. A value of 100% indicates no statistically significant performance regressions in benchmarks when using the DNN heuristic compared to the default compiler behavior.\n\n\n\n\n\nTransfer Learning: Instead of starting from scratch, the researchers use transfer learning to optimize the wavefront execution mode for a different GPU (the AMD Radeon™ RX 5700 XT), utilizing the final Q-table and behavior policy (πβ) from previous experiments as the starting point.\nEfficiency and Performance: With transfer learning, the model needed only 10 iterations over the set of graphics benchmarks to match or surpass previous frame rates in 94.4% of the benchmarks. Improvements included increases of up to 10.3% and an average improvement of 1.5%.\nBenchmarking: Experiments were conducted using the AMD production graphics compiler on over 270 graphics benchmarks, each with an average of 230 unique shaders. Figure 8 in the document provides a histogram of these results, showing the distribution of observed changes in frame rates.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#conclusion",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#conclusion",
    "title": "Paper Presentation",
    "section": "",
    "text": "Framework Development: The authors developed a GPU compiler autotuning framework using off-policy deep reinforcement learning (DRL) to generate heuristics that improve frame rates in graphics applications.\nContinuous Integration and Q-Learning: The framework combines continuous integration (CI) with Q-Learning to find optimal heuristic settings that maximize frame rate improvements across various graphics benchmarks.\nDeployment: By considering the rapid changes in software development, the trained models can be deployed as stable heuristics in evolving production compilers.\nGeneralized Gains: The framework demonstrates generalized performance gains across a large suite of graphics benchmarks and different GPUs.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#future-work",
    "href": "blogs/sharmila/paper_presentation_GPU_compiler_heuristics.html#future-work",
    "title": "Paper Presentation",
    "section": "",
    "text": "Explore Static Counters and Dynamic Properties: Investigate the relationship between the set of static counters and the dynamic properties that the neural network has learned to account for.\nExtend to Continuous Action Spaces: Aim to extend the framework to domains with continuous action spaces using techniques from deep Q-Learning.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Paper Presentation"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-09-20-Sharmila-HW01.html",
    "href": "blogs/sharmila/2024-09-20-Sharmila-HW01.html",
    "title": "Compiler Homework 01 - Trying Out Bril",
    "section": "",
    "text": "About Bril\nBril is a simple educational intermediate representation (IR) language that is used to teach and experiment with compiler and programming language concepts. It provides a set of operations such as arithmetic and logical operations, and basic control flow structures like conditional branches and loops. The idea behind Bril is to keep things minimal and easy to understand, so we can focus on the core concepts of compilers without getting distracted by complicated features found in more advanced languages.\nGoal of this Homework is to get familiar with Bril, so I chose to a write a simple benchmark under core without getting inputs(args).\nIn this blog, I’ll walk you through two parts of my Homework 01 assignment where I first write a benchmark in Bril and then develop a tool to analyze Bril programs.\n\n\nPart 1: Write a New Benchmark\nThe first part of the assignment involved writing a new benchmark in Bril. This helped me get familiar with Bril’s control flow, syntax, and basic operations.\nBenchmark: addsqevenodd.bril\nThe goal of this benchmark is to calculate the sum of squares of even and odd numbers from 1 to 10 separately. Here’s the Bril program I wrote by hand:\n\n@main {\n  sum_even: int = const 0;      \n  sum_odd: int = const 0;       \n  i: int = const 1;             \n  limit: int = const 10;        \n  one: int = const 1;           \n  two: int = const 2;           \n\n.loop:\n  square: int = mul i i;        \n  half: int = div i two;        \n  check: int = mul half two;    \n  is_even: bool = eq check i;   \n  br is_even .even_case .odd_case; \n\n.even_case:\n  sum_even: int = add sum_even square; \n  jmp .increment;\n\n.odd_case:\n  sum_odd: int = add sum_odd square;   \n\n.increment:\n  i: int = add i one;           \n  cond: bool = le i limit;      \n  br cond .loop .exit;          \n\n.exit:\n  print sum_even;               \n  print sum_odd;                \n}\n\nThis program defines a loop that iterates from 1 to 10, calculates the square of each number, and adds the result to two separate sums for even and odd numbers. Finally, it prints the sums of the squares of even and odd numbers\nAfter writing the program, I converted it to a JSON format, which is required for further processing:\nbril2json &lt; addsqevenodd.bril &gt; addsqevenodd.json\nDuring this step, I faced an issue where the Bril interpreter (brili) did not accept direct constant values in core instructions. To resolve this, I explicitly declared constants before using them in operations like add or mul. This was a good learning experience that reinforced the importance of proper initialization in IR.\nTo automate testing, I used turnt to create a test output for this benchmark. I added the following command to a turnt.toml file:\ncommand = “bril2json &lt; {filename} | brili -p {args}”\nThis command runs the benchmark through brili, captures the output, and saves it in an output file. After creating the test, I ran:\nturnt –save addsqevenodd.bril\nThis command saved the output in a file named addsqevenodd.out, which can be used for further validation.\n\n\nPart 2: Write a Bril Analyzer\nIn this part, I developed a small tool to analyze Bril programs using python. I chose to count the number of add and print instructions in the Bril program to better understand the operations performed in the code.\nHere’s the Python code I implemented to count the add and print instructions in a Bril program:\n\nimport json\nimport sys\n\n\ndef count_add_instrs(bril_program):\n\n    count = 0\n    print = 0\n    for func in bril_program['functions']:\n        for instr in func['instrs']:\n            if 'op' in instr:\n                if instr['op'] == 'add':\n                    count += 1\n\n    return count\n\ndef count_print_instrs(bril_program):\n\n    printcount = 0\n    for func in bril_program['functions']:\n        for instr in func['instrs']:\n            if 'op' in instr:\n                if instr['op'] == 'print':\n                    printcount += 1\n                    \n    return printcount\n\nif __name__ == \"__main__\":\n\n    json_file = \"/home/sharmila_ubuntu/Compiler HW 01/addsqevenodd.json\"\n    with open(json_file,'r') as f:\n        bril_program = json.load(f)\n    count = count_add_instrs(bril_program)\n    print(f\"Number of add instructions in addsqevenodd.json file : {count}\")\n    printcount = count_print_instrs(bril_program)\n    print(f\"Number of print instructions in addsqevenodd.json file : {printcount}\")\n\nThis script loads the Bril program in JSON format, iterates over its instructions, and counts how many add and print operations are present.\nI tested my analyzer using turnt by adding the following command to the turnt.toml file:\ncommand = “bril2json &lt; {filename} | python3 analyzebril.py”\nThis command takes the Bril program, converts it to JSON, and runs my Python script (analyzebril.py) to analyze the JSON file. The output file generated by this process contains the number of add and print instructions in the program.\n\n\nConclusion: Summary of this Homework 01\n\nPart 1: I wrote a benchmark (addsqevenodd.bril) that computes the sum of squares of even and odd numbers from 1 to 10 separately. I used bril2json to convert the program into JSON and ran it with brili. I also used turnt to create an automated test for the benchmark.\nPart 2: I created a Python tool to analyze the Bril program. The tool counts add and print instructions in the JSON representation of the Bril program. I tested the tool using turnt, ensuring it ran correctly.\n\n\n\nTest and Results:\nTo verify this analyzer I tested with 3 example: (obtained the later two examples from (https://github.com/sampsyo/bril/tree/main/benchmarks)\n\nInitially, I used my benchmark to test the implementation, that is, using both the addsqevenodd.bril file and its corresponding json file (addsqevenodd.json). The analyzed correctly counted 3 add instructions and 2 print instructions.\nAnd then, I verified using bubblesort.bril, loading it’s json file and analyzed correctly as 8 add instructions and 1 print instruction.\nFinally, I verifies using mat-inv.bril, loading it’s json file and analyzed correctly as 12 add instruction and 2 print instruction.\n\n\n\nChallenges Faced and Solution:\nThe hardest part of this task was dealing with the restriction on directly using constants in core Bril instructions. I initially overlooked this, but after debugging, I realized that every constant had to be initialized in a separate instruction. This helped me better understand Bril’s structure and rules. Moreover, learning to use turnt for automating tests was a valuable lesson, as it streamlines the testing process significantly.\n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 01 - Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/sharmila/2024-09-27-Sharmila-HW02.html",
    "href": "blogs/sharmila/2024-09-27-Sharmila-HW02.html",
    "title": "Compiler Homework 02 - Local Optimization",
    "section": "",
    "text": "Part 1: Dead Code Elimination\nDead Code Elimination (DCE) is an optimization technique in compiler design, aiming to remove instructions that do not affect the program’s final output.\nIn Homework 2, I implemented a “trivial” DCE, where instructions that are never used before being reassigned are deleted.\nThe Basic Idea behind this is to identify and remove the redundant assignment those are never implemented before reassignment.\nIn this blog, I’ll walk throught the implementation process and verify it with an example.\n\n\nImplementation Process:\nI used 3 key step to acheive this task: DCE, Reassignment Handling and Iteration until no changes.\nIn DCE step, identifies the variables that are used by collecting all arguments in the blocks and removes instructions where the destination variable (dest) is not in the used set (indicating it’s redundant). It modifies the blocks and flattens them back into the instruction list. At the next step, Reassignment Handling, removes redundant reassignments. If a variable is defined but then reassigned before its value is used, the earlier definition is deleted. The process is applied block by block.Finally, Iteration until no changes, repeatedly applies DCE and reassignment elimination until no further changes occur.\n\n\nExample:\n\n@main {\n  a: int = const 100;\n  a: int = const 42;\n  b: int = const 5;\n  sum: int = add a b;\n  c: int = id b;\n  sum: int = add c a;\n  print sum;\n}\n\nHere, the variable a is assigned the value 100, but it is immediately overwritten with the value 42, making the first assignment redundant. Similarly, the first computation of sum is also unnecessary as it is overwritten before being used. After running my implementation:\n\n@main {\n  a: int = const 42;\n  b: int = const 5;\n  c: int = id b;\n  sum: int = add c a;\n  print sum;\n}\n\nThe redundant instructions have been successfully removed. The total number of dynamic instructions has been reduced from 7 to 5. Thus it is verified that my implementation acheived the task of the homework.\nAnd another example to verify further, that the code delete instructions that are never used before they are reassigned.\n\n@main {\n a: int = const 100;\n a: int = const 42;\n b: int = const 5;\n sum: int = add a b;\n print sum;\n c: int = id b;\n sum: int = add c a;\n print sum;\n}\n\nAfter running the code:\n@main {\n  a: int = const 42;\n  b: int = const 5;\n  sum: int = add a b;\n  print sum;\n  c: int = id b;\n  sum: int = add c a;\n  print sum;\n}\n\nHere, the variable a is assigned the value 100, but it is immediately overwritten with the value 42, making the first assignment redundant. Since the first sum is used therefore it is not a redundant.\n\n\nPart 2: Local Value Numbering\nLocal Value Numbering (LVN) is an optimization technique used in compilers to eliminate redundant calculations. By assigning a unique number to each distinct computation, LVN helps minimize repeated evaluations of the same expression. In this part, the task is to implement Local value numbering and further to pair with DCE\n\n\nImplementation:\nPseudo Code of the implementation:\nDefine a named tuple ‘Value’ to represent a computation. Create a ‘Numbering’ class to manage unique identifiers for each computation. Implement the ‘last_writes’ function to identify the last write instructions for each variable. Implement the ‘read_first’ function to determine which variables are read before being written to. Define ‘lvn_block’ to process each block of instructions: Initialize mappings for variable-to-number and value-to-number. For each variable read first, assign a unique number. For each instruction, retrieve argument numbers and check for redundancy: If the computation has been seen, replace the instruction with an identity operation. If it’s a new computation, assign a fresh number and record it. Define ‘lvn’ to iterate through functions in the input data and apply ‘lvn_block’. Load JSON data, call ‘lvn’, and output the optimized result.\nThis code implementation is verfied by using an example:\n@main {\n  a: int = const 4;\n  b: int = const 2;\n  c: int = const 5;\n  sum1: int = add a b;\n  sum2: int = add a b;\n  prod1: int = mul sum1 sum2;\n  sum1: int = const 0;\n  sum2: int = const 0;\n  sum3: int = add a b;\n  prod2: int = mul sum3 sum3;\n  print prod2;\n}\nOutput of the implement:\n@main {\n  a: int = const 4;\n  b: int = const 2;\n  c: int = const 5;\n  lvn.3: int = add a b;                \n  sum2: int = id lvn.3;                \n  prod1: int = mul lvn.3 lvn.3;        \n  sum1: int = const 0;\n  sum2: int = const 0;\n  sum3: int = id lvn.3;                \n  prod2: int = id prod1;              \n  print prod1;                         \n}\nIn this output,the operation add a b is computed once and assigned a unique identifier lvn.3, effectively avoiding redundant calculations. Instead of recalculating add a b for sum2 and sum3, the code uses id lvn.3, indicating that these instructions simply take the value of lvn.3. Furthermore, the instruction prod1 utilizes lvn.3 to multiply with itself, showcasing the efficient reuse of computed values and optimizing the overall execution of the code.\n\n\nPairing with DCE:\nI used an example to verify\n@main {\n  a: int = const 4;\n  b: int = const 2;\n  c: int = const 5;\n  sum1: int = add a b;\n  sum2: int = add a b;\n  prod1: int = mul sum1 sum2;\n\n  sum1: int = const 0;\n  sum2: int = const 0;\n  c: int = const 10;\n  sum3: int = add a b;\n  prod2: int = mul sum3 sum3;\n\n  print prod2;\n}\nHere c variable is initiallized twice and not used before reassigned.\nOutput:\n@main { a: int = const 4; b: int = const 2; sum1: int = add a b; sum2: int = add a b; prod1: int = mul sum1 sum2;\nsum1: int = const 0; sum2: int = const 0; c: int = const 10; sum3: int = add a b; prod2: int = mul sum3 sum3;\nprint prod2; }\nThe implementation of Local Value Numbering optimizes computations by eliminating redundancy, thus enhancing performance. This example illustrates how LVN effectively manages computations through unique identifiers.\n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Compiler Homework 02 - Local Optimization"
    ]
  },
  {
    "objectID": "blogs/sharmila/project/Project.html",
    "href": "blogs/sharmila/project/Project.html",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "",
    "text": "In memory-intensive C++ applications, optimizing memory usage is critical, especially in environments with limited resources. This project introduces a Python-based approach that leverages runtime profiling data and machine learning (ML) techniques to enhance memory efficiency. The approach utilizes a RandomForestRegressor ML model to predict memory bottlenecks by analyzing complex interactions in profiling data. By identifying patterns in memory allocation, access, and release, the tool applies targeted optimizations such as replacing inefficient memory allocation calls, reallocation strategies, and ensuring proper deallocation of memory. The results demonstrate significant reductions in memory footprint and peak memory usage, providing valuable insights for developers aiming to improve memory-intensive applications.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/sharmila/project/Project.html#abstract",
    "href": "blogs/sharmila/project/Project.html#abstract",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "",
    "text": "In memory-intensive C++ applications, optimizing memory usage is critical, especially in environments with limited resources. This project introduces a Python-based approach that leverages runtime profiling data and machine learning (ML) techniques to enhance memory efficiency. The approach utilizes a RandomForestRegressor ML model to predict memory bottlenecks by analyzing complex interactions in profiling data. By identifying patterns in memory allocation, access, and release, the tool applies targeted optimizations such as replacing inefficient memory allocation calls, reallocation strategies, and ensuring proper deallocation of memory. The results demonstrate significant reductions in memory footprint and peak memory usage, providing valuable insights for developers aiming to improve memory-intensive applications.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/sharmila/project/Project.html#key-concepts",
    "href": "blogs/sharmila/project/Project.html#key-concepts",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Key Concepts",
    "text": "Key Concepts\nBefore diving into the implementation details, let’s clarify some important terms:\n\nMemory Allocation: Memory allocation refers to the process by which a program requests memory from the system to store data. Dynamic allocation (e.g., malloc in C++) is common in applications where memory needs are determined during runtime.\nPeak Memory Usage: Peak memory usage is the maximum amount of memory consumed by a program at any point during its execution. Minimizing peak usage is essential for avoiding crashes and improving performance in resource-constrained environments.\nMemory Profiling: Memory profiling involves analyzing how an application allocates and frees memory over time. Tools like Valgrind’s Massif generate detailed reports on memory usage, helping identify inefficiencies or leaks.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/sharmila/project/Project.html#goals",
    "href": "blogs/sharmila/project/Project.html#goals",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Goals",
    "text": "Goals\n\nIdentify and optimize memory bottlenecks in five example C++ programs.\nImplement a Python tool that processes profiling data, predicts inefficiencies, and applies memory optimizations.\nEvaluate the effectiveness of these optimizations empirically.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/sharmila/project/Project.html#design-and-implementation",
    "href": "blogs/sharmila/project/Project.html#design-and-implementation",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Design and Implementation",
    "text": "Design and Implementation\n\nStep 1: Collecting Memory Profiling Data\n\nMethod\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false\ngraph LR;\nA[Start Profiling] --&gt; B[Run Program with Massif];\nB --&gt; C[Generate Massif Output];\nC --&gt; D[Analyze Output with ms_print];\nD --&gt; E[Convert to CSV for Analysis];\n\n\n\n\n\n\n\nTo analyze memory usage, we used Valgrind’s Massif tool:\n\nInstall Valgrind: sudo apt install valgrind\nCompile C++ programs with debug information: g++ -g -o example1 example1.cpp\nRun with Massif: valgrind --tool=massif ./example1\nAnalyze results: ms_print massif.out.&lt;pid&gt;\nConvert results to CSV for further analysis.\n\n\n\n\n\nStep 2: Building the Python Tool\nThe Python tool processes profiling data and optimizes memory usage through:\n\nInput Data:\n\nProfiling Data: Memory allocation, peak memory usage, and other metrics from Massif converted to CSV.\nSource Code: C++ programs requiring optimization.\n\n\n\nMachine Learning Component:\nWe used a RandomForestRegressor to predict memory inefficiencies based on the profiling data. This ML model enabled accurate identification of memory bottlenecks by analyzing complex interactions between memory allocation and usage patterns.\n\nRandomForestRegressor Algorithm\n\nInitialize Parameters: Set the number of trees (n_trees) and other hyperparameters.\nBootstrap Sampling: For each tree, draw a bootstrap sample from the original data.\nTrain Trees: Train a decision tree on each bootstrap sample. Each node in the tree considers a random subset of features when splitting.\nAggregate Predictions: For regression, average the predictions of all the trees in the forest.\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\n    A[Start] --&gt; B[Initialize Parameters];\n    B --&gt; C[Bootstrap Sampling];\n    C --&gt; D[Train Decision Tree];\n    D --&gt; E{Repeat for n Trees};\n    E -- Yes --&gt; C;\n    E -- No --&gt; F[Aggregate Predictions];\n    F --&gt; G[Output Prediction];\n\n\n\n\n\n\n\nThis block diagram represents the workflow of the RandomForestRegressor. Here’s a step-by-step explanation:\n\nInitialize Parameters: Set the number of trees (n_trees) and other parameters like maximum depth, minimum samples per leaf, etc.\nBootstrap Sampling: Create multiple bootstrap samples (random subsets with replacement) from the original dataset.\nTrain Decision Tree: Train a decision tree on each bootstrap sample. During tree construction, each node selects a subset of features randomly and uses the best split among them.\nRepeat for n Trees: Repeat the bootstrap sampling and tree training for n trees.\nAggregate Predictions: For each test sample, predict the outcome using all the trained trees and then average these predictions to get the final output.\n\n\n\n\nOptimizations Implemented:\n\nCode Transformations:\n\nReplacing malloc with calloc for improved initialization.\nNullifying pointers after free to prevent dangling references.\n\nMemory Simulations:\n\nAdjusting allocation and peak values to simulate optimized memory behavior.\n\n\n\n\nMethod\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n%%| echo: false\ngraph TD;\nA[Input CSV and C++ Source] --&gt; B[Parse and Preprocess Data];\nB --&gt; C[Feature Engineering];\nC --&gt; D[Train RandomForest Regressor];\nD --&gt; E[Predict Memory Bottlenecks];\nE --&gt; F[Apply Code Transformations];\nF --&gt; G[Simulate Optimized Profile];\nG --&gt; H[Save Outputs];\n\n\n\n\n\n\n\n\nThe Python tool processes profiling data and optimizes memory usage through:\n\nFeature Engineering: Calculating Memory_Delta (difference between allocated and freed memory) and Memory_Utilization_Ratio (allocated memory divided by peak memory).\nSource Code Optimization: Replacing malloc with calloc and ensuring proper nullification after freeing memory.\nSimulated Profiling: Adjusting allocation and peak memory values to simulate optimization effects.\n\n\n\n\n\nStep 3: Comparing Profiles\nThe tool generates comparison charts showing memory allocation and peak memory usage before and after optimization. Empirical results highlight reductions in memory usage.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/sharmila/project/Project.html#results",
    "href": "blogs/sharmila/project/Project.html#results",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Results",
    "text": "Results\n\nExample Outputs\n\nExample 1: mem_alloc.cpp\n\n\n\n\nExample 1: mem_alloc.cpp\n\n\n\n\nTotal reduction in memory allocation: 37,833.81 KB\nReduction in peak memory usage: 788.47 KB\n\n\n\nExample 2: mem_fragmentation.cpp\n\n\n\n\nExample 2: mem_fragmentation.cpp\n\n\n\n\nTotal reduction in memory allocation: 1,256.08 KB\nReduction in peak memory usage: 7.98 KB\n\n\n\nExample 3: Cache_miss.cpp\n\n\n\n\nExample 3: Cache_miss.cpp\n\n\n\n\nTotal reduction in memory allocation: 2,415.55 KB\nReduction in peak memory usage: 397.83 KB\n\n\n\nExample 4: growing_ds.cpp\n\n\n\n\nExample 4: growing_ds.cpp\n\n\n\n\nTotal reduction in memory allocation: 7,388.50 KB\nReduction in peak memory usage: 622.00 KB\n\n\n\nExample 5: recursive.cpp\n\n\n\n\nExample 5: recursive.cpp\n\n\n\n\nTotal reduction in memory allocation: 43.20 KB\nReduction in peak memory usage: 7.20 KB",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/sharmila/project/Project.html#challenges",
    "href": "blogs/sharmila/project/Project.html#challenges",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Challenges",
    "text": "Challenges\n\nHardest Parts\n\nParsing Profiling Data: Extracting meaningful patterns from Massif’s output required converting data into a structured format (CSV).\nCode Transformations: Ensuring that optimizations like replacing malloc with calloc preserved functionality without introducing bugs.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/sharmila/project/Project.html#evaluation",
    "href": "blogs/sharmila/project/Project.html#evaluation",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Evaluation",
    "text": "Evaluation\nThe optimizations were successful, with notable reductions in memory allocation and peak usage across all examples. For instance, in mem_leak.cpp, the optimized memory allocation dropped by over 2 GB, showcasing the impact of targeted memory management strategies.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/sharmila/project/Project.html#conclusion",
    "href": "blogs/sharmila/project/Project.html#conclusion",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Conclusion",
    "text": "Conclusion\nThis project demonstrates that combining Python and ML techniques with traditional profiling tools can effectively optimize memory usage in C++ applications. By automating analysis and applying systematic optimizations, developers can significantly improve efficiency, paving the way for better performance in memory-constrained environments.\nThis is the code link for this project: https://github.com/gurusamyanandakuma-r/bril/tree/main/HW/Sharmila_Rohit_Project",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Sharmila",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/rohit/2024-11-02-Rohit-HW5.html",
    "href": "blogs/rohit/2024-11-02-Rohit-HW5.html",
    "title": "Homework 5",
    "section": "",
    "text": "code link: https://github.com/gurusamyanandakuma-r/bril/tree/main/HW/HW5_Rohit/llvm-div-check\n\n\nFor this homework, I implemented DivCheckPass which is an LLVM pass that identifies division instructions within functions. When a division instruction is found, the pass prints information about:\n\nThe function containing the division\nThe basic block containing the division\nThe division instruction itself\n\nThe pass is implemented as a Function Pass, meaning it operates on one function at a time. It uses LLVM’s instruction visitor pattern to examine each instruction and uses dyn_cast to identify binary operations that are specifically division operations.\n\n\n\nThe pass was tested by:\n\nCompiling a C++ Program: A test program (test.cpp) containing division operations was compiled with the pass plugin enabled.\nPass Output Analysis: The compilation output was examined to verify that the pass correctly identified and reported division instructions.\nMultiple Test Cases: The program included both safe divisions and potential divide-by-zero scenarios.\n\n\n\n#include &lt;stdio.h&gt;\nint divide(int a, int b) {\n    return a / b;\n}\nint main() {\n    int x = 10;\n    int y = 0;\n    return divide(x, y);\n}\n\n\n\n\n\nclang -Xclang -load -Xclang ./DivCheckPass.so -S -emit-llvm ../test.cpp -o -\nAnalyzing Function: _Z6divideii\n  Basic Block: %2\n    Found division instruction:   %7 = sdiv i32 %5, %6\nAnalyzing Function: main\n  Basic Block: %0\n; ModuleID = '../test.cpp'\nsource_filename = \"../test.cpp\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n; Function Attrs: noinline nounwind optnone uwtable\ndefine dso_local i32 @_Z6divideii(i32 %0, i32 %1) #0 {\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  store i32 %0, i32* %3, align 4\n  store i32 %1, i32* %4, align 4\n  %5 = load i32, i32* %3, align 4\n  %6 = load i32, i32* %4, align 4\n  %7 = sdiv i32 %5, %6\n  ret i32 %7\n}\n\n; Function Attrs: noinline norecurse nounwind optnone uwtable\ndefine dso_local i32 @main() #1 {\n  %1 = alloca i32, align 4\n  %2 = alloca i32, align 4\n  %3 = alloca i32, align 4\n  store i32 0, i32* %1, align 4\n  store i32 10, i32* %2, align 4\n  store i32 0, i32* %3, align 4\n  %4 = load i32, i32* %2, align 4\n  %5 = load i32, i32* %3, align 4\n  %6 = call i32 @_Z6divideii(i32 %4, i32 %5)\n  ret i32 %6\n}\n\nattributes #0 = { noinline nounwind optnone uwtable \"correctly-rounded-divide-sqrt-fp-math\"=\"false\" \"disable-tail-calls\"=\"false\" \"frame-pointer\"=\"all\" \"less-precise-fpmad\"=\"false\" \"min-legal-vector-width\"=\"0\" \"no-infs-fp-math\"=\"false\" \"no-jump-tables\"=\"false\" \"no-nans-fp-math\"=\"false\" \"no-signed-zeros-fp-math\"=\"false\" \"no-trapping-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" }\nattributes #1 = { noinline norecurse nounwind optnone uwtable \"correctly-rounded-divide-sqrt-fp-math\"=\"false\" \"disable-tail-calls\"=\"false\" \"frame-pointer\"=\"all\" \"less-precise-fpmad\"=\"false\" \"min-legal-vector-width\"=\"0\" \"no-infs-fp-math\"=\"false\" \"no-jump-tables\"=\"false\" \"no-nans-fp-math\"=\"false\" \"no-signed-zeros-fp-math\"=\"false\" \"no-trapping-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" }\n\n\n\nclang -Xclang -load -Xclang ./DivCheckPass.so ../test.cpp -o test ./test\nAnalyzing Function: _Z6divideii\n  Basic Block: %2\n    Found division instruction:   %7 = sdiv i32 %5, %6\nAnalyzing Function: main\n  Basic Block: %0\n\n\n\n\n\nThe pass successfully identified the division operation in the divide function\nIt correctly located the basic block containing the division\nIt properly printed the LLVM IR representation of the division instruction\n\n\n\n\n\n\nImplementing the pass presented several challenges, including understanding LLVM’s Intermediate Representation (IR) and the translation of high-level C++ constructs into LLVM IR.\nIntegrating the pass involved setting up the CMake build system, registering the pass correctly, and managing the pass loading mechanisms. Navigating LLVM’s type system to identify division operations and using dyn_cast for type safety also proved challenging. Currently, the implementation only detects division operations.\nFuture improvements could include inserting runtime checks for divide-by-zero, adding instrumentation for division operation statistics, handling floating-point divisions, and generating warnings for potential divide-by-zero scenarios.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Homework 5"
    ]
  },
  {
    "objectID": "blogs/rohit/2024-11-02-Rohit-HW5.html#llvm-division-check-pass-implementation",
    "href": "blogs/rohit/2024-11-02-Rohit-HW5.html#llvm-division-check-pass-implementation",
    "title": "Homework 5",
    "section": "",
    "text": "code link: https://github.com/gurusamyanandakuma-r/bril/tree/main/HW/HW5_Rohit/llvm-div-check\n\n\nFor this homework, I implemented DivCheckPass which is an LLVM pass that identifies division instructions within functions. When a division instruction is found, the pass prints information about:\n\nThe function containing the division\nThe basic block containing the division\nThe division instruction itself\n\nThe pass is implemented as a Function Pass, meaning it operates on one function at a time. It uses LLVM’s instruction visitor pattern to examine each instruction and uses dyn_cast to identify binary operations that are specifically division operations.\n\n\n\nThe pass was tested by:\n\nCompiling a C++ Program: A test program (test.cpp) containing division operations was compiled with the pass plugin enabled.\nPass Output Analysis: The compilation output was examined to verify that the pass correctly identified and reported division instructions.\nMultiple Test Cases: The program included both safe divisions and potential divide-by-zero scenarios.\n\n\n\n#include &lt;stdio.h&gt;\nint divide(int a, int b) {\n    return a / b;\n}\nint main() {\n    int x = 10;\n    int y = 0;\n    return divide(x, y);\n}\n\n\n\n\n\nclang -Xclang -load -Xclang ./DivCheckPass.so -S -emit-llvm ../test.cpp -o -\nAnalyzing Function: _Z6divideii\n  Basic Block: %2\n    Found division instruction:   %7 = sdiv i32 %5, %6\nAnalyzing Function: main\n  Basic Block: %0\n; ModuleID = '../test.cpp'\nsource_filename = \"../test.cpp\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n; Function Attrs: noinline nounwind optnone uwtable\ndefine dso_local i32 @_Z6divideii(i32 %0, i32 %1) #0 {\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  store i32 %0, i32* %3, align 4\n  store i32 %1, i32* %4, align 4\n  %5 = load i32, i32* %3, align 4\n  %6 = load i32, i32* %4, align 4\n  %7 = sdiv i32 %5, %6\n  ret i32 %7\n}\n\n; Function Attrs: noinline norecurse nounwind optnone uwtable\ndefine dso_local i32 @main() #1 {\n  %1 = alloca i32, align 4\n  %2 = alloca i32, align 4\n  %3 = alloca i32, align 4\n  store i32 0, i32* %1, align 4\n  store i32 10, i32* %2, align 4\n  store i32 0, i32* %3, align 4\n  %4 = load i32, i32* %2, align 4\n  %5 = load i32, i32* %3, align 4\n  %6 = call i32 @_Z6divideii(i32 %4, i32 %5)\n  ret i32 %6\n}\n\nattributes #0 = { noinline nounwind optnone uwtable \"correctly-rounded-divide-sqrt-fp-math\"=\"false\" \"disable-tail-calls\"=\"false\" \"frame-pointer\"=\"all\" \"less-precise-fpmad\"=\"false\" \"min-legal-vector-width\"=\"0\" \"no-infs-fp-math\"=\"false\" \"no-jump-tables\"=\"false\" \"no-nans-fp-math\"=\"false\" \"no-signed-zeros-fp-math\"=\"false\" \"no-trapping-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" }\nattributes #1 = { noinline norecurse nounwind optnone uwtable \"correctly-rounded-divide-sqrt-fp-math\"=\"false\" \"disable-tail-calls\"=\"false\" \"frame-pointer\"=\"all\" \"less-precise-fpmad\"=\"false\" \"min-legal-vector-width\"=\"0\" \"no-infs-fp-math\"=\"false\" \"no-jump-tables\"=\"false\" \"no-nans-fp-math\"=\"false\" \"no-signed-zeros-fp-math\"=\"false\" \"no-trapping-math\"=\"false\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"unsafe-fp-math\"=\"false\" \"use-soft-float\"=\"false\" }\n\n\n\nclang -Xclang -load -Xclang ./DivCheckPass.so ../test.cpp -o test ./test\nAnalyzing Function: _Z6divideii\n  Basic Block: %2\n    Found division instruction:   %7 = sdiv i32 %5, %6\nAnalyzing Function: main\n  Basic Block: %0\n\n\n\n\n\nThe pass successfully identified the division operation in the divide function\nIt correctly located the basic block containing the division\nIt properly printed the LLVM IR representation of the division instruction\n\n\n\n\n\n\nImplementing the pass presented several challenges, including understanding LLVM’s Intermediate Representation (IR) and the translation of high-level C++ constructs into LLVM IR.\nIntegrating the pass involved setting up the CMake build system, registering the pass correctly, and managing the pass loading mechanisms. Navigating LLVM’s type system to identify division operations and using dyn_cast for type safety also proved challenging. Currently, the implementation only detects division operations.\nFuture improvements could include inserting runtime checks for divide-by-zero, adding instrumentation for division operation statistics, handling floating-point divisions, and generating warnings for potential divide-by-zero scenarios.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Homework 5"
    ]
  },
  {
    "objectID": "blogs/rohit/2024-09-20-Rohit-HW1.html",
    "href": "blogs/rohit/2024-09-20-Rohit-HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "A Cyclic Redundancy Check (CRC) is an error-detecting code used in digital networks and storage devices to detect accidental changes to raw data. In embedded systems, CRCs are crucial for ensuring data integrity during communication, verifying data stored in memory, and validating firmware updates. They are popular due to their simplicity, efficiency in binary hardware, and effectiveness in detecting common transmission errors.\n\nIt ensure data integrity in communication protocols like UART, SPI, and I2C by detecting errors caused by noise or interference.\nHow CRC Works:\n\nPolynomial Division: The data block is treated as a large binary number and divided by a fixed polynomial. The remainder of this division is the CRC value.\nAppending CRC: The CRC value is appended to the data before transmission or storage.\nVerification: Upon retrieval, the same polynomial division is performed. If the remainder matches the CRC value, the data is considered intact; otherwise, it indicates corruption\n\nLearn more at: https://en.wikipedia.org/wiki/Cyclic_redundancy_check\n\n\n\n\n\nCRC calculations prominently involve memory allocation, extensively using ptr&lt;int&gt;.\nCRC algorithms typically involve a mix of arithmetic (e.g., addition, subtraction, Multiplication) and bitwise operations (e.g., XOR, shifts).\nThis makes CRC an excellent microbenchmark for evaluating how well your Bril implementation handles these critical operations, which are essential for many real-world applications.\n\n\n\n\n\nBril code for CRC is fully hand typed code of 300+ lines using VS Code, the most useful tool ever was Bril syntax highlighting Extension in VS Code.\nTurnt tool was used to test and save the output file.\nturnt.toml file:\n\n    [envs.test]\n    command = \"bril2json &lt; {filename} | brili -p {args}\"\n    output.out = \"-\"\n    output.prof = \"2\"\n\n\nLearn more at: https://github.com/cucapra/turnt?tab=readme-ov-file\n\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[Initialize Inputs]--&gt; B0[Get CRC Remainder] --&gt; B1[\"Check CRC\"] --&gt; B2[Print Outputs];\n\n\n\n\n\n\n\n\n\nGet Word, Divisor, check_value as an int.\ncheck_value will be 0 if the bits are missing and error.\nAppend Divisor size - 1 zeros to word.\nConvert the int into array, here in bril we can use ptr&lt;int&gt; allocations calling @toPtr function.\nReference to memory allocation in bril: https://www.cs.cornell.edu/courses/cs6120/2019fa/blog/manually-managed-memory/\n\n\n\n\n\nTo divide Word and Divisor we need to do xor in each bit.\nFirst copy first few bits from Word to Word_split having same length as Divisor.\nCall @xorfunction and overwrite the result to first few bits of Word.\nCall @shiftleft function to left shift the Word and decrement word_size.\nRepeat this process until Word_size &lt; Div_size.\nAt the end return Word as a crc_rem to @main.\nConvert crc_rem from ptr&lt;int&gt; to int by calling toNum function.\n\n\n\n\n\nIf: crc_rem == 0 then there is no error in bit and crc_check is true.\nElse: there is a error and crc_check is false.\n\n\n\n\n\nOuputs are bool value crc_check and int value crc_rem.\nWhen check_value is set to 0\n\nThis Indicates that there are error or missing bits.\ncrc_check value will be false\ncheck_rem will give out the correct check_value\nso again replacing 0 from check_rem with check_value and running the code will give us crc_check as true with check_rem = 0 .\n\nFull code: https://github.com/gurusamyanandakuma-r/bril/blob/main/benchmarks/mem/crc_check.bril\n\n\n\n\n\n\n\n\n\n\nInput:\n\n    word: int = const 1101101;\n    word_size: int = const 7;\n    divisor: int = const 10101;\n    divisor_size: int = const 5;\n    check_value: int = const 0;\n\nOutput:\n\n    false\n    1011\n\nTotal Dynamic Instructions:\n\n    total_dyn_inst: 1784\n\n\n\n\nInput:\n\n    word: int = const 1101101;\n    word_size: int = const 7;\n    divisor: int = const 10101;\n    divisor_size: int = const 5;\n    check_value: int = const 1011;\n\nOutput:\n\n    true\n    0\n\nTotal Dynamic Instructions:\n\n    total_dyn_inst: 1784\n\n\n\n\n\n\n\nInput:\n\n    word: int = const 11010011101100;\n    word_size: int = const 14;\n    divisor: int = const 1011;\n    divisor_size: int = const 4;\n    check_value: int = const 0;\n\nOutput:\n\n    false\n    100\n\nTotal Dynamic Instructions:\n\n    total_dyn_inst: 3061\n\n\n\n\nInput:\n\n    word: int = const 11010011101100;\n    word_size: int = const 14;\n    divisor: int = const 1011;\n    divisor_size: int = const 4;\n    check_value: int = const 100;\n\nOutput:\n\n    true\n    0\n\nTotal Dynamic Instructions:\n\n    total_dyn_inst: 3061",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Homework 1"
    ]
  },
  {
    "objectID": "blogs/rohit/2024-09-20-Rohit-HW1.html#part-1-benchmark",
    "href": "blogs/rohit/2024-09-20-Rohit-HW1.html#part-1-benchmark",
    "title": "Homework 1",
    "section": "",
    "text": "A Cyclic Redundancy Check (CRC) is an error-detecting code used in digital networks and storage devices to detect accidental changes to raw data. In embedded systems, CRCs are crucial for ensuring data integrity during communication, verifying data stored in memory, and validating firmware updates. They are popular due to their simplicity, efficiency in binary hardware, and effectiveness in detecting common transmission errors.\n\nIt ensure data integrity in communication protocols like UART, SPI, and I2C by detecting errors caused by noise or interference.\nHow CRC Works:\n\nPolynomial Division: The data block is treated as a large binary number and divided by a fixed polynomial. The remainder of this division is the CRC value.\nAppending CRC: The CRC value is appended to the data before transmission or storage.\nVerification: Upon retrieval, the same polynomial division is performed. If the remainder matches the CRC value, the data is considered intact; otherwise, it indicates corruption\n\nLearn more at: https://en.wikipedia.org/wiki/Cyclic_redundancy_check\n\n\n\n\n\nCRC calculations prominently involve memory allocation, extensively using ptr&lt;int&gt;.\nCRC algorithms typically involve a mix of arithmetic (e.g., addition, subtraction, Multiplication) and bitwise operations (e.g., XOR, shifts).\nThis makes CRC an excellent microbenchmark for evaluating how well your Bril implementation handles these critical operations, which are essential for many real-world applications.\n\n\n\n\n\nBril code for CRC is fully hand typed code of 300+ lines using VS Code, the most useful tool ever was Bril syntax highlighting Extension in VS Code.\nTurnt tool was used to test and save the output file.\nturnt.toml file:\n\n    [envs.test]\n    command = \"bril2json &lt; {filename} | brili -p {args}\"\n    output.out = \"-\"\n    output.prof = \"2\"\n\n\nLearn more at: https://github.com/cucapra/turnt?tab=readme-ov-file\n\n\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false \ngraph LR;\nA[Initialize Inputs]--&gt; B0[Get CRC Remainder] --&gt; B1[\"Check CRC\"] --&gt; B2[Print Outputs];\n\n\n\n\n\n\n\n\n\nGet Word, Divisor, check_value as an int.\ncheck_value will be 0 if the bits are missing and error.\nAppend Divisor size - 1 zeros to word.\nConvert the int into array, here in bril we can use ptr&lt;int&gt; allocations calling @toPtr function.\nReference to memory allocation in bril: https://www.cs.cornell.edu/courses/cs6120/2019fa/blog/manually-managed-memory/\n\n\n\n\n\nTo divide Word and Divisor we need to do xor in each bit.\nFirst copy first few bits from Word to Word_split having same length as Divisor.\nCall @xorfunction and overwrite the result to first few bits of Word.\nCall @shiftleft function to left shift the Word and decrement word_size.\nRepeat this process until Word_size &lt; Div_size.\nAt the end return Word as a crc_rem to @main.\nConvert crc_rem from ptr&lt;int&gt; to int by calling toNum function.\n\n\n\n\n\nIf: crc_rem == 0 then there is no error in bit and crc_check is true.\nElse: there is a error and crc_check is false.\n\n\n\n\n\nOuputs are bool value crc_check and int value crc_rem.\nWhen check_value is set to 0\n\nThis Indicates that there are error or missing bits.\ncrc_check value will be false\ncheck_rem will give out the correct check_value\nso again replacing 0 from check_rem with check_value and running the code will give us crc_check as true with check_rem = 0 .\n\nFull code: https://github.com/gurusamyanandakuma-r/bril/blob/main/benchmarks/mem/crc_check.bril\n\n\n\n\n\n\n\n\n\n\nInput:\n\n    word: int = const 1101101;\n    word_size: int = const 7;\n    divisor: int = const 10101;\n    divisor_size: int = const 5;\n    check_value: int = const 0;\n\nOutput:\n\n    false\n    1011\n\nTotal Dynamic Instructions:\n\n    total_dyn_inst: 1784\n\n\n\n\nInput:\n\n    word: int = const 1101101;\n    word_size: int = const 7;\n    divisor: int = const 10101;\n    divisor_size: int = const 5;\n    check_value: int = const 1011;\n\nOutput:\n\n    true\n    0\n\nTotal Dynamic Instructions:\n\n    total_dyn_inst: 1784\n\n\n\n\n\n\n\nInput:\n\n    word: int = const 11010011101100;\n    word_size: int = const 14;\n    divisor: int = const 1011;\n    divisor_size: int = const 4;\n    check_value: int = const 0;\n\nOutput:\n\n    false\n    100\n\nTotal Dynamic Instructions:\n\n    total_dyn_inst: 3061\n\n\n\n\nInput:\n\n    word: int = const 11010011101100;\n    word_size: int = const 14;\n    divisor: int = const 1011;\n    divisor_size: int = const 4;\n    check_value: int = const 100;\n\nOutput:\n\n    true\n    0\n\nTotal Dynamic Instructions:\n\n    total_dyn_inst: 3061",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Homework 1"
    ]
  },
  {
    "objectID": "blogs/rohit/2024-09-20-Rohit-HW1.html#part-2-analysis",
    "href": "blogs/rohit/2024-09-20-Rohit-HW1.html#part-2-analysis",
    "title": "Homework 1",
    "section": "PART 2: Analysis:",
    "text": "PART 2: Analysis:\n\nFinding Number of Functions and its Calls\n\nCode: https://github.com/gurusamyanandakuma-r/bril/blob/main/examples/Count_Function.py\n\n\nInitialize\n\nThis analysis is done in python.\nUsing bril2json converting .bril file to JSON.\nLoading into python file using prog = json.load(sys.stdin).\nJSON heap structure will be loaded as dictionary in python.\n\n\n\nProcess\n\nCounting all functions in bril.\nCounting function calls of a function.\n\n\n\nTest\n\nTurnt.toml file:\n\n    [envs.countFunc]\n    default = false\n    command = \"bril2json &lt; {filename} | python3 hw.py &gt; {base}.txt\"\n\nOutput is stored in .txt file.\n\n\nExample 1: CRC\n\nInput:\n\nfile: https://github.com/gurusamyanandakuma-r/bril/blob/main/benchmarks/mem/crc_check.bril\n\n\n\nOutput:\n```\nNumber of Functions:  11\n------------------------\nFunction Call Counts:\nmain     : 0\nrem      : 1\nn_zeros  : 2\ntoPtr    : 2\nwordSplit: 1\nxor      : 1\nshiftLeft: 1\nprint_ptr: 0\ncrc_rem  : 1\ncrc_check: 1\ntoNum    : 1\n\n\n##### Example 2: Fibonacci\n###### Input: \n- file: &lt;https://github.com/normrubin/bril/blob/main/benchmarks/core/fibonacci.bril&gt;\n\n###### Output:\n    ```\n    Number of Functions:  2\n    ------------------------\n    Function Call Counts:\n    main     : 0\n    Fibonacci: 3\n\n\n\nExample 3: Binary Search\n\nInput:\n\nfile: https://github.com/normrubin/bril/blob/main/benchmarks/mem/binary-search.bril\n\n\n\nOutput:\n    Number of Functions:  4\n    ------------------------\n    Function Call Counts:\n    pack         : 1\n    print_array  : 0\n    binary_search: 3\n    main         : 0",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Homework 1"
    ]
  },
  {
    "objectID": "blogs/rohit/project/Project.html",
    "href": "blogs/rohit/project/Project.html",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "",
    "text": "In memory-intensive C++ applications, optimizing memory usage is critical, especially in environments with limited resources. This project introduces a Python-based approach that leverages runtime profiling data and machine learning (ML) techniques to enhance memory efficiency. The approach utilizes a RandomForestRegressor ML model to predict memory bottlenecks by analyzing complex interactions in profiling data. By identifying patterns in memory allocation, access, and release, the tool applies targeted optimizations such as replacing inefficient memory allocation calls, reallocation strategies, and ensuring proper deallocation of memory. The results demonstrate significant reductions in memory footprint and peak memory usage, providing valuable insights for developers aiming to improve memory-intensive applications.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/rohit/project/Project.html#abstract",
    "href": "blogs/rohit/project/Project.html#abstract",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "",
    "text": "In memory-intensive C++ applications, optimizing memory usage is critical, especially in environments with limited resources. This project introduces a Python-based approach that leverages runtime profiling data and machine learning (ML) techniques to enhance memory efficiency. The approach utilizes a RandomForestRegressor ML model to predict memory bottlenecks by analyzing complex interactions in profiling data. By identifying patterns in memory allocation, access, and release, the tool applies targeted optimizations such as replacing inefficient memory allocation calls, reallocation strategies, and ensuring proper deallocation of memory. The results demonstrate significant reductions in memory footprint and peak memory usage, providing valuable insights for developers aiming to improve memory-intensive applications.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/rohit/project/Project.html#key-concepts",
    "href": "blogs/rohit/project/Project.html#key-concepts",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Key Concepts",
    "text": "Key Concepts\nBefore diving into the implementation details, let’s clarify some important terms:\n\nMemory Allocation: Memory allocation refers to the process by which a program requests memory from the system to store data. Dynamic allocation (e.g., malloc in C++) is common in applications where memory needs are determined during runtime.\nPeak Memory Usage: Peak memory usage is the maximum amount of memory consumed by a program at any point during its execution. Minimizing peak usage is essential for avoiding crashes and improving performance in resource-constrained environments.\nMemory Profiling: Memory profiling involves analyzing how an application allocates and frees memory over time. Tools like Valgrind’s Massif generate detailed reports on memory usage, helping identify inefficiencies or leaks.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/rohit/project/Project.html#goals",
    "href": "blogs/rohit/project/Project.html#goals",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Goals",
    "text": "Goals\n\nIdentify and optimize memory bottlenecks in five example C++ programs.\nImplement a Python tool that processes profiling data, predicts inefficiencies, and applies memory optimizations.\nEvaluate the effectiveness of these optimizations empirically.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/rohit/project/Project.html#design-and-implementation",
    "href": "blogs/rohit/project/Project.html#design-and-implementation",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Design and Implementation",
    "text": "Design and Implementation\n\nStep 1: Collecting Memory Profiling Data\n\nMethod\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\n%%| echo: false\ngraph LR;\nA[Start Profiling] --&gt; B[Run Program with Massif];\nB --&gt; C[Generate Massif Output];\nC --&gt; D[Analyze Output with ms_print];\nD --&gt; E[Convert to CSV for Analysis];\n\n\n\n\n\n\n\nTo analyze memory usage, we used Valgrind’s Massif tool:\n\nInstall Valgrind: sudo apt install valgrind\nCompile C++ programs with debug information: g++ -g -o example1 example1.cpp\nRun with Massif: valgrind --tool=massif ./example1\nAnalyze results: ms_print massif.out.&lt;pid&gt;\nConvert results to CSV for further analysis.\n\n\n\n\n\nStep 2: Building the Python Tool\nThe Python tool processes profiling data and optimizes memory usage through:\n\nInput Data:\n\nProfiling Data: Memory allocation, peak memory usage, and other metrics from Massif converted to CSV.\nSource Code: C++ programs requiring optimization.\n\n\n\nMachine Learning Component:\nWe used a RandomForestRegressor to predict memory inefficiencies based on the profiling data. This ML model enabled accurate identification of memory bottlenecks by analyzing complex interactions between memory allocation and usage patterns.\n\nRandomForestRegressor Algorithm\n\nInitialize Parameters: Set the number of trees (n_trees) and other hyperparameters.\nBootstrap Sampling: For each tree, draw a bootstrap sample from the original data.\nTrain Trees: Train a decision tree on each bootstrap sample. Each node in the tree considers a random subset of features when splitting.\nAggregate Predictions: For regression, average the predictions of all the trees in the forest.\n\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n\ngraph TD;\n    A[Start] --&gt; B[Initialize Parameters];\n    B --&gt; C[Bootstrap Sampling];\n    C --&gt; D[Train Decision Tree];\n    D --&gt; E{Repeat for n Trees};\n    E -- Yes --&gt; C;\n    E -- No --&gt; F[Aggregate Predictions];\n    F --&gt; G[Output Prediction];\n\n\n\n\n\n\n\nThis block diagram represents the workflow of the RandomForestRegressor. Here’s a step-by-step explanation:\n\nInitialize Parameters: Set the number of trees (n_trees) and other parameters like maximum depth, minimum samples per leaf, etc.\nBootstrap Sampling: Create multiple bootstrap samples (random subsets with replacement) from the original dataset.\nTrain Decision Tree: Train a decision tree on each bootstrap sample. During tree construction, each node selects a subset of features randomly and uses the best split among them.\nRepeat for n Trees: Repeat the bootstrap sampling and tree training for n trees.\nAggregate Predictions: For each test sample, predict the outcome using all the trained trees and then average these predictions to get the final output.\n\n\n\n\nOptimizations Implemented:\n\nCode Transformations:\n\nReplacing malloc with calloc for improved initialization.\nNullifying pointers after free to prevent dangling references.\n\nMemory Simulations:\n\nAdjusting allocation and peak values to simulate optimized memory behavior.\n\n\n\n\nMethod\n\n\n\n\n\n\n%%{init: {\"flowchart\": {\"htmlLabels\": false}} }%%\n%%| echo: false\ngraph TD;\nA[Input CSV and C++ Source] --&gt; B[Parse and Preprocess Data];\nB --&gt; C[Feature Engineering];\nC --&gt; D[Train RandomForest Regressor];\nD --&gt; E[Predict Memory Bottlenecks];\nE --&gt; F[Apply Code Transformations];\nF --&gt; G[Simulate Optimized Profile];\nG --&gt; H[Save Outputs];\n\n\n\n\n\n\n\n\nThe Python tool processes profiling data and optimizes memory usage through:\n\nFeature Engineering: Calculating Memory_Delta (difference between allocated and freed memory) and Memory_Utilization_Ratio (allocated memory divided by peak memory).\nSource Code Optimization: Replacing malloc with calloc and ensuring proper nullification after freeing memory.\nSimulated Profiling: Adjusting allocation and peak memory values to simulate optimization effects.\n\n\n\n\n\nStep 3: Comparing Profiles\nThe tool generates comparison charts showing memory allocation and peak memory usage before and after optimization. Empirical results highlight reductions in memory usage.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/rohit/project/Project.html#results",
    "href": "blogs/rohit/project/Project.html#results",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Results",
    "text": "Results\n\nExample Outputs\n\nExample 1: mem_alloc.cpp\n\n\n\n\nExample 1: mem_alloc.cpp\n\n\n\n\nTotal reduction in memory allocation: 37,833.81 KB\nReduction in peak memory usage: 788.47 KB\n\n\n\nExample 2: mem_fragmentation.cpp\n\n\n\n\nExample 2: mem_fragmentation.cpp\n\n\n\n\nTotal reduction in memory allocation: 1,256.08 KB\nReduction in peak memory usage: 7.98 KB\n\n\n\nExample 3: Cache_miss.cpp\n\n\n\n\nExample 3: Cache_miss.cpp\n\n\n\n\nTotal reduction in memory allocation: 2,415.55 KB\nReduction in peak memory usage: 397.83 KB\n\n\n\nExample 4: growing_ds.cpp\n\n\n\n\nExample 4: growing_ds.cpp\n\n\n\n\nTotal reduction in memory allocation: 7,388.50 KB\nReduction in peak memory usage: 622.00 KB\n\n\n\nExample 5: recursive.cpp\n\n\n\n\nExample 5: recursive.cpp\n\n\n\n\nTotal reduction in memory allocation: 43.20 KB\nReduction in peak memory usage: 7.20 KB",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/rohit/project/Project.html#challenges",
    "href": "blogs/rohit/project/Project.html#challenges",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Challenges",
    "text": "Challenges\n\nHardest Parts\n\nParsing Profiling Data: Extracting meaningful patterns from Massif’s output required converting data into a structured format (CSV).\nCode Transformations: Ensuring that optimizations like replacing malloc with calloc preserved functionality without introducing bugs.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/rohit/project/Project.html#evaluation",
    "href": "blogs/rohit/project/Project.html#evaluation",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Evaluation",
    "text": "Evaluation\nThe optimizations were successful, with notable reductions in memory allocation and peak usage across all examples. For instance, in mem_leak.cpp, the optimized memory allocation dropped by over 2 GB, showcasing the impact of targeted memory management strategies.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/rohit/project/Project.html#conclusion",
    "href": "blogs/rohit/project/Project.html#conclusion",
    "title": "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach",
    "section": "Conclusion",
    "text": "Conclusion\nThis project demonstrates that combining Python and ML techniques with traditional profiling tools can effectively optimize memory usage in C++ applications. By automating analysis and applying systematic optimizations, developers can significantly improve efficiency, paving the way for better performance in memory-constrained environments.\nThis is the code link for this project: https://github.com/gurusamyanandakuma-r/bril/tree/main/HW/Sharmila_Rohit_Project",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Rohit",
      "Project",
      "Machine Learning-Driven Memory Optimization for C++ Applications: A Python-Based Approach"
    ]
  },
  {
    "objectID": "blogs/matin/2024-11-01-HW5.html",
    "href": "blogs/matin/2024-11-01-HW5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Using The LLVM Plugin To Embed Modified Bitcode In Executable\nFor this homework I decided to write a pass which runs at the very end of the optimization pipeline that does the following: 1. It first clones the module being worked on. 2. It then finds all outlined functions and global variables annotated with the keyword patch_point and removes their definitions. 3. Embeds the cloned bitcode into a section of the final emitted ELF.\nEmbedding the LLVM IR bitcode of the ELF allows for easier generation of patched and instrumented binaries at runtime because it retains important information like the calling convention of the functions to be patched.\nI ran the compiler plugin on the following program:\n\n#include &lt;iostream&gt;\n\ntemplate &lt;typename T&gt;\n__attribute__((used, noinline, annotate(\"patch_point\"))) T myPathPoint(T a,\n                                                                       T b) {\n  return a + b;\n}\n\n\n\nint main() {\n  static constexpr int myArray[]{1, 4, 5, 6, 10};\n  int mySum{0};\n  for (const auto &el : myArray) {\n    mySum += el * myPathPoint(mySum, el);\n  }\n  std::cout &lt;&lt; \"Sum of my array: \" &lt;&lt; mySum &lt;&lt; std::endl;\n  return 0;\n}\nThe original intercepted llvm::Module by the compiler plugin for this program was the following:\n; ModuleID = '/home/matinraayai/CLionProjects/bril/llvm-plugin/example/Example.cpp'\nsource_filename = \"/home/matinraayai/CLionProjects/bril/llvm-plugin/example/Example.cpp\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-unknown-linux-gnu\"\n\nmodule asm \".globl _ZSt21ios_base_library_initv\"\n\n%\"class.std::basic_ostream\" = type { ptr, %\"class.std::basic_ios\" }\n%\"class.std::basic_ios\" = type { %\"class.std::ios_base\", ptr, i8, i8, ptr, ptr, ptr, ptr }\n%\"class.std::ios_base\" = type { ptr, i64, i64, i32, i32, i32, ptr, %\"struct.std::ios_base::_Words\", [8 x %\"struct.std::ios_base::_Words\"], i32, ptr, %\"class.std::locale\" }\n%\"struct.std::ios_base::_Words\" = type { ptr, i64 }\n%\"class.std::locale\" = type { ptr }\n\n$_Z11myPathPointIiET_S0_S0_ = comdat any\n\n@_ZZ4mainE7myArray = internal constant [5 x i32] [i32 1, i32 4, i32 5, i32 6, i32 10], align 16\n@_ZSt4cout = external dso_local global %\"class.std::basic_ostream\", align 8\n@.str = private unnamed_addr constant [18 x i8] c\"Sum of my array: \\00\", align 1\n@.str.1 = private unnamed_addr constant [12 x i8] c\"patch_point\\00\", section \"llvm.metadata\"\n@.str.2 = private unnamed_addr constant [69 x i8] c\"/home/matinraayai/CLionProjects/bril/llvm-plugin/example/Example.cpp\\00\", section \"llvm.metadata\"\n@llvm.global.annotations = appending global [1 x { ptr, ptr, ptr, i32, ptr }] [{ ptr, ptr, ptr, i32, ptr } { ptr @_Z11myPathPointIiET_S0_S0_, ptr @.str.1, ptr @.str.2, i32 4, ptr null }], section \"llvm.metadata\"\n@llvm.compiler.used = appending global [1 x ptr] [ptr @_Z11myPathPointIiET_S0_S0_], section \"llvm.metadata\"\n\n; Function Attrs: mustprogress noinline norecurse optnone uwtable\ndefine dso_local noundef i32 @main() #0 {\n  %1 = alloca i32, align 4\n  %2 = alloca i32, align 4\n  %3 = alloca ptr, align 8\n  %4 = alloca ptr, align 8\n  %5 = alloca ptr, align 8\n  %6 = alloca ptr, align 8\n  store i32 0, ptr %1, align 4\n  store i32 0, ptr %2, align 4\n  store ptr @_ZZ4mainE7myArray, ptr %3, align 8\n  store ptr @_ZZ4mainE7myArray, ptr %4, align 8\n  store ptr getelementptr inbounds (i32, ptr @_ZZ4mainE7myArray, i64 5), ptr %5, align 8\n  br label %7\n\n7:                                                ; preds = %22, %0\n  %8 = load ptr, ptr %4, align 8\n  %9 = load ptr, ptr %5, align 8\n  %10 = icmp ne ptr %8, %9\n  br i1 %10, label %11, label %25\n\n11:                                               ; preds = %7\n  %12 = load ptr, ptr %4, align 8\n  store ptr %12, ptr %6, align 8\n  %13 = load ptr, ptr %6, align 8\n  %14 = load i32, ptr %13, align 4\n  %15 = load i32, ptr %2, align 4\n  %16 = load ptr, ptr %6, align 8\n  %17 = load i32, ptr %16, align 4\n  %18 = call noundef i32 @_Z11myPathPointIiET_S0_S0_(i32 noundef %15, i32 noundef %17)\n  %19 = mul nsw i32 %14, %18\n  %20 = load i32, ptr %2, align 4\n  %21 = add nsw i32 %20, %19\n  store i32 %21, ptr %2, align 4\n  br label %22\n\n22:                                               ; preds = %11\n  %23 = load ptr, ptr %4, align 8\n  %24 = getelementptr inbounds i32, ptr %23, i32 1\n  store ptr %24, ptr %4, align 8\n  br label %7\n\n25:                                               ; preds = %7\n  %26 = call noundef nonnull align 8 dereferenceable(8) ptr @_ZStlsISt11char_traitsIcEERSt13basic_ostreamIcT_ES5_PKc(ptr noundef nonnull align 8 dereferenceable(8) @_ZSt4cout, ptr noundef @.str)\n  %27 = load i32, ptr %2, align 4\n  %28 = call noundef nonnull align 8 dereferenceable(8) ptr @_ZNSolsEi(ptr noundef nonnull align 8 dereferenceable(8) %26, i32 noundef %27)\n  %29 = call noundef nonnull align 8 dereferenceable(8) ptr @_ZNSolsEPFRSoS_E(ptr noundef nonnull align 8 dereferenceable(8) %28, ptr noundef @_ZSt4endlIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_)\n  ret i32 0\n}\n\n; Function Attrs: mustprogress noinline nounwind optnone uwtable\ndefine linkonce_odr dso_local noundef i32 @_Z11myPathPointIiET_S0_S0_(i32 noundef %0, i32 noundef %1) #1 comdat {\n  %3 = alloca i32, align 4\n  %4 = alloca i32, align 4\n  store i32 %0, ptr %3, align 4\n  store i32 %1, ptr %4, align 4\n  %5 = load i32, ptr %3, align 4\n  %6 = load i32, ptr %4, align 4\n  %7 = add nsw i32 %5, %6\n  ret i32 %7\n}\n\ndeclare dso_local noundef nonnull align 8 dereferenceable(8) ptr @_ZStlsISt11char_traitsIcEERSt13basic_ostreamIcT_ES5_PKc(ptr noundef nonnull align 8 dereferenceable(8), ptr noundef) #2\n\ndeclare dso_local noundef nonnull align 8 dereferenceable(8) ptr @_ZNSolsEi(ptr noundef nonnull align 8 dereferenceable(8), i32 noundef) #2\n\ndeclare dso_local noundef nonnull align 8 dereferenceable(8) ptr @_ZNSolsEPFRSoS_E(ptr noundef nonnull align 8 dereferenceable(8), ptr noundef) #2\n\ndeclare dso_local noundef nonnull align 8 dereferenceable(8) ptr @_ZSt4endlIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_(ptr noundef nonnull align 8 dereferenceable(8)) #2\n\nattributes #0 = { mustprogress noinline norecurse optnone uwtable \"frame-pointer\"=\"all\" \"min-legal-vector-width\"=\"0\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\nattributes #1 = { mustprogress noinline nounwind optnone uwtable \"frame-pointer\"=\"all\" \"min-legal-vector-width\"=\"0\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\nattributes #2 = { \"frame-pointer\"=\"all\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\n\n!llvm.linker.options = !{}\n!llvm.module.flags = !{!0, !1, !2}\n!llvm.ident = !{!3}\n\n!0 = !{i32 1, !\"wchar_size\", i32 4}\n!1 = !{i32 7, !\"uwtable\", i32 2}\n!2 = !{i32 7, !\"frame-pointer\", i32 2}\n!3 = !{!\"AMD clang version 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.2.0 24292 26466ce804ac523b398608f17388eb6d605a3f09)\"}\nThe modified bitcode looks like the following, with _Z11myPathPointIiET_S0_S0_ being identified as a patch point and its definition removed:\n; ModuleID = '/home/matinraayai/CLionProjects/bril/llvm-plugin/example/Example.cpp'\nsource_filename = \"/home/matinraayai/CLionProjects/bril/llvm-plugin/example/Example.cpp\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-unknown-linux-gnu\"\n\nmodule asm \".globl _ZSt21ios_base_library_initv\"\n\n%\"class.std::basic_ostream\" = type { ptr, %\"class.std::basic_ios\" }\n%\"class.std::basic_ios\" = type { %\"class.std::ios_base\", ptr, i8, i8, ptr, ptr, ptr, ptr }\n%\"class.std::ios_base\" = type { ptr, i64, i64, i32, i32, i32, ptr, %\"struct.std::ios_base::_Words\", [8 x %\"struct.std::ios_base::_Words\"], i32, ptr, %\"class.std::locale\" }\n%\"struct.std::ios_base::_Words\" = type { ptr, i64 }\n%\"class.std::locale\" = type { ptr }\n\n@_ZZ4mainE7myArray = external constant [5 x i32], align 16\n@_ZSt4cout = external global %\"class.std::basic_ostream\", align 8\n@.str = external unnamed_addr constant [18 x i8], align 1\n@.str.1 = external unnamed_addr constant [12 x i8], section \"llvm.metadata\"\n@.str.2 = external unnamed_addr constant [69 x i8], section \"llvm.metadata\"\n\n; Function Attrs: mustprogress noinline norecurse optnone uwtable\ndefine dso_local noundef i32 @main() #0 {\n  %1 = alloca i32, align 4\n  %2 = alloca i32, align 4\n  %3 = alloca ptr, align 8\n  %4 = alloca ptr, align 8\n  %5 = alloca ptr, align 8\n  %6 = alloca ptr, align 8\n  store i32 0, ptr %1, align 4\n  store i32 0, ptr %2, align 4\n  store ptr @_ZZ4mainE7myArray, ptr %3, align 8\n  store ptr @_ZZ4mainE7myArray, ptr %4, align 8\n  store ptr getelementptr inbounds (i32, ptr @_ZZ4mainE7myArray, i64 5), ptr %5, align 8\n  br label %7\n\n7:                                                ; preds = %22, %0\n  %8 = load ptr, ptr %4, align 8\n  %9 = load ptr, ptr %5, align 8\n  %10 = icmp ne ptr %8, %9\n  br i1 %10, label %11, label %25\n\n11:                                               ; preds = %7\n  %12 = load ptr, ptr %4, align 8\n  store ptr %12, ptr %6, align 8\n  %13 = load ptr, ptr %6, align 8\n  %14 = load i32, ptr %13, align 4\n  %15 = load i32, ptr %2, align 4\n  %16 = load ptr, ptr %6, align 8\n  %17 = load i32, ptr %16, align 4\n  %18 = call noundef i32 @_Z11myPathPointIiET_S0_S0_(i32 noundef %15, i32 noundef %17)\n  %19 = mul nsw i32 %14, %18\n  %20 = load i32, ptr %2, align 4\n  %21 = add nsw i32 %20, %19\n  store i32 %21, ptr %2, align 4\n  br label %22\n\n22:                                               ; preds = %11\n  %23 = load ptr, ptr %4, align 8\n  %24 = getelementptr inbounds i32, ptr %23, i32 1\n  store ptr %24, ptr %4, align 8\n  br label %7\n\n25:                                               ; preds = %7\n  %26 = call noundef nonnull align 8 dereferenceable(8) ptr @_ZStlsISt11char_traitsIcEERSt13basic_ostreamIcT_ES5_PKc(ptr noundef nonnull align 8 dereferenceable(8) @_ZSt4cout, ptr noundef @.str)\n  %27 = load i32, ptr %2, align 4\n  %28 = call noundef nonnull align 8 dereferenceable(8) ptr @_ZNSolsEi(ptr noundef nonnull align 8 dereferenceable(8) %26, i32 noundef %27)\n  %29 = call noundef nonnull align 8 dereferenceable(8) ptr @_ZNSolsEPFRSoS_E(ptr noundef nonnull align 8 dereferenceable(8) %28, ptr noundef @_ZSt4endlIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_)\n  ret i32 0\n}\n\n; Function Attrs: mustprogress noinline nounwind uwtable\ndeclare dso_local anyregcc noundef i32 @_Z11myPathPointIiET_S0_S0_(i32 noundef, i32 noundef) #1\n\ndeclare dso_local noundef nonnull align 8 dereferenceable(8) ptr @_ZStlsISt11char_traitsIcEERSt13basic_ostreamIcT_ES5_PKc(ptr noundef nonnull align 8 dereferenceable(8), ptr noundef) #2\n\ndeclare dso_local noundef nonnull align 8 dereferenceable(8) ptr @_ZNSolsEi(ptr noundef nonnull align 8 dereferenceable(8), i32 noundef) #2\n\ndeclare dso_local noundef nonnull align 8 dereferenceable(8) ptr @_ZNSolsEPFRSoS_E(ptr noundef nonnull align 8 dereferenceable(8), ptr noundef) #2\n\ndeclare dso_local noundef nonnull align 8 dereferenceable(8) ptr @_ZSt4endlIcSt11char_traitsIcEERSt13basic_ostreamIT_T0_ES6_(ptr noundef nonnull align 8 dereferenceable(8)) #2\n\nattributes #0 = { mustprogress noinline norecurse optnone uwtable \"frame-pointer\"=\"all\" \"min-legal-vector-width\"=\"0\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\nattributes #1 = { mustprogress noinline nounwind uwtable \"frame-pointer\"=\"all\" \"min-legal-vector-width\"=\"0\" \"no-trapping-math\"=\"true\" \"patch_point\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\nattributes #2 = { \"frame-pointer\"=\"all\" \"no-trapping-math\"=\"true\" \"stack-protector-buffer-size\"=\"8\" \"target-cpu\"=\"x86-64\" \"target-features\"=\"+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87\" \"tune-cpu\"=\"generic\" }\n\n!llvm.linker.options = !{}\n!llvm.module.flags = !{!0, !1, !2}\n!llvm.ident = !{!3}\n\n!0 = !{i32 1, !\"wchar_size\", i32 4}\n!1 = !{i32 7, !\"uwtable\", i32 2}\n!2 = !{i32 7, !\"frame-pointer\", i32 2}\n!3 = !{!\"AMD clang version 18.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-6.2.0 24292 26466ce804ac523b398608f17388eb6d605a3f09)\"}\nInspecting the compiled executable shows the addition of the .llvmbc section, indicating the LLVM bitcode has been successfully embedded inside the final executable and can be accessed by a runtime/loader:\nSection Headers:\n  [Nr] Name              Type             Address           Offset\n       Size              EntSize          Flags  Link  Info  Align\n  [ 0]                   NULL             0000000000000000  00000000\n       0000000000000000  0000000000000000           0     0     0\n  [ 1] .interp           PROGBITS         00000000002002a8  000002a8\n       000000000000001c  0000000000000000   A       0     0     1\n  [ 2] .note.ABI-tag     NOTE             00000000002002c4  000002c4\n       0000000000000020  0000000000000000   A       0     0     4\n  [ 3] .dynsym           DYNSYM           00000000002002e8  000002e8\n       0000000000000120  0000000000000018   A       7     1     8\n  [ 4] .gnu.version      VERSYM           0000000000200408  00000408\n       0000000000000018  0000000000000002   A       3     0     2\n  [ 5] .gnu.version_r    VERNEED          0000000000200420  00000420\n       0000000000000080  0000000000000000   A       7     3     4\n  [ 6] .gnu.hash         GNU_HASH         00000000002004a0  000004a0\n       0000000000000024  0000000000000000   A       3     0     8\n  [ 7] .dynstr           STRTAB           00000000002004c4  000004c4\n       000000000000017f  0000000000000000   A       0     0     1\n  [ 8] .rela.dyn         RELA             0000000000200648  00000648\n       0000000000000090  0000000000000018   A       3     0     8\n  [ 9] .rela.plt         RELA             00000000002006d8  000006d8\n       00000000000000a8  0000000000000018  AI       3    23     8\n  [10] .rodata           PROGBITS         0000000000200780  00000780\n       0000000000000036  0000000000000000 AMS       0     0     16\n  [11] .eh_frame_hdr     PROGBITS         00000000002007b8  000007b8\n       000000000000003c  0000000000000000   A       0     0     4\n  [12] .eh_frame         PROGBITS         00000000002007f8  000007f8\n       00000000000000c8  0000000000000000   A       0     0     8\n  [13] .text             PROGBITS         00000000002018c0  000008c0\n       00000000000001e4  0000000000000000  AX       0     0     16\n  [14] .init             PROGBITS         0000000000201aa4  00000aa4\n       000000000000001b  0000000000000000  AX       0     0     4\n  [15] .fini             PROGBITS         0000000000201ac0  00000ac0\n       000000000000000d  0000000000000000  AX       0     0     4\n  [16] .plt              PROGBITS         0000000000201ad0  00000ad0\n       0000000000000080  0000000000000000  AX       0     0     16\n  [17] .init_array       INIT_ARRAY       0000000000202b50  00000b50\n       0000000000000008  0000000000000000  WA       0     0     8\n  [18] .fini_array       FINI_ARRAY       0000000000202b58  00000b58\n       0000000000000008  0000000000000000  WA       0     0     8\n  [19] .dynamic          DYNAMIC          0000000000202b60  00000b60\n       00000000000001b0  0000000000000010  WA       7     0     8\n  [20] .got              PROGBITS         0000000000202d10  00000d10\n       0000000000000028  0000000000000000  WA       0     0     8\n  [21] .relro_padding    NOBITS           0000000000202d38  00000d38\n       00000000000002c8  0000000000000000  WA       0     0     1\n  [22] .data             PROGBITS         0000000000203d38  00000d38\n       0000000000000010  0000000000000000  WA       0     0     8\n  [23] .got.plt          PROGBITS         0000000000203d48  00000d48\n       0000000000000050  0000000000000000  WA       0     0     8\n  [24] .bss              NOBITS           0000000000203dc0  00000d98\n       0000000000000190  0000000000000000  WA       0     0     64\n  [25] .comment          PROGBITS         0000000000000000  00000d98\n       00000000000000a1  0000000000000001  MS       0     0     1\n  [26] .llvmbc           PROGBITS         0000000000000000  00000e39\n       00000000000010c4  0000000000000000           0     0     1\n  [27] .symtab           SYMTAB           0000000000000000  00001f00\n       0000000000000390  0000000000000018          29    21     8\n  [28] .shstrtab         STRTAB           0000000000000000  00002290\n       000000000000010c  0000000000000000           0     0     1\n  [29] .strtab           STRTAB           0000000000000000  0000239c\n       0000000000000279  0000000000000000           0     0     1\n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Homework 5"
    ]
  },
  {
    "objectID": "blogs/matin/2024-09-28-HW2.html",
    "href": "blogs/matin/2024-09-28-HW2.html",
    "title": "Homework 2: Trivial Dead Code Elimination and Local Value Numbering Passes",
    "section": "",
    "text": "Part 1: Trivial Dead Code Elimination\nThe first part of homework 2 is implemented under the examples/tdce_matin.py file in my BRIL fork. It consists of two function passes that are run in the following order: 1. A pass which iteratively detects unused variables across the entire function and removes them. 2. A local version of the pass, which removes any re-definitions of a variable.\nThe test folder under examples/test/tdce under my BRIL fork contains the turnt script and test cases to test my TDCE implementation.\n\n\nPart 2: Local Value Numbering\nThe second part of homework 2 is implemented under the examples/lvn_matin.py file in my BRIL fork. It implements the vanilla LVN algorithm with support for renaming re-assigned variables.\nSimilar to TDCE, I updated the test folder under examples/test/lvn in my BRIL fork to run my LVN implementation with the existing test cases. As my implementation does not support constant folding and identity propogation, I had to update the expected result of some of the tests.\n\n\nCorrectness Evidence\nBesides the passing tests, I applied both my passes to the fizzbuzz benchmark in BRIL and found a decrease in the number of instructions executed as well as no change in the output of the benchmark, futher demonstrating the correctness of the implementation:\n\nWithout LVN + TDCE:\n\nbril2json &lt; ../benchmarks/core/fizz-buzz.bril | brili 10 -p\n1\n2\n-2\n4\n-3\n-2\n7\n8\n-2\ntotal_dyn_inst: 332\n\nWith LVN + TDCE:\n\nbril2json &lt; ../benchmarks/core/fizz-buzz.bril | python3 lvn_matin.py |\n python3 tdce_matin.py | brili 10 -p\n1\n2\n-2\n4\n-3\n-2\n7\n8\n-2\ntotal_dyn_inst: 278\n\n\nChallenges\nOverall, I found the hard part being working with BRIL.\n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Homework 2: Trivial Dead Code Elimination and Local Value Numbering Passes"
    ]
  },
  {
    "objectID": "blogs/matin/2024-10-11-HW3.html",
    "href": "blogs/matin/2024-10-11-HW3.html",
    "title": "Homework 3: Dataflow Analysis",
    "section": "",
    "text": "For this homework I implemented a liveness analysis in BRIL. I implemented it using the pseudo code that was covered in class. My implementation can be found here.\nI tested my implementation on the tests already under the df folder, as well as other tests for other programs. I added a small portion at the end which checks if the live-ins of the successors of each block is included in the live-outs of each block.\nSample Outputs:\nbril2json &lt; ../../../benchmarks/mem/adj2csr.bril | python3 ../../df_matin.py\n\nb1:\n  in:  max, seq\n  out: ∅\nb1:\n  in:  rng, size\n  out: arr, i, max, one, rng, size, zero\nloop:\n  in:  arr, i, max, one, rng, size, zero\n  out: arr, i, max, one, rng, size, zero\nbody:\n  in:  arr, i, max, one, rng, size, zero\n  out: arr, i, max, one, rng, size, val, zero\nif_body:\n  in:  arr, i, max, one, rng, size, zero\n  out: arr, i, max, one, rng, size, val, zero\nif_done:\n  in:  arr, i, max, one, rng, size, val, zero\n  out: arr, i, max, one, rng, size, zero\nloop_end:\n  in:  arr, i, max, one, rng, size, zero\n  out: arr, i, max, one, rng, size, zero\ndone:\n  in:  arr\n  out: ∅\nb1:\n  in:  arr, size\n  out: arr, i, one, size\nloop:\n  in:  arr, i, one, size\n  out: arr, i, one, size\nbody:\n  in:  arr, i, one, size\n  out: arr, i, one, size\nloop_end:\n  in:  arr, i, one, size\n  out: arr, i, one, size\ndone:\n  in:  ∅\n  out: ∅\nb1:\n  in:  size\n  out: arr, i, one, size, zero\nloop:\n  in:  arr, i, one, size, zero\n  out: arr, i, one, size, zero\nbody:\n  in:  arr, i, one, size, zero\n  out: arr, i, one, size, zero\nloop_end:\n  in:  arr, i, one, size, zero\n  out: arr, i, one, size, zero\ndone:\n  in:  arr\n  out: ∅\nb1:\n  in:  adjmat, csr_edges, csr_offset, num_nodes\n  out: adjmat, csr_edges, csr_offset, num_edges, num_nodes, one, row\niter_row:\n  in:  adjmat, csr_edges, csr_offset, num_edges, num_nodes, one, row\n  out: adjmat, col, csr_edges, csr_offset, num_edges, num_nodes, one, row\niter_col:\n  in:  adjmat, col, csr_edges, csr_offset, num_edges, num_nodes, one, row\n  out: adjmat, col, csr_edges, csr_offset, num_edges, num_nodes, one, row\ncol_body:\n  in:  adjmat, col, csr_edges, csr_offset, num_edges, num_nodes, one, row\n  out: adjmat, col, csr_edges, csr_offset, num_edges, num_nodes, one, row\nif_body:\n  in:  adjmat, col, csr_edges, csr_offset, num_edges, num_nodes, one, row\n  out: adjmat, col, csr_edges, csr_offset, num_edges, num_nodes, one, row\ncol_end:\n  in:  adjmat, col, csr_edges, csr_offset, num_edges, num_nodes, one, row\n  out: adjmat, col, csr_edges, csr_offset, num_edges, num_nodes, one, row\ncol_done:\n  in:  adjmat, csr_edges, csr_offset, num_edges, num_nodes, one, row\n  out: adjmat, csr_edges, csr_offset, num_edges, num_nodes, one, row\nrow_end:\n  in:  adjmat, csr_edges, csr_offset, num_edges, num_nodes, one, row\n  out: adjmat, csr_edges, csr_offset, num_edges, num_nodes, one, row\nrow_done:\n  in:  num_edges\n  out: ∅\nb1:\n  in:  num_nodes, seed\n  out: ∅\nbril2json &lt; ../../../benchmarks/mixed/cholesky.bril | python3 ../../df_matin.py\n\nb1:\n  in:  ∅\n  out: ∅\nb1:\n  in:  sqsize\n  out: arr, i, one, sqsize, zero\nloop:\n  in:  arr, i, one, sqsize, zero\n  out: arr, i, one, sqsize, zero\nbody:\n  in:  arr, i, one, sqsize, zero\n  out: arr, i, one, sqsize, zero\nloop_end:\n  in:  arr, i, one, sqsize, zero\n  out: arr, i, one, sqsize, zero\ndone:\n  in:  arr\n  out: ∅\nb1:\n  in:  arr, size\n  out: arr, i, one, size\nloop:\n  in:  arr, i, one, size\n  out: arr, i, one, size\nbody:\n  in:  arr, i, one, size\n  out: arr, i, one, size\nloop_end:\n  in:  arr, i, one, size\n  out: arr, i, one, size\ndone:\n  in:  ∅\n  out: ∅\nb1:\n  in:  arr1, arr2, dest, size\n  out: arr1, arr2, dest, one, row, size\nrow.loop:\n  in:  arr1, arr2, dest, one, row, size\n  out: arr1, arr2, dest, one, row, size\nrow.body:\n  in:  arr1, arr2, dest, one, row, size\n  out: arr1, arr2, col, dest, one, row, size\ncol.loop:\n  in:  arr1, arr2, col, dest, one, row, size\n  out: arr1, arr2, col, dest, one, row, size\ncol.body:\n  in:  arr1, arr2, col, dest, one, row, size\n  out: arr1, arr2, col, dest, i, one, row, size, sum\nsum.loop:\n  in:  arr1, arr2, col, dest, i, one, row, size, sum\n  out: arr1, arr2, col, dest, i, one, row, size, sum\nsum.body:\n  in:  arr1, arr2, col, dest, i, one, row, size, sum\n  out: arr1, arr2, col, dest, i, one, row, size, sum\nsum.loop_end:\n  in:  arr1, arr2, col, dest, i, one, row, size, sum\n  out: arr1, arr2, col, dest, i, one, row, size, sum\nsum.done:\n  in:  arr1, arr2, col, dest, one, row, size, sum\n  out: arr1, arr2, col, dest, one, row, size\ncol.loop_end:\n  in:  arr1, arr2, col, dest, one, row, size\n  out: arr1, arr2, col, dest, one, row, size\ncol.done:\n  in:  arr1, arr2, dest, one, row, size\n  out: arr1, arr2, dest, one, row, size\nrow.loop_end:\n  in:  arr1, arr2, dest, one, row, size\n  out: arr1, arr2, dest, one, row, size\nrow.done:\n  in:  ∅\n  out: ∅\nb1:\n  in:  input, output, size\n  out: input, one, output, row, size\nrow.loop:\n  in:  input, one, output, row, size\n  out: input, one, output, row, size\nrow.body:\n  in:  input, one, output, row, size\n  out: col, input, one, output, row, size\ncol.loop:\n  in:  col, input, one, output, row, size\n  out: col, input, one, output, row, size\ncol.body:\n  in:  col, input, one, output, row, size\n  out: col, input, one, output, row, size\ncol.loop_end:\n  in:  col, input, one, output, row, size\n  out: col, input, one, output, row, size\ncol.done:\n  in:  input, one, output, row, size\n  out: input, one, output, row, size\nrow.loop_end:\n  in:  input, one, output, row, size\n  out: input, one, output, row, size\nrow.done:\n  in:  ∅\n  out: ∅\nb1:\n  in:  input\n  out: n, notdone, precision, x\nfor.cond.4:\n  in:  n, notdone, precision, x\n  out: n, notdone, precision, x\nfor.body.4:\n  in:  n, notdone, precision, x\n  out: diff, n, notdone, precision, root\nthen.18:\n  in:  diff, n, notdone, precision, root\n  out: diff, n, notdone, precision, root\nelse.18:\n  in:  diff, n, notdone, precision, root\n  out: diff, n, notdone, precision, root\nendif.18:\n  in:  diff, n, notdone, precision, root\n  out: n, notdone, precision, root\nthen.25:\n  in:  n, precision, root\n  out: n, notdone, precision, root\nelse.25:\n  in:  n, notdone, precision, root\n  out: n, notdone, precision, root\nendif.25:\n  in:  n, notdone, precision, root\n  out: n, notdone, precision, x\nfor.end.4:\n  in:  x\n  out: ∅\nb1:\n  in:  arr1, arr2, size\n  out: arr1, arr2, i, one, size\ni.loop:\n  in:  arr1, arr2, i, one, size\n  out: arr1, arr2, i, one, size\ni.body:\n  in:  arr1, arr2, i, one, size\n  out: arr1, arr2, i, j, one, size\nj.loop:\n  in:  arr1, arr2, i, j, one, size\n  out: arr1, arr2, i, j, one, size\nj.body:\n  in:  arr1, arr2, i, j, one, size\n  out: arr1, arr2, i, j, k, one, size\nk.loop:\n  in:  arr1, arr2, i, j, k, one, size\n  out: arr1, arr2, i, j, k, one, size\nk.body:\n  in:  arr1, arr2, i, j, k, one, size\n  out: arr1, arr2, i, j, k, one, size\nk.loop_end:\n  in:  arr1, arr2, i, j, k, one, size\n  out: arr1, arr2, i, j, k, one, size\nk.done:\n  in:  arr1, arr2, i, j, one, size\n  out: arr1, arr2, i, j, one, size\nj.loop_end:\n  in:  arr1, arr2, i, j, one, size\n  out: arr1, arr2, i, j, one, size\nj.done:\n  in:  arr1, arr2, i, one, size\n  out: arr1, arr2, i, one, size\ni.loop_end:\n  in:  arr1, arr2, i, one, size\n  out: arr1, arr2, i, one, size\ni.done:\n  in:  ∅\n  out: ∅\nb1:\n  in:  ∅\n  out: ∅\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Matin",
      "Homework 3: Dataflow Analysis"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-10-11-HW3-Kellner.html",
    "href": "blogs/oscar/2024-10-11-HW3-Kellner.html",
    "title": "HW3 - Data Flow Analysis",
    "section": "",
    "text": "In this assignment, we model a dataflow framework to allow analysus for Bril programs in Lua through utilization of a control flow graph (CFG), which we also write a program to construct for us. A control flow graph represents all the possible paths that a program may take during execution in graph notation, delimited in blocks. Thus, we also use the gen_blocks program we had rewritten in the previous assignment.\nUsing the provided examples for direction, we first verify the correctness of the generated CFG by generating a GraphViz file and comparing with the same output that the example provides. Then, we attempt to implement a framework for dataflow analysis, allowing a user-defined analysis to work with the CFG we generated.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW3 - Data Flow Analysis"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-10-11-HW3-Kellner.html#overview",
    "href": "blogs/oscar/2024-10-11-HW3-Kellner.html#overview",
    "title": "HW3 - Data Flow Analysis",
    "section": "",
    "text": "In this assignment, we model a dataflow framework to allow analysus for Bril programs in Lua through utilization of a control flow graph (CFG), which we also write a program to construct for us. A control flow graph represents all the possible paths that a program may take during execution in graph notation, delimited in blocks. Thus, we also use the gen_blocks program we had rewritten in the previous assignment.\nUsing the provided examples for direction, we first verify the correctness of the generated CFG by generating a GraphViz file and comparing with the same output that the example provides. Then, we attempt to implement a framework for dataflow analysis, allowing a user-defined analysis to work with the CFG we generated.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW3 - Data Flow Analysis"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-10-11-HW3-Kellner.html#part-1-control-flow-graph",
    "href": "blogs/oscar/2024-10-11-HW3-Kellner.html#part-1-control-flow-graph",
    "title": "HW3 - Data Flow Analysis",
    "section": "Part 1: Control Flow Graph",
    "text": "Part 1: Control Flow Graph\nOne challenge of working with Lua is that, as powerful and versatile tables are (essentially being an analogue of Python’s dictionaries, or hash tables), are that there appears to be no standard implementation for other common data types such as sets, arrays, and so on. A lot of boilerplate must be written to emulate the same functionality that Python allows natively or within their standard library. One of these data structures is the OrderedDict, which keeps the order of key-value pairs stored in the order in which they were added. Lua does not keep a consistent order in its key-value pairs, so without any kind of ordering, the CFG will not maintain its order as its being created and it causes the graph to be formed incorrectly. We fix this by keeping an extra table specifically to keep track of the order in which blocks are added, which must be passed around alongside the block list across functions.\nWe write cfg_dot.lua to generate GraphViz files to represent bril programs in CFG form to test our implementation. A small shell script generates an PNG of the CFG for all functions that was stored in the GraphViz file. The CFG for the merge sort program is included, which includes a decent amount of control flow to demonstrate the CFG program.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW3 - Data Flow Analysis"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-10-11-HW3-Kellner.html#part-2-analysis-tool",
    "href": "blogs/oscar/2024-10-11-HW3-Kellner.html#part-2-analysis-tool",
    "title": "HW3 - Data Flow Analysis",
    "section": "Part 2: Analysis Tool",
    "text": "Part 2: Analysis Tool\nThe analysis tool, dataflow.lua and dataflow_run.lua, provides a small framework in which a user defined analysis can be run on an existing CFG for a bril program that is generated through use of the cfg.lua module created in part 1. A couple of boilerplate utility functions were implemented to allow for set operations. To allow for easier testing against the existing implementation, the output was carefully formatted to recreate that which the .out test files appear.\nOne of the issues encountered when debugging is that the program would loop indefinitely within the df_worklist function. It was resolved when a list of visited blocks was kept up to date, however it was remained to be seen whether it ended up affecting the correctness of the program.\nUnfortunately the output does not exactly match that of the provided example. Small errors appear in mismatching between the liveness of variables, particularly at the starts and ends of programs or functions. A small modification of the example turnt file allowed for comparison between the given and expected output.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW3 - Data Flow Analysis"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-09-27-HW2-Kellner.html",
    "href": "blogs/oscar/2024-09-27-HW2-Kellner.html",
    "title": "HW2 - Minor Local Optimizations",
    "section": "",
    "text": "In this assignment, we try to inplement two common forms of local optimization: trivial dead code elimination and value numbering. In this context, local refers to optimizations within the block level, meaning optimizations are isolated between any kind of control flow, which may include unconditional branches. Despite the limited scope of the optimizations, these two methods are relatively inexpensive and can be used in conjunction with each other to reduce unnecessary instructions from a program.\nI had struggled a lot with this homework in particular, partially due to my insistence on using Lua (a comparable scripting language to Python but with less library support) to get more familiar with the language. This also required rewriting plenty of utility functions to achieve some of the same functionality that was previously given, such as “gen_blocks”. The time constraints have not been friendly.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW2 - Minor Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-09-27-HW2-Kellner.html#overview",
    "href": "blogs/oscar/2024-09-27-HW2-Kellner.html#overview",
    "title": "HW2 - Minor Local Optimizations",
    "section": "",
    "text": "In this assignment, we try to inplement two common forms of local optimization: trivial dead code elimination and value numbering. In this context, local refers to optimizations within the block level, meaning optimizations are isolated between any kind of control flow, which may include unconditional branches. Despite the limited scope of the optimizations, these two methods are relatively inexpensive and can be used in conjunction with each other to reduce unnecessary instructions from a program.\nI had struggled a lot with this homework in particular, partially due to my insistence on using Lua (a comparable scripting language to Python but with less library support) to get more familiar with the language. This also required rewriting plenty of utility functions to achieve some of the same functionality that was previously given, such as “gen_blocks”. The time constraints have not been friendly.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW2 - Minor Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-09-27-HW2-Kellner.html#part-1-trivial-dead-code-elimination-tdce",
    "href": "blogs/oscar/2024-09-27-HW2-Kellner.html#part-1-trivial-dead-code-elimination-tdce",
    "title": "HW2 - Minor Local Optimizations",
    "section": "Part 1: Trivial Dead Code Elimination (TDCE)",
    "text": "Part 1: Trivial Dead Code Elimination (TDCE)\nFor each of these optimizations, we will need to split our program into a series of blocks, which as mentioned before is separated between control flow instructions, such as jmp, ret, and br. Within these blocks, we can analyze what variables are used within this block and if any dead instructions are found. We can classify “trivial dead code” as those that write to a variable, and subsequently never use said variable in any accessible code path within the block.\nWe can run this process as many times as we like until our code converges to a point where no changes are further made after a round of execution (or “pass”). A single TDCE pass may not eliminate all dead code on the first iteration, as (for example) some useless variables may be used to assign other useless variables, which our TDCE detects as being “used” until the dependent dead variables are also eliminated.\nImplementing this was not too difficult - just simply iterate through all the instructions for a given block, and keep a table of used variables. Then, reconstruct the sequence of instructions, excluding any instructions that write to variables not within our table, and finally rewrite the trimmed block back to the original function. Repeat for all blocks, and you have a single TDCE pass. I opted to write an option for a user-specified number of passes if they like, with &lt; 1 defaulting to repeating the TDCE until the program detects that no more instructions were eliminated in a given pass.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW2 - Minor Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-09-27-HW2-Kellner.html#part-2-local-value-numbering-lvn",
    "href": "blogs/oscar/2024-09-27-HW2-Kellner.html#part-2-local-value-numbering-lvn",
    "title": "HW2 - Minor Local Optimizations",
    "section": "Part 2: Local Value Numbering (LVN)",
    "text": "Part 2: Local Value Numbering (LVN)\nLocal value numbering works somewhat similarly to TDCE in that it will iterate through the instructions for a given block and keep track of some statistics to determine what code to modify. In this case, LVN aims to reduce redundancy within a block, by enumaterating expressions (as opposed to variables) it encounters and eliminating any values that share the same operation and operands.\nThere are a few methods that can be employed in conjunction in order to remove redundancy:\n\nConstant folding: If operands within an instruction are constants, we can “fold” the result directly into the code so the machine does not have to recalculate it at runtime. This may also include algebraic identities, but I did not attempt to support it here.\nPropagation: Redundant variables can be eliminated if multiple variables are assigned to be equal to each other and are not each used for different purposes.\nOther properties: Making use various mathematical properties / boolean logic to find redundant expressions, such as (a + b) and (b + a), or (x == x).\n\nDebugging this code has proven to be very frustrating as most of the flow for execution involves piping in multiple commands through standard input / output (usually to accomodate for Turnt and Bril utilities), so any attempt to print debugging information onto the screen will modify the output and cause any subsequent programs to fail. Work became somewhat disorganized when I had opted to save a lot of intermediate output files and additional testing programs just to observe the outcome of a single change. I will admit that I had to restructure / rewrite a lot of code multiple times to find a good balance, and in doing so I had revised a lot of the code in accordance to the example program after having made my own attempts at implementation.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW2 - Minor Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/oscar/2024-09-27-HW2-Kellner.html#testing",
    "href": "blogs/oscar/2024-09-27-HW2-Kellner.html#testing",
    "title": "HW2 - Minor Local Optimizations",
    "section": "Testing",
    "text": "Testing\nBecause Brili serves mostly to simulate code, our method of checking performance of our local optimization is to simply count the number of instructions reduced between the optimization passes. While this code does not aim specifically to target loops, it would be in our best interst to try and optimize repeated blocks of code so the gross number of instructions executed at runtime is reduced.\nI’m still struggling a lot to understand how turnt works. From my understanding, it doesn’t seem to work past one directory level? I’ve needed to copy all of my source files and also benchmark folders just to make the process a little more convenient, but it is still pretty frustrating.\nUnfortunately, there seems to be a few bugs in the local optimizations, as there are a select few programs that appear to fail the consistency check after optimizations (comparing to the existing .out file). There are two bril programs that are the culprits - mat-mul.bril and adj2csr.bril. Upon further inspection, both of these programs appear to use a random number generator of sorts, but the important constants may be filtered out before they are run. However, after optimizations, the rest of the programs I have tested on appear to produce an identical output. I have tested the TDCE and LVN programs, as well as the two chained after another, on all mem and core benchmarks.\nHere are some stats on the mem set of benchmarks:\nBaseline:\ntotal_dyn_inst: 56629\ntotal_dyn_inst: 6851\ntotal_dyn_inst: 78\ntotal_dyn_inst: 253\ntotal_dyn_inst: 121202\ntotal_dyn_inst: 88\ntotal_dyn_inst: 1006454\ntotal_dyn_inst: 121\ntotal_dyn_inst: 47\ntotal_dyn_inst: 1990407\ntotal_dyn_inst: 193\ntotal_dyn_inst: 677\ntotal_dyn_inst: 11029\ntotal_dyn_inst: 279\ntotal_dyn_inst: 27333\ntotal_dyn_inst: 264\ntotal_dyn_inst: 3482\ntotal_dyn_inst: 98\ntotal_dyn_inst: 86036\nTDCE only:\ntotal_dyn_inst: 56584\ntotal_dyn_inst: 6851\ntotal_dyn_inst: 75\ntotal_dyn_inst: 253\ntotal_dyn_inst: 120652\ntotal_dyn_inst: 88\ntotal_dyn_inst: 959702\ntotal_dyn_inst: 121\ntotal_dyn_inst: 47\ntotal_dyn_inst: 1990407\ntotal_dyn_inst: 193\ntotal_dyn_inst: 677\ntotal_dyn_inst: 11024\ntotal_dyn_inst: 279\ntotal_dyn_inst: 27011\ntotal_dyn_inst: 264\ntotal_dyn_inst: 3455\ntotal_dyn_inst: 88\ntotal_dyn_inst: 86036\nLVN only:\ntotal_dyn_inst: 56584\ntotal_dyn_inst: 6851\ntotal_dyn_inst: 78\ntotal_dyn_inst: 253\ntotal_dyn_inst: 121202\ntotal_dyn_inst: 88\ntotal_dyn_inst: 1006454\ntotal_dyn_inst: 121\ntotal_dyn_inst: 47\ntotal_dyn_inst: 1990407\ntotal_dyn_inst: 193\ntotal_dyn_inst: 677\ntotal_dyn_inst: 11029\ntotal_dyn_inst: 279\ntotal_dyn_inst: 27011\ntotal_dyn_inst: 264\ntotal_dyn_inst: 3482\ntotal_dyn_inst: 98\ntotal_dyn_inst: 86036\nLVN | TDCE:\ntotal_dyn_inst: 56584\ntotal_dyn_inst: 6851\ntotal_dyn_inst: 75\ntotal_dyn_inst: 253\ntotal_dyn_inst: 120652\ntotal_dyn_inst: 88\ntotal_dyn_inst: 959702\ntotal_dyn_inst: 121\ntotal_dyn_inst: 47\ntotal_dyn_inst: 1990407\ntotal_dyn_inst: 193\ntotal_dyn_inst: 677\ntotal_dyn_inst: 11024\ntotal_dyn_inst: 279\ntotal_dyn_inst: 27011\ntotal_dyn_inst: 264\ntotal_dyn_inst: 3455\ntotal_dyn_inst: 88\ntotal_dyn_inst: 86036\nUnfortunately, as it stands only TDCE seems to create a marginal difference in performance - ironically LVN tends to increase the number of instructions likely leading to worse performance. Though, with working within a small scope such as individual blocks, it may be expected that programs with a lot of control flow would not see much benefit from block-level optimization. Perhaps with more aggressive scheduling and more awareness of instruction count, I could get LVN to improve on its ability, however I have struggled a lot with the programming as it is given the time constraints.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Oscar",
      "HW2 - Minor Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/michael/11-26-2024-EATSS-Review.html",
    "href": "blogs/michael/11-26-2024-EATSS-Review.html",
    "title": "Paper Presentation – EATSS",
    "section": "",
    "text": "I reviewed and presented on a paper recently, titled Energy-Aware Tile Size Selection for Affine Programs on GPUs (EATSS), authored by Malith Jayaweera, Martin Kong, Yanzhi Wang, and David Kaeli. This paper developed a technique by which tile sizes are selected for affine programs, with power consumption and performance as optimization targets.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Paper Presentation -- EATSS"
    ]
  },
  {
    "objectID": "blogs/michael/11-26-2024-EATSS-Review.html#introduction",
    "href": "blogs/michael/11-26-2024-EATSS-Review.html#introduction",
    "title": "Paper Presentation – EATSS",
    "section": "",
    "text": "I reviewed and presented on a paper recently, titled Energy-Aware Tile Size Selection for Affine Programs on GPUs (EATSS), authored by Malith Jayaweera, Martin Kong, Yanzhi Wang, and David Kaeli. This paper developed a technique by which tile sizes are selected for affine programs, with power consumption and performance as optimization targets.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Paper Presentation -- EATSS"
    ]
  },
  {
    "objectID": "blogs/michael/11-26-2024-EATSS-Review.html#relevance",
    "href": "blogs/michael/11-26-2024-EATSS-Review.html#relevance",
    "title": "Paper Presentation – EATSS",
    "section": "Relevance",
    "text": "Relevance\nPower consumption in recent years has become a critical consideration when designing not only hardware systems, but also algorithms. As hardware power usage has improved, large scale software has yet to do much to make similar improvements. This paper is one of the first to explore this field of optimizing GPU programs for both power and performance, which is a field ripe for future work.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Paper Presentation -- EATSS"
    ]
  },
  {
    "objectID": "blogs/michael/11-26-2024-EATSS-Review.html#background",
    "href": "blogs/michael/11-26-2024-EATSS-Review.html#background",
    "title": "Paper Presentation – EATSS",
    "section": "Background",
    "text": "Background\n\nAffine Kernels\nAffine kernels are chunks of code which contain nested loops with affine bounds. These types of loops are amenable to polyhedral optimization techniques (one of which is loop tiling).\n\n\nLoop Tiling\nLoop tiling is an optimization technique which aims to increase data locality, and take advantage of the system memory heirarchy, generally to improve system performance (i.e. execution time). This is a common optimization technique in affine programs.\n\n\nGPUs\nThis technique is designed with GPUs in mind, and was examined on a set of NVIDIA GPUs. More specifically, these GPUs all share the same memory heirarchy, and enable something called “coalesced memory accesses” or “CMAs”. CMAs are a particular phenomenon, by which if there are many threads in a warp acessing contiguous elements, then fewer memory accesses are required, as nearby elements are pulled in with a single memory transaction as opposed to multiple. This is a core tenet of this algorithm.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Paper Presentation -- EATSS"
    ]
  },
  {
    "objectID": "blogs/michael/11-26-2024-EATSS-Review.html#the-algorithm",
    "href": "blogs/michael/11-26-2024-EATSS-Review.html#the-algorithm",
    "title": "Paper Presentation – EATSS",
    "section": "The Algorithm",
    "text": "The Algorithm\nEATSS aims to find tile sizes which maximize the “performance per watt” (PPW) of a given workload. Typically, optimization strategies optimize for performance (i.e. runtime) or code size, and performance per watt is just another optimization target. In order to determine the performance per watt of a workload, one must understand the speed at which the workload can be completed, and also the amount of energy to do so. However, the authors do not do any power measurements in optimizing their program. Instead, they use a proxy, which is the following objective function:\n\\[OBJ = \\prod_{\\text{i is par}} T_{i} + \\sum(H_{i} \\times T_{i}) \\]\nThe idea here is that this objective function, when maximized, results in the high inter-thread data sharing. This leads to more coalesced memory accesses. Why is this useful? Coalesced memory accesses result in fewer overall memory transactions, and less data movement. The authors claim, and demonstrate, that this results in less power usage. This is a neat idea, and makes a lot of sense to the hardware-aware reader.\nIn \\(OBJ\\), there are a handful of terms, which define the solution space. The authors use a SMT solver (Z3) to search this space for an optimal solution.\nThis compiler chooses tile sizes for affine programs, but has some additional nice features. One of which is it supports single precision and double precision data types. It also allows for varying splits between how much of combined memory is allocated to the L1-cache, and how much is allocated to shared memory.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Paper Presentation -- EATSS"
    ]
  },
  {
    "objectID": "blogs/michael/11-26-2024-EATSS-Review.html#results",
    "href": "blogs/michael/11-26-2024-EATSS-Review.html#results",
    "title": "Paper Presentation – EATSS",
    "section": "Results",
    "text": "Results\nThe authors test their optimization technique on two NVIDIA GPUs, a GA100, and a Xavier. The authors determine that their system performs very well on many benchmarks tested, though by varying degrees on each of the GPUs. Many of the comparisons the authors make are against “PPCG” (the polyhedral parallel code generator). Their optimizer operates on the code generated by PPCG by default, and looks to improve results. One comparison the authors conduct is their performance results against those generated by PPCG with a varying set of constraints. Namely, they run between 200-800 different configurations of PPCG for given kernels, and compare the results developed by EATSS against the entire space. The authors find that on some kernels, such as 2mm, 3mm, and gemm, the technique developed by the authors is better than any that PPCG finds in the search space. However, this is not true on all benchmarks, and the geometric mean PPW achieved by EATSS is lower than the average-best performance achieved by PPCG (which is to say, searching with PPCG) generally delivers better results.\nThe authors report additional results, such as that certain kernels (BLAS3-like kernels for example) perform better with more shared memory, but low dimensional kernels benefit from a higher proportion of allocation to the L1-cache. This shows how the ability to allocate cache and shared memory is a useful feature of the compiler. The authors find that their metric is a good approximation for power usage, as they find a high correlation between L2 cache reads and average power consumption (0.85 on 2mm, 0.75 on gemm). However, they also find that this correlation does not hold on kernels with O(1) memory reuse (it does hold if the memory reuse is ~O(n)).\nAdditionally, the authors compare their results against cuDNN and cuBLAS, two libraries with highly optimized affine kernels. They show that EATSS delivers 75% of cuBLAS and cuDNN’s PPW on the GA100, and performs significantly better than these libraries on Xavier (2.1x PPW).\nAnd finally, the authors compare against an autotuner ytop. This autotuner evelops worse solutions in every case observed than EATSS, and takes 17 minutes to get a solution, whereas EATSS takes on the order 1 second to converge to a solution.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Paper Presentation -- EATSS"
    ]
  },
  {
    "objectID": "blogs/michael/11-26-2024-EATSS-Review.html#what-could-be-improved",
    "href": "blogs/michael/11-26-2024-EATSS-Review.html#what-could-be-improved",
    "title": "Paper Presentation – EATSS",
    "section": "What Could be Improved?",
    "text": "What Could be Improved?\nThere are some areas of improvement in this paper. One is that the authors only run this technique on a small set of GPUs, and they are both NVIDIA GPUs. Their results demonstrate that performance actually varies quite a bit between the two, so I would argue that in order to really understand the impact of this technique, a wider set of hardare is required. For example, the memory heirarchy on AMD GPUs is markedly different from that on NVIDIA, and it’s not clear whether the proxy for energy usage really holds across GPU types. If I were to give one piece of advice to these authors, it would be to run experiments on more GPUs. During discussion of this work with others, this point seemed to resonate quite strongly as something that could have been improved.\nAnother shortcoming is that some of their comparisons are not really apples-to-apples comparisons. cuBLAS and cuDNN both use tensor cores in their implementation, whereas EATSS does not (beacuse PPCG does not). If EATSS could be modified to do so, that could bolster the usefulness of their results significantly.\nTwo other changes I would have liked to see, but are less critical, are with respect to their power evaluation, and lack of analysis as to why EATSS does not find the best result.\nThe measurement of how much power the GPUs under test are using is based on averages of measurements collected from nvidia-smi and tegrastats as opposed to physical measurements. It has been shown in the past that nvidia-smi undersamples with respect to power consumption, and so the power metrics under test could be (though unlikely) missing useful information.\nThis paper also does not perform further analysis as to why EATSS does not find the optimal solution in all cases. In some contexts, this is clear, such as for programs with limited CMAs. However, it seems as though an analysis of what other factors are at play here would be useful for the reader, and for those developing future techniques in this space.\nAn interesting comment made by a colleague was that the runtime of their algorithm should blow up with greater loop nesting, and this is correct. The authors show that with 5 loop nests, convergence time is 2.2s, whereas it is only 1.4s with 3 loop nests. That said, this increase in runtime does not seem crazy to me, but it would have been nice for the authors to comment on it further.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Paper Presentation -- EATSS"
    ]
  },
  {
    "objectID": "blogs/michael/11-26-2024-EATSS-Review.html#whats-good",
    "href": "blogs/michael/11-26-2024-EATSS-Review.html#whats-good",
    "title": "Paper Presentation – EATSS",
    "section": "What’s Good?",
    "text": "What’s Good?\nNow that I’ve complained sufficiently, I’ll say that I think this is quite a useful paper, and the technique propsed looks very strong. EATSS is able to find a solution very quickly (in 1.4s), which is really quite fast for what it’s doing, and it is a much faster optimization technique than running PPCG 200-800 times, or using ytop (which takes ~17s). It even seems as though this technique could be used in JIT compilers, which is fantastic.\nThe authors also run their optimizer on a very large set (21) of benchmarks, of varying types. This means that the results they present are likely quite reliable on the GA100 and Xavier GPUs.\nThe contribution of this paper is also quite novel, and the performance of the optimized code is qutie strong, even on par with the highly optimized libraries of cuBLAS and cuDNN (though, these libraries are not optimized for PPW, but just performance).",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Paper Presentation -- EATSS"
    ]
  },
  {
    "objectID": "blogs/michael/11-26-2024-EATSS-Review.html#conclusion",
    "href": "blogs/michael/11-26-2024-EATSS-Review.html#conclusion",
    "title": "Paper Presentation – EATSS",
    "section": "Conclusion",
    "text": "Conclusion\nI think this paper is quite good, and I would say it’s worth reading for those working in the GPU or compilers space. The results are quite clearly strong, and the tool is sufficiently validated with respect to hardware. I would like to see additional work done to expand the space of hardware explored, and also a more rigorous analysis of their optimization function, but that seems like feasible future work. Overall, I am glad I read this paper and am aware of the work, and I would reccomend it to others.\nAdditionally, I also found that this paper motivated a useful and hearty discussion, and so I would reccomend it to professors interested in talking about GPU optimization techniques with their students.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "Paper Presentation -- EATSS"
    ]
  },
  {
    "objectID": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html",
    "href": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html",
    "title": "EECE7309 Homework 4 – Dominance",
    "section": "",
    "text": "This assignment asks us to develop code to find the dominators for a basic block, construct the dominance tree, and compute the dominance frontier for each basic block in the program.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 4 -- Dominance"
    ]
  },
  {
    "objectID": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html#introduction",
    "href": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html#introduction",
    "title": "EECE7309 Homework 4 – Dominance",
    "section": "",
    "text": "This assignment asks us to develop code to find the dominators for a basic block, construct the dominance tree, and compute the dominance frontier for each basic block in the program.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 4 -- Dominance"
    ]
  },
  {
    "objectID": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html#dominance",
    "href": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html#dominance",
    "title": "EECE7309 Homework 4 – Dominance",
    "section": "Dominance",
    "text": "Dominance\nA basic block A dominates another basic block B when there is no path in the program which reaches B and does not pass through A. Understanding the dominance relationship between basic blocks can be quite useful for program analysis, and so it is useful to have this tool with which we may determine the dominance relationships in a given program. Additionally, it’s useful to know where a basic block’s dominance “ends” for program analysis techniques. Dominance relationships can further be represented in a graph, which we call a dominance tree. The dominance tree does not always indicate the dominance frontier, particularly in the case of loops. Here, we will show these graphs as Mermaid diagrams.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 4 -- Dominance"
    ]
  },
  {
    "objectID": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html#implementation",
    "href": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html#implementation",
    "title": "EECE7309 Homework 4 – Dominance",
    "section": "Implementation",
    "text": "Implementation\nThe implementation for this program is a set of short python scripts, which can be found using the link at the bottom of this page. To do this effectively, we defined a class for a control flow graph, and gave it four important methods. The first identifies the dominators of a block (compute_dominators()). The dominators of a block are the intersection of all blocks which dominate the predecessors of said block, as well as the block itself. To do this algorithmically, we must iterate at least once through all nodes in the graph. We may iterate more times particularly if there are loops in the control flow, and we can iterate util we reach convergence at an answer. The second identifies the strict dominators of a block (compute_strict_dominators()). Strict dominance is simply dominance but without the property of reflexivity. We compute the strict dominators of a block by taking its dominators and removing itself. The third identifies the immediate dominators of a block. Immediate dominators are the direct parents of blocks in the dominator tree, and indicate when a block strictly dominates another block, but does not dominate any other node which strictly dominates this block. The fourth is a method for computing the dominance frontier of a block. This is done by taking a block, and first investigating whether its successors are dominated by this block. If they are not, then they are included in the dominance frontier. If they are, then we need to investigate the successors of said block. We continue this until we’ve investigated all blocks.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 4 -- Dominance"
    ]
  },
  {
    "objectID": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html#challenges-faced",
    "href": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html#challenges-faced",
    "title": "EECE7309 Homework 4 – Dominance",
    "section": "Challenges Faced",
    "text": "Challenges Faced\nOne challenge I faced was simply in understanding how to efficiently compute the dominators of a basic block. It is fairly clear when considering the physical representation of a graph what the dominators of a node are, however it took me a moment to determine it using mathematical representations of graphs. Another challenge I faced was when computing the dominance frontiers, I at first implemented an algorithm which did not properly allow nodes to be their own dominance frontiers in certain scenarios. However, this proved to be a relatively easy fix.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 4 -- Dominance"
    ]
  },
  {
    "objectID": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html#testing",
    "href": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html#testing",
    "title": "EECE7309 Homework 4 – Dominance",
    "section": "Testing",
    "text": "Testing\nI tested my program on five different program inputs, generated in part courtesy of large language models. The programs which I used as test cases have varying types of control flow, and appear to give good coverage of possible program inputs.\nThe first, and most basic code I tested was a program containing a simple if statement.\n@main {\n  x: int = const 5;\n  zero: int = const 0;\n  cond: bool = gt x zero;\n  br cond .true .continue;\n.true:\n  one: int = const 1;\n  x: int = add x one;\n.continue:\n  one: int = const 1;\n  x: int = sub x one;\n  ret x;\n}\n\nWhich has the following CFG:\n\n\n\n\n\ngraph TD\n    true --&gt; continue\n    .start --&gt; continue\n    .start --&gt; true\n\n\n\n\n\n\nHere, we expect that .start strictly dominates both true and continue, and neither true nor continue strictly dominate any nodes. Further, the dominance frontier for all nodes should be empty.\nThis generated the following output:\nNodes and their dominators:\n{'true': {'.start', 'true'}, 'continue': {'continue', '.start'}, '.start': {'.start'}}\n\nNodes and their strict dominators:\n{'true': {'.start'}, 'continue': {'.start'}, '.start': set()}\n\nNodes and their immediate dominators:\n{'true': '.start', 'continue': '.start', '.start': None}\n\nNodes and their dominance frontiers:\n{'true': {'continue'}, 'continue': set(), '.start': set()}\nAnd the following dominance tree:\n\n\n\n\n\ngraph TD\n    .start --&gt; true\n    .start --&gt; continue\n\n\n\n\n\n\nThese results are as expected. One note, is in the printed results from this program, an empty set (set()) and None are used interchangably.\nNext, I examined a program with control flow similar to a switch statement, shown below:\n@main {\n  x: int = const 2;\n  one: int = const 1;\n  two: int = const 2;\n  cond1: bool = eq x one;\n  br cond1 .case1 .check_case2;\n.check_case2:\n  cond2: bool = eq x two;\n  br cond2 .case2 .default;\n.case1:\n  ret one;\n.case2:\n  ret two;\n.default:\n  zero: int = const 0;\n  ret zero;\n}\nWith the following CFG:\n\n\n\n\n\ngraph TD\n    check_case2 --&gt; default\n    check_case2 --&gt; case2\n    .start --&gt; case1\n    .start --&gt; check_case2\n\n\n\n\n\n\nWhat’s interesting here is we expect the dominance graph to look the same as the CFG, because there is no re-connection of branches. Using this as input, we get the following results:\nNodes and their dominators:\n{'case1': {'case1', '.start'}, 'check_case2': {'.start', 'check_case2'}, '.start': {'.start'}, 'case2': {'case2', '.start', 'check_case2'}, 'default': {'default', '.start', 'check_case2'}}\n\nNodes and their strict dominators:\n{'case1': {'.start'}, 'check_case2': {'.start'}, '.start': set(), 'case2': {'.start', 'check_case2'}, 'default': {'.start', 'check_case2'}}\n\nNodes and their immediate dominators:\n{'case1': '.start', 'check_case2': '.start', '.start': None, 'case2': 'check_case2', 'default': 'check_case2'}\n\nNodes and their dominance frontiers:\n{'case1': set(), 'check_case2': set(), '.start': set(), 'case2': set(), 'default': set()}\nWith the dominance graph:\n\n\n\n\n\ngraph TD\n    .start --&gt; case1\n    .start --&gt; check_case2\n    check_case2 --&gt; case2\n    check_case2 --&gt; default\n\n\n\n\n\n\nAs we can see, this graph is the same as the CFG (though mirrored on the vertical axis in this case).\nThe next program I examined was that of a while loop:\n@main {\n  x: int = const 0;\n.loop:\n  ten: int = const 10;\n  cond: bool = lt x ten;\n  br cond .body .exit;\n.body:\n  one: int = const 1;\n  x: int = add x one;\n  jmp .loop;\n.exit:\n  ret x;\n}\nWhich has the CFG:\n\n\n\n\n\ngraph TD\n    .start --&gt; loop\n    loop --&gt; exit\n    loop --&gt; body\n    body --&gt; loop\n\n\n\n\n\n\nHere, we expect that .start dominates all nodes, loop dominates all nodes but .start, and exit and body do not strictly dominate any nodes. Further, the dominance frontier for .start and exit should be no nodes, however loop and body should both have a dominance frontier of loop. This is due to the loop structure of this function. Analyizing this program, we get the following results:\nNodes and their dominators:\n{'.start': {'.start'}, 'loop': {'.start', 'loop'}, 'body': {'.start', 'body', 'loop'}, 'exit': {'.start', 'exit', 'loop'}}\n\nNodes and their strict dominators:\n{'.start': set(), 'loop': {'.start'}, 'body': {'.start', 'loop'}, 'exit': {'.start', 'loop'}}\n\nNodes and their immediate dominators:\n{'.start': None, 'loop': '.start', 'body': 'loop', 'exit': 'loop'}\n\nNodes and their dominance frontiers:\n{'.start': set(), 'loop': {'loop'}, 'body': {'loop'}, 'exit': set()}\n\n\n\n\n\ngraph TD\n    .start --&gt; loop\n    loop --&gt; body\n    loop --&gt; exit\n\n\n\n\n\n\nThese results align with what we expect.\nNow for a more complex looping program, we investigate a program with a loop which contains a branch internally, seen below:\n@main {\n  x: int = const 0;\n.loop:\n  ten: int = const 10;\n  cond: bool = lt x ten;\n  br cond .body .exit;\n.body:\n  two: int = const 2;\n  mod: int = mod x two;\n  zero: int = const 0;\n  cond2: bool = eq mod zero;\n  br cond2 .even .odd;\n.even:\n  two: int = const 2;\n  x: int = add x two;\n  jmp .merge;\n.odd:\n  one: int = const 1;\n  x: int = add x one;\n  jmp .merge;\n.merge:\n  jmp .loop;\n.exit:\n  ret x;\n}\n\n\n\n\n\ngraph TD\n    .start --&gt; loop\n    loop --&gt; body\n    loop --&gt; exit\n    body --&gt; odd\n    body --&gt; even\n    even --&gt; merge\n    odd --&gt; merge\n    merge --&gt; loop\n\n\n\n\n\n\nHere we expect that start dominates all directly connected successors, as do loop and body. Even, odd, and merge should strictly dominate no nodes, and should all have the immediate dominator of body. The dominance frontier of start and exit should both be empty, whereas body, loop, and merge should have a dominance frontier of only loop. Even and odd should contain merge as their dominance frontier.\nWe generate the following results, which align with our expectations:\nNodes and their dominators:\n{'.start': {'.start'}, 'loop': {'.start', 'loop'}, 'body': {'.start', 'loop', 'body'}, 'exit': {'exit', '.start', 'loop'}, 'even': {'even', '.start', 'loop', 'body'}, 'odd': {'odd', '.start', 'loop', 'body'}, 'merge': {'merge', '.start', 'loop', 'body'}}\n\nNodes and their strict dominators:\n{'.start': set(), 'loop': {'.start'}, 'body': {'.start', 'loop'}, 'exit': {'.start', 'loop'}, 'even': {'.start', 'loop', 'body'}, 'odd': {'.start', 'loop', 'body'}, 'merge': {'.start', 'loop', 'body'}}\n\nNodes and their immediate dominators:\n{'.start': None, 'loop': '.start', 'body': 'loop', 'exit': 'loop', 'even': 'body', 'odd': 'body', 'merge': 'body'}\n\nNodes and their dominance frontiers:\n{'.start': set(), 'loop': {'loop'}, 'body': {'loop'}, 'exit': set(), 'even': {'merge'}, 'odd': {'merge'}, 'merge': {'loop'}}\n\n\n\n\n\ngraph TD\n    .start --&gt; loop\n    loop --&gt; body\n    loop --&gt; exit\n    body --&gt; even\n    body --&gt; odd\n    body --&gt; merge\n\n\n\n\n\n\nNow, finally, we investigate a program which contains a nested loop:\n@main {\n  x: int = const 0;\n  y: int = const 0;\n.outer_loop:\n  ten: int = const 10;\n  cond_outer: bool = lt x ten;\n  br cond_outer .inner_loop .exit;\n.inner_loop:\n  five: int = const 5;\n  cond_inner: bool = lt y five;\n  br cond_inner .inner_body .increment_outer;\n.inner_body:\n  one: int = const 1;\n  y: int = add y one;\n  jmp .inner_loop;\n.increment_outer:\n  one: int = const 1;\n  x: int = add x one;\n  y: int = const 0;\n  jmp .outer_loop;\n.exit:\n  ret x;\n}\n\n\n\n\n\ngraph TD\n    .start --&gt; outer_loop\n    outer_loop --&gt; inner_loop\n    outer_loop --&gt; exit\n    inner_loop --&gt; inner_body\n    inner_loop --&gt; increment_outer\n    inner_body --&gt; inner_loop\n    increment_outer --&gt; outer_loop\n\n\n\n\n\n\nHere, we expect that start and outer_loop strictly dominate all successor nodes, and the successors of those nodes. Exit should only dominate itself, as should inner_body and increment_outer. inner_loop should strictly dominate inner_body and increment_outer. .start and exit should have an empty dominance frontier. inner_loop, increment_outer, and outer_loop should contain outer_loop in their dominance frontiers. inner_body should have inner_loop as its dominance frontier. The results of our program are shown below:\nNodes and their dominators:\n{'.start': {'.start'}, 'outer_loop': {'.start', 'outer_loop'}, 'inner_loop': {'inner_loop', '.start', 'outer_loop'}, 'exit': {'.start', 'exit', 'outer_loop'}, 'inner_body': {'inner_body', 'inner_loop', '.start', 'outer_loop'}, 'increment_outer': {'inner_loop', '.start', 'increment_outer', 'outer_loop'}}\n\nNodes and their strict dominators:\n{'.start': set(), 'outer_loop': {'.start'}, 'inner_loop': {'.start', 'outer_loop'}, 'exit': {'.start', 'outer_loop'}, 'inner_body': {'inner_loop', '.start', 'outer_loop'}, 'increment_outer': {'inner_loop', '.start', 'outer_loop'}}\n\nNodes and their immediate dominators:\n{'.start': None, 'outer_loop': '.start', 'inner_loop': 'outer_loop', 'exit': 'outer_loop', 'inner_body': 'inner_loop', 'increment_outer': 'inner_loop'}\n\nNodes and their dominance frontiers:\n{'.start': set(), 'outer_loop': {'outer_loop'}, 'inner_loop': {'inner_loop', 'outer_loop'}, 'exit': set(), 'inner_body': {'inner_loop'}, 'increment_outer': {'outer_loop'}}\n\n\n\n\n\ngraph TD\n    .start --&gt; outer_loop\n    outer_loop --&gt; inner_loop\n    outer_loop --&gt; exit\n    inner_loop --&gt; inner_body\n    inner_loop --&gt; increment_outer",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 4 -- Dominance"
    ]
  },
  {
    "objectID": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html#code",
    "href": "blogs/michael/10-18-2024-HW4-MichaelMaurer.html#code",
    "title": "EECE7309 Homework 4 – Dominance",
    "section": "Code",
    "text": "Code\nThe code used to generate the above results can be found here. Some of the test cases were initially generated by ChatGPT, but were modified to create better test cases (and for correctness).",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 4 -- Dominance"
    ]
  },
  {
    "objectID": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html",
    "href": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html",
    "title": "EECE7309 Homework 2 – Local Optimizations",
    "section": "",
    "text": "This assignment introduces trivial dead code elimination, and local value numbering. Python code which performs these optimizations are available at the link provided in the Code section.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 2 -- Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#introduction",
    "href": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#introduction",
    "title": "EECE7309 Homework 2 – Local Optimizations",
    "section": "",
    "text": "This assignment introduces trivial dead code elimination, and local value numbering. Python code which performs these optimizations are available at the link provided in the Code section.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 2 -- Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#part-1-trivial-dead-code-elimination",
    "href": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#part-1-trivial-dead-code-elimination",
    "title": "EECE7309 Homework 2 – Local Optimizations",
    "section": "Part 1: Trivial Dead Code Elimination",
    "text": "Part 1: Trivial Dead Code Elimination\nIn the first part of this assignment, we implemented trivial dead code elimination. By this, we mean removing instructions which are not used before reassignment. Another way to say this, is that an instruction is not dead if it is used, or if it reaches the end of the basic block, before it is reassigned.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 2 -- Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#challenges-faced",
    "href": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#challenges-faced",
    "title": "EECE7309 Homework 2 – Local Optimizations",
    "section": "Challenges Faced",
    "text": "Challenges Faced\nThe primary challenge I faced here was in attempting to implement more sophisticated dead code elimination, such as at the global scope. I thought this would be interesting to explore, but abandoned it as it is out of scope for this assignment. What proved particularly challenging on this front was handling the case where a variable is used across blocks, where the control flow forms cycles. The code I wrote was considering certain instructions to be unused when there were possible paths in which the instructions were used. When only performing local DCE, this task became much more achievable.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 2 -- Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#implementation",
    "href": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#implementation",
    "title": "EECE7309 Homework 2 – Local Optimizations",
    "section": "Implementation",
    "text": "Implementation\nTo perform local DCE, first I wrote some code which could partition every function into blocks. With these blocks, we have all the code chunks which are candidates for local DCE. Then, determining dead code was simply determining which instructions are not used by any later instructions in the block. This was done repetitively, as some instructions may on first pass not be considered dead, however they are only used by a dead instruction which will be eliminated.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 2 -- Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#testing",
    "href": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#testing",
    "title": "EECE7309 Homework 2 – Local Optimizations",
    "section": "Testing",
    "text": "Testing\nMy initial line of testing was very curated, and used the following source code:\n@main {\n    a: int = const 4;           # Will be eliminated\n    prod: int = id a;\n    b: int = const 2;\n    sum1: int = add a b;\n    sum2: int = add a b;        # Will be eliminated\n    prod: int = mul sum1 sum2;  # Will be eliminated\n    sum2: int = id a;\n    prod: int = id sum2;\n    print prod;\n}\nHere, we would expect that the first two instructions assigning prod would be eliminated, and then as a follow-on effect, the first assignment of sum2 also becomes dead code. What we observe aligns with these expectations, and is shown below:\n@main {\n    a: int = const 4;\n    b: int = const 2;\n    sum1: int = add a b;\n    sum2: int = id a;\n    prod: int = id sum2;\n    print prod;\n}\nTo further test my dead code elimination, I leveraged the brench tool provided by bril. This tool allows for a pipeline to be created, which source code is passed through. Then, using turnt the results of running the optimized code can be compared to what is expected. In the code repository, there is a .csv file containing the results of running brench against all benchmarks in the bril repository. We observe that all tests pass (or timeout in the case of function_call, which times out with or without optimization). Further, and perhaps dissapointingly, we notice that the number of instructions does not change after optimization for these benchmarks. However, this may be unsurprising, as these benchmarks are in a public code-base, and are less likely to have trivial dead code.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 2 -- Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#part-2-local-value-numbering",
    "href": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#part-2-local-value-numbering",
    "title": "EECE7309 Homework 2 – Local Optimizations",
    "section": "Part 2: Local Value Numbering",
    "text": "Part 2: Local Value Numbering\nFor the second part of this assignment, we implemented local value numbering to optimize the number of dynamic instructions executed by a program. The strategy here is to identify where computations have happened before in the code, and where possible do not re-evaluate them, and instead favor a copy.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 2 -- Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#implementation-1",
    "href": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#implementation-1",
    "title": "EECE7309 Homework 2 – Local Optimizations",
    "section": "Implementation",
    "text": "Implementation\nI used a very straightforward method of implementing LVN, as was discussed in our lectures as well as those from Cornell. The primary difference that one may notice in the code is that there is more significant exception handling which must be done to achieve consistent results.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 2 -- Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#challenges-faced-1",
    "href": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#challenges-faced-1",
    "title": "EECE7309 Homework 2 – Local Optimizations",
    "section": "Challenges Faced",
    "text": "Challenges Faced\nMy initial implementation appeared to work well on some programs, but returned incorrect results on others. This was quite puzzling, but looking at the code which was being produced, it quickly became clear what was going wrong. As a demonstration, consider the following code from dot-product.bril:\nvectorA: ptr&lt;int&gt; = alloc size;\n...\nvectorB: ptr&lt;int&gt; = alloc size;\nThis particular benchmark was failing when testing my initial code, and investigating the results showed the following:\nvectorA: ptr&lt;int&gt; = alloc size;\n...\nvectorB: ptr&lt;int&gt; = id vectorA;\nThis is not good! My hashing / replacement scheme was based entirely off of the instruction operation and the arguments involved, and so in this case, I registered the instruction alloc size as redundant and simply assigned vectorB to vectorA. For simpler instructions this makes sense, however as we are trying to have these vectors point at some dynamically allocated memory, this causes serious issues. So, to handle this, I created a list of special instructions which were not to be optimized in this manner, including functions like alloc.\nAfter this, all but one of the benchmarks was passing, with the lone failure being riemann.bril. Again, this appeared to be a result of the hash function I implemented. Consider the following code:\nleft: float = call @left_riemann a b n;\nprint left;\nmidpoint: float = call @midpoint_riemann a b n;\nprint midpoint;\nright: float = call @right_riemann a b n;\nMy setup was optimizing this code to the following:\nleft: float = call @left_riemann a b n;\nprint left;\nmidpoint: float = id @midpoint_riemann left;\nprint midpoint;\nright: float = id @right_riemann left;\nThis realization came much quicker, as it was clear I omitted certain important values in my hashing function, in this case being the function name which is called. Before this, my algorithm determined that left, midpoint, and right all hashed to callabn, indicating that they are the same value, when of course this is not the case.\nAfter these issues were resolved, everything worked quite well. I spent some time laying the groundwork for constant folding, however I was not able to finish this in reasonable time.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 2 -- Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#testing-1",
    "href": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#testing-1",
    "title": "EECE7309 Homework 2 – Local Optimizations",
    "section": "Testing",
    "text": "Testing\nThe primary line of testing for the LVN code was using brench, and using the code which I have developed for this assignment, in the repository linked below, it can be verified that all bril benchmarks pass through a brench test.\nWhat was interesting was that some benchmarks had a substantial decrease in number of dynamic instructions. One in particular was the quadratic.bril benchmark. What I noticed was that there were calls to @sqrt which took the same inputs, and then these were optimized out using local value numbering. This reduced the number of executed instructions from 785 to 412 (a 47.5% decrease!).",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 2 -- Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#dce-and-lvn",
    "href": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#dce-and-lvn",
    "title": "EECE7309 Homework 2 – Local Optimizations",
    "section": "DCE and LVN",
    "text": "DCE and LVN\nIn the code submitted with this assignment, the provided brench setup actually tests both optimizations, and passes the input code through dce.py first, followed by lvn.py. The results indicate that all tests still pass, and therefore we can say these optimizations work together!",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 2 -- Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#code",
    "href": "blogs/michael/09-27-2024-HW2-MichaelMaurer.html#code",
    "title": "EECE7309 Homework 2 – Local Optimizations",
    "section": "Code",
    "text": "Code\nThe code for this assignment is contained in a public GitHub repository which I set up. Notably, to run brench using the provided brench_config.toml file, bril is expected to be two directories above this one. This can be changed by modifying the benchmarks variable in brench_config.toml.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Michael",
      "EECE7309 Homework 2 -- Local Optimizations"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html",
    "href": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html",
    "title": "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR",
    "section": "",
    "text": "This work presents the development of a software infrastructure for a Hardware GEMM Accelerator, utilizing MLIR to bridge the gap between high-level computational models and low-level hardware execution. The project implements a non-intrusive, robust framework. It seamlessly integrates with existing machine learning libraries, optimizing crucial matrix multiplication operations and enhancing model performance on custom hardware.\n\n\nThe rapid evolution of applications, such as machine learning, has significantly heightened the demand for high-quality computational power. As machine learning models continue to grow in complexity, and datasets increase in size, there is a sharp surge in the need for computational capabilities, energy efficiency, customization, and flexibility.\nThis surge has compelled hardware manufacturers to develop specialized accelerators, including GPUs (Graphics Processing Units), TPUs (Tensor Processing Units), and FPGAs (Field-Programmable Gate Arrays), designed to provide more efficient computational power. These heterogeneous hardware platforms excel at achieving higher energy efficiency by striking a balance between computational task allocation, energy consumption, and computational performance, consequently improving the power efficiency ratio. These platforms also excel in offering increased customization and flexibility to meet the diverse requirements of various applications.\nIn the realm of machine learning, different tasks often demand distinct computational resources. For instance, image-related deep learning applications frequently rely on parallel processing, while U-Net structure-based Transformer models, such as those used in large language models, may place more emphasis on the sequential processing capabilities of CPUs.\nMoreover, with Moore’s Law nearing its limit, traditional hardware upgrades are no longer as effective in contributing to overall system performance speedup as they once were. In light of this, the significance of optimizing software implementation, the compilation process, and intermediate abstract layers becomes evident. Researchers and developers are actively exploring various approaches to application optimization, including Intermediate Representation (IR), Single Instruction Multiple Data (SIMD), and the eco-system of CUDA (Compute Unified Device Architecture).\n\n\n\nHowever, it is important to note that while hardware capabilities have advanced significantly, software adaptation has not kept pace. Hardware manufacturers provide features, but software applications often need to be adapted to leverage these features, leading to increased development costs.\nIn the embedded systems domain, a usual approach for chip manufacturers is to provide their customized build-up toolchain. For instance, to embed neural network models on chipset, ST provides the STM32Cube.AI tool for its STM32 series, and Rockchip provides RKNN-SDK to support the integrated NPU. Both toolsets attached detailed documentations about how to migrate the code. In this case, software developers often need to invest considerable effort in adapting their applications to utilize these features efficiently. This adaptation process can be time-consuming and costly, highlighting the need for more seamless integration between hardware and software.\nIn response to these challenges, a growing trend is the adoption of unified heterogeneous development architectures. One such example is OpenACC, an open standard for parallel programming of heterogeneous systems, which aims to simplify the process of optimizing applications for different hardware accelerators.\n\n\n\nIn this context, we propose a software and hardware co-design method, where optimization occurs at multiple layers, and a portion of computationally intensive instructions is migrated to hardware accelerators using Multi-Level Intermediate Representation (MLIR). MLIR presents an innovative approach to building reusable and extensible compiler infrastructure. Its goal is to reduce software fragmentation and enhance compilation for heterogeneous hardware, thereby significantly reducing the cost of developing domain-specific compilers and facilitating their integration with existing frameworks. This approach enables efficient utilization of hardware capabilities while reducing the burden of manual adaptation, ultimately enhancing development efficiency and lowering costs. Our aim is that the end users can easily adopt those machine learning models written in standard language and libraries, with the minimum effort to learn how to interact with our hardware.\nThe primary challenge addressed in this project is the efficient utilization of a Hardware GEMM Accelerator using a software infrastructure based on MLIR. The solution involves the implementation of a non-intrusive framework that integrates seamlessly with existing technologies, optimizing the performance of matrix multiplication operations crucial in many machine learning algorithms.\nThe integration of MLIR with GEMM Acceleration hardware brings several benefits:\n\nEnhanced performance optimization for machine learning models on customized hardware.\nFlexibility in representing, analyzing, and transforming graphs at multiple levels of abstraction, including Torch operations and hardware operations (torch-mlir).\nFacilitation of connections between hardware and high-level frameworks like PyTorch. The same approach may cater to various hardware configurations including TPUs and custom ASICs once the corresponding backend is added.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html#intro",
    "href": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html#intro",
    "title": "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR",
    "section": "",
    "text": "This work presents the development of a software infrastructure for a Hardware GEMM Accelerator, utilizing MLIR to bridge the gap between high-level computational models and low-level hardware execution. The project implements a non-intrusive, robust framework. It seamlessly integrates with existing machine learning libraries, optimizing crucial matrix multiplication operations and enhancing model performance on custom hardware.\n\n\nThe rapid evolution of applications, such as machine learning, has significantly heightened the demand for high-quality computational power. As machine learning models continue to grow in complexity, and datasets increase in size, there is a sharp surge in the need for computational capabilities, energy efficiency, customization, and flexibility.\nThis surge has compelled hardware manufacturers to develop specialized accelerators, including GPUs (Graphics Processing Units), TPUs (Tensor Processing Units), and FPGAs (Field-Programmable Gate Arrays), designed to provide more efficient computational power. These heterogeneous hardware platforms excel at achieving higher energy efficiency by striking a balance between computational task allocation, energy consumption, and computational performance, consequently improving the power efficiency ratio. These platforms also excel in offering increased customization and flexibility to meet the diverse requirements of various applications.\nIn the realm of machine learning, different tasks often demand distinct computational resources. For instance, image-related deep learning applications frequently rely on parallel processing, while U-Net structure-based Transformer models, such as those used in large language models, may place more emphasis on the sequential processing capabilities of CPUs.\nMoreover, with Moore’s Law nearing its limit, traditional hardware upgrades are no longer as effective in contributing to overall system performance speedup as they once were. In light of this, the significance of optimizing software implementation, the compilation process, and intermediate abstract layers becomes evident. Researchers and developers are actively exploring various approaches to application optimization, including Intermediate Representation (IR), Single Instruction Multiple Data (SIMD), and the eco-system of CUDA (Compute Unified Device Architecture).\n\n\n\nHowever, it is important to note that while hardware capabilities have advanced significantly, software adaptation has not kept pace. Hardware manufacturers provide features, but software applications often need to be adapted to leverage these features, leading to increased development costs.\nIn the embedded systems domain, a usual approach for chip manufacturers is to provide their customized build-up toolchain. For instance, to embed neural network models on chipset, ST provides the STM32Cube.AI tool for its STM32 series, and Rockchip provides RKNN-SDK to support the integrated NPU. Both toolsets attached detailed documentations about how to migrate the code. In this case, software developers often need to invest considerable effort in adapting their applications to utilize these features efficiently. This adaptation process can be time-consuming and costly, highlighting the need for more seamless integration between hardware and software.\nIn response to these challenges, a growing trend is the adoption of unified heterogeneous development architectures. One such example is OpenACC, an open standard for parallel programming of heterogeneous systems, which aims to simplify the process of optimizing applications for different hardware accelerators.\n\n\n\nIn this context, we propose a software and hardware co-design method, where optimization occurs at multiple layers, and a portion of computationally intensive instructions is migrated to hardware accelerators using Multi-Level Intermediate Representation (MLIR). MLIR presents an innovative approach to building reusable and extensible compiler infrastructure. Its goal is to reduce software fragmentation and enhance compilation for heterogeneous hardware, thereby significantly reducing the cost of developing domain-specific compilers and facilitating their integration with existing frameworks. This approach enables efficient utilization of hardware capabilities while reducing the burden of manual adaptation, ultimately enhancing development efficiency and lowering costs. Our aim is that the end users can easily adopt those machine learning models written in standard language and libraries, with the minimum effort to learn how to interact with our hardware.\nThe primary challenge addressed in this project is the efficient utilization of a Hardware GEMM Accelerator using a software infrastructure based on MLIR. The solution involves the implementation of a non-intrusive framework that integrates seamlessly with existing technologies, optimizing the performance of matrix multiplication operations crucial in many machine learning algorithms.\nThe integration of MLIR with GEMM Acceleration hardware brings several benefits:\n\nEnhanced performance optimization for machine learning models on customized hardware.\nFlexibility in representing, analyzing, and transforming graphs at multiple levels of abstraction, including Torch operations and hardware operations (torch-mlir).\nFacilitation of connections between hardware and high-level frameworks like PyTorch. The same approach may cater to various hardware configurations including TPUs and custom ASICs once the corresponding backend is added.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html#detail-design",
    "href": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html#detail-design",
    "title": "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR",
    "section": "Detail Design",
    "text": "Detail Design\n\n\n\nSystem Diagram\n\n\nThe system diagram illustrates the overall framework of this project.\nTo be clear, blocks with grey backgrounds are not supported or tested in this project. Grey dot lines illustrate the structure of the relative components but are not used in this work. The circled number located on each arrow represents the sequence of execution flow. Dash lines with numbers attached also represent the data flow.\nWe separate the toolchain into 4 layers, which are High-Level Framework, torch-mlir, llvm-mlir, and system with hardware.\nHigh-Level Framework is the interface for end users, usually, in this project, this layer is programmed in Python with PyTorch library defining the model. Torch-mlir layer is based on torch-mlir project from the LLVM community. It contains the definition of torch dialect, the opt tool, and several backends. Backends defined the lowering sequence and the outcome from mlir layer. Their role is to instruct mlir layer to convert high-level model representation into code that can be executed on specific hardware. The llvm-mlir layer is based on the LLVM project. The pipelined lowering process utilizes a bunch of conversions from dialect to dialect to get the final result in progressive descent. At the bottom, the system and hardware layer is the actual component to execute all instructions.\nFor this project, we use the structure of LinalgOnTensor Backend. It focuses on converting linear algebra operations in MLIR into low-level code that can operate directly on tensors. This backend optimizes these operations so that they can be mapped more efficiently to accelerator hardware that supports linear algebra operations. During the progressive lowering sequence, we insert our passes to redirect the behavior of execution before a pattern we recognize is broken down into pieces that may be hard to identify.\n\nStep-by-Step Design Explanation\nHere’s how this system operates, following the execution flow shown in the system diagram:\n\nUser defined a PyTorch model and passed it to torch-mlir library. The library compiles the torch model into mlir script in torch dialect by a pybinded torch-mlir-opt execution call. The user gets the handle to the mlir model.\nUser chooses to use our customized HwaccLinalgOnTensor Backend. The handle of the torch dialect model is passed into our backend implementation.\nHwaccLinalgOnTensor backend lowers the torch dialect operations into other dialects that are officially supported by MLIR. Then the backend generates the sequence of optimizing and lowering towards LLVM IR for MLIR toolset.\nOpt tool inside MLIR gets the pipeline sequence from the layer above and iteratively lowers the user script. During this step, our redirection of linalg.matmul is applied. The matmul operation will turn into a C function call instead of nested loops of ALU instructions with loads and stores.\nThe final outcome after the pipeline lowering is recognizable for LLVM Jit. Therefore, the instructions are stored in the Execution Engine, with a C-typed function call style wrapped, waiting for a user call from the High-Level Framework.\nAt this moment, the matmul operation is required but not implemented. Torch backend needs to get the executable function and register it to the Execution Engine.\nUser starts the inference / training of the pytorch model. Causing the execution of CPU Runner Jit.\nWhen the LLVM IR script reaches its end, a return call is executed, which pushes the data descriptor to the upper levels of this system. The processed data rises step by step and becomes runtime output by a numpy library call which converts the descriptor into numpy.ndarray and return to the user. At this stage, the cycle from high-level model specification to low-level hardware execution is completed.\n\nThis workflow is designed to efficiently translate high-level machine learning models into optimized, executable code that can run on various hardware, including specialized accelerators.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html#experimental-explore",
    "href": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html#experimental-explore",
    "title": "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR",
    "section": "Experimental Explore",
    "text": "Experimental Explore\n\nTechnical problem and solution\nDuring the exploration of the existing MLIR and Torch-Mlir project, it is easy to get lost of function definition, especially for beginners. The fast-paced evolution of the project led to discrepancies between the provided documentation and the actual implementation. The inadequate and scattered documentation for MLIR and torch-mlir makes the understanding of code even harder. For most of the cases, the explanation of code and errors are documented in the MLIR discourse forum. Therefore, the learning curve is steep in the beginning.\n\nLack of systematic up-to-date documentation\nOne typical case is the Python package location documented inside development.md. The actual location of outcomes that the environment variable PYTHONPATH needs to store has been changed multiple times in recent history. Till the time of writing this report, the path issue is still not fixed.\n\n\nLimited explanation about memref interoperability and solution\nAnother example of a problem I was facing is the C interpretation format of memref type in MLIR. Although memref dialect is relatively stable. Limited documentation for interaction outer the scope of MLIR. This problem stuck me for quite a long time, seeking where is the data located after the pointer I received.\nIt turns out that the array of data that the memref represents is not directly sent to the C interface. Instead, the memref object delivers a pointer to a descriptor structure that contains the information about the tensor. This was found by reading the numpy interfacing source code for return value handling. Based on the numpy library-defined name of the memref descriptor, I found the relative online discussion about the memref C interface.\nBy the way, it is worth noticing that the ranked memref object and the unranked one use totally different descriptor structures. Also, the descriptor size of memref objects is different based on rank. In my observation, the element type is not included in the descriptor, which makes it hard for the C library to handle all types of memref input in one function interface.\nLuckily, we got the backend written in Python. This is the key to solving this problem. As the instructions are lowered to LLVM IR level, the func call name can be accessed by Python at this stage. I implemented a function name parser referencing how the numpy interface did. After extracting the call name with the input and output memref type, the Python code links the call to a type-matched C library symbol by the redirection ability of the function lookup table in Python.\n\n\n\nTestbench\nTo test whether our HWACC-MLIR structure is executing correctly or not, we introduced a testbench. It is a MNIST handwritten digit classification task. The convolutional neural network is written as a class inherited from torch.nn.Module.\nIts architecture is as follows:\n\nInput Layer: The network accepts a single-channel (grayscale) image of size 28x28 pixels.\nFirst Convolutional Layer: This layer expands the depth of the input from 1 channel to 16 channels using a 3x3 convolutional kernel with a stride of 1 and padding of 1. The output remains at 28x28 pixels.\nReLU Activation: Following the first convolutional operation, a ReLU activation function is applied to introduce non-linearity, enabling the network to learn more complex patterns.\nSecond Convolutional Layer: The second convolutional layer further increases the depth from 16 to 32 channels, with the same kernel size, stride, and padding as the first layer. The output size remains 28x28 pixels.\nReLU Activation: A second ReLU activation function is applied after the second convolutional layer.\nMax Pooling Layer: This layer performs a 2x2 max pooling operation with a stride of 2, reducing the spatial dimensions from 28x28 to 14x14 pixels while retaining the 32 channels.\nFlatten Operation: Before entering the fully connected layers, the 32x14x14 tensor is flattened into a 1D tensor of 6272 elements (32 * 14 * 14).\nFirst Fully Connected (Dense) Layer: The flattened tensor is passed through a fully connected layer reducing its size to 1024 nodes.\nReLU Activation: A ReLU activation is applied after the fully connected layer.\nDropout: A dropout layer with a dropout rate of 30 percent is used to prevent overfitting during training.\nSecond Fully Connected (Dense) Layer: The final fully connected layer reduces the size from 1024 nodes to 10 nodes, corresponding to the number of classes for classification.\n\n\n\n\nImgClassifier-demo: Torch NN Structure",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html#testing-results",
    "href": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html#testing-results",
    "title": "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR",
    "section": "Testing / Results",
    "text": "Testing / Results\n\nEnv Setup\n\nPyVenv and dependencies\nFollow the official development instruction from torch-mlir.\nTo build the whole project in-tree with JIT modules enabled, add the following paramenters to CMake:\n  -DTORCH_MLIR_ENABLE_PYTORCH_EXTENSIONS=ON \\\n  -DTORCH_MLIR_ENABLE_JIT_IR_IMPORTER=ON \\\n\n\nRuntime Env activation\nAdditional command line variables are required to run the whole JIT toolchain:\n#!/usr/bin/env bash\n__TORCH_MLIR_PATH__=&lt;PRJ_PATH&gt;\nconda activate &lt;PRJ_CONDA_ENV&gt;\nexport PYTHONPATH=$__TORCH_MLIR_PATH__/build/python_packages/torch_mlir:$__TORCH_MLIR_PATH__/projects/pt1/examples:$__TORCH_MLIR_PATH__/build/tools/torch-mlir/python_packages/torch_mlir\nexport TORCH_MLIR_PREFIX=$__TORCH_MLIR_PATH__/build/bin\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$__TORCH_MLIR_PATH__/build/lib\necho \"Use 'conda deactivate' to exit this venv\"\nThe reason for adding additional LD_LIBRARY_PATH is that we encapsulate our HWACC-GEMM func interface as a HAL lib call towards the JIT toolchain. Therefore, each time when the LLVM JIT runs into such a command of llvm.call @linalg_matmul_*() it needs to be remapped into some registered functions that are provided by the shared library. And we provided such func call in LD_LIBRARY_PATH, and operation binding in hwaccBackendLibCall.py.\n\n\n\nUnit Test\n\nAdding / Removing w/ MatchAndRewrite(MatmulOp)\nA MatchAndRewrite(MatmulOp) pattern is added to the Hwacc Pass. In this pattern match test, operations that can be successfully cast into MatmulOp will be substituted by a CallOp as shown below:\nLogicalResult matchAndRewrite(Operation *op, PatternRewriter &rewriter) const override {\n\n  SingletonLogger::getInstance() &lt;&lt; \"Test:[\" &lt;&lt; op-&gt;getName().getStringRef().str() &lt;&lt; \"]\\t\";\n\n  auto matmulOp = dyn_cast&lt;MatmulOp&gt;(op);\n\n  if (!isa&lt;MatmulOp&gt;(op) || !matmulOp.hasPureBufferSemantics()) {\n    SingletonLogger::getInstance() &lt;&lt; \"Fail To Match!\" &lt;&lt; \"\\n\";\n    return rewriter.notifyMatchFailure(\n        op, \"expected matmul op with buffer semantics\");\n  }\n  SingletonLogger::getInstance() &lt;&lt; \"Capture! \" &lt;&lt; matmulOp-&gt;getName().getStringRef().str() &lt;&lt; \"\\n\";\n  \n  ...\n}\nTo replicate the capture process use the command line of:\n&lt;prj&gt;/build/bin/mlir-opt  -pass-pipeline='builtin.module(func.func(convert-linalg-to-hwacc))' ./my_torch_jit_app1.mlir_lowering_pipeline.before_linalg_matmul_lowering.mlir \nAfter the optimization process, the filter matches correctly to its rule. the following debug log is documented:\n# debug.log\n...\nTest:[scf.for]        Fail To Match!\nTest:[memref.alloc]   Fail To Match!\nTest:[arith.addf]     Fail To Match!\nTest:[linalg.matmul]  Capture! linalg.matmul\nTest:[linalg.yield]   Fail To Match!\n...\n\n\nAdding / Removing -convert-linalg-to-hwacc pass\nA new pass was added to linalg/passes.td called convert-linalg-to-hwacc. Its corresponding C implementation is createConvertLinalgToHwaccPass(). Complete the rest part of the implementation of this pass, rebuild, and test. Changing the pipeline lowering sequence in refbackend.py.\nBefore adding -convert-linalg-to-hwacc, the lowered mlir instructions until -refback-munge-memref-copy is shown in below. The linalg operation is not expanded.\n...\nalloc_15 = memref.alloc() {alignment = 64 : i64} : memref&lt;64x1024xf32&gt;\nlinalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(alloc_14 : memref&lt;64x1024xf32&gt;) outs(alloc_15 : memref&lt;64x1024xf32&gt;) {\n  ^bb0(in: f32, out: f32):\n    linalg.yield in : f32\n}\nmemref.dealloc alloc_14 : memref&lt;64x1024xf32&gt;\n// Not yet expanded, whole op showing as a matmul sementic. \nlinalg.matmul ins(collapse_shape, 1 : memref&lt;64x6272xf32&gt;, memref&lt;6272x1024xf32&gt;) outs(alloc_15 : memref&lt;64x1024xf32&gt;)\nmemref.dealloc alloc_13 : memref&lt;64x32x14x14xf32&gt;\nalloc_16 = memref.alloc() {alignment = 64 : i64} : memref&lt;64x1024xf32&gt;\nlinalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(alloc_15, 3 : memref&lt;64x1024xf32&gt;, memref&lt;1024xf32&gt;) outs(alloc_16 : memref&lt;64x1024xf32&gt;) {\n  ^bb0(in: f32, in_21: f32, out: f32):\n    8 = arith.addf in, in_21 : f32\n    9 = arith.cmpf ugt, 8, cst_0 : f32\n...\nThe linalg operation is changed to scf.for loop. Remember the implementation of match and rewrite is referencing from Loops.cpp, this is the reason for observing such behavior here.\n...\nalloc_15 = memref.alloc() {alignment = 64 : i64} : memref&lt;64x1024xf32&gt;\nlinalg.generic {indexing_maps = [#map2, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(alloc_14 : memref&lt;64x1024xf32&gt;) outs(alloc_15 : memref&lt;64x1024xf32&gt;) {\n^bb0(in: f32, out: f32):\n  linalg.yield in : f32\n}\nmemref.dealloc alloc_14 : memref&lt;64x1024xf32&gt;\n// Matmul expanded into scf.for loops. \nscf.for arg1 = c0 to c64 step c1 {\n  scf.for arg2 = c0 to c1024 step c1 {\n    scf.for arg3 = c0 to c6272 step c1 {\n      8 = memref.load collapse_shape[arg1, arg3] : memref&lt;64x6272xf32&gt;\n      9 = memref.load 1[arg3, arg2] : memref&lt;6272x1024xf32&gt;\n      10 = memref.load alloc_15[arg1, arg2] : memref&lt;64x1024xf32&gt;\n      11 = arith.mulf 8, 9 : f32\n      12 = arith.addf 10, 11 : f32\n      memref.store 12, alloc_15[arg1, arg2] : memref&lt;64x1024xf32&gt;\n    }\n  }\n}\nmemref.dealloc alloc_13 : memref&lt;64x32x14x14xf32&gt;\nalloc_16 = memref.alloc() {alignment = 64 : i64} : memref&lt;64x1024xf32&gt;\nlinalg.generic {indexing_maps = [#map2, #map3, #map2], iterator_types = [\"parallel\", \"parallel\"]} ins(alloc_15, 3 : memref&lt;64x1024xf32&gt;, memref&lt;1024xf32&gt;) outs(alloc_16 : memref&lt;64x1024xf32&gt;) {\n^bb0(in: f32, in_21: f32, out: f32):\n  8 = arith.addf in, in_21 : f32\n  9 = arith.cmpf ugt, 8, cst_0 : f32\n...\nAfter adding our pass indicator to the lowering sequence, the lowered mlir instructions script is shown as the following.\n...\n%1479 = llvm.getelementptr %1457[%1478] : (!llvm.ptr, i64) -&gt; !llvm.ptr, f32 loc(#loc21)\nllvm.store %1475, %1479 : f32, !llvm.ptr loc(#loc21)\n%1480 = llvm.add %1469, %5  : i64 loc(#loc21)\nllvm.br ^bb255(%1480 : i64) loc(#loc21)\n^bb257:  // pred: ^bb255\n%1481 = llvm.add %1467, %5  : i64 loc(#loc21)\nllvm.br ^bb253(%1481 : i64) loc(#loc21)\n^bb258:  // pred: ^bb253\nllvm.call @free(%1414) : (!llvm.ptr) -&gt; () loc(#loc21)\n// Kept as a C style func call, that can be preserved into HAL lib tiling optimization. \nllvm.call @linalg_matmul_view64x1024xf32_view1024x10xf32_view64x10xf32(%1368, %1375, %1379, %1359, %1360, %1360, %1361, %29, %27, %33, %19, %20, %20, %21, %1450, %1457, %1461, %1441, %1442, %1442, %1443) : (!llvm.ptr, !llvm.ptr, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64, i64, !llvm.ptr, !llvm.ptr, i64, i64, i64, i64, i64) -&gt; () loc(#loc21)\nllvm.call @free(%1368) : (!llvm.ptr) -&gt; () loc(#loc25)\n%1482 = llvm.mlir.constant(64 : index) : i64 loc(#loc21)\n%1483 = llvm.mlir.constant(10 : index) : i64 loc(#loc21)\n%1484 = llvm.mlir.constant(1 : index) : i64 loc(#loc21)\n%1485 = llvm.mlir.constant(640 : index) : i64 loc(#loc21)\n%1486 = llvm.mlir.zero : !llvm.ptr loc(#loc21)\n%1487 = llvm.getelementptr %1486[640] : (!llvm.ptr) -&gt; !llvm.ptr, f32 loc(#loc21)\n%1488 = llvm.ptrtoint %1487 : !llvm.ptr to i64 loc(#loc21)\n%1489 = llvm.mlir.constant(64 : index) : i64 loc(#loc21)\n...\n\n\n\nSystem Test\nAfter activating the customed Python virtual environment. Import the MNIST Handwritten Digit Testbench from the previous section. Attach the Torch-Milr backend with our HWACC pass integrated into the PyTorch model. The demo code showing in below is the basic outline of the test application.\n# ... \nimport torch_mlir\nfrom torch_mlir_e2e_test.hwacc_linalg_on_tensors_backends import refbackend\n# ... \nmy_nn_in_torch_mlir = torch_mlir.compile(my_model, torch.ones(64, 1, 28, 28), output_type=\"Linalg-on-Tensors\")\nhwacc_backend = refbackend.RefBackendHwaccLinalgOnTensorsBackend()\nmy_nn_llvm_ir = hwacc_backend.compile(my_nn_in_torch_mlir)\nexec_jit_handle = hwacc_backend.load(my_nn_llvm_ir)\n# Run inference\nprint(\"PyTorch Native: \")\nfor img, label in test_data_loader:\n    img, label = Variable(img), Variable(label)\n    outputs = my_model(img)\n    _, pred = torch.max(outputs.data, 1)\n    test_count+=img.size()[0]\n    test_correct += torch.sum(pred == label.data)\n    print(\"  Test Correct {:4d}/{:4d} --&gt; {:.4f}\".format(\n        test_correct, test_count, 100.0*test_correct/test_count))\n    if MAX_TEST_COUNT &lt;= test_count: break\nprint(\"Test Accuracy is:{:.4f}\".format(  100 * test_correct / test_count  ))\n\nprint(\"torch-mlir: \")\nfor img, label in test_data_loader:\n    img, label = Variable(img), Variable(label)\n    outputs = exec_jit_handle.forward(img.numpy())\n    outputs = torch.from_numpy(outputs)\n    _, pred = torch.max(outputs.data, 1)\n    test_count+=img.size()[0]\n    test_correct += torch.sum(pred == label.data)\n    print(\"  Test Correct {:4d}/{:4d} --&gt; {:.4f}\".format(\n        test_correct, test_count, 100.0*test_correct/test_count))\n    if MAX_TEST_COUNT &lt;= test_count: break\nprint(\"Test Accuracy is:{:.4f}\".format(  100 * test_correct / test_count  ))\n\nData Accuracy\nAfter execution, the accuracy of result overtime from both implementations are shown in below. There’s no difference between the pure pytorch backend, and our implementation. Which means our method of remapping such matmul function to HWACC HAL lib call is success in simulation.\n\n\n\nResult Accuracy\n\n\n\n\nTiming\nWe are curious about how the performances are between different configurations. After testing each configuration 20 times, we averaged the timing result and summarized the timing result as listed in following.\n\n\n\n\n\n\n\n\n\n\n\nClass\nExec Item\nItem Time (sec)\nRequired (orig_pytorch)\nRequired (mlir_hwacc)\nRequired (mlir_cfloop)\n\n\n\n\nInit\npytorch nn model loading\n1.7\n\\(\\checkmark\\)\n\\(\\checkmark\\)\n\\(\\checkmark\\)\n\n\nInit\ntorch_mlir compile+load\n0.8\n\n\\(\\checkmark\\)\n\\(\\checkmark\\)\n\n\nInference\noriginal pytorch calc (orig)\n1.7\n\\(\\checkmark\\)\n\n\n\n\nInference\nmlir using hwacc (hwacc)\n6.4\n\n\\(\\checkmark\\)\n\n\n\nInference\nmlir using scf.loop (loop)\n29.5\n\n\n\\(\\checkmark\\)\n\n\nTotal\nTime Cost\n\n3.4\n8.9\n32\n\n\n\nAs we observed in this table, the original pytorch calculation is the fastest, then our implementation using HWACC, and the official torch-mlir solution is the slowest. It is not surprising that the original pytorch implementation reaches the best score since it utilizes the BLAS library to do the calculations, and BLAS will try to adapt all kinds of localized accelerating methods such as multicore parallel, loop unrolling, SIMD instructions, e.g. AVX on X86 or NEON on ARM machine.\nAn exciting finding is that our HWACC implementation is about 4.6 times faster than the loop solution based on Amdahl’s Law.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html#conclusion",
    "href": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html#conclusion",
    "title": "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR",
    "section": "Conclusion",
    "text": "Conclusion\n\nSummary\nThe project successfully demonstrates the integration of a Hardware GEMM Accelerator with an MLIR-based software infrastructure, showing significant improvements in computational efficiency. Despite challenges such as inconsistent documentation and evolving MLIR implementations, the project culminates in a versatile system capable of translating high-level neural network models into optimized code for diverse hardware architectures. This infrastructure promises to simplify the adoption of machine learning models, reducing the effort required to achieve efficient hardware acceleration.\n\n\nFuture outlook\nIn the future, the project aims to expand the capabilities of the MLIR-based software infrastructure for Hardware GEMM Accelerators. We envision enhancing support for a wider range of machine learning models and computational patterns, thus fostering broader adoption across various domains. Also, our approach is now limited to C-interfaced call mapping, we hope to find a better and more elegant way to interact with our hardware implementation.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html#code-repo",
    "href": "blogs/Qucheng/2024-12-01-Qucheng-Prj.html#code-repo",
    "title": "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR",
    "section": "Code Repo",
    "text": "Code Repo\nCode are distributed in different sections. To get the starting point, check out the Torch-MLIR repo, this is the root repo that has submodules as link to other two repos used below.\n\nTorch-MLIR toolchain: https://github.com/jiangquchengpeter/torch-mlir-hwacc\nHWACC HAL Shared Lib: https://github.com/jiangquchengpeter/gemm-hwacc-backend\nLLVM with HWACC pass: https://github.com/jiangquchengpeter/llvm-project-hwacc",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Project: Build non-intrusive Software Infrastructure of a GEMM HWACC using MLIR"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html",
    "title": "Homework 3: Data Flow",
    "section": "",
    "text": "This assignment focuses on implementing dataflow analysis for the Bril intermediate language to support three major analyses: liveness, availability, and busy expressions. These analyses play a crucial role in various compiler optimizations, including Trivial Dead Code Elimination (DCE) and Local Value Numbering (LVN) that we discussed and implemented in previous homework, HW2. The goal is to analyze the flow of data within a program, identify redundant or unnecessary computations, and determine which values are still “live” or “in use” at various points in a program.\nThe report details the design of a flexible framework for these analyses, the implementation strategy for each type of dataflow analysis, and the use of visualization to display the results. The integration and testing process ensures the framework’s correctness through various benchmark tests, and the results are analyzed to demonstrate its effectiveness.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 3: Data Flow"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html#introduction",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html#introduction",
    "title": "Homework 3: Data Flow",
    "section": "",
    "text": "This assignment focuses on implementing dataflow analysis for the Bril intermediate language to support three major analyses: liveness, availability, and busy expressions. These analyses play a crucial role in various compiler optimizations, including Trivial Dead Code Elimination (DCE) and Local Value Numbering (LVN) that we discussed and implemented in previous homework, HW2. The goal is to analyze the flow of data within a program, identify redundant or unnecessary computations, and determine which values are still “live” or “in use” at various points in a program.\nThe report details the design of a flexible framework for these analyses, the implementation strategy for each type of dataflow analysis, and the use of visualization to display the results. The integration and testing process ensures the framework’s correctness through various benchmark tests, and the results are analyzed to demonstrate its effectiveness.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 3: Data Flow"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html#interactive-demo",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html#interactive-demo",
    "title": "Homework 3: Data Flow",
    "section": "Interactive Demo",
    "text": "Interactive Demo\n\n\n\n 🚧 Use the link HERE if not showing",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 3: Data Flow"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html#detail-design",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html#detail-design",
    "title": "Homework 3: Data Flow",
    "section": "Detail Design",
    "text": "Detail Design\nCheck EECE7398_ST_Compiler/HW3 for source code.\n\nAbstraction Classes\nThe abstraction class remain the same structure as last homework submittion, see detail in HW2 - Abstraction Classes.\nI add some additional features and fix something like __repr__ for some of the classes, therefore use current ./bril_model for further development.\n\n\nGraph Result Demonstration\nSince this task is about analizing some properties of the data-flow graph, a better way to demonstrate it clear and straight forward is to show the computated result in visual format. I developed the framework to view analyze results using vis.js and networkx.\n\n\n🚧 Expand HERE for HD Video\n\n\n\n\n\n\n\n\n\n\nDataflow Analysis Implementation\nThe script df_analysis.py performs dataflow analysis on Bril programs, supporting three types of analysis: liveness, availability, and busy expressions. The implementation is designed to be generic, allowing for easy extension to other types of dataflow analysis with minimal code changes.\n\n\n1. Argument Parsing and Setup\nThe script begins by utilizing the argparse module to handle command-line argument parsing. This allows the user to specify three key arguments when running the script:\n\nANALYSIS: Specifies the type of dataflow analysis to perform. The valid options are 'liveness', 'availability', or 'busy'. This argument is required and ensures the script knows what kind of analysis to perform on the Bril program.\nDEMO_BRIL_FILE: This is the path to the Bril program that will be analyzed. The file contains the intermediate representation (IR) of the program.\n--save-dir: An optional argument that specifies where to save the generated output HTML files (visualizations). If not provided, the default directory ./save is used.\n\nBy parsing these arguments, the script can dynamically handle different types of analysis and ensure the correct files are processed and results saved to the right locations.\nEECE7398_ST_Compiler(601e0e9) :: HW3/df_analysis.py :: Lines 20 to 25\n import argparse \n parser = argparse.ArgumentParser(description='Generate data flow graph for a bril script') \n parser.add_argument('ANALYSIS', type=str, help='Analysis: [liveness, availability, busy]') \n parser.add_argument('DEMO_BRIL_FILE', type=str, help='Path to the bril file') \n parser.add_argument('--save-dir', type=str, default='./save', help='Path to save the generated html files') \n args = parser.parse_args() \n\n\n\n2. Validation of Arguments\nOnce the arguments are parsed, the script proceeds to validate them:\n\nANALYSIS: The provided value is converted to lowercase for consistency. The script checks whether the specified analysis type is one of the allowed options: 'liveness', 'availability', or 'busy'. If the user specifies an unsupported analysis, the script prints an error message and exits.\nDEMO_BRIL_FILE: The script checks if the provided file path exists on the system. If not, an error message is displayed, and the script terminates.\nIf all validations pass, the script proceeds with the execution; otherwise, it prints the usage information (parser.print_help()) to guide the user on how to run the script correctly.\n\nThis ensures that only valid input and analysis types are processed.\nEECE7398_ST_Compiler(601e0e9) :: HW3/df_analysis.py :: Lines 27 to 43\n ANALYSIS = args.ANALYSIS.lower() \n DEMO_BRIL_FILE = args.DEMO_BRIL_FILE \n # how to get the save-dir value \n SAVE_DIR = args.save_dir \n  \n # make sure args are allowed \n correct = True \n if ANALYSIS not in ['liveness', 'availability', 'busy']: \n     print(f\"Analysis &lt;{ANALYSIS}&gt; not supported\") \n     correct = False \n if not os.path.exists(DEMO_BRIL_FILE): \n     print(f\"File &lt;{DEMO_BRIL_FILE}&gt; not found\") \n     correct = False \n if not correct: \n     # print help and exit \n     parser.print_help() \n     exit(1) \n\n\n\n3. Loading the Bril Script\nThe script loads the Bril program into a BrilScript object, which represents the structure of the input program. This object provides an abstraction over the Bril instructions and functions, making it easier to manipulate and analyze.\n\nThe BrilScript object is initialized with the name of the file, and the control flow graph (CFG) for each function in the Bril script is constructed. The CFG represents how control flows between basic blocks (sequences of instructions without branches) in each function, which is crucial for performing dataflow analysis.\napp_graph is a dictionary that maps each BrilFunction to a tuple consisting of a networkx directed graph (the CFG) and a mapping of labels to their corresponding instructions.\n\nThe CFG provides the foundation for performing various types of dataflow analysis, as it organizes the program’s structure into a form that can be traversed and analyzed.\nEECE7398_ST_Compiler(601e0e9) :: HW3/df_analysis.py :: Lines 135 to 137\n bbs = bm.BrilScript(script_name=os.path.basename(DEMO_BRIL_FILE), file_dir=os.path.dirname(DEMO_BRIL_FILE)) \n app_graph: Dict[bm.BrilFunction, Tuple[nx.DiGraph, Dict[bm.BrilInstruction_Label, List[bm.BrilInstruction]]]] = {} \n update_to_graph(bbs, app_graph) \n\n\n\n4. Generating GEN, KILL, and EXPR Sets\nFor each basic block, the script calculates three sets:\n\nGEN: The set of variables used before being assigned a new value in the block. These are the variables that “generate” dependencies in the block.\nKILL: The set of variables that are redefined or “killed” within the block.\nEXPR: The set of expressions that are still available at the end of the block.\n\nThe function get__args_used_before_assign__assigned__calc_expr_available_at_bb_end iterates over the instructions in a basic block. It collects the following information: - used_first: Variables that are used before any assignment in the block. - written: Variables that are assigned new values within the block. - avail_exprs: Expressions that remain available for future use at the end of the block, ensuring that they haven’t been invalidated by any assignments.\nThe GEN, KILL, and EXPR sets are essential for performing dataflow analysis like liveness and availability, which rely on tracking how variables and expressions evolve over the program.\nEECE7398_ST_Compiler(601e0e9) :: HW3/df_analysis.py :: Lines 150 to 168\n def get__args_used_before_assign__assigned__calc_expr_available_at_bb_end(instrs: List[bm.BrilInstruction]) -&gt; Tuple[Set[str],Set[str],Set[Expr]]: \n     \"\"\" \n     Given a list of instructions, return the set of variables that are used before defined and the set of variables that are modified. \n     This is from HW2, but modified to return the sets instead of printing them. \n     \"\"\" \n     used_first: Set[str] = set() \n     written: Set[str] = set() \n     avail_exprs: Set[Expr] = set() \n     for instr in instrs: \n         used_first.update(set(instr.args if instr.args else []) - written) \n         if instr.dest: \n             # check if the dest was used in generating any of the expressions, if so, remove it from exprs \n             for expr in list(avail_exprs): \n                 if instr.dest in expr.args: \n                     avail_exprs.remove(expr) \n             written.add(instr.dest) \n             if instr.args and instr.op not in ['id', 'const', 'call']: \n                 avail_exprs.add(Expr(instr.op, instr.args)) \n     return used_first, written, avail_exprs \n\n\n\n5. Updating GEN, KILL, and EXPR Sets\nAfter calculating the GEN, KILL, and EXPR sets for each basic block, the script updates the CFG with this information. The update_gen_kill_sets function iterates over each node (basic block) in the CFG and stores the calculated GEN, KILL, and EXPR sets in the node’s attributes.\nThis step enriches the CFG with the necessary dataflow information, setting up the foundation for more complex analyses like determining which variables are live or which expressions are available.\nEECE7398_ST_Compiler(601e0e9) :: HW3/df_analysis.py :: Lines 173 to 180\n def update_gen_kill_sets(app_graph: Dict[bm.BrilFunction, Tuple[nx.DiGraph, Dict[bm.BrilInstruction_Label, List[bm.BrilInstruction]]]]): \n     for _, (fdg, _) in app_graph.items(): \n         for each_node, each_node_data in fdg.nodes(data=True): \n             each_block: List[bm.BrilInstruction] = each_node_data.get('instructions', None) \n             _gen, _kill, _expr = gen_kill_expr_sets(each_block) if each_block else (set(), set(), set()) \n             fdg.nodes[each_node]['GEN'] = _gen \n             fdg.nodes[each_node]['KILL'] = _kill \n             fdg.nodes[each_node]['EXPR'] = _expr \n\n\n\n6. Generic Dataflow Analysis\nThe script defines a generic dataflow analysis function _fdg_update_bare_bone, which can be adapted for different types of analyses (liveness, availability, busy expressions). This function operates by traversing the CFG and updating the IN and OUT sets of each node (basic block):\n\nIN: The set of variables or expressions that are live or available at the entry to a basic block.\nOUT: The set of variables or expressions that are live or available at the exit of a basic block.\n\nThe function takes as input a specific analysis function (e.g., for liveness or availability) and applies it to update the IN and OUT sets for each node. The function also checks whether the IN and OUT sets change during the analysis—if they do, it indicates that the dataflow information has propagated, and further iterations are required.\nThe generic nature of this function minimizes code duplication and allows the script to easily switch between different types of analyses.\nEECE7398_ST_Compiler(601e0e9) :: HW3/df_analysis.py :: Lines 202 to 213\n def _fdg_update_bare_bone(specific_analysis_func: Callable[[nx.DiGraph, str], Tuple[Set[str], Set[str]]], fdg: nx.DiGraph) -&gt; bool: \n     # here we extract the common part of the analysis \n     has_changed = False \n     for this_node, _ in fdg.nodes(data=True): \n         _print_this_node_name = this_node.replace('\\n', '\\\\n') \n         _in, _out = specific_analysis_func(fdg, this_node) \n         print(f\"Node: {_print_this_node_name}, IN: {_in}, OUT: {_out}\") \n         if _in != _get_node_in_set(fdg, this_node) or _out != _get_node_out_set(fdg, this_node): \n             fdg.nodes[this_node]['IN'] = _in \n             fdg.nodes[this_node]['OUT'] = _out \n             has_changed = True \n     return has_changed \n\n\n\n7. Specific Analysis Functions\nThe script defines specific analysis functions for each type of dataflow analysis:\n\nLiveness Analysis: Computes the IN and OUT sets based on the liveness of variables. It updates the sets by looking at the successors of each node and determining which variables are still live at the entry of a basic block.\nAvailability Analysis: Determines which expressions are available at the entry and exit of a block. It considers the predecessor nodes and checks whether the expressions remain valid by avoiding any variables that were killed in the block.\nBusy Expressions Analysis: Identifies which expressions must be computed before reaching the next use of a variable. This analysis is useful for identifying common subexpressions that can be optimized.\n\nEach function is passed to the generic _fdg_update_bare_bone function to perform the corresponding analysis.\nEECE7398_ST_Compiler(601e0e9) :: HW3/df_analysis.py :: Lines 215 to 258\n def _fdg_update_internal_liveness_sets(fdg: nx.DiGraph, this_node: str) -&gt; Tuple[Set[str], Set[str]]: \n     \"\"\" \n         this.gen = {v | v is used before defined here} \n         this.kill = {v | v is assigned here} \n         IN = this.gen + (OUT - this.kill) \n         OUT = union(successors' IN) \n     \"\"\" \n     _print_this_node_name = this_node.replace('\\n', '\\\\n') \n     _temp_succ_req = [_get_node_in_set(fdg, each_succ_node) for each_succ_node in _get_node_succ_set(fdg, this_node)] \n     _out: Set[str] = set.union(*_temp_succ_req) if _temp_succ_req else set() \n     print(f\"Node: {_print_this_node_name}, Succ: {_get_node_succ_set(fdg, this_node)}=&gt;{_temp_succ_req}, OUT: {_out}\") \n     _in: Set[str] = set.union(_get_node_gen_set(fdg, this_node), set.difference(_out, _get_node_kill_set(fdg, this_node))) \n     return _in, _out \n  \n def _fdg_update_internal_availability_sets(fdg: nx.DiGraph, this_node: str) -&gt; Tuple[Set[Expr], Set[Expr]]: \n     \"\"\" \n         this.exprs = {e | e is available at the end of the block} \n         IN = intersection(predeccessors' OUT) \n         OUT = (IN + this.exprs) - OUT(expr: any(var modified here exist in expr)) \n     \"\"\" \n     _print_this_node_name = this_node.replace('\\n', '\\\\n') \n     _temp_pred_give = [_get_node_out_set(fdg, each_pred_node) for each_pred_node in _get_node_pred_set(fdg, this_node)] \n     _in: Set[Expr] = set.intersection(*_temp_pred_give) if _temp_pred_give else set() \n     print(f\"Node: {_print_this_node_name}, Pred: {_get_node_pred_set(fdg, this_node)}=&gt;{_temp_pred_give}, IN: {_in}\") \n     # gatter all exprs that were computed before the end of this block  \n     _out: Set[Expr] = set.union(_get_node_in_set(fdg, this_node), _get_node_expr_set(fdg, this_node)) \n     # remove such expr that include variables that were modified in this block \n     _out = set([each_expr for each_expr in _out if not set.intersection(set(each_expr.args), _get_node_kill_set(fdg, this_node))]) \n     return _in, _out \n  \n def _fdg_update_internal_busy_sets(fdg: nx.DiGraph, this_node: str) -&gt; Tuple[Set[Expr], Set[Expr]]: \n     \"\"\" \n         IN = (OUT - OUT(expr: any(var modified here exist in expr)) + this.exprs \n         OUT = intersection(successors' IN) \n     \"\"\" \n     _print_this_node_name = this_node.replace('\\n', '\\\\n') \n     _temp_succ_req = [_get_node_in_set(fdg, each_succ_node) for each_succ_node in _get_node_succ_set(fdg, this_node)] \n     _out: Set[Expr] = set.intersection(*_temp_succ_req) if _temp_succ_req else set() \n     print(f\"Node: {_print_this_node_name}, Succ: {_get_node_succ_set(fdg, this_node)}=&gt;{_temp_succ_req}, OUT: {_out}\") \n     # remove such expr that include variables that were modified in this block \n     _in: Set[Expr] = set([each_expr for each_expr in _out if not set.intersection(set(each_expr.args), _get_node_kill_set(fdg, this_node))]) \n     # add all exprs that are computed in this block \n     _in = set.union(_in, _get_node_expr_set(fdg, this_node)) \n     return _in, _out \n\n\n\n8. Running the Analysis\nThe update_analysis_sets function orchestrates the execution of the selected analysis. Based on the user’s input (liveness, availability, or busy), the corresponding analysis function is retrieved from a mapping (ANALYSIS_FUNC).\nThe analysis is executed iteratively, updating the CFG with the calculated IN and OUT sets until no further changes occur. This iterative approach is necessary because dataflow analysis typically involves propagating information through the CFG until a fixed point is reached (i.e., the point where further iterations don’t change the dataflow sets).\nEECE7398_ST_Compiler(601e0e9) :: HW3/df_analysis.py :: Lines 266 to 277\n def update_analysis_sets(analysis_type_str: str , app_graph: Dict[bm.BrilFunction, Tuple[nx.DiGraph, Dict[bm.BrilInstruction_Label, List[bm.BrilInstruction]]]]): \n     analysis_func = ANALYSIS_FUNC.get(analysis_type_str, None) \n     if not analysis_func: \n         print(f\"Analysis &lt;{analysis_type_str}&gt; not supported\") \n         return \n     has_changed = True \n     while has_changed: \n         has_changed = False \n         print(f\"Updating {analysis_type_str} sets\") \n         for _, (fdg, _) in app_graph.items(): \n             has_changed |= _fdg_update_bare_bone(analysis_func, fdg) \n         print() \n\n\n\n9. Visualization\nThe script concludes by generating a visual representation of the CFG using the pyvis library. It creates an interactive HTML file that displays the CFG along with the computed IN, OUT, GEN, KILL, and EXPR sets for each basic block.\n\nThe dump_into_pv_graph function takes the CFG as input and converts it into a visual format. For each node (basic block) in the CFG, the IN, OUT, GEN, KILL, and EXPR sets are formatted as strings and added as attributes to the node in the visualization.\nThe visualization allows users to interact with the CFG, inspect the dataflow information for each node, and understand the results of the analysis in a more intuitive manner.\n\nThis final step provides a clear and accessible way to verify the results of the analysis.\nEECE7398_ST_Compiler(601e0e9) :: HW3/df_analysis.py :: Lines 318 to 376\n     for node in net.nodes: \n         node['IN'] = _in = generate_set_str(node.pop('IN', set())) \n         node['OUT'] = _out = generate_set_str(node.pop('OUT', set())) \n         node['GEN'] = _gen = generate_set_str(node.pop('GEN', set())) \n         node['KILL'] = _kill = generate_set_str(node.pop('KILL', set())) \n         node['EXPR'] = _expr = generate_set_str([str(x) for x in node.pop('EXPR', set())]) \n  \n         node['title'] = '' \n         if 'instructions' in node: \n             # remove 'data' key from node, and set 'title' key to the string representation of the data \n             node['title'] = \"\\n  \".join([obj.to_briltxt() if hasattr(obj, 'to_briltxt') else str(obj) for obj in node.pop('instructions', [])]) \n             node['shape'] = 'box' \n         node['title'] += \"\\n\" \n         node['title'] += f\"\\nGEN: { _gen }\" \n         node['title'] += f\"\\nKILL: { _kill }\" \n         node['title'] += f\"\\nEXPR: { _expr }\" \n         node['title'] += f\"\\nIN: { _in }\" \n         node['title'] += f\"\\nOUT: { _out }\" \n  \n         # title layout change (word replace): \n         #  strip: remove leading/trailing spaces, tabs, newlines, and carriage returns \n         #  double return -&gt; dash line \n         #  double space -&gt; full corner single space \n         node['title'] = node['title'].strip(' \\t\\n\\r').replace('\\n\\n', '\\n--------\\n').replace('  ', '\\u3000') \n  \n         # node['label'] = f\"IN: {_in}\" + '\\n' + node['label'] + '\\n' + f\"OUT: {_out}\" \n  \n         if node['id'] in (ENTRY_POINT_NAME, RETURN_POINT_NAME): \n             node['color'] = 'grey' \n             node['shape'] = 'circle' \n  \n     for edge in net.edges: \n         _reason = edge.pop('reason', None) \n         if _reason: \n             edge['label'] = _reason \n         _src_node, _dst_node = _get_node_by_name(edge['from']), _get_node_by_name(edge['to']) \n         if _src_node and _dst_node and 'OUT' in _src_node and 'IN' in _dst_node: \n             src_id, src_out, dst_id, dst_in = _src_node['id'], _src_node['OUT'], _dst_node['id'], _dst_node['IN'] \n             if src_out == CONST_EMPTY_STR: src_out = dst_in \n             if dst_in == CONST_EMPTY_STR: dst_in = src_out \n             new_label = f\"{src_id}.OUT:{src_out}\\n{dst_id}.IN:{dst_in}\" if src_out != dst_in else src_out \n             edge['title'] = edge.get('title', \"\") + edge.get('label', \"\")  # move label to popup title \n             edge['label'] = new_label  # set new label \n      \n     return net \n  \n # Safe linux fs name \n safe_fs_name = lambda raw_string: \"\".join(c if c.isalnum() or c in (' ', '.', '_') else '_' for c in raw_string) \n  \n for each_func in bbs.functions: \n     # Remove illegal characters for Linux filesystem \n     save_dir = os.path.join(SAVE_DIR, safe_fs_name(bbs.script_name), ANALYSIS) \n     save_file = f\"{safe_fs_name(each_func.name)}.html\" \n     # get the function directed graph \n     fdg, _ = app_graph.get(each_func) \n     # mkdir -p ./save_dir \n     os.makedirs(f\"./{save_dir}\", exist_ok=True) \n     # save to html \n     dump_into_pv_graph(fdg).save_graph(os.path.join(save_dir, save_file)) \n\n\n\n\nConclusion\nThe script of df_analysis.py provides a flexible and extensible framework for performing dataflow analysis on Bril programs. By defining generic functions and specific analysis functions, it minimizes code duplication and allows for easy addition of new analysis types. The visualization step helps in understanding the results of the analysis by generating an interactive HTML representation of the CFG.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 3: Data Flow"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html#integration-and-testing",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html#integration-and-testing",
    "title": "Homework 3: Data Flow",
    "section": "Integration and Testing",
    "text": "Integration and Testing\nThe integration of the optimizations was done manually and thoroughly tested to ensure correctness. Testing was carried out using both class examples and official test cases from the Bril repository.\nManual Testing: To validate the correctness of the dataflow analysis, I manually executed the script on several examples provided during class lectures. This helped confirm that the basic dataflow functionality (liveness, availability, and busy expression analysis) was functioning as expected. To test each step of the workflow, use the playground.ipynb to break between steps and check output of each cell.\nClass Examples: I tested the implementation with in-class examples, which serve as benchmarks for the expected output of liveness and availability analysis. Each example was carefully compared with the correct CFG and dataflow analysis results.\nBril Benchmark Tests: Additionally, I tested the system on Bril’s official dataflow test cases found in bril/examples/test/df/*.bril. These benchmark examples cover edge cases and typical control flow scenarios, further ensuring the robustness of the implementation.\nOther Benchmarks: Beyond the standard tests, I applied the analysis to select benchmarks from bril/benchmarks/core. These more complex scenarios helped confirm that the system could scale effectively while maintaining correct results.\nAcross all test cases, the output CFGs and the results of liveness, availability, and busy expressions analysis were consistent with the expected behavior. No discrepancies were found during manual inspection or benchmark comparisons.\nBTW, to rerun all results that I submitted to this repo, just simply use make.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 3: Data Flow"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html#results-and-analysis",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html#results-and-analysis",
    "title": "Homework 3: Data Flow",
    "section": "Results and Analysis",
    "text": "Results and Analysis\nTo make life easier, the Bril scripts that are tested in this HW are copied/created in the ./example folder. The three types of dataflow analyses (liveness, availability, busy) were tested on the following :\n\nbirthday.bril\ncheck-primes.bril\ncond-args.bril\ncond.bril\nfact.bril\nin_class_example_1.bril\nin_class_example_2.bril\nin_class_example_3.bril\nis-decreasing.bril\n\nTo view the graph, set up a http server and use any of your favourite modern web browsers to open the .html file that just generated.\nExample:\n❯ cd HW3\n❯ python -m http.server\nServing HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ...\n# Goto http://&lt;server-ip&gt;:8000/output/in_class_example_1.bril/liveness/in_class_example_1.html\n\nLiveness Analysis\nLiveness analysis was correctly computed in all test cases. The variables that were “live” at each point in the program were accurately tracked. The results were particularly effective in identifying opportunities for Trivial DCE. For instance, variables that were defined but never live in subsequent instructions were correctly flagged, highlighting code that could be safely eliminated.\n\n\nAvailability Analysis\nAvailability analysis was applied successfully, and all expressions available at the entry and exit of each basic block were correctly computed. This analysis is critical for Local Value Numbering (LVN), where equivalent expressions are consolidated. In benchmarks such as in_class_example_3.bril, the implementation showed how redundant computations could be eliminated by detecting available expressions early.\n\n\nBusy Expressions Analysis\nBusy expressions analysis worked as expected, identifying expressions that were critical to compute before the next use. This was particularly useful in loops, where understanding which expressions are “busy” helps optimize the repeated evaluation of the same values.\n\n\nComparison and Insights\nFor all test cases, I carefully compared the output CFG and dataflow graph to the expected results. No differences were found in any of the test cases, confirming that the implementation is consistent with the theoretical expectations for dataflow analysis.\nThe visual representation of the analysis results (generated using vis.js and networkx) provided an intuitive way to verify the correctness of the IN, OUT, GEN, KILL, and EXPR sets for each basic block. This helped to easily identify points of optimization and confirmed that the framework was operating as intended.\n\n\nRaw test log\nCheck the following pages: \n\nTypical Examples:\nin_class_example_1.bril/liveness\n\n\n\nimage\n\n\nin_class_example_3.bril/busy\n\n\n\nimage",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 3: Data Flow"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html#conclusion-1",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW3.html#conclusion-1",
    "title": "Homework 3: Data Flow",
    "section": "Conclusion",
    "text": "Conclusion\nThe dataflow analysis framework for Bril successfully implements liveness, availability, and busy expressions analyses, all of which are foundational for key compiler optimizations. Trivial Dead Code Elimination (DCE) and Local Value Numbering (LVN) will extremely benefit from these analyses by eliminating redundant computations and identifying unused variables.\nThe testing of the framework on various Bril programs—ranging from simple in-class examples to more complex benchmark scripts—demonstrates that the analyses work correctly and efficiently. The interactive visualizations of the control flow graph (CFG) and associated dataflow sets provide valuable insights, making it easier to verify correctness and identify optimization opportunities.\nThis project establishes a strong foundation for performing local optimizations on intermediate representations like Bril. Future work could extend the framework to handle interprocedural analyses, multi-block optimizations, or more advanced global dataflow analyses. By further refining these optimizations, we can significantly enhance the efficiency of compiled programs.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 3: Data Flow"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-09-20-Qucheng-HW1.html",
    "href": "blogs/Qucheng/2024-09-20-Qucheng-HW1.html",
    "title": "Homework 1: Exploring Bril",
    "section": "",
    "text": "For this assignment, I explored the Bril intermediate representation and wrote a new benchmark along with a tool to analyze and transform Bril programs.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 1: Exploring Bril"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-09-20-Qucheng-HW1.html#introduction",
    "href": "blogs/Qucheng/2024-09-20-Qucheng-HW1.html#introduction",
    "title": "Homework 1: Exploring Bril",
    "section": "",
    "text": "For this assignment, I explored the Bril intermediate representation and wrote a new benchmark along with a tool to analyze and transform Bril programs.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 1: Exploring Bril"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-09-20-Qucheng-HW1.html#detail-design",
    "href": "blogs/Qucheng/2024-09-20-Qucheng-HW1.html#detail-design",
    "title": "Homework 1: Exploring Bril",
    "section": "Detail Design",
    "text": "Detail Design\nThe homework was divided into two main parts. I’ll walk through the details of my implementation, challenges faced, and the testing process.\n\nPart 1: Creating and Running a New Benchmark\nPart 2: Implementing a Bril Program Stub-Analyzer",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 1: Exploring Bril"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-09-20-Qucheng-HW1.html#conclusion",
    "href": "blogs/Qucheng/2024-09-20-Qucheng-HW1.html#conclusion",
    "title": "Homework 1: Exploring Bril",
    "section": "Conclusion",
    "text": "Conclusion\nThis assignment provided a comprehensive introduction to Bril and its tooling ecosystem. By creating benchmarks and developing a custom analyzer, I gained insights into compiler design and intermediate representations.\nHardest Part: The most challenging aspect was managing dependencies and environment setup for the Bril toolchain. Careful attention to the makefile and troubleshooting through GitHub Discussions helped overcome these issues.\nTesting and Results: I validated my benchmarks and analyzer using various Bril programs. The tools successfully transformed and analyzed the input programs, demonstrating the utility of Bril in simple code analysis and transformation tasks.\nThis experience has equipped me with practical skills in working with compiler tools and representations, paving the way for more complex analyses in future assignments.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 1: Exploring Bril"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW4.html",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW4.html",
    "title": "Homework 4: Dominance",
    "section": "",
    "text": "Dominance analysis is a fundamental concept in compiler optimization, crucial for understanding the structure of control flow within programs. It involves identifying dominance relationships among blocks in a Control Flow Graph (CFG), where a program is broken down into basic blocks connected by directed edges representing the possible flow of execution. This analysis is often used for advanced compiler optimizations like code motion, dead code elimination, and Static Single Assignment (SSA) form conversion.\nThis blog post delves into the key concepts of dominance analysis, including dominators, immediate dominators, and dominance frontiers. These concepts form the basis of various control flow optimizations and transformations.\n\n\n\n\n\nIn a CFG, a block BB1 is said to dominate another block BB2 if every path from the entry block to BB2 must pass through BB1 . The entry block itself trivially dominates all other blocks in the graph because all execution paths must originate from it.\nThe concept of dominators can be useful to identify regions in the program where specific code fragments will always execute before others. This information is essential for optimizations such as moving invariant computations out of loops and removing unreachable code.\n\n\n\nAmong the set of blocks that dominate a particular block, the one closest to in terms of control flow is known as its immediate dominator. The immediate dominator is unique for each block (except the entry block) and forms the basis of the dominator tree, a hierarchical structure that reveals the dominance relationships within the CFG.\nIn the dominator tree, each node represents a basic block, and an edge from BB1 to BB2 indicates that BB1 is the immediate dominator of BB2 . This tree structure helps in visualizing the control dependencies in a program.\n\n\n\nThe dominance frontier of a block BB1 is the set of all blocks where the dominance relationship breaks. More formally, a block BBX is in the dominance frontier of BB1 if BB1 dominates a predecessor of BBX , but BB1 does not strictly dominate BBX itself.\nDominance frontiers are used to identify the points in the CFG where variables need to be merged or “phi-inserted” during SSA form conversion. This is important because it ensures that all possible values of a variable reaching a certain block are correctly accounted for.\n\n\n\n\nIn the context of compiler design, the CFG represents the program’s execution paths, with nodes corresponding to basic blocks and edges denoting control flow between them. Dominance analysis provides a way to understand the flow of control within a program by identifying how execution paths interact and where they converge or diverge.\n\n\nDominators can be seen as checkpoints that ensure certain computations always happen before others. For example, if a block BB1 dominates BB2 , we can be certain that the execution of BB2 implies the execution of BB1 at some point. This property allows for optimizations such as moving loop-invariant computations out of loops, as the computations are guaranteed to occur before each iteration of the loop.\n\n\n\nThe dominator tree provides a structured way to represent the dominance relationships within the CFG. By identifying the immediate dominator for each block, we can construct a tree where each block is a child of its immediate dominator. This structure is useful for identifying regions of code that are tightly coupled in terms of control flow, aiding in optimizations such as code motion and dead code elimination.\n\n\n\nThe dominance frontier is particularly useful in scenarios where multiple control paths converge. It helps identify the blocks where different execution paths must be reconciled, especially in the context of SSA form, where variables may have different values depending on the path taken to reach a block. By inserting “phi functions” at the dominance frontier, the compiler can correctly handle the merging of variable values from different paths.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 4: Dominance"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW4.html#introduction",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW4.html#introduction",
    "title": "Homework 4: Dominance",
    "section": "",
    "text": "Dominance analysis is a fundamental concept in compiler optimization, crucial for understanding the structure of control flow within programs. It involves identifying dominance relationships among blocks in a Control Flow Graph (CFG), where a program is broken down into basic blocks connected by directed edges representing the possible flow of execution. This analysis is often used for advanced compiler optimizations like code motion, dead code elimination, and Static Single Assignment (SSA) form conversion.\nThis blog post delves into the key concepts of dominance analysis, including dominators, immediate dominators, and dominance frontiers. These concepts form the basis of various control flow optimizations and transformations.\n\n\n\n\n\nIn a CFG, a block BB1 is said to dominate another block BB2 if every path from the entry block to BB2 must pass through BB1 . The entry block itself trivially dominates all other blocks in the graph because all execution paths must originate from it.\nThe concept of dominators can be useful to identify regions in the program where specific code fragments will always execute before others. This information is essential for optimizations such as moving invariant computations out of loops and removing unreachable code.\n\n\n\nAmong the set of blocks that dominate a particular block, the one closest to in terms of control flow is known as its immediate dominator. The immediate dominator is unique for each block (except the entry block) and forms the basis of the dominator tree, a hierarchical structure that reveals the dominance relationships within the CFG.\nIn the dominator tree, each node represents a basic block, and an edge from BB1 to BB2 indicates that BB1 is the immediate dominator of BB2 . This tree structure helps in visualizing the control dependencies in a program.\n\n\n\nThe dominance frontier of a block BB1 is the set of all blocks where the dominance relationship breaks. More formally, a block BBX is in the dominance frontier of BB1 if BB1 dominates a predecessor of BBX , but BB1 does not strictly dominate BBX itself.\nDominance frontiers are used to identify the points in the CFG where variables need to be merged or “phi-inserted” during SSA form conversion. This is important because it ensures that all possible values of a variable reaching a certain block are correctly accounted for.\n\n\n\n\nIn the context of compiler design, the CFG represents the program’s execution paths, with nodes corresponding to basic blocks and edges denoting control flow between them. Dominance analysis provides a way to understand the flow of control within a program by identifying how execution paths interact and where they converge or diverge.\n\n\nDominators can be seen as checkpoints that ensure certain computations always happen before others. For example, if a block BB1 dominates BB2 , we can be certain that the execution of BB2 implies the execution of BB1 at some point. This property allows for optimizations such as moving loop-invariant computations out of loops, as the computations are guaranteed to occur before each iteration of the loop.\n\n\n\nThe dominator tree provides a structured way to represent the dominance relationships within the CFG. By identifying the immediate dominator for each block, we can construct a tree where each block is a child of its immediate dominator. This structure is useful for identifying regions of code that are tightly coupled in terms of control flow, aiding in optimizations such as code motion and dead code elimination.\n\n\n\nThe dominance frontier is particularly useful in scenarios where multiple control paths converge. It helps identify the blocks where different execution paths must be reconciled, especially in the context of SSA form, where variables may have different values depending on the path taken to reach a block. By inserting “phi functions” at the dominance frontier, the compiler can correctly handle the merging of variable values from different paths.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 4: Dominance"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW4.html#detail-design",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW4.html#detail-design",
    "title": "Homework 4: Dominance",
    "section": "Detail Design",
    "text": "Detail Design\n\nAbstraction Classes\nThe abstraction class remain the same structure as last homework submittion, see detail in HW3 - Abstraction Classes.\nI add one additional feature: __lt__() for class BrilInstruction_Label, therefore use current ./bril_model for further development.\n\n\nDominance Analysis Implementation\nThe script dom_analysis.py is designed to perform dominance analysis on Bril program. It includes functionalities for generating a Control Flow Graph (CFG), computing dominators for each block, constructing the dominator tree, and identifying the dominance frontier. Below is a breakdown of the key parts of the code with brief notes on their functions and corresponding source code line indxx.\n\n1. Imports and Argument Parsing\n\nThe code imports necessary libraries like os, networkx, pyvis, and matplotlib for graph visualization, as well as custom modules from bril_model.\nIt also sets up argument parsing to allow the user to specify the Bril file to analyze and an optional save directory for generated outputs.\n\n\n\nℹ️ View source code : EECE7398_ST_Compiler/HW4/dom_analysis.py @ 582ec4a.\n\n # %% [markdown] \n # # Playground - data flow graph  \n  \n # %% \n import os \n import networkx as nx \n import pyvis as pv \n import matplotlib.pyplot as plt \n import graphviz as gv \n from typing import Iterable, Tuple, Set, List, Dict, OrderedDict, Optional \n import bril_model as bm \n from bril_model.form_blocks import form_blocks \n from hashlib import sha256 \n fast_hash = lambda str: sha256(str).hexdigest()[:3] \n  \n  \n # usage: &lt;this_script&gt; DEMO_BRIL_FILE [--save-dir=./save | --mute-output]  \n  \n import argparse \n parser = argparse.ArgumentParser(description='Generate CFG with dominance frontier and Dominator-Tree for a bril script') \n parser.add_argument('DEMO_BRIL_FILE', type=str, help='Path to the bril file') \n parser.add_argument('--save-dir', type=str, default='./save', help='Path to save the generated html files') \n parser.add_argument('--mute-output', action='store_true', help='Mute the output of the graphviz render', default=False) \n args = parser.parse_args() \n  \n DEMO_BRIL_FILE = args.DEMO_BRIL_FILE \n SAVE_DIR = args.save_dir \n MUTE_OUTPUT = args.mute_output \n  \n # DEMO_BRIL_FILE = \"../bril/examples/test/dom/loopcond.bril\" \n # DEMO_BRIL_FILE = \"../bril/examples/test/dom/while.bril\" \n  \n # %% \n # make sure args are allowed \n args_correct = True \n if not os.path.exists(DEMO_BRIL_FILE): \n     print(f\"File &lt;{DEMO_BRIL_FILE}&gt; not found\") \n     args_correct = False \n if not args_correct: \n     # print help and exit \n     parser.print_help() \n     exit(1) \n  \n # %% \n ENTRY_POINT_NAME = 'ENTRY' \n RETURN_POINT_NAME = 'RETURN' \n\n\n\n2. Basic Block Definition\n\nThis section defines the BasicBlock class, representing a basic block in the CFG. Each basic block consists of a label and a list of instructions.\nIt also includes properties for successors (succ) and predecessors (pred), which form the edges in the CFG.\nThe class implements various methods for equality checks, hashing, and string representation.\n\n\n\nℹ️ View source code : EECE7398_ST_Compiler/HW4/dom_analysis.py @ 582ec4a.\n\n class BasicBlock(): \n     def __init__(self, label: bm.BrilInstruction_Label, instrs: List[bm.BrilInstruction]): \n         if not isinstance(label, bm.BrilInstruction_Label): \n             raise TypeError(f\"Expected label, got {type(label)}\") \n         if label not in instrs: \n             instrs.insert(0, label) \n         self.instrs = instrs \n         self.succ: Set[BasicBlock] = set() \n         self.pred: Set[BasicBlock] = set() \n      \n     @property \n     def label(self): \n         return self.instrs[0] \n      \n     def __hash__(self) -&gt; int: \n         return hash(self.label) \n     def __eq__(self, o: object) -&gt; bool: \n         if not isinstance(o, BasicBlock): \n             return False \n         return self.label == o.label \n     def __lt__(self, o: object) -&gt; bool: \n         if not isinstance(o, BasicBlock): \n             return False \n         # return block is always the last one \n         if self.label.label == RETURN_POINT_NAME: return False \n         if o.label.label == RETURN_POINT_NAME: return True \n         # entry block is always the first one \n         if self.label.label == ENTRY_POINT_NAME: return True \n         if o.label.label == ENTRY_POINT_NAME: return False \n         return self.label &lt; o.label \n  \n     def __str__(self): \n         if self.label.label in {ENTRY_POINT_NAME, RETURN_POINT_NAME}: return self.label.label \n         _instrs = '\\n  '.join([x.to_briltxt() for x in self.instrs[1:]]) \n         if _instrs: \n             _instrs = f\"\\n  {_instrs}\" \n         return f\"{self.label.to_briltxt()}{_instrs}\" \n     def __repr__(self): \n         return f\"{self.__class__.__name__} :: {self.label.to_briltxt()}..[{len(self.instrs) - 1}]\" \n\n\n\n3. Forming Basic Blocks from Bril Functions\n\nThe function iter_func_blocks generates basic blocks from the provided Bril script by iterating over each function in the script and forming blocks based on instruction labels.\nFor functions without explicit labels, it creates anonymous basic blocks.\n\n\n\nℹ️ View source code : EECE7398_ST_Compiler/HW4/dom_analysis.py @ 582ec4a.\n\n def iter_func_blocks(bs: bm.BrilScript) -&gt; Iterable[Tuple[bm.BrilFunction, List[BasicBlock]]]: \n     for each_func in bs.functions: \n         bbs: List[BasicBlock] = list() \n         anonymous_id = 0 \n         for each_block in form_blocks(each_func.instrs): \n             this_block_label: bm.BrilInstruction_Label = None \n             if isinstance(each_block[0], bm.BrilInstruction_Label): \n                 this_block_label = each_block[0] \n             else: \n                 this_block_label = bm.BrilInstruction_Label(dict(label='_f{}._anon{}'.format(fast_hash(each_func.name.encode()), anonymous_id))) \n                 anonymous_id += 1 \n             bbs.append(BasicBlock(this_block_label, each_block)) \n         yield (each_func, bbs) \n\n\n\n4. Generating the CFG\n\nThe generate_func_cfg_dict function builds the CFG by establishing the control flow between basic blocks. It adds entry and return blocks to represent the start and end points of a function.\nIt iterates over the blocks and updates the predecessors and successors based on control flow instructions (jmp, br, ret).\n\n\n\nℹ️ View source code : EECE7398_ST_Compiler/HW4/dom_analysis.py @ 582ec4a.\n\n def generate_func_cfg_dict(bscript: bm.BrilScript) -&gt; Dict[bm.BrilFunction, List[BasicBlock]]: \n     app_bbs_dict: OrderedDict[bm.BrilFunction, List[BasicBlock]] = {} \n     for each_func, basic_blocks in iter_func_blocks(bscript): \n         # print(\"Function: {}\".format(each_func.name)) \n         entry_bb = BasicBlock(bm.BrilInstruction_Label(dict(label=ENTRY_POINT_NAME)), []) \n         return_bb = BasicBlock(bm.BrilInstruction_Label(dict(label=RETURN_POINT_NAME)), []) \n         prev_bb: Optional[BasicBlock] = None \n         is_first: bool = True \n         for each_bb in basic_blocks: \n             if prev_bb: \n                 # reason: fallthrough edge \n                 prev_bb.succ.add(each_bb) \n                 each_bb.pred.add(prev_bb) \n             elif is_first: \n                 # reason: entry edge \n                 entry_bb.succ.add(each_bb) \n                 each_bb.pred.add(entry_bb) \n                 is_first = False \n              \n             # get last instruction if exists \n             final_instr = each_bb.instrs[-1] if each_bb.instrs else None \n             if final_instr is None: \n                 # empty block, skip \n                 prev_bb = each_bb \n             elif final_instr.op in ['jmp', 'br']: \n                 # reaching control flow instruction \n                 for redirect_instr_to_bb_label in final_instr.labels: \n                     next_bb = next((bb for bb in basic_blocks if bb.label.label == redirect_instr_to_bb_label), None) \n                     if next_bb is None: \n                         raise ValueError(f\"Cannot find block with label {redirect_instr_to_bb_label}\") \n                     # reason: control flow edge \n                     each_bb.succ.add(next_bb) \n                     next_bb.pred.add(each_bb) \n                 prev_bb = None \n             elif final_instr.op in ['ret']: \n                 # reaching return instruction \n                 # reason: return edge \n                 each_bb.succ.add(return_bb) \n                 return_bb.pred.add(each_bb) \n                 prev_bb = None \n                 # explicit return block, no fallthrough \n             else: \n                 # normal instruction, prepare for fallthrough \n                 prev_bb = each_bb \n         # check if last block has no fallthrough \n         if prev_bb: \n             # reason: return edge \n             prev_bb.succ.add(return_bb) \n             return_bb.pred.add(prev_bb) \n         # add entry/return block \n         basic_blocks.insert(0, entry_bb) \n         basic_blocks.append(return_bb) \n         app_bbs_dict[each_func] = basic_blocks \n     return app_bbs_dict \n\n\n\n5. Computing Dominators\n\nThe generate_dom_dict function calculates the dominators for each block using an iterative fixed-point algorithm.\nInitially, every block is assumed to dominate itself and all others. The algorithm repeatedly refines the dominator sets by intersecting the dominator sets of each block’s predecessors.\n\n\n\nℹ️ View source code : EECE7398_ST_Compiler/HW4/dom_analysis.py @ 582ec4a.\n\n def generate_dom_dict(bbs: List[BasicBlock]) -&gt; dict[BasicBlock, set[BasicBlock]]: \n     # Initialize dominator sets for each block \n     # Initially, every block dominates every other block \n     dom: Dict[BasicBlock, Set[BasicBlock]] = {bb: set(bbs) for bb in bbs} \n     # The entry block only dominates itself \n     bb_entry = getBasicBlockByLabel(bbs, ENTRY_POINT_NAME) \n     dom[bb_entry] = set([bb_entry]) \n  \n     changed: bool = True \n     while changed: \n         changed = False \n         for each_bb in bbs: \n             if each_bb == bb_entry: continue \n             # The dominator set of a bb is the intersection of the dominator sets of its predecessors \n             this_bb_dom = set.intersection(*[dom[each_pred] for each_pred in each_bb.pred] or [set()]) \n             # The dominator set of a bb should includes itself \n             this_bb_dom.add(each_bb) \n             # Update if changed \n             if dom[each_bb] != this_bb_dom: \n                 dom[each_bb] = this_bb_dom \n                 changed = True \n     return dom \n\n\n\n6. Immediate Dominator Calculation\n\nThe get_lease_superset_dom function identifies the immediate dominator for a given block by finding the closest dominator in the dominator set.\n\n\n\nℹ️ View source code : EECE7398_ST_Compiler/HW4/dom_analysis.py @ 582ec4a.\n\n # get the last dom of a given block \n def get_lease_superset_dom(dom_dict: dict[BasicBlock, set[BasicBlock]], bb: BasicBlock) -&gt; Optional[BasicBlock]: \n     return next((test_dom for test_dom in dom_dict[bb] if ((dom_dict[bb] - dom_dict[test_dom]) == set([bb]))), None) \n\n\n\n7. Dominance Frontier Computation\n\nThe get_dom_frontier function determines the dominance frontier for a given block. It checks blocks where the dominance relationship changes based on control flow.\n\n\n\nℹ️ View source code : EECE7398_ST_Compiler/HW4/dom_analysis.py @ 582ec4a.\n\n # get the dom frontiers of a given block. (One-liner version, the well-commented version is hidden in previous commits) \n def get_dom_frontier(dom: Dict[BasicBlock, Set[BasicBlock]], bb: BasicBlock) -&gt; Set[BasicBlock]: \n     return set([test_with_bb for test_with_bb in dom.keys() if (bb not in dom[test_with_bb] or bb == test_with_bb) and (bb in set.union(*[dom[each_tbp] for each_tbp in test_with_bb.pred] or [set()]))]) \n\n\n\n8. Graph Visualization\n\nThe code includes helper functions for generating graph visualizations using Graphviz. The functions create_graphviz_dot_node and create_graphviz_dot_edge create nodes and edges for the CFG and dominator tree graphs.\nThe create_graphviz_dot function produces separate visualizations for the CFG and dominator tree.\n\n\n\nℹ️ View source code : EECE7398_ST_Compiler/HW4/dom_analysis.py @ 582ec4a.\n\n def create_graphviz_dot(app_graph: Dict[bm.BrilFunction, List[BasicBlock]]) -&gt; Tuple[gv.Digraph, gv.Digraph]: \n     dot_cfg = gv.Digraph('CFG', comment='Control Flow Graph') \n     dot_domt = gv.Digraph('DOMTREE', comment='Dominator Tree') \n     for each_func, bbs in app_graph.items(): \n         _func_show_str = str(each_func).replace('\\t', '  ') \n         print(f\" {_func_show_str} \".center(80, '=')) \n         dom = generate_dom_dict(bbs) \n         _max_bb_label_len = max([len(bb.label.label) for bb in bbs]) \n  \n         # each func is a subgraph \n         with dot_cfg.subgraph(name=f\"cluster_{each_func.name}\") as dot_cfg_func, dot_domt.subgraph(name=f\"cluster_{each_func.name}\") as dot_domt_func: \n             dot_cfg_func.attr(label=each_func.name) \n             dot_cfg_func.attr(color='#f7f7f7') \n             dot_cfg_func.attr(style='filled') \n             dot_cfg_func.attr(rankdir='TB') \n             dot_domt_func.attr(label=each_func.name) \n             dot_domt_func.attr(color='#f7f7f7') \n             dot_domt_func.attr(style='filled') \n             dot_domt_func.attr(rankdir='TB') \n  \n             print(\" Dom-Front \".center(80, '-')) \n             # generate cfg dot graph \n             for each_bb in bbs: \n                 dom_frontiers = get_dom_frontier(dom, each_bb) \n                 print(f\"{each_bb.label.label.ljust(_max_bb_label_len)} : {', '.join([x.label.label for x in sorted(dom_frontiers)])}\") \n                 create_graphviz_dot_node(dot_cfg_func, each_func, each_bb) \n                 for each_df in dom_frontiers: \n                     create_graphviz_dot_edge(dot_cfg_func, each_func, each_bb, each_df, style='dashed', color='#66ccff', label='DF', fontsize='10', fontcolor='#66ccff', constraint='false') \n                 for each_succ in each_bb.succ: \n                     create_graphviz_dot_edge(dot_cfg_func, each_func, each_bb, each_succ) \n  \n             # generate domt dot graph \n             print(\" Dom & Dom-Tree \".center(80, '-')) \n             for each_bb in dom.keys(): \n                 least_superset_dom = get_lease_superset_dom(dom, each_bb) \n                 print(f\"{each_bb.label.label.ljust(_max_bb_label_len)} &lt;- {(least_superset_dom.label.label if least_superset_dom else '[ROOT]').ljust(_max_bb_label_len)} : {', '.join([x.label.label for x in sorted(dom[each_bb])])}\") \n                 create_graphviz_dot_node(dot_domt_func, each_func, each_bb) \n                 if least_superset_dom: \n                     create_graphviz_dot_edge(dot_domt_func, each_func, least_superset_dom, each_bb) \n              \n             # print the script in briltxt format for reference \n             print(\" Script \".center(80, '-')) \n             for each_bb in bbs: \n                 for each_instr in each_bb.instrs: \n                     print( (\"\" if isinstance(each_instr, bm.BrilInstruction_Label) else \"  \") + each_instr.to_briltxt()) \n              \n             print() \n     return dot_cfg, dot_domt \n\n\n\n9. Main Execution\n\nThe script loads the specified Bril file and constructs the CFG for the functions in the script.\nIt then generates the dot files for the CFG and dominator tree, saving them in the specified directory.\n\n\n\nℹ️ View source code : EECE7398_ST_Compiler/HW4/dom_analysis.py @ 582ec4a.\n\n # %% \n # load the bril script \n bscript = bm.BrilScript(script_name=os.path.basename(DEMO_BRIL_FILE), file_dir=os.path.dirname(DEMO_BRIL_FILE)) \n print(bscript) \n app_graph: Dict[bm.BrilFunction, List[BasicBlock]] = generate_func_cfg_dict(bscript) \n # print(\"Functions: {}\".format(', '.join([f\"[{idx}]={x.name}\" for idx, x in enumerate(app_graph.keys())]))) \n  \n print() \n dot_cfg, dot_domt = create_graphviz_dot(app_graph) \n  \n if MUTE_OUTPUT: \n     exit(0) \n  \n # Finally, create the save directory if not exists, and save the generated dot files \n if not os.path.exists(SAVE_DIR): \n     os.makedirs(SAVE_DIR) \n  \n _gv_ret = dot_cfg.render(os.path.join(SAVE_DIR, bscript.script_name+\".cfg.gv\"), view=False) \n print(_gv_ret) \n _gv_ret = dot_domt.render(os.path.join(SAVE_DIR, bscript.script_name+\".domt.gv\"), view=False) \n print(_gv_ret) \n\n\n\n\nIntegration and Testing\nThe integration and testing phase focuses on verifying the correctness of the dominance analysis by evaluating its results against various test cases. The goal is to ensure that the dominance rules defined in the dominator tree are upheld for different basic block execution paths. This involves checking if any execution sequence violates the dominance relationships established by the analysis. To achieve this, the following steps are used:\n\nGeneral Testing Approach:\n\nThe basic and general idea of verification is to iterate over all possible paths of execution in the Control Flow Graph (CFG) and validate the dominance properties. Specifically, we ensure that if block BB1 dominates block BB2, then any execution path reaching BB2 must pass through BB1.\nFor each basic block, we confirm that its set of dominators conforms to the expectations as derived from the dominator tree.\nThe results are compared against reference outputs from well-understood examples discussed in class, as well as other edge cases designed to stress test the analysis.\nAll analysis output are checked manually, and stored as the golden snapshot using turnt toolchain. To test any modification, use command make turnt to submit a fast check if anything have changed.\n\nTest Cases:\n\nTo thoroughly validate the implementation, several test cases in .bril format are used, which cover a variety of CFG structures. These include:\n\nBranching and Merging: Simple branches where different paths converge into a single block. This tests if the dominance frontier is correctly identified.\nLoop Structures:\n\nNatural Loops: A basic loop where the execution repeatedly returns to the entry point. This checks if the analysis can handle back edges correctly.\nCross Loops: Where multiple loops share a common point, testing the algorithm’s handling of intertwined control flows.\nSelf-Loops: Where a block jumps to itself, testing the edge case of dominance with cyclic behavior.\n\nComplex Branches: Multi-level branching scenarios that test cascading decision-making processes.\nSelect Cases with Fallthrough: A switch-like structure where control can fall through to subsequent cases, testing non-trivial flow paths.\nCycle with/without Header: Examining cases where cycles may or may not have a designated entry block, testing the analysis of entry points and loop headers.\nMultiple Entry Points: CFGs with more than one entry point, challenging the assumptions about where execution begins.\n\n\nTesting Execution:\n\nEach test case is analyzed using the implemented dominance utilities, and the results are compared against expected outputs derived manually or from reference solutions provided in class.\nThe verification process involves:\n\nDominators Check: Ensuring that for each block BB1, its dominator set only contains blocks that dominate BB1 as per the expected results.\nImmediate Dominators Check: Validating the immediate dominator for each block against the reference dominator tree structure.\nDominance Frontier Check: Confirming that the dominance frontier of each block matches the expected set of blocks where the dominance relationship changes.\n\n\n\n\n\nResults and Analysis\n👉 Check HERE to see all graphical testcase results.\nThe analysis results demonstrate the correctness and robustness of the dominance utilities. The generated dominance information (dominator sets, dominator tree, and dominance frontier) matches the expected results for all test cases. Below is a summary of the findings:\n\nBranching and Merging:\n\nThe dominance analysis correctly identifies the blocks that dominate each branch and accurately determines the dominance frontiers where paths converge. \n\n\n\n\nℹ️ View log : Natural-Loops.golden-log @ 16fcb67.\n\n BrilScript ::  Natural-Loops.bril &lt;1 func&gt; @ example/in_class_example_dom \n  \n ================ BrilFunction ::  main (  ) -&gt; None: &lt;14 instr&gt; ================ \n ---------------------------------- Dom-Front ----------------------------------- \n ENTRY  :  \n entry  :  \n loop   : loop \n if     : loop \n then   : endif \n else   : endif \n endif  : loop \n exit   :  \n RETURN :  \n -------------------------------- Dom & Dom-Tree -------------------------------- \n ENTRY  &lt;- [ROOT] : ENTRY \n entry  &lt;- ENTRY  : ENTRY, entry \n loop   &lt;- entry  : ENTRY, entry, loop \n if     &lt;- loop   : ENTRY, entry, if, loop \n then   &lt;- if     : ENTRY, entry, if, loop, then \n else   &lt;- if     : ENTRY, else, entry, if, loop \n endif  &lt;- if     : ENTRY, endif, entry, if, loop \n exit   &lt;- loop   : ENTRY, entry, exit, loop \n RETURN &lt;- exit   : ENTRY, entry, exit, loop, RETURN \n ------------------------------------ Script ------------------------------------ \n .ENTRY: \n .entry: \n   jmp  .loop; \n .loop: \n   br cond .if .exit; \n .if: \n   br cond .then .else; \n .then: \n   jmp  .endif; \n .else: \n   jmp  .endif; \n .endif: \n   jmp  .loop; \n .exit: \n   ret ; \n .RETURN: \n\n\nLoop Structures:\n\nNatural Loops: The algorithm successfully handles back edges and identifies the loop entry points as dominators for the loop body.\nCross Loops and Multiple Entry Points: The implementation accurately distinguishes between blocks that dominate each shared section, even when the control flow is complex.\nSelf-Loops: Dominance analysis treats the block as dominating itself, which is correctly identified in the results. \n\n\n\n\nℹ️ View log : Cycle-with-Header-One-Entry-Point.golden-log @ 16fcb67.\n\n BrilScript ::  Cycle-with-Header-One-Entry-Point.bril &lt;1 func&gt; @ example/in_class_example_dom \n  \n ================ BrilFunction ::  main (  ) -&gt; None: &lt;12 instr&gt; ================ \n ---------------------------------- Dom-Front ----------------------------------- \n ENTRY  :  \n entry  :  \n H1     : H1 \n A      : H1 \n H2     : H1, H2 \n B      : H1, H2 \n exit   :  \n RETURN :  \n -------------------------------- Dom & Dom-Tree -------------------------------- \n ENTRY  &lt;- [ROOT] : ENTRY \n entry  &lt;- ENTRY  : ENTRY, entry \n H1     &lt;- entry  : ENTRY, H1, entry \n A      &lt;- H1     : ENTRY, A, H1, entry \n H2     &lt;- A      : ENTRY, A, H1, H2, entry \n B      &lt;- H2     : ENTRY, A, B, H1, H2, entry \n exit   &lt;- H1     : ENTRY, H1, entry, exit \n RETURN &lt;- exit   : ENTRY, H1, entry, exit, RETURN \n ------------------------------------ Script ------------------------------------ \n .ENTRY: \n .entry: \n   jmp  .H1; \n .H1: \n   br if_cond .A .exit; \n .A: \n   jmp  .H2; \n .H2: \n   br cond .B .H1; \n .B: \n   br cond .H2 .H1; \n .exit: \n   ret ; \n .RETURN: \n\n\n\nℹ️ View log : Cycle-without-Header-Two-Entry-Points.golden-log @ 16fcb67.\n\n BrilScript ::  Cycle-without-Header-Two-Entry-Points.bril &lt;1 func&gt; @ example/in_class_example_dom \n  \n ================ BrilFunction ::  main (  ) -&gt; None: &lt;8 instr&gt; ================= \n ---------------------------------- Dom-Front ----------------------------------- \n ENTRY  :  \n entry  :  \n if     :  \n loop2  : loop1 \n loop1  : loop2 \n RETURN :  \n -------------------------------- Dom & Dom-Tree -------------------------------- \n ENTRY  &lt;- [ROOT] : ENTRY \n entry  &lt;- ENTRY  : ENTRY, entry \n if     &lt;- entry  : ENTRY, entry, if \n loop2  &lt;- if     : ENTRY, entry, if, loop2 \n loop1  &lt;- if     : ENTRY, entry, if, loop1 \n RETURN &lt;- [ROOT] : RETURN \n ------------------------------------ Script ------------------------------------ \n .ENTRY: \n .entry: \n   jmp  .if; \n .if: \n   br cond .loop1 .loop2; \n .loop2: \n   jmp  .loop1; \n .loop1: \n   jmp  .loop2; \n .RETURN: \n\n\nComplex Branches and Fallthrough Cases:\n\nFor cases with multiple branches and fallthrough behavior, the dominance frontier calculation correctly identifies the regions where the execution paths merge.\nThis indicates that the implementation can handle cases where control flow does not follow a simple pattern, such as switch statements with fallthrough. \n\n\n\n\nℹ️ View log : Branch-Another-Example.golden-log @ 16fcb67.\n\n BrilScript ::  Branch-Another-Example.bril &lt;1 func&gt; @ example/in_class_example_dom \n  \n ================ BrilFunction ::  main (  ) -&gt; None: &lt;10 instr&gt; ================ \n ---------------------------------- Dom-Front ----------------------------------- \n ENTRY  :  \n n1     :  \n n2     : n2 \n n3     : n5 \n n4     : n5 \n n5     : n2 \n RETURN :  \n -------------------------------- Dom & Dom-Tree -------------------------------- \n ENTRY  &lt;- [ROOT] : ENTRY \n n1     &lt;- ENTRY  : ENTRY, n1 \n n2     &lt;- n1     : ENTRY, n1, n2 \n n3     &lt;- n2     : ENTRY, n1, n2, n3 \n n4     &lt;- n2     : ENTRY, n1, n2, n4 \n n5     &lt;- n2     : ENTRY, n1, n2, n5 \n RETURN &lt;- [ROOT] : RETURN \n ------------------------------------ Script ------------------------------------ \n .ENTRY: \n .n1: \n   jmp  .n2; \n .n2: \n   br cond1 .n3 .n4; \n .n3: \n   jmp  .n5; \n .n4: \n   jmp  .n5; \n .n5: \n   jmp  .n2; \n .RETURN: \n\n\n\nℹ️ View log : Branch-Complex-Example.golden-log @ 16fcb67.\n\n BrilScript ::  Branch-Complex-Example.bril &lt;1 func&gt; @ example/in_class_example_dom \n  \n ================ BrilFunction ::  main (  ) -&gt; None: &lt;18 instr&gt; ================ \n ---------------------------------- Dom-Front ----------------------------------- \n ENTRY  :  \n n0     :  \n n5     : n4, n8 \n n1     : n4, n8 \n n7     : n8 \n n6     : n4 \n n2     : n4 \n n4     : n8 \n n3     : n8 \n n8     :  \n RETURN :  \n -------------------------------- Dom & Dom-Tree -------------------------------- \n ENTRY  &lt;- [ROOT] : ENTRY \n n0     &lt;- ENTRY  : ENTRY, n0 \n n5     &lt;- n0     : ENTRY, n0, n5 \n n1     &lt;- n0     : ENTRY, n0, n1 \n n7     &lt;- n5     : ENTRY, n0, n5, n7 \n n6     &lt;- n5     : ENTRY, n0, n5, n6 \n n2     &lt;- n1     : ENTRY, n0, n1, n2 \n n4     &lt;- n0     : ENTRY, n0, n4 \n n3     &lt;- n1     : ENTRY, n0, n1, n3 \n n8     &lt;- n0     : ENTRY, n0, n8 \n RETURN &lt;- n8     : ENTRY, n0, n8, RETURN \n ------------------------------------ Script ------------------------------------ \n .ENTRY: \n .n0: \n   br cond1 .n5 .n1; \n .n5: \n   br cond2 .n7 .n6; \n .n1: \n   br cond3 .n2 .n3; \n .n7: \n   jmp  .n8; \n .n6: \n   jmp  .n4; \n .n2: \n   jmp  .n4; \n .n4: \n   jmp  .n8; \n .n3: \n   jmp  .n8; \n .n8: \n   ret ; \n .RETURN: \n\n\nOther Edge Cases:\n\nTests involving cycles without a clear header, and cases with two or more entry points, show that the analysis correctly computes dominators based on the actual paths from any entry to each block.\nThe results reveal that the dominance utilities can generalize beyond typical loop constructs, making them suitable for analyzing diverse control flow patterns.\n\nOther Real Benchmarks: \n\n\n\nℹ️ View log : in_class_example_1.golden-log @ 16fcb67.\n\n BrilScript ::  in_class_example_1.bril &lt;2 func&gt; @ example/in_class_example_df \n  \n ==== BrilFunction ::  in_class_example_1 ( input&lt;int&gt; ) -&gt; None: &lt;33 instr&gt; ==== \n ---------------------------------- Dom-Front ----------------------------------- \n ENTRY             :  \n _ffb1._anon0      :  \n assign_input_to_x :  \n is_x_gt_1         : is_x_gt_1 \n loop_start        : is_x_gt_1 \n is_y_gt_3         : is_x_gt_1 \n y_gt_3            : loop_cont1 \n loop_cont1        : is_x_gt_1 \n is_z_gt_0         : is_x_gt_1 \n z_gt_0            : loop_cont2 \n loop_cont2        : is_x_gt_1 \n print_output      :  \n RETURN            :  \n -------------------------------- Dom & Dom-Tree -------------------------------- \n ENTRY             &lt;- [ROOT]            : ENTRY \n _ffb1._anon0      &lt;- ENTRY             : ENTRY, _ffb1._anon0 \n assign_input_to_x &lt;- _ffb1._anon0      : ENTRY, _ffb1._anon0, assign_input_to_x \n is_x_gt_1         &lt;- assign_input_to_x : ENTRY, _ffb1._anon0, assign_input_to_x, is_x_gt_1 \n loop_start        &lt;- is_x_gt_1         : ENTRY, _ffb1._anon0, assign_input_to_x, is_x_gt_1, loop_start \n is_y_gt_3         &lt;- loop_start        : ENTRY, _ffb1._anon0, assign_input_to_x, is_x_gt_1, is_y_gt_3, loop_start \n y_gt_3            &lt;- is_y_gt_3         : ENTRY, _ffb1._anon0, assign_input_to_x, is_x_gt_1, is_y_gt_3, loop_start, y_gt_3 \n loop_cont1        &lt;- is_y_gt_3         : ENTRY, _ffb1._anon0, assign_input_to_x, is_x_gt_1, is_y_gt_3, loop_cont1, loop_start \n is_z_gt_0         &lt;- loop_cont1        : ENTRY, _ffb1._anon0, assign_input_to_x, is_x_gt_1, is_y_gt_3, is_z_gt_0, loop_cont1, loop_start \n z_gt_0            &lt;- is_z_gt_0         : ENTRY, _ffb1._anon0, assign_input_to_x, is_x_gt_1, is_y_gt_3, is_z_gt_0, loop_cont1, loop_start, z_gt_0 \n loop_cont2        &lt;- is_z_gt_0         : ENTRY, _ffb1._anon0, assign_input_to_x, is_x_gt_1, is_y_gt_3, is_z_gt_0, loop_cont1, loop_cont2, loop_start \n print_output      &lt;- is_x_gt_1         : ENTRY, _ffb1._anon0, assign_input_to_x, is_x_gt_1, print_output \n RETURN            &lt;- print_output      : ENTRY, _ffb1._anon0, assign_input_to_x, is_x_gt_1, print_output, RETURN \n ------------------------------------ Script ------------------------------------ \n .ENTRY: \n ._ffb1._anon0: \n   zero: int = const 0; \n   one: int = const 1; \n   two: int = const 2; \n   three: int = const 3; \n   four: int = const 4; \n   x: int = const 0; \n   y: int = const 0; \n   z: int = const 0; \n .assign_input_to_x: \n   x: int = id input; \n .is_x_gt_1: \n   cond1: bool = gt x one; \n   br cond1 .loop_start .print_output; \n .loop_start: \n   y: int = div x two; \n .is_y_gt_3: \n   cond2: bool = gt y three; \n   br cond2 .y_gt_3 .loop_cont1; \n .y_gt_3: \n   x: int = sub x y; \n .loop_cont1: \n   z: int = sub x four; \n .is_z_gt_0: \n   cond3: bool = gt z zero; \n   br cond3 .z_gt_0 .loop_cont2; \n .z_gt_0: \n   x: int = div x two; \n .loop_cont2: \n   z: int = sub z one; \n   jmp  .is_x_gt_1; \n .print_output: \n   print x; \n   ret ; \n .RETURN: \n  \n ================ BrilFunction ::  main (  ) -&gt; None: &lt;2 instr&gt; ================= \n ---------------------------------- Dom-Front ----------------------------------- \n ENTRY        :  \n _f0d6._anon0 :  \n RETURN       :  \n -------------------------------- Dom & Dom-Tree -------------------------------- \n ENTRY        &lt;- [ROOT]       : ENTRY \n _f0d6._anon0 &lt;- ENTRY        : ENTRY, _f0d6._anon0 \n RETURN       &lt;- _f0d6._anon0 : ENTRY, _f0d6._anon0, RETURN \n ------------------------------------ Script ------------------------------------ \n .ENTRY: \n ._f0d6._anon0: \n   input: int = const 10; \n   call in_class_example_1; \n .RETURN: \n\n* Check HERE to see all graphical test case results. ** Check HERE to see all golden snapshots.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 4: Dominance"
    ]
  },
  {
    "objectID": "blogs/Qucheng/2024-10-11-Qucheng-HW4.html#conclusion",
    "href": "blogs/Qucheng/2024-10-11-Qucheng-HW4.html#conclusion",
    "title": "Homework 4: Dominance",
    "section": "Conclusion",
    "text": "Conclusion\nDominance analysis is a key part of CFG-based optimizations in compilers. It involves: - Identifying dominators to understand the flow of control. - Using the immediate dominator to build the dominator tree. - Computing the dominance frontier to manage variable merging.\nThese concepts enable powerful optimizations and transformations that improve program efficiency and correctness. In the following sections, we will explore how to implement dominance analysis in a compiler framework, test the correctness of the results, and apply these concepts to optimize real programs.\nThe integration and testing of the dominance analysis show that the implementation is reliable and performs well across a range of control flow scenarios. By comparing the analysis results with known references and carefully designed test cases, we ensure the correctness of the dominator tree construction and dominance frontier calculations.\nThe next steps involve using this verified dominance analysis for further compiler optimizations, such as SSA form conversion, dead code elimination, and loop-invariant code motion, leveraging the dominance relationships identified in this phase.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Qucheng",
      "Homework 4: Dominance"
    ]
  },
  {
    "objectID": "blogs/aymane/readme2.html",
    "href": "blogs/aymane/readme2.html",
    "title": "Homework 2 – Implementing DCE and LVN",
    "section": "",
    "text": "This assignment assumes that the input program is a single block. Multi block programs might not work as intended. Parsing the program into multiple blocks is handled by the helper functions inside cfg.py.\n\nPart 1: implementing trivial deadcode elimination\nI begin by setting up a class to perform dead code elimination, the class takes as input the program, and parses it before begining to perform the dce optimization. The block_dce() function performs the heavy lifting. Despite the fact that my implementation only works for a single block, I’ve tried to modularize the code such that a multi block implementation is easier.\n# perform deadcode elimination on a single block\ndef block_dce(self, block):\n    for instr in block:\n        if \"args\" not in instr.keys():\n            continue\n        self.used.update(instr[\"args\"])\n    \n    for i in range(len(block)-1, -1, -1):\n        if \"dest\" in block[i].keys() and block[i][\"dest\"] not in self.used:\n            rm_instr = block.pop(i)\n            # print(f\"Instruction removed:\\n {rm_instr}. Destination {rm_instr[\"dest\"]} was not used\")   \n    return block\nThe function loops over all instructions in the block adding only the instructions that have arguments to a set. This is done to keep track of all arguments that are used in the block. The second pass, loops over all instrucitons backwards.\nIf an instruction has a destination variable that doesn’t exists in the set of used variables we know that it is a dead instruction, therefore we can delete it.\nLet’s look at an example typescript program:\nlet x = 5n;  \nlet y = 13n;\n\nlet a = x + y;\nlet b = x + y;\nlet c = a * 2n;     # c is not used\nlet d = b * 2n;     \n\nlet e = a + 15n;    # e is not used\nlet f = d - b;\nlet g = f + 1n;     # g is not used\n\nconsole.log(f);\nWe should be able to eliminate 3 instructions.\nprogram.bril: text representation. program_j.bril: json representation. program.bril_dce: json representation with the dce pass.\nWe want to compare the number of instructions between program_j.bril and program.bril_dce and make sure the outputs are the same.\n$ wc program_j.bril && brili &lt; program_j.bril\n     280     421    4942 program_j.bril\n18\n$ wc program.bril_dce && brili &lt; program.bril_dce\n     249     375    6781 program.bril_dce\n18\nWe notice a reduction in the number of lines from 280, down to 249. Most of this reduction is a direct result of the json format returning to a new line after every entry. So most lines only contain a bracket, or a comma. The output (18) is the same for both programs.\n\n\nPart 2: implementing local value numbering\nMy lvn implementation involves two main functions: vn_gen() to generate the value number, and lvn() which performs the lvn pass.\nThe vn_gen() function returns the value number of a variable. If the variable doesn’t exist, we add it to the table, and increment the value number for the next variable.\n    def vn_gen(self, var):\n        if var not in self.vn2var.keys():\n            self.vn2var[var] = self.vn\n            self.vn += 1\n        return self.vn2var[var] \n    \n    def lvn(self, block):\n        for instr in block:\n            if \"dest\" in instr.keys():\n                if \"args\" in instr.keys():\n                    values = [self.vn_gen(arg) for arg in instr[\"args\"]]\n                    hash_entry = (instr[\"op\"], *values)\n                    canonical_var = instr[\"dest\"]\n                else:   \n                    val = instr[\"value\"]\n                    values = [self.vn_gen(instr[\"dest\"])]\n                    hash_entry = (instr[\"op\"], val)\n                    canonical_var = instr[\"dest\"]\n                \n                if hash_entry in self.hash_table.keys():\n                    vn = self.hash_table[hash_entry][\"vn\"]\n                    canonical_var = self.hash_table[hash_entry][\"canncl_var\"]\n                    self.vn2var[instr[\"dest\"]] = vn\n                    instr[\"dest\"] = canonical_var\n                \n                else:\n                    new_vn = self.vn_gen(instr[\"dest\"])\n                    self.hash_table[hash_entry] = {\"vn\": new_vn, \"canncl_var\": canonical_var}\n            else:\n                continue\n        return block\nThe lvn() function loops over all instructions in a block. If an instruction doesn’t contain a destination, we skip it. Otherwise, we check if the instruction contains arguments, if so, we calculate the value numebers of all arguments, and create the hash entry using those value numbers. If the instruction doesn’t contain arguments, then we only need to compute the value number of the destination variable, and use it to generate a hash entry.\nFinally, we need to check if the hash entry already exists in the table. If so, we update the structs to avoid recomputing the same value.\nIn the program below, the sum2 operation is redundant. Let’s see how the lvn algorithm modifices the code:\n@main {\n    a: int = const 4;\n    b: int = const 2;\n    sum1: int = add a b;\n    sum2: int = add a b;\n    prod: int = mul sum1 sum2;\n    print prod;\n}\nRuning the lvn pass we notice that the sum2 instruction was rewritten in terms of sum1\npython3 lvn.py test.bril && bril2txt &lt; test.bril_lvn\n@main {\n  a: int = const 4;\n  b: int = const 2;\n  sum1: int = add a b;\n  sum2: int = const sum1;\n  prod: int = mul sum1 sum2;\n  print prod;\n}\n\nValue Number Table\n{'a': 1, 'b': 2, 'sum1': 3, 'sum2': 3, 'prod': 4}\nHash Table:\n{('const', 4): {'vn': 1, 'canncl_var': 'a'}, ('const', 2): {'vn': 2, 'canncl_var': 'b'}, ('add', 1, 2): {'vn': 3, 'canncl_var': 'sum1'}, ('mul', 3, 3): {'vn': 4, 'canncl_var': 'prod'}}\n\n\nThings I found challenging\nHandling the formatting of the input and output to each command was a little tricky. It’s sometimes confusing trying to keep track of which format is being used at a specific instance when piping different commands into each other.\nFor example, it seems like the brili interpreter can not read a program from stdin, instead the filename has to be provided as input brili &lt; {filename}. This makes it difficult to pipe programs into brili, which requires running bril2txt and bril2json commands manually to guarantee correct execution.\nTo make things simple for next time, I will make sure that all programs I write accept input from stdin, and output to stdout.\nCode can be found here\n\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Homework 2 -- Implementing DCE and LVN"
    ]
  },
  {
    "objectID": "blogs/aymane/mlir-transform-dialect.html",
    "href": "blogs/aymane/mlir-transform-dialect.html",
    "title": "Project Presentation - The MLIR Transform Dialect",
    "section": "",
    "text": "I recently explored the MLIR (Multi-Level Intermediate Representation) Transform dialect. The framework enables control over compiler transformations, at different levels of abstraction. It also utilizes the concept of a schedule to manage complex transformation sequences within the IR. This approach provides engineers with fine grained control over transformations enabling the optimization of code for a wide range of architectures.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Project Presentation - The MLIR Transform Dialect"
    ]
  },
  {
    "objectID": "blogs/aymane/mlir-transform-dialect.html#what-is-the-transform-dialect",
    "href": "blogs/aymane/mlir-transform-dialect.html#what-is-the-transform-dialect",
    "title": "Project Presentation - The MLIR Transform Dialect",
    "section": "What is the Transform Dialect?",
    "text": "What is the Transform Dialect?\nThe MLIR transform dialect is designed to enable precise control over compiler transformations by using the compiler IR itself. This approach allows transformations to be embedded with the IR being transformed, an approach that doesn’t require rebuilding the compiler.\nThe main goal of the Transform dialect is to orchestrate fine-grain transformations on individual operations or sets of operations within the IR. Traditional compiler passes can be thought of as a “monolithic black box” that apply transformations to the entire program. Whereas the Transform dialect enables finer control over optimizations for a more targeted approach.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Project Presentation - The MLIR Transform Dialect"
    ]
  },
  {
    "objectID": "blogs/aymane/mlir-transform-dialect.html#why-the-transform-dialect",
    "href": "blogs/aymane/mlir-transform-dialect.html#why-the-transform-dialect",
    "title": "Project Presentation - The MLIR Transform Dialect",
    "section": "Why the Transform Dialect?",
    "text": "Why the Transform Dialect?\nIn order to fully utilize the hardware capabilities, target specific compiler optimizations must be leveraged. During compilation, domain specific knowledge can be incorporated to squeeze even more performance out of the hardware. Nowadays, programmers are mostly limited to using high level compiler directives like pragmas to influence passes. This rigid approach lacks modularity and extensibility, meaning that low level program optimizations must be performed for each and every target platform. Additionally, the Transform dialect incurs less than 2.6% compile time overhead while providing robust and scalable features.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Project Presentation - The MLIR Transform Dialect"
    ]
  },
  {
    "objectID": "blogs/aymane/mlir-transform-dialect.html#the-end-of-moores-law",
    "href": "blogs/aymane/mlir-transform-dialect.html#the-end-of-moores-law",
    "title": "Project Presentation - The MLIR Transform Dialect",
    "section": "The End of Moore’s Law",
    "text": "The End of Moore’s Law\nFor decades, computer performance was riding the transistor shrinking wave. As transistors got smaller, chips could pack more and more transistors per unit area. The slowing down of Moore’s law has left engineers looking for new ways of obtaining more performance improvements.\n\n\n\ncomputing_landscape\n\n\nThe end of Moore’s law paves the way for specialization and hardware acceleration. Since general purpose hardware is at a saturation point, computing systems now offload certain tasks and workloads to specialized units called accelerators. As opposed to general purpose processing units, accelerators are built with a single specific usage in mind. For example, we are experiencing firsthand the rise of SoCs that integrate all kinds of processors and different heterogeneous systems tailored for domain specific workloads.\nThe complexity needed to manage this ever growing compute platform diversity is not trivial. However, with the help of the compiler, we can bridge the gap between high level software constructs and lower level platform specific implementations. Given the current landscape, frameworks such as LLVM and MLIR aim to consolidate, standardize and solve many of the problems caused by the ever increasing number of specialized architectures.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Project Presentation - The MLIR Transform Dialect"
    ]
  },
  {
    "objectID": "blogs/aymane/mlir-transform-dialect.html#payload-and-schedule-approach",
    "href": "blogs/aymane/mlir-transform-dialect.html#payload-and-schedule-approach",
    "title": "Project Presentation - The MLIR Transform Dialect",
    "section": "Payload and Schedule Approach",
    "text": "Payload and Schedule Approach\nThe transform dialect is not the first to separate the payload (ie. program) from the schedule (ie. the transformations performed on the program), in fact frameworks such as Halide, TVM, and TACO have been using this concept before the existence of the Transform dialect. Although, they seperate the schedule from the payload, these frameworks focus on domain specific optimizations, have predefined software stacks and are not well integrated into a generic compiler unlike the Transform dialect. The goal is to provide a general purpose solution that can be deployed for various envrionments and targets.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Project Presentation - The MLIR Transform Dialect"
    ]
  },
  {
    "objectID": "blogs/aymane/mlir-transform-dialect.html#modularity-and-extensibility",
    "href": "blogs/aymane/mlir-transform-dialect.html#modularity-and-extensibility",
    "title": "Project Presentation - The MLIR Transform Dialect",
    "section": "Modularity and Extensibility",
    "text": "Modularity and Extensibility\nThe MLIR Transform dialect is designed with modularity at its core. Users have the ability to define new transformations that can integrate seamlessly into existing workflows. One of the standout features is its extensibility. Users can inject additional operations into the dialect using mechanisms like TransformDialectExtension, allowing for custom transformations without altering the core dialect. Additionally, users can compose new transformations by combining existing ones or creating entirely new operations. This allows for precise control over compiler transformations and facilitates the highly iterative workflows needed for research purposes.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Project Presentation - The MLIR Transform Dialect"
    ]
  },
  {
    "objectID": "blogs/aymane/mlir-transform-dialect.html#hardware-specific-optimizations-and-search-methods",
    "href": "blogs/aymane/mlir-transform-dialect.html#hardware-specific-optimizations-and-search-methods",
    "title": "Project Presentation - The MLIR Transform Dialect",
    "section": "Hardware Specific Optimizations and Search Methods",
    "text": "Hardware Specific Optimizations and Search Methods\nIn practice, the transform dialect can be used to perform a sequence of optimizations that result in efficient code generation for MLIR operations. The Transform Dialect is particularly effective in optimizing linear algebra computations such as matrix multiplication and convolution.\nComposing existing transformations can lead to highly efficient code tailored to specific architectures. Furthermore, the Transform dialect allows for the customization of compiler transformations to best use the characteristics of the target hardware such as the cache block size or the number of registers. This flexibility is crucial for maximizing performance across a wide range of accelerators.\nThe separation of the payload and schedule allows for the use of search methods to explore optimization spaces effectively. This enables autotuning transformation parameters with the goal of finding the most optimal set of parameters on specific workloads and for specific targets.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Project Presentation - The MLIR Transform Dialect"
    ]
  },
  {
    "objectID": "blogs/aymane/mlir-transform-dialect.html#challenges-and-learning-curve",
    "href": "blogs/aymane/mlir-transform-dialect.html#challenges-and-learning-curve",
    "title": "Project Presentation - The MLIR Transform Dialect",
    "section": "Challenges and Learning Curve",
    "text": "Challenges and Learning Curve\nWhile the transform dialect offers powerful capabilities, it requires a solid understanding of MLIR’s infrastructure. Familiarity with existing transformation patterns and how they interact with the transform dialect is crucial for effectively leveraging its potential.\nFor most of my time, I have treated compilers as a black box, passing in an input file and getting a binary in return. Taking a compiler course has sparked my interest even further. More recently, I have been spending time reading about LLVM and MLIR. Despite the steep learning curve, I believe these to be important and promising frameworks I need to add to my arsenal.\nFor those interested in diving deeper, I recommend starting with MLIR’s basic tutorials before tackling the transform dialect. It’s a rewarding journey that opens up new possibilities in compiler design and optimization.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Project Presentation - The MLIR Transform Dialect"
    ]
  },
  {
    "objectID": "blogs/aymane/readme.html",
    "href": "blogs/aymane/readme.html",
    "title": "Homework 1 – Trying Out Bril",
    "section": "",
    "text": "To compile from typescript to bril and execute the program run the following:\n$ ts2bril bench.ts &gt; bench.bril && brili &lt; bench.bril\n727\nThe typescript benchmark I wrote performs a series of simple arithmetic, logical and alu operations. The function outputs 727 with the default input value 10.\n\n\nWhen writing the typescript benchmark, the input function argument was hardcoded to have value 10. So I couldn’t parametrized the turnt tests. I wanted to figure out a way to pass the input as a command line argument from stdin in order to generate different tests. Now I could do this by importing a typescript package to handle user input. But I’ve decided to manually modify my benchmark at the bril level.\nInitially, the typescript benchmark I wrote resulted in the following bril ir:\nbench.bril:\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        {\n          \"op\": \"const\",\n          \"value\": 10,\n          \"dest\": \"v31\",\n          \"type\": \"int\"\n        },\n        ...\n      ]\n      \"args\": []\n    },\n    ...\n  ]\n}\nThe value 10 is passed in as a constant to main. In order to parametrize it, a few things need to be modified. First, we need to change the op field from const to id. As explained in the bril documentation, the id opcode is a type-insensitive identity we can use to pass variables around. Additonally, we also need specify the arguments passed to main. To do so, we pass a argument to the function. So the new bril representation becomes:\nbench-param.bril:\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"args\": [\n        {\n          \"name\": \"val\",\n          \"type\": \"int\"\n        }\n      ],\n      \"instrs\": [\n        {\n          \"args\": [\n            \"val\"\n          ],\n          \"dest\": \"v31\",\n          \"op\": \"id\",\n          \"type\": \"int\"\n        }, \n      ],\n    },\n    ...\n  ]\n}\n\nNow we can pass any value to the function:\n$ brili &lt; bench-param.bril 12\n2796\nTo more easily view these changes, let’s look at the text formatting of bril by running these commands to convert from json to text.\n$ bril2txt &lt; bench.bril &gt; bench.txt && bril2txt &lt; bench-param.bril &gt; bench-param.txt\nbench.txt\n@main {\n  v31: int = const 10;\n  ...\n}\nbench-param.txt\n@main(val: int) {\n  v31: int = id val;\n  ...\n}",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Homework 1 -- Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/aymane/readme.html#passing-command-line-arugments",
    "href": "blogs/aymane/readme.html#passing-command-line-arugments",
    "title": "Homework 1 – Trying Out Bril",
    "section": "",
    "text": "When writing the typescript benchmark, the input function argument was hardcoded to have value 10. So I couldn’t parametrized the turnt tests. I wanted to figure out a way to pass the input as a command line argument from stdin in order to generate different tests. Now I could do this by importing a typescript package to handle user input. But I’ve decided to manually modify my benchmark at the bril level.\nInitially, the typescript benchmark I wrote resulted in the following bril ir:\nbench.bril:\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        {\n          \"op\": \"const\",\n          \"value\": 10,\n          \"dest\": \"v31\",\n          \"type\": \"int\"\n        },\n        ...\n      ]\n      \"args\": []\n    },\n    ...\n  ]\n}\nThe value 10 is passed in as a constant to main. In order to parametrize it, a few things need to be modified. First, we need to change the op field from const to id. As explained in the bril documentation, the id opcode is a type-insensitive identity we can use to pass variables around. Additonally, we also need specify the arguments passed to main. To do so, we pass a argument to the function. So the new bril representation becomes:\nbench-param.bril:\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"args\": [\n        {\n          \"name\": \"val\",\n          \"type\": \"int\"\n        }\n      ],\n      \"instrs\": [\n        {\n          \"args\": [\n            \"val\"\n          ],\n          \"dest\": \"v31\",\n          \"op\": \"id\",\n          \"type\": \"int\"\n        }, \n      ],\n    },\n    ...\n  ]\n}\n\nNow we can pass any value to the function:\n$ brili &lt; bench-param.bril 12\n2796\nTo more easily view these changes, let’s look at the text formatting of bril by running these commands to convert from json to text.\n$ bril2txt &lt; bench.bril &gt; bench.txt && bril2txt &lt; bench-param.bril &gt; bench-param.txt\nbench.txt\n@main {\n  v31: int = const 10;\n  ...\n}\nbench-param.txt\n@main(val: int) {\n  v31: int = id val;\n  ...\n}",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Aymane",
      "Homework 1 -- Trying Out Bril"
    ]
  },
  {
    "objectID": "blogs/yashaswini/PaperPresentation-ProgaML.html",
    "href": "blogs/yashaswini/PaperPresentation-ProgaML.html",
    "title": "Paper Presentation - Yashaswini",
    "section": "",
    "text": "This paper adresses the following issues: 1. Hand-tuning Heuristics for changes in software or hardware is time consuming and never-ending 2. Machine learning approaches do don’t capture the structure of programs and are unable to reason about program behaviour​ - Unnessecary emphasis on naming conventions​ - Using compilation to remove noise also omits information​ - Related statemed separated sequentially fall to vanishing gradients and catastrophic forgetting​\n\n\n\nPrevious methods of representing IR code for input to ML algorithms: 1. AST-code2vec - AST paths to embed programs​ - highly effective at software engineering tasks such as algorithm classification, where the code was written by humans​ - puts more weight on names rather than code structure 2. Neural Code Comprehension - Encoder uses Contextual Flow Graphs (XFG) built from LLVM-IR statements to create inputs for neural networks​ - Combining DFGs and CFGs, the XFG representation omits important information such as order of instruction operands​ 3. Control and Data Flow graphs - uses only instruction opcodes to compute latent representations​ - Omits data types, the presence of variables and constants, and the ordering of operands\n\n\n\nPrograml offers a new graphical representation of IR that combines - control flow - data flow - call flow - input encoding\nto bypass issues mentioned earlier.\nThey tested agains 3 main problem types - Traditional compiler analysis - Heterogeneous device mapping - Algorithm classification with the caveat that this representation is not meant for these purposes and should not be used to substitute current methods, but need to be able to pass them in order to be a valid solution\n\n\n\n\nProGraML aims to create a toolbox for eventual machine learning application in optimization compilers​.\nsolver issues that other state of the art representations have not addresses\nMay aid other endevours in the future.\n\n\n\n\n\nIt does not replace any of the current tools and cannot stand alone in this task\nIt cannot currently be put to use as it is only a proof of concept\nwhile it may be useful in the future, we cannot say for sure what changes may happen over the years.\n\n\n\n\n\nwe disscussed the reason that the paper focused so much on telling us that the ProGraML representation is not meant to replace current methods, and reasoned about its nature as a toolbox for the future.\nwe discussed its use in amchine learning and wherther it could be sed with large language models or other use cases.\n\n\n\n\n\nOverall the paper is well written ad they present their findings well.\nthey have good data to back up thier conclutions\nthey do not gloss over the shortcomings of their work\nhowever, the usefulness of the representation has not been fully showcased. some more work may need to be done for a final product of this toolbox to be useful to users.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Paper Presentation - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/PaperPresentation-ProgaML.html#introduction",
    "href": "blogs/yashaswini/PaperPresentation-ProgaML.html#introduction",
    "title": "Paper Presentation - Yashaswini",
    "section": "",
    "text": "This paper adresses the following issues: 1. Hand-tuning Heuristics for changes in software or hardware is time consuming and never-ending 2. Machine learning approaches do don’t capture the structure of programs and are unable to reason about program behaviour​ - Unnessecary emphasis on naming conventions​ - Using compilation to remove noise also omits information​ - Related statemed separated sequentially fall to vanishing gradients and catastrophic forgetting​",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Paper Presentation - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/PaperPresentation-ProgaML.html#background",
    "href": "blogs/yashaswini/PaperPresentation-ProgaML.html#background",
    "title": "Paper Presentation - Yashaswini",
    "section": "",
    "text": "Previous methods of representing IR code for input to ML algorithms: 1. AST-code2vec - AST paths to embed programs​ - highly effective at software engineering tasks such as algorithm classification, where the code was written by humans​ - puts more weight on names rather than code structure 2. Neural Code Comprehension - Encoder uses Contextual Flow Graphs (XFG) built from LLVM-IR statements to create inputs for neural networks​ - Combining DFGs and CFGs, the XFG representation omits important information such as order of instruction operands​ 3. Control and Data Flow graphs - uses only instruction opcodes to compute latent representations​ - Omits data types, the presence of variables and constants, and the ordering of operands",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Paper Presentation - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/PaperPresentation-ProgaML.html#main-contribution",
    "href": "blogs/yashaswini/PaperPresentation-ProgaML.html#main-contribution",
    "title": "Paper Presentation - Yashaswini",
    "section": "",
    "text": "Programl offers a new graphical representation of IR that combines - control flow - data flow - call flow - input encoding\nto bypass issues mentioned earlier.\nThey tested agains 3 main problem types - Traditional compiler analysis - Heterogeneous device mapping - Algorithm classification with the caveat that this representation is not meant for these purposes and should not be used to substitute current methods, but need to be able to pass them in order to be a valid solution",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Paper Presentation - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/PaperPresentation-ProgaML.html#merits",
    "href": "blogs/yashaswini/PaperPresentation-ProgaML.html#merits",
    "title": "Paper Presentation - Yashaswini",
    "section": "",
    "text": "ProGraML aims to create a toolbox for eventual machine learning application in optimization compilers​.\nsolver issues that other state of the art representations have not addresses\nMay aid other endevours in the future.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Paper Presentation - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/PaperPresentation-ProgaML.html#shortcomings",
    "href": "blogs/yashaswini/PaperPresentation-ProgaML.html#shortcomings",
    "title": "Paper Presentation - Yashaswini",
    "section": "",
    "text": "It does not replace any of the current tools and cannot stand alone in this task\nIt cannot currently be put to use as it is only a proof of concept\nwhile it may be useful in the future, we cannot say for sure what changes may happen over the years.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Paper Presentation - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/PaperPresentation-ProgaML.html#class-discussion",
    "href": "blogs/yashaswini/PaperPresentation-ProgaML.html#class-discussion",
    "title": "Paper Presentation - Yashaswini",
    "section": "",
    "text": "we disscussed the reason that the paper focused so much on telling us that the ProGraML representation is not meant to replace current methods, and reasoned about its nature as a toolbox for the future.\nwe discussed its use in amchine learning and wherther it could be sed with large language models or other use cases.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Paper Presentation - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/PaperPresentation-ProgaML.html#conclusion",
    "href": "blogs/yashaswini/PaperPresentation-ProgaML.html#conclusion",
    "title": "Paper Presentation - Yashaswini",
    "section": "",
    "text": "Overall the paper is well written ad they present their findings well.\nthey have good data to back up thier conclutions\nthey do not gloss over the shortcomings of their work\nhowever, the usefulness of the representation has not been fully showcased. some more work may need to be done for a final product of this toolbox to be useful to users.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Paper Presentation - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/09-27-2024-HW2-YashaswiniMakaram.html",
    "href": "blogs/yashaswini/09-27-2024-HW2-YashaswiniMakaram.html",
    "title": "Homework2 - Yashaswini",
    "section": "",
    "text": "starting from the end of the function, the optimzer keeps a record of the variables used.\nif an instruction is assigning value to a variable that is not used later in the function, then it is eliminated\nonce a used variable is defined in the code, it is removed form the list of used varibles. ## Testing:\n\ntest case: ’’’ { “functions”: [ { “name”: “main”, “instrs”: [ { “op”: “const”, “type”: “int”, “dest”: “v1”, “value”: 10 }, { “op”: “const”, “type”: “int”, “dest”: “v2”, “value”: 2 }, { “op”: “const”, “type”: “int”, “dest”: “v3”, “value”: 1 }, { “op”: “add”, “type”: “int”, “dest”: “out”, “args”: [“v1”, “v2”] }, { “op”: “print”, “args”: [“out”] } ] } ] } ’’’\noutput: ’’’ { “functions”: [ { “name”: “main”, “instrs”: [ { “op”: “const”, “type”: “int”, “dest”: “v1”, “value”: 10 }, { “op”: “const”, “type”: “int”, “dest”: “v2”, “value”: 2 }, { “op”: “add”, “type”: “int”, “dest”: “out”, “args”: [“v1”, “v2”] }, { “op”: “print”, “args”: [“out”] } ] } ] } ’’’\nAs you can see above, satrting with the last instruction, out is the only variable used. Going backwards the instruction is assigning value to out which is used so this line stays and the inputs, v1 and v2 are added to used variables, and out is removed from used variables. the next instrction up is assigning value to v3, which is not used. there fore this instruction is eliminated and its inputs are not added to used variables.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework2 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/09-27-2024-HW2-YashaswiniMakaram.html#about-the-code",
    "href": "blogs/yashaswini/09-27-2024-HW2-YashaswiniMakaram.html#about-the-code",
    "title": "Homework2 - Yashaswini",
    "section": "",
    "text": "starting from the end of the function, the optimzer keeps a record of the variables used.\nif an instruction is assigning value to a variable that is not used later in the function, then it is eliminated\nonce a used variable is defined in the code, it is removed form the list of used varibles. ## Testing:\n\ntest case: ’’’ { “functions”: [ { “name”: “main”, “instrs”: [ { “op”: “const”, “type”: “int”, “dest”: “v1”, “value”: 10 }, { “op”: “const”, “type”: “int”, “dest”: “v2”, “value”: 2 }, { “op”: “const”, “type”: “int”, “dest”: “v3”, “value”: 1 }, { “op”: “add”, “type”: “int”, “dest”: “out”, “args”: [“v1”, “v2”] }, { “op”: “print”, “args”: [“out”] } ] } ] } ’’’\noutput: ’’’ { “functions”: [ { “name”: “main”, “instrs”: [ { “op”: “const”, “type”: “int”, “dest”: “v1”, “value”: 10 }, { “op”: “const”, “type”: “int”, “dest”: “v2”, “value”: 2 }, { “op”: “add”, “type”: “int”, “dest”: “out”, “args”: [“v1”, “v2”] }, { “op”: “print”, “args”: [“out”] } ] } ] } ’’’\nAs you can see above, satrting with the last instruction, out is the only variable used. Going backwards the instruction is assigning value to out which is used so this line stays and the inputs, v1 and v2 are added to used variables, and out is removed from used variables. the next instrction up is assigning value to v3, which is not used. there fore this instruction is eliminated and its inputs are not added to used variables.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework2 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/09-27-2024-HW2-YashaswiniMakaram.html#about-the-code-1",
    "href": "blogs/yashaswini/09-27-2024-HW2-YashaswiniMakaram.html#about-the-code-1",
    "title": "Homework2 - Yashaswini",
    "section": "About the Code",
    "text": "About the Code\n\ngiven a block of code the program starts from the top and assigns a vlaue number to each variable and computation.\nif a variable is reasigned it’s value number changes. the value number of a computation is the same only if both the value numbers of the inputs and the operation is the same\nif two instuctions have the same value number, then the instruction is changed by copying the previously computed value.\nall subsiquent instructions that use that variable will check the value table, and choose the earliest variable that has that value\n\n##Testing\ntest case: ’’’ { “functions”: [ { “name”: “main”, “instrs”: [ { “op”: “const”, “type”: “int”, “dest”: “v1”, “value”: 10 }, { “op”: “const”, “type”: “int”, “dest”: “v2”, “value”: 5 }, { “op”: “add”, “type”: “int”, “dest”: “sum1”, “args”: [“v1”, “v2”] }, { “op”: “add”, “type”: “int”, “dest”: “sum2”, “args”: [“v1”, “v2”] }, { “op”: “mul”, “type”: “int”, “dest”: “prod1”, “args”: [“sum1”, “sum2”] }, { “op”: “add”, “type”: “int”, “dest”: “sum3”, “args”: [“sum1”, “prod1”] }, { “op”: “add”, “type”: “int”, “dest”: “sum3”, “args”: [“sum3”, “sum2”] }, { “op”: “print”, “args”: [“sum3”] } ] } ] }\n’’’\noutput:\n’’’ { “functions”: [ { “name”: “main”, “instrs”: [ { “op”: “const”, “type”: “int”, “dest”: “v1”, “value”: 10 }, { “op”: “const”, “type”: “int”, “dest”: “v2”, “value”: 5 }, { “op”: “add”, “type”: “int”, “dest”: “sum1”, “args”: [“v1”, “v2”] }, { “op”: “copy”, “type”: “int”, “dest”: “sum2”, “args”: [“sum1”] }, { “op”: “mul”, “type”: “int”, “dest”: “prod1”, “args”: [“sum1”, “sum1”] }, { “op”: “add”, “type”: “int”, “dest”: “sum3”, “args”: [“sum1”, “prod1”] }, { “op”: “add”, “type”: “int”, “dest”: “sum3”, “args”: [“sum3”, “sum1”] }, { “op”: “print”, “args”: [“sum3”] } ] } ] }\n’’’ In this case, the local value numbering does not reducte the number of instructions, however it does reduce the number of computations.\nin order to remove the unused instructions, we can now run the dead code elimination.\noutput: ’’’ { “functions”: [ { “name”: “main”, “instrs”: [ { “op”: “const”, “type”: “int”, “dest”: “v1”, “value”: 10 }, { “op”: “const”, “type”: “int”, “dest”: “v2”, “value”: 5 }, { “op”: “add”, “type”: “int”, “dest”: “sum1”, “args”: [“v1”, “v2”] }, { “op”: “mul”, “type”: “int”, “dest”: “prod”, “args”: [“sum1”, “sum1”] }, { “op”: “add”, “type”: “int”, “dest”: “sum3”, “args”: [“sum1”, “prod”] }, { “op”: “add”, “type”: “int”, “dest”: “sum3”, “args”: [“sum3”, “sum1”] }, { “op”: “print”, “args”: [“sum3”] } ] } ] }\n’’’\nnow the duplicated instuction is removed as sum2 is never used.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework2 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/09-20-2024-HW1-YashaswiniMakaram.html",
    "href": "blogs/yashaswini/09-20-2024-HW1-YashaswiniMakaram.html",
    "title": "Homework1 - Yashaswini",
    "section": "",
    "text": "I created a program that iterates through an array of values both suming and multiplying the values.\n\n\nthe code was written in text forma ns used a pointer to initialize the array. store and load instructions were used to access the array and .loop with a jmp was used for the iterations\n\n\n\nThe code was tested with different size arrays with the answers check using an external calculator.\n\n\n\nThe main challenge for this was finding the best way to populate the array with values. brili is very simple and using just stores and loads to index into the array gave added complexity.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework1 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/09-20-2024-HW1-YashaswiniMakaram.html#about-the-code",
    "href": "blogs/yashaswini/09-20-2024-HW1-YashaswiniMakaram.html#about-the-code",
    "title": "Homework1 - Yashaswini",
    "section": "",
    "text": "the code was written in text forma ns used a pointer to initialize the array. store and load instructions were used to access the array and .loop with a jmp was used for the iterations",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework1 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/09-20-2024-HW1-YashaswiniMakaram.html#testing",
    "href": "blogs/yashaswini/09-20-2024-HW1-YashaswiniMakaram.html#testing",
    "title": "Homework1 - Yashaswini",
    "section": "",
    "text": "The code was tested with different size arrays with the answers check using an external calculator.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework1 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/09-20-2024-HW1-YashaswiniMakaram.html#challenges",
    "href": "blogs/yashaswini/09-20-2024-HW1-YashaswiniMakaram.html#challenges",
    "title": "Homework1 - Yashaswini",
    "section": "",
    "text": "The main challenge for this was finding the best way to populate the array with values. brili is very simple and using just stores and loads to index into the array gave added complexity.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework1 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/09-20-2024-HW1-YashaswiniMakaram.html#python-tool",
    "href": "blogs/yashaswini/09-20-2024-HW1-YashaswiniMakaram.html#python-tool",
    "title": "Homework1 - Yashaswini",
    "section": "Python Tool",
    "text": "Python Tool\nThe python tool was run on the a new add.json file based off of my benchmark from part 1\nimport json\n\ndef load_bril_program(filename):\n    \"\"\"Load the Bril program from a JSON file.\"\"\"\n    with open(filename, 'r') as file:\n        return json.load(file)\n\ndef count_add_instructions(program):\n    \"\"\"Count the number of 'add' instructions in the Bril program.\"\"\"\n    add_count = 0\n    for func in program['functions']:\n        for instr in func['instrs']:\n            if instr.get('op') == 'add':\n                add_count += 1\n    return add_count\n\ndef add_print_before_jumps(program):\n    \"\"\"Insert a 'print' instruction before every 'jmp' instruction.\"\"\"\n    for func in program['functions']:\n        new_instrs = []\n        for instr in func['instrs']:\n            if instr.get('op') == 'jmp':\n                # Add a print instruction before the jump\n                new_instr = {\n                    \"op\": \"print\",\n                    \"args\": [\"jmp\"]\n                }\n                new_instrs.append(new_instr)\n            new_instrs.append(instr)\n        func['instrs'] = new_instrs\n    return program\n\ndef save_bril_program(program, filename):\n    \"\"\"Save the transformed Bril program back to a JSON file.\"\"\"\n    with open(filename, 'w') as file:\n        json.dump(program, file, indent=2)\n\n# Main function to load, transform, and save the Bril program\ndef main():\n    # Load the Bril program from a file\n    bril_program = load_bril_program('add.json')\n\n    # Count the number of 'add' instructions\n    add_count = count_add_instructions(bril_program)\n    print(f\"Number of 'add' instructions: {add_count}\")\n\n    # Add 'print' before 'jmp' instructions\n    transformed_program = add_print_before_jumps(bril_program)\n\n    # Save the transformed Bril program to a file\n    save_bril_program(transformed_program, 'output_add.json')\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework1 - Yashaswini"
    ]
  },
  {
    "objectID": "blogs/yashaswini/09-20-2024-HW1-YashaswiniMakaram.html#output",
    "href": "blogs/yashaswini/09-20-2024-HW1-YashaswiniMakaram.html#output",
    "title": "Homework1 - Yashaswini",
    "section": "Output",
    "text": "Output\nthe program found 2 add instrucitons and produced and output file shown below\n{\n  \"functions\": [\n    {\n      \"instrs\": [\n        {\n          \"dest\": \"c5\",\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"value\": 5\n        },\n        {\n          \"args\": [\n            \"c5\"\n          ],\n          \"dest\": \"v0\",\n          \"op\": \"alloc\",\n          \"type\": {\n            \"ptr\": \"int\"\n          }\n        },\n        {\n          \"dest\": \"j\",\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"value\": 1\n        },\n        {\n          \"dest\": \"c1\",\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"value\": 1\n        },\n        {\n          \"args\": [\n            \"v0\",\n            \"c1\"\n          ],\n          \"op\": \"store\"\n        },\n        {\n          \"dest\": \"c2\",\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"value\": 2\n        },\n        {\n          \"args\": [\n            \"v0\",\n            \"c2\"\n          ],\n          \"op\": \"store\"\n        },\n        {\n          \"dest\": \"c3\",\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"value\": 3\n        },\n        {\n          \"args\": [\n            \"v0\",\n            \"c2\"\n          ],\n          \"op\": \"store\"\n        },\n        {\n          \"dest\": \"c4\",\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"value\": 4\n        },\n        {\n          \"args\": [\n            \"v0\",\n            \"c2\"\n          ],\n          \"op\": \"store\"\n        },\n        {\n          \"dest\": \"c5\",\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"value\": 5\n        },\n        {\n          \"args\": [\n            \"v0\",\n            \"c2\"\n          ],\n          \"op\": \"store\"\n        },\n        {\n          \"dest\": \"sum\",\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"value\": 0\n        },\n        {\n          \"dest\": \"i\",\n          \"op\": \"const\",\n          \"type\": \"int\",\n          \"value\": 0\n        },\n        {\n          \"label\": \"loop\"\n        },\n        {\n          \"args\": [\n            \"i\",\n            \"c5\"\n          ],\n          \"dest\": \"cond\",\n          \"op\": \"lt\",\n          \"type\": \"bool\"\n        },\n        {\n          \"args\": [\n            \"cond\"\n          ],\n          \"labels\": [\n            \"body\",\n            \"end\"\n          ],\n          \"op\": \"br\"\n        },\n        {\n          \"label\": \"body\"\n        },\n        {\n          \"args\": [\n            \"v0\"\n          ],\n          \"dest\": \"current\",\n          \"op\": \"load\",\n          \"type\": \"int\"\n        },\n        {\n          \"args\": [\n            \"sum\",\n            \"current\"\n          ],\n          \"dest\": \"sum\",\n          \"op\": \"add\",\n          \"type\": \"int\"\n        },\n        {\n          \"args\": [\n            \"i\",\n            \"j\"\n          ],\n          \"dest\": \"i\",\n          \"op\": \"add\",\n          \"type\": \"int\"\n        },\n        {\n          \"op\": \"print\",\n          \"args\": [\n            \"jmp\"\n          ]\n        },\n        {\n          \"labels\": [\n            \"loop\"\n          ],\n          \"op\": \"jmp\"\n        },\n        {\n          \"label\": \"end\"\n        },\n        {\n          \"args\": [\n            \"sum\"\n          ],\n          \"op\": \"print\"\n        },\n        {\n          \"args\": [\n            \"v0\"\n          ],\n          \"op\": \"free\"\n        },\n        {\n          \"op\": \"ret\"\n        }\n      ],\n      \"name\": \"main\"\n    }\n  ]\n}\n##Testing\na good way to otest this is using small scripts where the number of adds is known or can be calculates.\nI could also develop a tool to reverse the addition of prints before jmp and compare the output to the original code using turnt.",
    "crumbs": [
      "EECS 7398",
      "Blogs",
      "Yashaswini",
      "Homework1 - Yashaswini"
    ]
  },
  {
    "objectID": "Class_Overview/What_to_do.html",
    "href": "Class_Overview/What_to_do.html",
    "title": "How to submit assignments",
    "section": "",
    "text": "Assignments get submitted as blog postings. In setting up the class web site I used quarto which lets you write a blog post in markdown (no messy html required). I recommend you use it as well. quarto converts markdown files to html and revealjs (for slides)",
    "crumbs": [
      "EECS 7398",
      "How to submit assignments"
    ]
  },
  {
    "objectID": "Class_Overview/What_to_do.html#mechanics-of-writing-a-blog",
    "href": "Class_Overview/What_to_do.html#mechanics-of-writing-a-blog",
    "title": "How to submit assignments",
    "section": "",
    "text": "Assignments get submitted as blog postings. In setting up the class web site I used quarto which lets you write a blog post in markdown (no messy html required). I recommend you use it as well. quarto converts markdown files to html and revealjs (for slides)",
    "crumbs": [
      "EECS 7398",
      "How to submit assignments"
    ]
  },
  {
    "objectID": "Class_Overview/What_to_do.html#submitting-via-pull-requests",
    "href": "Class_Overview/What_to_do.html#submitting-via-pull-requests",
    "title": "How to submit assignments",
    "section": "Submitting via pull requests",
    "text": "Submitting via pull requests\nTo add a blog post (which you must do for homework, discussion leading, and project reports), use a pull request.\nYou’ll want to create a text file in the blog directory with your new post. Use a filename like YYYY-MM-DD-title.qmd, where the date is the discussion day or the project deadline and the title is up to you.\nEach quarto file starts with some meta data. For example the sample file has\n---\nformat:\n\n\n\n\n---\nauthor: norm\nformat:\n html: default\ntitle: sample blog\n---\n\n\n\n\n\nThe rest of the text file is the Markdown text of your blog post.\nIf you want to use math in your blog post, you can use latex dollar signs like $\\pi$ for \\(\\pi\\) for inline math and $$ e^{i\\pi} + 1 = 0 $$ for \\[ e^{i\\pi} + 1 = 0 \\].\nTo include images or other resources in your post, make your post into a directory. That is, make a new directory called YYYY-MM-DD-title inside blog. Then, put your text in a file called index.qmd inside that. Put your images in the same directory and refer to them with relative paths. See the QUARTO docs on for more details.\nYou can preview your writing with any Markdown renderer. To see what it will look like when published, install quarto and type quarto render to preview the entire site. Visual code has a really nice quarto extension that can preview pages.",
    "crumbs": [
      "EECS 7398",
      "How to submit assignments"
    ]
  },
  {
    "objectID": "Class_Overview/What_to_do.html#homework",
    "href": "Class_Overview/What_to_do.html#homework",
    "title": "How to submit assignments",
    "section": "Homework",
    "text": "Homework\nTo reinforce the specific compiler techniques we cover in class, you will implement them on your own. In lessons, we will discuss the high-level ideas and provide pseudo-code; your task is to translate these into working code and collect empirical evidence to demonstrate their effectiveness. Completing these implementations will reveal practical challenges that are not apparent from a high-level overview.\nTesting your implementation is crucial. Your goal is to provide convincing evidence that your implementation performs as intended. For instance, an optimization should generally make programs faster without causing any errors. While formal proofs of these properties are likely out of scope, you will need to find alternative ways to gather evidence. Avoid relying solely on existing test cases in the Bril repository, as they are typically insufficient. instead, consider using all the benchmarks available in the repo.\nYou may work individually or in groups of 2–3 students. Upon completing an implementation, follow these steps:\n\nConsider putting all your code online in an open-source repository, such as GitHub (optional but recommended). Create a fork of the class repository if desired.\nSubmit the assignment on Canvas by providing a text file with a URL to your open-source implementation. If you prefer not to open-source your code, you can upload the code itself.\nWrite a brief post in the lesson’s associated GitHub Discussions thread, covering the following topics (one paragraph each is sufficient):\n\nSummarize what you did.\nExplain how you tested your implementation. What test inputs did you use? Do you have any quantitative results?\nDescribe the hardest part of the task and how you addressed this challenge.\n\n\nEnsure all implementation tasks are your own work or done with your group. Although sample implementations for many tasks are available in the GitHub repository, you are not allowed to use this code. Similarly, you may not use implementations open-sourced by past students. I recommend not looking at these implementations while working on your tasks to ensure you genuinely learn the material. However, if you absolutely need to refer to them, you are responsible for managing your own learning process.",
    "crumbs": [
      "EECS 7398",
      "How to submit assignments"
    ]
  },
  {
    "objectID": "Class_Overview/What_to_do.html#paper-reading-discussion",
    "href": "Class_Overview/What_to_do.html#paper-reading-discussion",
    "title": "How to submit assignments",
    "section": "Paper Reading & Discussion",
    "text": "Paper Reading & Discussion\nPaper discussions are on GitHub Discussions.\nAnother part of this course involves reading and discussing research papers. For each paper (see the schedule), everyone will participate in the discussion in two ways: asynchronously on GitHub Discussions threads before class, and synchronously in class. For every paper, there will be a Discussions topic; post at least one message with your thoughts on the paper before the class discussion. Your comment doesn’t need to be long—just a couple of sentences is fine. You can also respond to others’ thoughts on the thread.\nFor some papers, you will be the discussion leader. Leaders have three extra responsibilities: monitoring and replying to the asynchronous discussion, moderating and guiding the in-class discussion, and synthesizing ideas into a blog post afterward.\nLeader Responsibilities\nAt least a week before the discussion day:\n1) Create a GitHub Discussions thread in the Reading category for your topic.\nDuring the lead-up to the discussion day:\n1) Monitor the GitHub Discussions thread for your topic. Answer questions and offer additional insights as needed.\n1) Collect a list of questions for the in-class discussion. You can create your own or select the best from the online discussion.\nOn the discussion day:\nModerate the discussion. Provide enough background to get to the discussion questions and facilitate the conversation.\nDue one week after the discussion day:\n\nWrite a post about the paper for our course blog. The post should include:\n\nBackground information necessary to understand the paper.\nA detailed summary of the main contributions.\nCritical analysis of the merits and shortcomings of the work.\nDiscussion of the paper’s role in history and its connections to the current computing landscape.\n\n\nIncorporate the best ideas from the online and in-class discussions. You can present your own opinions, the class consensus, or both.\n\n\nWriting the Blog Post\nWhile summarizing the paper, avoid letting direct summary dominate your post. Keep the technical explanation to about a quarter of the length. Prioritize breadth over depth in your summary, and highlight specific contributions instead of covering the entire paper.\nFocus most of your writing on your own commentary: context, criticism, and discussion. Choose a title for your blog post that reflects the main point you want to make about the paper, rather than just the paper’s title.\nFor inspiration, check out previous cs6120 blog posts. However, avoid reading posts about your paper, if they exist.\nPublishing\nPublish the post to the course GitHub repository by opening a pull request. Once your PR is open, announce it on the appropriate Discussions thread to let others know.",
    "crumbs": [
      "EECS 7398",
      "How to submit assignments"
    ]
  },
  {
    "objectID": "Class_Overview/What_to_do.html#project-proposal",
    "href": "Class_Overview/What_to_do.html#project-proposal",
    "title": "How to submit assignments",
    "section": "project Proposal",
    "text": "project Proposal\nThe first deadline is the project proposal. Open a GitHub issue answering these three questions:\nWhat will you do? How will you do it? How will you empirically measure success?\nYou should also list the GitHub usernames of everyone in the group. After you send the PR, submit its URL to the “Project Proposal” assignment on canvas.\nThe instructor will have feedback on how to approach your project.\nImplementation\nThe main phase, of course, is implementing the thing you said you would implement. I recommend you keep a “lab notebook” to log your thoughts, attempts, and frustrations—this will come in handy for the report you’ll write about the project.\nI strongly recommend that you develop your code as an open-source project. Use a publicly-visible version control repository on a host like GitHub, and include an open source license. When you create your repository, comment on your proposal GitHub issue with a link. (If you have a specific objection to open-sourcing your code, that’s OK—include a description of how you’ll share your code privately with me.)\nEvaluation\nA major part of your project is an empirical evaluation. To design your evaluation strategy, you will need to consider at least these things:\nWhere will you get the input code you’ll use in your evaluation? How will you check the correctness of your implementation? If you’ve implemented an optimization, for example, “correctness” means that the transformed programs behave the same way as the original programs. How will you measure the benefit (in performance, energy, complexity, etc.) of your implementation?\nHow will you present the data you collect from your empirical evaluation? Other questions may be relevant depending on the project you choose. Consider the SIGPLAN empirical evaluation guidelines when you design your methodology.",
    "crumbs": [
      "EECS 7398",
      "How to submit assignments"
    ]
  },
  {
    "objectID": "Class_Overview/What_to_do.html#project-experience-report",
    "href": "Class_Overview/What_to_do.html#project-experience-report",
    "title": "How to submit assignments",
    "section": "project Experience Report",
    "text": "project Experience Report\nFor the main project deadline, you will write up the project’s outcomes in the form of a post on the course blog. Your writeup should answer these questions in excruciating, exhaustive detail:\nWhat was the goal? What did you do? (Include both the design and the implementation.) What were the hardest parts to get right? Were you successful? (Report rigorously on your empirical evaluation.) As with paper discussions, you can optionally include a video to go along with your blog post.\nTo submit your report, open a pull request in the course’s GitHub repository to add your post to the blog. In your PR description, please include “closes #N” where N is the issue number for your proposal",
    "crumbs": [
      "EECS 7398",
      "How to submit assignments"
    ]
  },
  {
    "objectID": "ai.html",
    "href": "ai.html",
    "title": "machine learning",
    "section": "",
    "text": "Start with compiler data structures\nSSA, DDG, CFG\nHuman expert maps these to a feature vector\n\nnumber of instructions\nmean dependence depth\nbranch count\nloop next\n…\ntrip count\n\ncollect many examples of programs - calculate feature vector"
  },
  {
    "objectID": "ai.html#pre---ai-methods",
    "href": "ai.html#pre---ai-methods",
    "title": "machine learning",
    "section": "",
    "text": "Start with compiler data structures\nSSA, DDG, CFG\nHuman expert maps these to a feature vector\n\nnumber of instructions\nmean dependence depth\nbranch count\nloop next\n…\ntrip count\n\ncollect many examples of programs - calculate feature vector"
  },
  {
    "objectID": "notebooks/representation.html",
    "href": "notebooks/representation.html",
    "title": "Representation",
    "section": "",
    "text": "The representation of a program - is what we read in and read out when transforming a program. What kind of properties make a good representation?\nOne possible representation is called concrete syntax form Programs are text - surface syntax- just what you would type into an editor.\n\nvalue = 8\nresult = 1\nfor i in range(value):\n  result = result + i\nprint(result)\n\n29\n\n\nWhat is good and what is bad about this representation?\nWhat is the level of abstraction? How do you understand the semantics.\nForm 2 - Abstract syntax form\nTree structure - Nodes are parts of the program, edges show how they are connected. We can write this as a list or a graph\n\n\nFunctionDef(\n    name='pgm',\n    args=arguments(\n        posonlyargs=[],\n        args=[],\n        kwonlyargs=[],\n        kw_defaults=[],\n        defaults=[]),\n    body=[\n        Assign(\n            targets=[\n                Name(id='value', ctx=Store())],\n            value=Constant(value=8)),\n        Assign(\n            targets=[\n                Name(id='result', ctx=Store())],\n            value=Constant(value=1)),\n        For(\n            target=Name(id='i', ctx=Store()),\n            iter=Call(\n                func=Name(id='range', ctx=Load()),\n                args=[\n                    Name(id='value', ctx=Load())],\n                keywords=[]),\n            body=[\n                Assign(\n                    targets=[\n                        Name(id='result', ctx=Store())],\n                    value=BinOp(\n                        left=Name(id='result', ctx=Load()),\n                        op=Mult(),\n                        right=Name(id='i', ctx=Load())))],\n            orelse=[]),\n        Expr(\n            value=Call(\n                func=Name(id='print', ctx=Load()),\n                args=[\n                    Name(id='result', ctx=Load())],\n                keywords=[]))],\n    decorator_list=[])\n\n\n\ndot_dia\n\n\n\n\n\n\n\n\nAST tree representation An AST is a tree structure, nodes like if, test, body, assign Each node is one concept from the program\nRecursive function can walk over the tree, one chunk of code for each node.\n\nGood - each type of node is different, making special cases are easy\nBad - each type of node is different so analysis has to know about every type, making general cases hard\n\nThis is the classic way to write an interpreter. Simple (non optimizing) compilers often use this format.\n\n\nPrograms are lists of instructions. Like an assembly instructions. Same sort of representation as LLVM.\n\n\n    let value = 8\n    let result = 1\n    for (let i = 0;i &lt; value;i = i+ 1)\n    {\n        result = result * i\n    }\n    console.log(result )\n\n\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}\n\n\n\n\nos.system('ts2bril images/toy.ts | bril2txt')\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}\n\n\n0\n\n\nLooks like assembly but no limit on registers, no condition codes. fully typed, no complex addressing modes.\nsyntax-\nDeclare functions, labels, instructions\ninstruction:\n1) variable type = opcode arguments 2) opcode list of arguments\n\n\n\nWhat is the abstract syntax form for this?\n\n\n\n\n\nRepresentation is a directed graph. Nodes are instructions, edges indicate possible flow of control, one entry and one exit node.\nHere is a simple program:\n    @main {\n        v: int = const 5;\n        print v;\n    }\n\n\n\n\n\nflowchart LR\n  A[const] --&gt; B[print]\n\n\n\n\n\n\na second example\n    @main {\n        v: int = const 4;\n        jmp  .somewhere;\n        v: int = const 2;\n        .somewhere;\n        print v;\n    }\nWhat does the control flow graph look like?\n\n\n\n\n\nflowchart LR\n  A[const 4] --&gt; B[jmp]\n  B --&gt; C[print]\n  D[const 2] --&gt; C\n\n\n\n\n\n\nnotice label does not produce a node\nEasy to see a dead instruction.\nThird example:\n    @main {\n        v: int = const 4;\n        b: bool = const false;\n        br b .there .here;\n    .here:\n        v: int = const 2;\n    .there;\n        print v;\n    }\n\n\n\n\n\nflowchart LR\n  A[v: int const 4] --&gt; B[b: bool const false]\n  B --&gt; C[br b .there, .false]\n  C --&gt; D[v: const 2]\n  C --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\nwhich is the true and which is the false, could mark the edges or use a convention\nWhich is the entry, which is the exit?\nThere is a long chain of instructions entered at the top, exit at the bottom, no branches inside.\nBasic blocks (cfg form 2) 1) nodes can be a sequence of instructions. 1) jumps and branches can only be at the end of a sequence 1) only label has to be at the start 1) every instruction in the sequence executes the same number of times\n\n\n\n\n\nflowchart LR\n  A[v: int const 4\\nb : bool\\n br ] \n  A --&gt; D[v: const 2]\n  A --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\nAs we construct basic blocks, we can add instructions up till something that ends the block (terminator)\nOption: do all blocks end in a terminator or not?\ngiven a block b, the predecessors of b are the blocks b_in where there is an edge bin-&gt;b. And the successors of B are the b_out where b-&gt;b_out is an edge\n\n\n\n\n\njust find all the basic blocks\nadd the control flow edges\n\npsuedo code\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\nstep 2 we need a map from labels to basic blocks\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\n    \n\nfor block in blocks:\n   last = block[-1]\n   if last is a jmp (one successor)\n      add edge from block to last.dest \n   else if last is a br (two successors)\n      add two edges from block to last.true, last.false \n   else  fall through \n      add edge to next block (if it exists)\n\nwith open(\"images/add.json\", 'r') as f:\n  bril_program = f.read()\n  print(bril_program)\n\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v0\", \"value\": 1 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v1\", \"value\": 2 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"v2\",\n          \"args\": [\"v0\", \"v1\"] },\n        { \"op\": \"print\", \"args\": [\"v2\"] }\n      ],\n      \"args\": []\n    }\n  ]\n}"
  },
  {
    "objectID": "notebooks/representation.html#a-more-regular-representation",
    "href": "notebooks/representation.html#a-more-regular-representation",
    "title": "Representation",
    "section": "",
    "text": "Programs are lists of instructions. Like an assembly instructions. Same sort of representation as LLVM.\n\n\n    let value = 8\n    let result = 1\n    for (let i = 0;i &lt; value;i = i+ 1)\n    {\n        result = result * i\n    }\n    console.log(result )\n\n\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}\n\n\n\n\nos.system('ts2bril images/toy.ts | bril2txt')\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}\n\n\n0\n\n\nLooks like assembly but no limit on registers, no condition codes. fully typed, no complex addressing modes.\nsyntax-\nDeclare functions, labels, instructions\ninstruction:\n1) variable type = opcode arguments 2) opcode list of arguments"
  },
  {
    "objectID": "notebooks/representation.html#what-is-good-and-what-is-about-this-reorientation",
    "href": "notebooks/representation.html#what-is-good-and-what-is-about-this-reorientation",
    "title": "Representation",
    "section": "",
    "text": "What is the abstract syntax form for this?"
  },
  {
    "objectID": "notebooks/representation.html#extract-info-from-this-repreentation.",
    "href": "notebooks/representation.html#extract-info-from-this-repreentation.",
    "title": "Representation",
    "section": "",
    "text": "Representation is a directed graph. Nodes are instructions, edges indicate possible flow of control, one entry and one exit node.\nHere is a simple program:\n    @main {\n        v: int = const 5;\n        print v;\n    }\n\n\n\n\n\nflowchart LR\n  A[const] --&gt; B[print]\n\n\n\n\n\n\na second example\n    @main {\n        v: int = const 4;\n        jmp  .somewhere;\n        v: int = const 2;\n        .somewhere;\n        print v;\n    }\nWhat does the control flow graph look like?\n\n\n\n\n\nflowchart LR\n  A[const 4] --&gt; B[jmp]\n  B --&gt; C[print]\n  D[const 2] --&gt; C\n\n\n\n\n\n\nnotice label does not produce a node\nEasy to see a dead instruction.\nThird example:\n    @main {\n        v: int = const 4;\n        b: bool = const false;\n        br b .there .here;\n    .here:\n        v: int = const 2;\n    .there;\n        print v;\n    }\n\n\n\n\n\nflowchart LR\n  A[v: int const 4] --&gt; B[b: bool const false]\n  B --&gt; C[br b .there, .false]\n  C --&gt; D[v: const 2]\n  C --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\nwhich is the true and which is the false, could mark the edges or use a convention\nWhich is the entry, which is the exit?\nThere is a long chain of instructions entered at the top, exit at the bottom, no branches inside.\nBasic blocks (cfg form 2) 1) nodes can be a sequence of instructions. 1) jumps and branches can only be at the end of a sequence 1) only label has to be at the start 1) every instruction in the sequence executes the same number of times\n\n\n\n\n\nflowchart LR\n  A[v: int const 4\\nb : bool\\n br ] \n  A --&gt; D[v: const 2]\n  A --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\nAs we construct basic blocks, we can add instructions up till something that ends the block (terminator)\nOption: do all blocks end in a terminator or not?\ngiven a block b, the predecessors of b are the blocks b_in where there is an edge bin-&gt;b. And the successors of B are the b_out where b-&gt;b_out is an edge"
  },
  {
    "objectID": "notebooks/representation.html#what-is-an-algorithm-that-forms-a-cfg",
    "href": "notebooks/representation.html#what-is-an-algorithm-that-forms-a-cfg",
    "title": "Representation",
    "section": "",
    "text": "just find all the basic blocks\nadd the control flow edges\n\npsuedo code\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\nstep 2 we need a map from labels to basic blocks\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\n    \n\nfor block in blocks:\n   last = block[-1]\n   if last is a jmp (one successor)\n      add edge from block to last.dest \n   else if last is a br (two successors)\n      add two edges from block to last.true, last.false \n   else  fall through \n      add edge to next block (if it exists)\n\nwith open(\"images/add.json\", 'r') as f:\n  bril_program = f.read()\n  print(bril_program)\n\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v0\", \"value\": 1 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v1\", \"value\": 2 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"v2\",\n          \"args\": [\"v0\", \"v1\"] },\n        { \"op\": \"print\", \"args\": [\"v2\"] }\n      ],\n      \"args\": []\n    }\n  ]\n}"
  },
  {
    "objectID": "notebooks/possible_papers.html",
    "href": "notebooks/possible_papers.html",
    "title": "possible papers",
    "section": "",
    "text": "Glow: Graph Lowering Compiler Techniques for Neural Networks Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron, Summer Deng, Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman Levenstein, Jack Montgomery, Bert Maher, Satish Nadathur, Jakob Olesen, Jongsoo Park, Artem Rakhov, Misha Smelyanski chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/1805.00907\nA Deep Learning Based Cost Model for Automatic Code Optimization. Riyadh Baghdadi, Massinissa Merouani, Mohamed-Hicham Leghettas, Kamel Abdous, Taha Arbaoui, Karima Benatchba, Saman Amarasinghe. Proceedings of the Fourth Conference on Machine Learning and Systems (MLSys).\nhttps://dl.acm.org/doi/abs/10.1145/3213846.3213848?casa_token=cbgdY_Wgz9kAAAAA:IMKfnKAYKl3t9wXFen_yauFHY__vyUHcqSgjENz7RB2QEGeTC1L70FEC5vM9FnKBWdAiL6tw1uC4 Compiler fuzzing through deep learning\n“Effective Superword Level Parallelism for Multimedia Extension Architectures” by Samuel Larsen and Saman Amarasinghe (2000)\nEnergy-Aware Tile Size Selection for Affine Programs on GPUs, M. Jayaweera, M. Kong, Y. Wang, D. Kaeli, Pre-print, Artifact\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/02aa_reps.html",
    "href": "notebooks/02aa_reps.html",
    "title": "2a Representation",
    "section": "",
    "text": "The representation of a program - is what we read in and read out when transforming a program. What kind of properties make a good representation?\nOne possible representation is called concrete syntax form Programs are text - surface syntax- just what you would type into an editor.\n\nvalue = 8\nresult = 1\nfor i in range(value):\n  result = result + i\nprint(result)\n\n29\n\n\nWhat is good and what is bad about this representation?\nWhat is the level of abstraction? How do you understand the semantics.\nForm 2 - Abstract syntax form\nTree structure - Nodes are parts of the program, edges show how they are connected. We can write this as a list or a graph\n\n\nFunctionDef(\n    name='pgm',\n    args=arguments(\n        posonlyargs=[],\n        args=[],\n        kwonlyargs=[],\n        kw_defaults=[],\n        defaults=[]),\n    body=[\n        Assign(\n            targets=[\n                Name(id='value', ctx=Store())],\n            value=Constant(value=8)),\n        Assign(\n            targets=[\n                Name(id='result', ctx=Store())],\n            value=Constant(value=1)),\n        For(\n            target=Name(id='i', ctx=Store()),\n            iter=Call(\n                func=Name(id='range', ctx=Load()),\n                args=[\n                    Name(id='value', ctx=Load())],\n                keywords=[]),\n            body=[\n                Assign(\n                    targets=[\n                        Name(id='result', ctx=Store())],\n                    value=BinOp(\n                        left=Name(id='result', ctx=Load()),\n                        op=Mult(),\n                        right=Name(id='i', ctx=Load())))],\n            orelse=[]),\n        Expr(\n            value=Call(\n                func=Name(id='print', ctx=Load()),\n                args=[\n                    Name(id='result', ctx=Load())],\n                keywords=[]))],\n    decorator_list=[])\n\n\n\ndot_dia\n\n\n\n\n\n\n\n\nAST tree representation An AST is a tree structure, nodes like if, test, body, assign Each node is one concept from the program\nRecursive function can walk over the tree, one chunk of code for each node.\n\nGood - each type of node is different, making special cases are easy\nBad - each type of node is different so analysis has to know about every type, making general cases hard\n\nThis is the classic way to write an interpreter. Simple (non optimizing) compilers often use this format.\n\n\nPrograms are lists of instructions. Like an assembly instructions. Same sort of representation as LLVM.\n\n\n    let value = 8\n    let result = 1\n    for (let i = 0;i &lt; value;i = i+ 1)\n    {\n        result = result * i\n    }\n    console.log(result )\n\n\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}\n\n\n\n\nos.system('ts2bril images/toy.ts | bril2txt')\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}\n\n\n0\n\n\nLooks like assembly but no limit on registers, no condition codes. fully typed, no complex addressing modes.\nsyntax-\nDeclare functions, labels, instructions\ninstruction:\n1) variable type = opcode arguments 2) opcode list of arguments\n\n\n\nWhat is the abstract syntax form for this?\n\n\n\n\n\nRepresentation is a directed graph. Nodes are instructions, edges indicate possible flow of control, one entry and one exit node.\nHere is a simple program:\n    @main {\n        v: int = const 5;\n        print v;\n    }\n\n\n\n\n\nflowchart LR\n  A[const] --&gt; B[print]\n\n\n\n\n\n\na second example\n    @main {\n        v: int = const 4;\n        jmp  .somewhere;\n        v: int = const 2;\n        .somewhere;\n        print v;\n    }\nWhat does the control flow graph look like?\n\n\n\n\n\nflowchart LR\n  A[const 4] --&gt; B[jmp]\n  B --&gt; C[print]\n  D[const 2] --&gt; C\n\n\n\n\n\n\nnotice label does not produce a node\nEasy to see a dead instruction.\nThird example:\n    @main {\n        v: int = const 4;\n        b: bool = const false;\n        br b .there .here;\n    .here:\n        v: int = const 2;\n    .there;\n        print v;\n    }\n\n\n\n\n\nflowchart LR\n  A[v: int const 4] --&gt; B[b: bool const false]\n  B --&gt; C[br b .there, .false]\n  C --&gt; D[v: const 2]\n  C --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\nwhich is the true and which is the false, could mark the edges or use a convention\nWhich is the entry, which is the exit?\nThere is a long chain of instructions entered at the top, exit at the bottom, no branches inside.\nBasic blocks (cfg form 2) 1) nodes can be a sequence of instructions. 1) jumps and branches can only be at the end of a sequence 1) only label has to be at the start 1) every instruction in the sequence executes the same number of times\n\n\n\n\n\nflowchart LR\n  A[v: int const 4\\nb : bool\\n br ] \n  A --&gt; D[v: const 2]\n  A --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\nAs we construct basic blocks, we can add instructions up till something that ends the block (terminator)\nOption: do all blocks end in a terminator or not?\ngiven a block b, the predecessors of b are the blocks b_in where there is an edge bin-&gt;b. And the successors of B are the b_out where b-&gt;b_out is an edge\n\n\n\n\n\njust find all the basic blocks\nadd the control flow edges\n\npsuedo code\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\nstep 2 we need a map from labels to basic blocks\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\n    \n\nfor block in blocks:\n   last = block[-1]\n   if last is a jmp (one successor)\n      add edge from block to last.dest \n   else if last is a br (two successors)\n      add two edges from block to last.true, last.false \n   else  fall through \n      add edge to next block (if it exists)\n\nwith open(\"images/add.json\", 'r') as f:\n  bril_program = f.read()\n  print(bril_program)\n\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v0\", \"value\": 1 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v1\", \"value\": 2 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"v2\",\n          \"args\": [\"v0\", \"v1\"] },\n        { \"op\": \"print\", \"args\": [\"v2\"] }\n      ],\n      \"args\": []\n    }\n  ]\n}"
  },
  {
    "objectID": "notebooks/02aa_reps.html#a-more-regular-representation",
    "href": "notebooks/02aa_reps.html#a-more-regular-representation",
    "title": "2a Representation",
    "section": "",
    "text": "Programs are lists of instructions. Like an assembly instructions. Same sort of representation as LLVM.\n\n\n    let value = 8\n    let result = 1\n    for (let i = 0;i &lt; value;i = i+ 1)\n    {\n        result = result * i\n    }\n    console.log(result )\n\n\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}\n\n\n\n\nos.system('ts2bril images/toy.ts | bril2txt')\n\n@main {\n  v0: float = const 8;\n  value: float = id v0;\n  v1: float = const 1;\n  result: float = id v1;\n  v3: float = const 0;\n  i: float = id v3;\n.for.cond.2:\n  v4: float = id i;\n  v5: float = id value;\n  v6: bool = flt v4 v5;\n  br v6 .for.body.2 .for.end.2;\n.for.body.2:\n  v7: float = id result;\n  v8: float = id i;\n  v9: float = fmul v7 v8;\n  result: float = id v9;\n  v10: float = id i;\n  v11: float = const 1;\n  v12: float = fadd v10 v11;\n  i: float = id v12;\n  jmp .for.cond.2;\n.for.end.2:\n  v13: float = id result;\n  print v13;\n  v14: int = const 0;\n}\n\n\n0\n\n\nLooks like assembly but no limit on registers, no condition codes. fully typed, no complex addressing modes.\nsyntax-\nDeclare functions, labels, instructions\ninstruction:\n1) variable type = opcode arguments 2) opcode list of arguments"
  },
  {
    "objectID": "notebooks/02aa_reps.html#what-is-good-and-what-is-about-this-reorientation",
    "href": "notebooks/02aa_reps.html#what-is-good-and-what-is-about-this-reorientation",
    "title": "2a Representation",
    "section": "",
    "text": "What is the abstract syntax form for this?"
  },
  {
    "objectID": "notebooks/02aa_reps.html#extract-info-from-this-repreentation.",
    "href": "notebooks/02aa_reps.html#extract-info-from-this-repreentation.",
    "title": "2a Representation",
    "section": "",
    "text": "Representation is a directed graph. Nodes are instructions, edges indicate possible flow of control, one entry and one exit node.\nHere is a simple program:\n    @main {\n        v: int = const 5;\n        print v;\n    }\n\n\n\n\n\nflowchart LR\n  A[const] --&gt; B[print]\n\n\n\n\n\n\na second example\n    @main {\n        v: int = const 4;\n        jmp  .somewhere;\n        v: int = const 2;\n        .somewhere;\n        print v;\n    }\nWhat does the control flow graph look like?\n\n\n\n\n\nflowchart LR\n  A[const 4] --&gt; B[jmp]\n  B --&gt; C[print]\n  D[const 2] --&gt; C\n\n\n\n\n\n\nnotice label does not produce a node\nEasy to see a dead instruction.\nThird example:\n    @main {\n        v: int = const 4;\n        b: bool = const false;\n        br b .there .here;\n    .here:\n        v: int = const 2;\n    .there;\n        print v;\n    }\n\n\n\n\n\nflowchart LR\n  A[v: int const 4] --&gt; B[b: bool const false]\n  B --&gt; C[br b .there, .false]\n  C --&gt; D[v: const 2]\n  C --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\nwhich is the true and which is the false, could mark the edges or use a convention\nWhich is the entry, which is the exit?\nThere is a long chain of instructions entered at the top, exit at the bottom, no branches inside.\nBasic blocks (cfg form 2) 1) nodes can be a sequence of instructions. 1) jumps and branches can only be at the end of a sequence 1) only label has to be at the start 1) every instruction in the sequence executes the same number of times\n\n\n\n\n\nflowchart LR\n  A[v: int const 4\\nb : bool\\n br ] \n  A --&gt; D[v: const 2]\n  A --&gt; E[print v]\n  D --&gt; E\n\n\n\n\n\n\nAs we construct basic blocks, we can add instructions up till something that ends the block (terminator)\nOption: do all blocks end in a terminator or not?\ngiven a block b, the predecessors of b are the blocks b_in where there is an edge bin-&gt;b. And the successors of B are the b_out where b-&gt;b_out is an edge"
  },
  {
    "objectID": "notebooks/02aa_reps.html#what-is-an-algorithm-that-forms-a-cfg",
    "href": "notebooks/02aa_reps.html#what-is-an-algorithm-that-forms-a-cfg",
    "title": "2a Representation",
    "section": "",
    "text": "just find all the basic blocks\nadd the control flow edges\n\npsuedo code\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\nstep 2 we need a map from labels to basic blocks\n\nin: instructions - list of instructions\nout blocks - list of lists of instructions\n\ncurrent_block = []\nfor i in instructions:\n    if i is not a label:\n       block.append(i)\n    if i is a label or terminator:\n        blocks.append(current_block)\n        current_block = []\n    \n\nfor block in blocks:\n   last = block[-1]\n   if last is a jmp (one successor)\n      add edge from block to last.dest \n   else if last is a br (two successors)\n      add two edges from block to last.true, last.false \n   else  fall through \n      add edge to next block (if it exists)\n\nwith open(\"images/add.json\", 'r') as f:\n  bril_program = f.read()\n  print(bril_program)\n\n{\n  \"functions\": [\n    {\n      \"name\": \"main\",\n      \"instrs\": [\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v0\", \"value\": 1 },\n        { \"op\": \"const\", \"type\": \"int\", \"dest\": \"v1\", \"value\": 2 },\n        { \"op\": \"add\", \"type\": \"int\", \"dest\": \"v2\",\n          \"args\": [\"v0\", \"v1\"] },\n        { \"op\": \"print\", \"args\": [\"v2\"] }\n      ],\n      \"args\": []\n    }\n  ]\n}"
  },
  {
    "objectID": "homework/4_hw.html",
    "href": "homework/4_hw.html",
    "title": "homework - dominance",
    "section": "",
    "text": "Implement the dominance utilities-\n\nFind dominators for a function,\nconstruct the dominance tree,\ncompute the dominance frontier.\n\nDevise a way to test your implementations. Can you find a way to confirm that block A dominates blockB. When you code this,\nremember that checking their output could use slow naive algorithms.\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Homework",
      "homework -  dominance"
    ]
  },
  {
    "objectID": "homework/5_hw.html",
    "href": "homework/5_hw.html",
    "title": "Homework 5 ssa",
    "section": "",
    "text": "Implement the into SSA and out of SSA transformations on Bril functions. Watch out for variables that are undefined on some paths. The script “is_ssa.py can check if a program is really in SSA and the Bril interpreter bili supports phi functions so you can execute code in the midpoint of your round trip. Measure the overhead (does the final program have more instructions (static or dynamic) the original, be sure to report the overhead in your writeup.\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Homework",
      "Homework 5 ssa"
    ]
  },
  {
    "objectID": "homework/6_extra_credit.html",
    "href": "homework/6_extra_credit.html",
    "title": "Homework 6 loop optimization",
    "section": "",
    "text": "Implement and evaluate a loop optimization, either start with Bril or LLVM, you can use the ssa form of Bril if you want. If you use Bril you will have to find the natural loops, if you use LLVM you can call LoopPass but other parts of the implementation will be tricker, Pick an optimization (I’d suggest loop invariant code motion) but any of the others mentioned in class would be fine. Evaluate its performance, in Bril you can use the Bril benchmarks, in LLVM select an existing benchmark such as Embench and feel free to violate the sigplan guidelines SIGPLAN empirical evaluation guidelines by cherry-picking a convenient subset.\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Homework",
      "Homework 6 loop optimization"
    ]
  },
  {
    "objectID": "homework/project.html",
    "href": "homework/project.html",
    "title": "project",
    "section": "",
    "text": "Half way through the course, you should submit a project proposal, I’ll review it to make sure you are not tackling too big a challenge or going to far afield.\nAt the end of the course, you will complete a compiler research project. This is an open- ended project that can be on any topic in the field of compilers.\nThe final product is an experience report where you rigorously evaluate the success of your implementation. You can work individually or in groups of 2–3 students. When you finish an implementation, write it up. Your writeup should answer these questions in a good degree of detail: ● What was the goal? ● What did you do? (include both the design and the implementation) ● What were the hardest parts to get right? ● Were you successful? (report rigorously on your empirical evaluation)\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Homework",
      "project"
    ]
  },
  {
    "objectID": "homework/hw5_llvm.html",
    "href": "homework/hw5_llvm.html",
    "title": "hw 5 llvm",
    "section": "",
    "text": "Follow the LLVM tutorial blog post far enough to implement a pass that changes program execution.\nThis is intentionally open-ended. You can be as ambitious or as unambitious as you want. An example of an unambitious but acceptable task would be to print out a message every time the program uses floating-point division.\nAn example of an ambitious task would be to implement an optimization on LLVM IR and make sure it speeds things up in actual wall-clock time execution.\nFind a real-ish C/C++ program somewhere and run your pass on it to observe the results.\n\n\n\n Back to top",
    "crumbs": [
      "EECS 7398",
      "Homework",
      "hw 5 llvm"
    ]
  }
]